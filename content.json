{"meta":{"title":"ggq","subtitle":"","description":"","author":"ggq","url":"https://gouguoqiang.github.io","root":"/"},"pages":[{"title":"about","date":"2022-10-10T02:10:55.000Z","updated":"2022-10-10T02:11:45.270Z","comments":true,"path":"about/index.html","permalink":"https://gouguoqiang.github.io/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2023-02-20T01:03:34.000Z","updated":"2023-02-20T01:04:33.413Z","comments":true,"path":"categories/index.html","permalink":"https://gouguoqiang.github.io/categories/index.html","excerpt":"","text":""},{"title":"archives","date":"2022-10-10T01:30:43.000Z","updated":"2022-10-10T01:31:11.074Z","comments":true,"path":"archives/index.html","permalink":"https://gouguoqiang.github.io/archives/index.html","excerpt":"","text":""},{"title":"tags","date":"2023-02-20T01:03:28.000Z","updated":"2023-02-20T01:04:56.472Z","comments":true,"path":"tags/index.html","permalink":"https://gouguoqiang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"基础知识-位运算","slug":"算法/1基础知识/5位运算","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-22T10:20:37.612Z","comments":true,"path":"2022/09/02/算法/1基础知识/5位运算/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/5%E4%BD%8D%E8%BF%90%E7%AE%97/","excerpt":"","text":"001. 整数除法 难度简单189收藏分享切换为英文接收动态反馈 给定两个整数 a 和 b ，求它们的除法的商 a/b ，要求不得使用乘号 &#39;*&#39;、除号 &#39;/&#39; 以及求余符号 &#39;%&#39; 。 注意： 整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2 假设我们的环境只能存储 32 位有符号整数，其数值范围是 [−231, 231−1]。本题中，如果除法结果溢出，则返回 231 − 1 示例 1： 123输入：a = 15, b = 2输出：7解释：15/2 = truncate(7.5) = 7 示例 2： 123输入：a = 7, b = -3输出：-2解释：7/-3 = truncate(-2.33333..) = -2 示例 3： 12输入：a = 0, b = 1输出：0 示例 4： 12输入：a = 1, b = 1输出：1 12345678910111213141516171819202122232425262728// 用 - 法 22/3(从大到小测试) = 7(都可以用二进制表示,)...1class Solution &#123; public int divide(int a, int b) &#123; // //二进制模拟除法 &amp; | ^ &gt;&gt; &lt;&lt; 正常左右移不动符号位 &gt;&gt;&gt;(移动符号位) // 用 - 法 22/3(从大到小测试) = 7(都可以用二进制表示,)...1 // 都转化为整数 考虑特殊情况 MIN_VALUE // 3 &lt;&lt; 31 位的话存在越界 用 a &gt;&gt; int sign = (a &gt; 0) ^ (b &gt; 0) ? -1 : 1; if(a == Integer.MIN_VALUE &amp;&amp; b == -1) return Integer.MAX_VALUE; a = Math.abs(a); // abs(MIN)还为MIN 右移采用 &gt;&gt;&gt; 变为正数进行处理 b = Math.abs(b); // 如果 MIN // 如果都是正数 // 减法代替除法 int res = 0; for(int i = 31 ; i &gt;= 0 ; i--) &#123; // i == 0 时 变成 MIN // 从最大的二进制开始减 // 比如 22 / 3 // 20 - (3 &lt;&lt;2) res += (1&lt;&lt;2) 10 - 6 res+=(1&lt;&lt;1) 4 - 3 = 1 res+=(1&lt;&lt;0) if((a &gt;&gt;&gt; i) - b &gt;= 0) &#123; a -= (b &lt;&lt; i); res += (1 &lt;&lt; i); &#125; &#125; return sign == 1 ? res : -res; &#125;&#125; 002. 二进制加法1234567891011121314151617181920212223242526class Solution &#123; public String addBinary(String a, String b) &#123; // 处理进位 三个数相加 更新 进位的值 1 || 0 int len1 = a.length(); int len2 = b.length(); int i = len1; int j = len2; int more = 0; StringBuilder sb = new StringBuilder(); while(i &gt; 0 || j &gt; 0 || more &gt; 0) &#123; int a1 = 0; int b1 = 0; if(i &gt; 0) &#123; a1 = a.charAt(--i) - &#x27;0&#x27;; &#125; if(j &gt; 0) &#123; b1 = b.charAt(--j) - &#x27;0&#x27;; &#125; int temp = a1 + b1 + more; sb.append(temp % 2); more = temp / 2; &#125; return sb.reverse().toString(); &#125;&#125; 003. 前 n 个数字二进制中 1 的个数12345678910class Solution &#123; public int[] countBits(int n) &#123; // 如果知道 i &lt;&lt; 1 的个数 那么 i的个数就是 i&amp;1 + int[] ans = new int[n+1]; for (int i = 1; i &lt;= n; i++) &#123; ans[i] = ans[i&gt;&gt;1] + (i &amp; 1); &#125; return ans; &#125;&#125; 004. 只出现一次的数字 给你一个整数数组 nums ，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 。请你找出并返回那个只出现了一次的元素。 示例 1： 输入：nums &#x3D; [2,2,3,2]输出：3示例 2： 输入：nums &#x3D; [0,1,0,1,0,1,100]输出：100 提示： 1 &lt;&#x3D; nums.length &lt;&#x3D; 3 * 104-231 &lt;&#x3D; nums[i] &lt;&#x3D; 231 - 1nums 中，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 进阶：你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int singleNumber(int[] nums) &#123; // 二进制 % 3 每一位 int[] num = new int[32]; for(int i = 0 ; i &lt; nums.length ; i++) &#123; for(int j = 0 ; j &lt; 32 ; j++) &#123; num[j] += nums[i] &amp; 1; nums[i] &gt;&gt;&gt;= 1; &#125; &#125; int res = 0; for(int i = 0 ; i &lt; 32 ; i++) &#123; num[i] %= 3; res += (num[i] &lt;&lt; i); &#125; return res; &#125;&#125;class Solution &#123; public int singleNumber(int[] nums) &#123; Map&lt;Integer, Integer&gt; freq = new HashMap&lt;Integer, Integer&gt;(); for (int num : nums) &#123; freq.put(num, freq.getOrDefault(num, 0) + 1); &#125; int ans = 0; for (Map.Entry&lt;Integer, Integer&gt; entry : freq.entrySet()) &#123; int num = entry.getKey(), occ = entry.getValue(); if (occ == 1) &#123; ans = num; break; &#125; &#125; return ans; &#125;&#125; 005.单词长度的最大乘积 给定一个字符串数组 words，请计算当两个字符串 words[i] 和 words[j] 不包含相同字符时，它们长度的乘积的最大值。假设字符串中只包含英语的小写字母。如果没有不包含相同字符的一对字符串，返回 0。 示例 1: 输入: words &#x3D; [“abcw”,”baz”,”foo”,”bar”,”fxyz”,”abcdef”]输出: 16解释: 这两个单词为 “abcw”, “fxyz”。它们不包含相同字符，且长度的乘积最大。示例 2: 输入: words &#x3D; [“a”,”ab”,”abc”,”d”,”cd”,”bcd”,”abcd”]输出: 4解释: 这两个单词为 “ab”, “cd”。示例 3: 输入: words &#x3D; [“a”,”aa”,”aaa”,”aaaa”]输出: 0解释: 不存在这样的两个单词。 提示： 2 &lt;&#x3D; words.length &lt;&#x3D; 10001 &lt;&#x3D; words[i].length &lt;&#x3D; 1000words[i] 仅包含小写字母 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/aseY1I著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 123456789101112131415161718192021class Solution &#123; public int maxProduct(String[] words) &#123; int n = words.length; int[] mark = new int[n]; for (int i = 0; i &lt; n; i++) &#123; int m = words[i].length(); for (int j = 0; j &lt; m; j++) &#123; mark[i] |= (1 &lt;&lt; words[i].charAt(j) - &#x27;a&#x27;); &#125; &#125; int res = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = i+1; j &lt; n; j++) &#123; if ((mark[i] &amp; mark[j]) == 0) &#123; res = Math.max(words[i].length() * words[j].length(),res); &#125; &#125; &#125; return res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"位运算","slug":"位运算","permalink":"https://gouguoqiang.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"}]},{"title":"一. 基础知识AcWing题目","slug":"算法/1基础知识/AcWing","date":"2022-09-02T03:51:56.000Z","updated":"2023-01-25T16:44:43.469Z","comments":true,"path":"2022/09/02/算法/1基础知识/AcWing/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/AcWing/","excerpt":"","text":"第K个数12345678910111213141516171819202122232425class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; int n = nums.length; k = n - k + 1; return quickSelect(nums,0,n-1,k); // 这里的k不是坐标 &#125; public int quickSelect(int[] q, int l, int r, int k) &#123; if (l &gt;= r) return q[l]; int x = q[l], i = l - 1, j = r + 1; // j]左边都小于 x 右边都大于等于 while (i &lt; j) &#123; while(q[++i] &lt; x); while(q[--j] &gt; x); if (i &lt; j) &#123; int t = q[i]; q[i] = q[j]; q[j] = t; &#125; &#125; int lnum = j - l + 1; if (k &lt;= lnum) return quickSelect(q,l,j,k); return quickSelect(q,j+1,r,k - lnum); &#125;&#125; 均分纸牌有N堆纸牌，编号分别为 1,2,…,N 每堆上有若干张，但纸牌总数必为 N 的倍数。 可以在任一堆上取若干张纸牌，然后移动。 移牌规则为：在编号为 1 的堆上取的纸牌，只能移到编号为 2 的堆上；在编号为 N 的堆上取的纸牌，只能移到编号为 N−1 的堆上；其他堆上取的纸牌，可以移到相邻左边或右边的堆上。 现在要求找出一种移动方法，用最少的移动次数使每堆上纸牌数都一样多。 例如 N&#x3D;4，4 堆纸牌数分别为：(9,8,17,6) 移动 3 次可达到目的： 从第三堆取四张牌放入第四堆，各堆纸牌数量变为:(9,8,13,10) 从第三堆取三张牌放入第二堆，各堆纸牌数量变为:(9,11,10,10) 从第二堆取一张牌放入第一堆，各堆纸牌数量变为:(10,10,10,10) 输入格式第一行包含整数 N。 第二行包含 N 个整数，A1,A2,…,AN 表示各堆的纸牌数量。 输出格式输出使得所有堆的纸牌数量都相等所需的最少移动次数。 数据范围1≤N≤1001≤N≤100,1≤Ai≤100001≤Ai≤10000 输入样例：1249 8 17 6 输出样例：13 思路:模拟多进少补 处理每一个数 多进少补 代码:123456789int t = 0; for (int i = 1; i &lt;= n; i++) t += arr[i]; t /= n; for (int i = 1; i &lt;= n; i++) arr[i] -= t; int tnt = 0, ans = 0; for (int i = 1; i &lt;= n; i++) &#123; tnt += arr[i]; ans++; if(tnt == 0) ans--; &#125; 七夕祭七夕节因牛郎织女的传说而被扣上了「情人节」的帽子。 于是 TYVJ 今年举办了一次线下七夕祭。 Vani 同学今年成功邀请到了 cl 同学陪他来共度七夕，于是他们决定去 TYVJ 七夕祭游玩。 TYVJ 七夕祭和 11 区的夏祭的形式很像。 矩形的祭典会场由 N 排 M 列共计 N×M 个摊点组成。 虽然摊点种类繁多，不过 cl 只对其中的一部分摊点感兴趣，比如章鱼烧、苹果糖、棉花糖、射的屋……什么的。 Vani 预先联系了七夕祭的负责人 zhq，希望能够通过恰当地布置会场，使得各行中 cl 感兴趣的摊点个数一样多，并且各列中 cl 感兴趣的摊点数也一样多。 不过 zhq 告诉 Vani，摊点已经随意布置完毕了，如果想满足 cl 的要求，唯一的调整方式就是交换两个相邻的摊点。 两个摊点相邻，当且仅当他们处在同一行或者同一列的相邻位置上。 由于 zhq 率领的 TYVJ 开发小组成功地扭曲了空间，每一行或每一列的第一个位置和最后一个位置也算作相邻。 现在 Vani 想知道他的两个要求最多能满足多少个。 在此前提下，至少需要交换多少次摊点。 输入格式第一行包含三个整数 N 和 M 和 T，T 表示 cl 对多少个摊点感兴趣。 接下来 T 行，每行两个整数 x,y，表示 cl 对处在第 x行第 y 列的摊点感兴趣。 输出格式首先输出一个字符串。 如果能满足 Vani 的全部两个要求，输出 both； 如果通过调整只能使得各行中 cl 感兴趣的摊点数一样多，输出 row； 如果只能使各列中 cl 感兴趣的摊点数一样多，输出 column； 如果均不能满足，输出 impossible。 如果输出的字符串不是 impossible， 接下来输出最小交换次数，与字符串之间用一个空格隔开。 数据范围1≤N,M≤1000000≤T≤min(N∗M,100000),1≤x≤N,1≤y≤M 输入样例：123452 3 41 32 12 22 3 输出样例：1row 1 思路: 1 2 3 4 5 只交换相邻两个人的牌 需要交换多少次每个人都一样 设平均值为m &#x3D;3, 去一个通俗的就可以带入所有的 需要借: 1: m-1 2: 2-(m-1) &#x3D; 3-m, m - (3-m) &#x3D; 2m -3 3: 3 - (2m-3) &#x3D; 6 - 2m, m - (6-2m) &#x3D; 3m - 6 4: 4-(3m-6) &#x3D; 10 -3m, 4m - 10; 5: 5m - 15; 观察常数为前缀和, 如果不取首尾只能相邻 那就是前缀和的 每项绝对值相加 但是现在可以选取一个点做为分割点 转化为前缀和货仓选址问题(设置一个仓库位置 到其他所有点的总和距离最短,求中位数(要先排序)) 最少交换次数 排成一个环 取每个数减去中位数的新数组的前缀和 求出都减去平均值数组的前缀和做为新数组, 对新数组做为(“货仓选址”); 代码:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.*;public class Main&#123; static int N = 100005; static long[] row = new long[N], col = new long[N]; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int T = in.nextInt(); // 统计每行每列 for (int i = 1; i &lt;= T; i ++) &#123; int x = in.nextInt(); int y = in.nextInt(); row[x] ++; col[y] ++; &#125; for (int i = 1; i &lt;= n; i++) &#123; row[0] += row[i]; &#125; for (int i = 1; i &lt;= m; i++) &#123; col[0] += col[i]; &#125; if (T % n == 0 &amp;&amp; T % m == 0) &#123; long ans = get(row,n)+get(col,m); System.out.println(&quot;both&quot; + &quot; &quot; + ans); &#125; else if (T % n == 0) &#123; System.out.println(&quot;row&quot; + &quot; &quot; + get(row,n)); &#125; else if (T % m == 0) &#123; System.out.println(&quot;column&quot; + &quot; &quot; + get(col,m)); &#125; else &#123; System.out.println(&quot;impossible&quot;); &#125; &#125; public static long get(long[] a, int n) &#123; long avg = a[0] / n; long[] s = new long[N]; for (int i = 1; i &lt;= n; i++) &#123; a[i] -= avg; s[i] = s[i-1] + a[i]; &#125; long ans = 0; Arrays.sort(s,1,n); long mid = s[(1 + n) / 2]; for (int i = 1; i &lt;= n; i++) &#123; ans += Math.abs(s[i] - mid); &#125; return ans; &#125;&#125; 逆序对数量给定一个长度为 n 的整数数列，请你计算数列中的逆序对的数量。 逆序对的定义如下：对于数列的第 i 个和第 j 个元素，如果满足 i&lt;j 且 a[i]&gt;a[j]，则其为一个逆序对；否则不是。 输入格式第一行包含整数 n，表示数列的长度。 第二行包含 n 个整数，表示整个数列。 输出格式输出一个整数，表示逆序对的个数。 数据范围1≤n≤100000，数列中的元素的取值范围 [1,109][1,109]。 输入样例：1262 3 4 5 6 1 输出样例：15 思路:分析左右两半部分，如果左半部分 q[i] 大于右半部分的 q[j]，那么从 i 到 mid 都可以和 j 组成逆序对，逆序对个数 res +&#x3D; mid - i + 1 代码:12345678910111213141516171819202122232425262728293031323334class Main&#123; static int N = 100010; static long res = 0; static int[] q = new int[N]; static int[] tmp = new int[N]; public static void main(String[] args) throws IOException&#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); int n = Integer.parseInt(br.readLine()); String[] s = br.readLine().split(&quot; &quot;); for(int i = 0; i &lt; n; i++)&#123; q[i] = Integer.parseInt(s[i]); &#125; mergeSort(0, n - 1); System.out.println(res); &#125; public static void mergeSort(int l, int r)&#123; if(l &gt;= r) return; int mid = l + r &gt;&gt; 1; mergeSort(l, mid); mergeSort(mid + 1, r); int k = 0, i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r)&#123; if(q[i] &lt;= q[j]) tmp[k++] = q[i++]; else &#123; res += mid - i + 1; tmp[k++] = q[j++]; &#125; &#125; while(i &lt;= mid) tmp[k++] = q[i++]; while(j &lt;= r) tmp[k++] = q[j++]; for(i = l, j = 0; i &lt;= r; i++, j++) q[i] = tmp[j]; &#125;&#125; 选择排序从头到尾找最小的跟第一个未排序交换 n方 插入排序在已排好序的中插入应该插的地方 12345678910111213int[] arr = &#123;1,3,5,6,2,1,23,5,6&#125;int n = arr.length;for (int i = 1; i &lt; n; i++) &#123; int t = arr[i]; int j = i - 1; for (; j &gt;= 0; j--) &#123; if (arr[j] &lt;= t) break; arr[j+1] = arr[j]; &#125; arr[j+1] = t; &#125; 希尔排序插入排序的优化 堆排序从最后一个根节点开始后层序遍历 上浮 为一个大顶堆 之后在交换第一个与最后一个为排序的 796. 子矩阵的和输入一个 nn 行 mm 列的整数矩阵，再输入 qq 个询问，每个询问包含四个整数 x1,y1,x2,y2x1,y1,x2,y2，表示一个子矩阵的左上角坐标和右下角坐标。 对于每个询问输出子矩阵中所有数的和。 输入格式第一行包含三个整数 n，m，qn，m，q。 接下来 nn 行，每行包含 mm 个整数，表示整数矩阵。 接下来 qq 行，每行包含四个整数 x1,y1,x2,y2x1,y1,x2,y2，表示一组询问。 输出格式共 qq 行，每行输出一个询问的结果。 数据范围1≤n,m≤10001≤n,m≤1000,1≤q≤2000001≤q≤200000,1≤x1≤x2≤n1≤x1≤x2≤n,1≤y1≤y2≤m1≤y1≤y2≤m,−1000≤矩阵内元素的值≤1000−1000≤矩阵内元素的值≤1000 输入样例：12345673 4 31 7 2 43 6 2 82 1 2 31 1 2 22 1 3 41 3 3 4 输出样例：123172721 Solution1234567891011121314n, m, q = map(int, input().split())a = [list(map(int, input().split())) for _ in range(n)]s = [[0] * (m + 1) for _ in range(n + 1)]for i in range(1, n + 1): for j in range(1, m + 1): s[i][j] = s[i - 1][j] + s[i][j - 1] - s[i - 1][j - 1] + a[i - 1][j - 1] while q: x1, y1, x2, y2 = map(int, input().split()) print(s[x2][y2] - s[x1 - 1][y2] - s[x2][y1 - 1] + s[x1 - 1][y1 - 1]) q -= 1 798. 差分矩阵输入一个 nn 行 mm 列的整数矩阵，再输入 qq 个操作，每个操作包含五个整数 x1,y1,x2,y2,cx1,y1,x2,y2,c，其中 (x1,y1)(x1,y1) 和 (x2,y2)(x2,y2) 表示一个子矩阵的左上角坐标和右下角坐标。 每个操作都要将选中的子矩阵中的每个元素的值加上 cc。 请你将进行完所有操作后的矩阵输出。 输入格式第一行包含整数 n,m,qn,m,q。 接下来 nn 行，每行包含 mm 个整数，表示整数矩阵。 接下来 qq 行，每行包含 55 个整数 x1,y1,x2,y2,cx1,y1,x2,y2,c，表示一个操作。 输出格式共 nn 行，每行 mm 个整数，表示所有操作进行完毕后的最终矩阵。 数据范围1≤n,m≤10001≤n,m≤1000,1≤q≤1000001≤q≤100000,1≤x1≤x2≤n1≤x1≤x2≤n,1≤y1≤y2≤m1≤y1≤y2≤m,−1000≤c≤1000−1000≤c≤1000,−1000≤矩阵内元素的值≤1000−1000≤矩阵内元素的值≤1000 输入样例：12345673 4 31 2 2 13 2 2 11 1 1 11 1 2 2 11 3 2 3 23 1 3 4 1 输出样例：1232 3 4 14 3 4 12 2 2 2 Solution1234567891011121314151617181920212223242526272829303132n, m, q = map(int, input().split())s = [[0] * (m + 1) for _ in range(n + 1)]for i in range(1, n + 1): s[i] = [0] + list(map(int,input().split()))# +1 是因为存储是从1,1 开始 + 2是因为 对末尾不越界a = [[0] * (m + 2) for _ in range(n + 2)]def insert(x1, y1, x2, y2, c): a[x1][y1] += c a[x2 + 1][y1] -= c a[x1][y2 + 1] -= c a[x2 + 1][y2 + 1] += cfor i in range(1, n + 1): for j in range(1, m + 1): insert(i, j, i, j, s[i][j])while q &gt; 0: x1, y1, x2, y2, c = map(int, input().split()) insert(x1, y1, x2, y2, c) q -= 1for i in range(1, n +1): for j in range(1, m + 1): s[i][j] = s[i - 1][j] + s[i][j - 1] - s[i - 1][j - 1] + a[i][j] print(s[i][j], end=&quot; &quot;) print() 100. 增减序列给定一个长度为 nn 的数列 a1,a2,…,ana1,a2,…,an，每次可以选择一个区间 [l,r][l,r]，使下标在这个区间内的数都加一或者都减一。 求至少需要多少次操作才能使数列中的所有数都一样，并求出在保证最少次数的前提下，最终得到的数列可能有多少种。 输入格式第一行输入正整数 nn。 接下来 nn 行，每行输入一个整数，第 i+1i+1 行的整数代表 aiai。 输出格式第一行输出最少操作次数。 第二行输出最终能得到多少种结果。 数据范围0&lt;n≤1050&lt;n≤105,0≤ai&lt;21474836480≤ai&lt;2147483648 输入样例：1234541122 输出样例：1212 1234567891011121314151617181920212223242526272829303132333435363738394041# 差分数组#，每一次选取Bi和Bj，2&lt;=i,j&lt;=n2&lt;=i,j&lt;=n,而且这两个数，一个为正数，一个为负数，至于为什么要是正负配对，因为我们是要这个B序列2~n都要为0，所以这样负数增加，正数减少，就可以最快地到达目标为0的状态。# 至于那些无法配对的数BkBk可以选B1或者Bn+1，这两个不影响的数，进行修改# 原数组是差分数组的前缀和# 原数组都一样 意味着差分数组第一个数为这个数 其他都为0# 最少操作数min(p,q)+abs(p−q)=max(p,q)，然后最终序列a可能会有abs(p−q)+1种情况。p为b序列中正数之和，而q为b序列中负数之和# abs(p−q)+1中情况: 不能配队用操作第一个数 不操作第一个数# insert(l,r,c ) d[l] += c d[r + 1] -= c 这种操作将后面每个数变成0 两两一组?# 结论 最少操作 等于 max(z, -f) 不同结果的个数 abs(z + f) 个未配对的 选第一个数或最后一个没影响的减# 减 for 0次 - abs(z + f) 次第一个数n = int(input())s = [0] * (n + 1)a = [0] * (n + 2)def insert(l, r, c): a[l] += c a[r + 1] -= c for i in range(1, n + 1): s[i] = int(input()) insert(i, i, s[i]) z, f = 0, 0for i in range(2, n + 1): if a[i] &gt; 0: z += a[i] else: f += a[i] ans = max(z, -f)# 减 for 0次 - abs(z + f) 次第一个数count = 1 + abs(z + f) print(ans)print(count) 99. 激光炸弹地图上有 NN 个目标，用整数 Xi,YiXi,Yi 表示目标在地图上的位置，每个目标都有一个价值 WiWi。 注意：不同目标可能在同一位置。 现在有一种新型的激光炸弹，可以摧毁一个包含 R×RR×R 个位置的正方形内的所有目标。 激光炸弹的投放是通过卫星定位的，但其有一个缺点，就是其爆炸范围，即那个正方形的边必须和 x，yx，y 轴平行。 求一颗炸弹最多能炸掉地图上总价值为多少的目标。 输入格式第一行输入正整数 NN 和 RR，分别代表地图上的目标数目和正方形的边长，数据用空格隔开。 接下来 NN 行，每行输入一组数据，每组数据包括三个整数 Xi,Yi,WiXi,Yi,Wi，分别代表目标的 xx 坐标，yy 坐标和价值，数据用空格隔开。 输出格式输出一个正整数，代表一颗炸弹最多能炸掉地图上目标的总价值数目。 数据范围0≤R≤1090≤R≤1090&lt;N≤100000&lt;N≤10000,0≤Xi,Yi≤50000≤Xi,Yi≤50000≤Wi≤10000≤Wi≤1000 输入样例：1232 10 0 11 1 1 输出样例：11 Solution12345678910111213141516171819202122232425262728293031T, r = map(int, input().split())N = 5010a = [[0] * N for _ in range(N)]s = [[0] * N for _ in range(N)]n = 0for i in range(T): x, y, w = map(int, input().split()) a[x][y] += w n = max(n, x) n = max(n, y) n += 1 for i in range(1, n + 1): for j in range(1, n + 1): s[i][j] = s[i-1][j] + s[i][j-1] -s[i-1][j-1] + a[i-1][j-1] ans = 0if r &gt;= n: ans = s[n][n]for i in range(r, n + 1): for j in range(r, n + 1): x1,y1,x2,y2 = i - r + 1, j - r + 1, i, j ans = max(ans, s[x2][y2] - s[x1-1][y2] - s[x2][y1-1] + s[x1-1][y1-1]) print(ans) 799. 最长连续不重复子数组12345678910111213141516set = [0] * 100010n = int(input())a = list(map(int, input().split()))ans = 0j = 0for i in range(n): x = a[i] while set[x] &gt; 0: out = a[j] set[out] -= 1 j += 1 set[x] += 1 ans = max(i - j + 1, ans)print(ans) 800. 数组元素的目标和给定两个升序排序的有序数组 AA 和 BB，以及一个目标值 xx。 数组下标从 00 开始。 请你求出满足 A[i]+B[j]&#x3D;xA[i]+B[j]&#x3D;x 的数对 (i,j)(i,j)。 数据保证有唯一解。 输入格式第一行包含三个整数 n,m,xn,m,x，分别表示 AA 的长度，BB 的长度以及目标值 xx。 第二行包含 nn 个整数，表示数组 AA。 第三行包含 mm 个整数，表示数组 BB。 输出格式共一行，包含两个整数 ii 和 jj。 数据范围数组长度不超过 105105。同一数组内元素各不相同。1≤数组元素≤1091≤数组元素≤109 输入样例：1234 5 61 2 4 73 4 6 8 9 输出样例：11 1 1234567891011121314151617# 没必要四指针n, m, x = map(int, input().split())a = list(map(int, input().split()))b = list(map(int, input().split()))j = m - 1for i in range(n): while j &gt; 0 and a[i] + b[j] &gt; x: j -= 1 if j &gt; 0 and a[i] + b[j] == x: print(i,j) break","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"一  . 基础知识LeetCode","slug":"算法/1基础知识/LeetCode","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-27T01:10:08.592Z","comments":true,"path":"2022/09/02/算法/1基础知识/LeetCode/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/LeetCode/","excerpt":"","text":"API6233.1234567891011class Solution &#123; public double[] convertTemperature(double celsius) &#123; Double res1 = celsius + 273.15; Double res2 = celsius * 1.80 + 32; String str1 = String.format(&quot;%.5f&quot;,res1); String str2 = String.format(&quot;%.5f&quot;,res2); return new double[] &#123;Double.parseDouble(str1),Double.parseDouble(str2)&#125;; &#125;&#125; 双指针6234. 最小公倍数为 K 的子数组数目123456789101112131415161718192021222324252627282930313233//错误代码class Solution &#123; public int subarrayLCM(int[] nums, int k) &#123; int n = nums.length; int res = 0; for (int i = 0, j = 0; i &lt; n; i++) &#123; if (isLCM(nums,j,i,k)) res ++; else &#123; while(++j &lt; i &amp;&amp; !isLCM(nums,j,i,k)) res++; &#125; &#125; return res; &#125; public boolean isLCM(int[] nums, int j, int i, int k) &#123; for (int m = 1; m &lt;= k; m++) &#123; boolean flag = false; for (int n = j; n &lt;= i; n++) &#123; if (m % nums[n] != 0) &#123; flag = true; break; &#125; &#125; if (!flag &amp;&amp; m == k) return true; if (!flag &amp;&amp; m != k) return false; &#125; return false; &#125;&#125; 6250. 商店的最少代价给你一个顾客访问商店的日志，用一个下标从 0 开始且只包含字符 &#39;N&#39; 和 &#39;Y&#39; 的字符串 customers 表示： 如果第 i 个字符是 &#39;Y&#39; ，它表示第 i 小时有顾客到达。 如果第 i 个字符是 &#39;N&#39; ，它表示第 i 小时没有顾客到达。 如果商店在第 j 小时关门（0 &lt;= j &lt;= n），代价按如下方式计算： 在开门期间，如果某一个小时没有顾客到达，代价增加 1 。 在关门期间，如果某一个小时有顾客到达，代价增加 1 。 请你返回在确保代价 最小 的前提下，商店的 最早 关门时间。 注意，商店在第 j 小时关门表示在第 j 小时以及之后商店处于关门状态。 示例 1： 123456789输入：customers = &quot;YYNY&quot;输出：2解释：- 第 0 小时关门，总共 1+1+0+1 = 3 代价。- 第 1 小时关门，总共 0+1+0+1 = 2 代价。- 第 2 小时关门，总共 0+0+0+1 = 1 代价。- 第 3 小时关门，总共 0+0+1+1 = 2 代价。- 第 4 小时关门，总共 0+0+1+0 = 1 代价。在第 2 或第 4 小时关门代价都最小。由于第 2 小时更早，所以最优关门时间是 2 。 示例 2： 123输入：customers = &quot;NNNNN&quot;输出：0解释：最优关门时间是 0 ，因为自始至终没有顾客到达。 示例 3： 123输入：customers = &quot;YYYY&quot;输出：4解释：最优关门时间是 4 ，因为每一小时均有顾客到达。 提示： 1 &lt;= customers.length &lt;= 105 customers 只包含字符 &#39;Y&#39; 和 &#39;N&#39; 。 12345class Solution &#123; public int bestClosingTime(String customers) &#123; // 前缀和统计1的个数 那么0的个数就是 &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"一. 基础知识Java模板","slug":"算法/1基础知识/6java模板","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-01T08:19:17.683Z","comments":true,"path":"2022/09/02/算法/1基础知识/6java模板/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/6java%E6%A8%A1%E6%9D%BF/","excerpt":"","text":"快速排序123456789101112131415public void quick_sort(int[] q, int l, int r) &#123; if (l &gt;= r) return ; int x = q[l], i = l - 1, j = r + 1; while (i &lt; j) &#123; do i ++; while(q[i] &lt; x); do j --; while(q[j] &gt; x); if (i &lt; j) &#123; int t = q[i]; q[i] = q[j]; q[j] = t; &#125; &#125; quick_sort(q,l,j); quick_sort(q,j+1,r); &#125; 归并排序123456789101112131415161718public static void merge_sort(int[] q, int l, int r) &#123; if (l &gt;= r) return ; int mid = l + (r - l) / 2; merge_sort(q,l,mid); merge_sort(q,mid + 1,r); int k = 0, i = l, j = mid + 1; while (i &lt;= mid &amp;&amp; j &lt;= r) &#123; if (q[i] &lt; q[j]) &#123; temp[k++] = q[i++]; &#125; else &#123; temp[k++] = q[j++]; &#125; &#125; while (i &lt;= mid) temp[k++] = q[i++]; while (j &lt;= r) temp[k++] = q[j++]; for (i = l, j = 0; j &lt; k; j++, i++) q[i] = temp[j]; &#125; 二分12345678910111213141516171819// 二分check public static boolean check(int mid, int target) &#123; return q[mid] &gt;= target; &#125; // 二分模板 找target 有多个 返回第一个的下标 public static int bin_search(int target) &#123; int n = q.length; int l = 0, r = n - 1; while (l &lt;= r) &#123; int mid = l + (r - l) / 2; if (check(mid,target)) &#123; r = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; if (l &gt;= n || q[l] != target) return -1; return l; &#125; 离散化","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"搜索leetCode","slug":"算法/2搜索/LeetCode","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-22T13:01:48.719Z","comments":true,"path":"2022/09/02/算法/2搜索/LeetCode/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/LeetCode/","excerpt":"","text":"Leetcode79 单词搜索123456789101112131415161718192021222324252627282930class Solution &#123; int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; int n, m; public boolean exist(char[][] board, String word) &#123; //dfs n = board.length; m = board[0].length; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; if (dfs(board,i,j,word,0)) return true; &#125; &#125; return false; &#125; public boolean dfs(char[][] board, int x, int y, String word, int u) &#123; if (board[x][y] != word.charAt(u)) return false; if (u == word.length()-1) return true; board[x][y] = &#x27;.&#x27;; for (int i = 0; i &lt; 4; i++) &#123; int a = x + dx[i], b = y + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) continue; if (dfs(board,a,b,word,u+1)) return true; &#125; board[x][y] = word.charAt(u); return false; &#125;&#125; 46 全排列 无重复数顺序一: 枚举每个位置放哪个数 顺序二: 枚举每个数放到那个位置上 1234567891011121314151617181920212223242526272829//第一种搜索顺序class Solution &#123; int n; boolean[] st; List&lt;Integer&gt; path = new ArrayList&lt;&gt;(); List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; n = nums.length; st = new boolean[n]; dfs(0,nums,path); return res; &#125; public void dfs(int u, int[] nums, List&lt;Integer&gt; path) &#123; if (u == n) &#123; res.add(new ArrayList(path)); return ; &#125; for (int i = 0; i &lt; n; i++) &#123; if (!st[i]) &#123; path.add(nums[i]); st[i] = true; dfs(u + 1, nums, path); st[i] = false; path.remove(path.size() - 1); &#125; &#125; &#125;&#125; 47 有重复数 全排列 123456789101112131415161718192021222324252627282930313233class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; boolean[] st; public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; n = nums.length; st = new boolean[n]; ans = new ArrayList&lt;&gt;(); path = new ArrayList&lt;&gt;(n); Arrays.sort(nums); for (int i = 0; i &lt; n; i++) &#123; path.add(null); &#125; dfs(nums,0,0); return ans; &#125; public void dfs(int[] nums, int u, int start) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; for (int i = start; i &lt; n; i++) &#123; if (!st[i]) &#123; st[i] = true; path.set(i,nums[u]); dfs(nums,u+1,u+1 &lt; n &amp;&amp; nums[u] == nums[u+1] ? i+1 : 0); st[i] = false; &#125; &#125; &#125;&#125; 78 子集二进制枚举 1234567891011121314151617181920212223242526class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; //dfs 搜索顺序,枚举每个数选还是不选 n = nums.length; path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(nums,0); return ans; &#125; public void dfs(int[] nums, int u) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; int k = 1; for (int i = 0; i &lt;= k; i++) &#123; dfs(nums,u+1); path.add(nums[u]); &#125; for (int i = 0; i &lt;= k; i++) path.remove(path.size() - 1); &#125;&#125; 123456789101112131415161718192021222324252627class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; // 搜索顺序 枚举每个数选还是不选 n = nums.length; path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(nums,0); return ans; &#125; public void dfs(int[] nums, int u) &#123; if (u == n) &#123; ans.add(new ArrayList(path)); return ; &#125; // 选第u位 path.add(nums[u]); dfs(nums,u+1); path.remove(path.size() - 1); // 不选 dfs(nums,u+1); &#125;&#125; 90 子集II 先枚举每个数字,然后枚举每个数字选多少个 1234567891011121314151617181920212223242526272829class Solution &#123; List&lt;List&lt;Integer&gt;&gt; res; List&lt;Integer&gt; path; int n; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; res = new ArrayList&lt;&gt;(); path = new ArrayList&lt;&gt;(); n = nums.length; Arrays.sort(nums); dfs(nums,0); return res; &#125; private void dfs(int[] nums, int u)&#123; if (u == n) &#123; res.add(new ArrayList&lt;&gt;(path)); return ; &#125; int k = 0; while(u + k &lt; n &amp;&amp; nums[u+k] == nums[u]) k++; for (int i = 0; i &lt;= k; i++) &#123; dfs(nums,u+k); path.add(nums[u]); &#125; for (int i = 0; i &lt;= k; i++) path.remove(path.size()-1); &#125;&#125; 216 组合总和III12345678910111213141516171819202122232425262728293031class Solution &#123; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; combinationSum3(int k, int n) &#123; // 组合 搜索顺序: 依次枚举每个数 是否选 path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(1,k,n); return ans; &#125; public void dfs(int u, int k,int n) &#123; if (k == 0) &#123; if (n == 0) &#123; ans.add(new ArrayList&lt;&gt;(path)); &#125; return ; &#125; if (u == 10) return ; dfs(u+1,k,n); path.add(u); dfs(u+1,k-1,n-u); path.remove(path.size() - 1); &#125;&#125;// 依次枚举每个位置 可以选哪些数 200 岛屿数量123456789101112131415161718192021222324252627282930313233343536 boolean[][] st; int n; int m; int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,1,0,-1&#125;; public int numIslands(char[][] grid) &#123; n = grid.length; m = grid[0].length; st = new boolean[n][m]; int cnt = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; if (grid[i][j] == &#x27;1&#x27; &amp;&amp; !st[i][j]) &#123; bfs(grid,i,j); cnt++; &#125; &#125; &#125; return cnt; &#125; public void bfs(char[][] g, int a, int b ) &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); q.offer(new int[]&#123;a,b&#125;); st[a][b] = true; while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int x = t[0] + dx[i], y = t[1] + dy[i]; if (x &gt;= n || x &lt; 0 || y &lt; 0 || y &gt;= m || st[x][y] || g[x][y] != &#x27;1&#x27;) continue; q.offer(new int[]&#123;x,y&#125;); st[x][y] = true; &#125; &#125; &#125;&#125; 八皇后","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"2搜索","slug":"算法/2搜索","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"搜索","slug":"搜索","permalink":"https://gouguoqiang.github.io/tags/%E6%90%9C%E7%B4%A2/"}]},{"title":"图论-拓扑排序","slug":"算法/4图论/拓扑排序","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-22T10:26:33.858Z","comments":true,"path":"2022/09/02/算法/4图论/拓扑排序/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/","excerpt":"","text":"环检测算法 dfs即可 拓扑排序的一部分 bfs也可 拓扑排序 两种方式DFS与BFS DFS模板现在你总共有 numCourses 门课需要选，记为 0 到 numCourses - 1。给你一个数组 prerequisites ，其中 prerequisites[i] &#x3D; [ai, bi] ，表示在选修课程 ai 前 必须 先选修 bi 。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示：[0,1] 。返回你为了学完所有课程所安排的学习顺序。可能会有多个正确的顺序，你只要返回 任意一种 就可以了。如果不可能完成所有课程，返回 一个空数组 。 示例 1： 输入：numCourses &#x3D; 2, prerequisites &#x3D; [[1,0]]输出：[0,1]解释：总共有 2 门课程。要学习课程 1，你需要先完成课程 0。因此，正确的课程顺序为 [0,1] 。示例 2： 输入：numCourses &#x3D; 4, prerequisites &#x3D; [[1,0],[2,0],[3,1],[3,2]]输出：[0,2,1,3]解释：总共有 4 门课程。要学习课程 3，你应该先完成课程 1 和课程 2。并且课程 1 和课程 2 都应该排在课程 0 之后。因此，一个正确的课程顺序是 [0,1,2,3] 。另一个正确的排序是 [0,2,1,3] 。示例 3： 输入：numCourses &#x3D; 1, prerequisites &#x3D; []输出：[0] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; List&lt;Integer&gt; seq; public int[] findOrder(int numCourses, int[][] prerequisites) &#123; topo(prerequisites,numCourses); int n = seq.size(); int[] ans = new int[n]; for (int i = 0; i &lt; n; i++) &#123; ans[i] = seq.get(i)-1; &#125; if (loop) return new int[]&#123;&#125;; return ans; &#125; List&lt;Integer&gt; g[]; boolean[] instk; boolean[] vis; boolean loop; public void topo(int[][] edges, int n) &#123; g = new List[n+1]; for (int i = 1; i &lt;= n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[0]+1].add(edge[1]+1); &#125; instk = new boolean[n+1]; vis = new boolean[n+1]; seq = new ArrayList&lt;&gt;(); //防止不连通 for (int i = 1; i &lt;= n; i++) &#123; dfs(i); if (loop) break; &#125; &#125; public void dfs(int root) &#123; if (vis[root]) &#123; if (instk[root]) &#123; loop = true; &#125; return ; &#125; instk[root] = vis[root] = true; for (int node : g[root]) &#123; dfs(node); &#125; seq.add(root); instk[root] = false; &#125;&#125; BFS模板123456789101112131415161718192021222324252627282930313233343536373839public int[] findOrder(int n, int[][] edges) &#123; if (!topo(edges,n)) return new int[]&#123;&#125;; return seq; &#125; List&lt;Integer&gt; g[]; // d[i] 存储 结点i的入度 int[] d; int[] seq; public boolean topo(int[][] edges, int n) &#123; g = new List[n]; d = new int[n]; seq = new int[n]; for (int i = 0; i&lt; n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[1]].add(edge[0]); d[edge[0]] ++; &#125; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); for (int i = 0; i &lt; n; i++) &#123; if (d[i] == 0) &#123; q.offer(i); &#125; &#125; int index = 0; while (!q.isEmpty()) &#123; int t = q.poll(); seq[index++] = t; for (int node : g[t]) &#123; if (--d[node] == 0) &#123; q.offer(node); &#125; &#125; &#125; return index == n; &#125;&#125; 直观地说就是，让你把⼀幅图「拉平」，⽽且这个「拉平」的图⾥⾯，所有箭头⽅向都是⼀致的，⽐如上图 所有箭头都是朝右的。 很显然，如果⼀幅有向图中存在环，是⽆法进⾏拓扑排序的，因为肯定做不到所有箭头⽅向⼀致；反过来， 如果⼀幅图是「有向⽆环图」，那么⼀定可以进⾏拓扑排序。 拓扑排序是正常建有向图后序遍历的逆 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123; public int[][] buildMatrix(int k, int[][] rowConditions, int[][] colConditions) &#123; int[][] ans = new int[k][k]; int[] xs = new int[k + 1]; int[] ys = new int[k + 1]; List&lt;Integer&gt; rows = topo(rowConditions, k); List&lt;Integer&gt; cols = topo(colConditions, k); if(rows == null || cols == null) &#123; return new int[0][]; &#125; for(int i = 0; i &lt; k; i++) &#123; xs[rows.get(i)] = i; ys[cols.get(i)] = i; &#125; for(int i = 1; i &lt;= k; i++) &#123; ans[xs[i]][ys[i]] = i; &#125; return ans; &#125; List&lt;Integer&gt;[] g; boolean[] instk; boolean[] visited; List&lt;Integer&gt; seq; boolean loop; public void dfs(int root) &#123; if(visited[root]) &#123; if(instk[root]) &#123; loop = true; &#125; return; &#125; visited[root] = instk[root] = true; for(int node : g[root]) &#123; dfs(node); &#125; seq.add(root); instk[root] = false; &#125; public List&lt;Integer&gt; topo(int[][] edges, int n) &#123; loop = false; g = new List[n + 1]; for(int i = 0; i &lt;= n; i++) &#123; g[i] = new ArrayList(); &#125; for(int[] e : edges) &#123; g[e[1]].add(e[0]); &#125; instk = new boolean[n + 1]; visited = new boolean[n + 1]; seq = new ArrayList(); for(int i = 1; i &lt;= n; i++) &#123; dfs(i); &#125; return loop ? null : seq; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"拓扑排序","slug":"拓扑排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"}]},{"title":"图论-最小生成树","slug":"算法/4图论/最小生成树","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-22T08:56:09.430Z","comments":true,"path":"2022/09/02/算法/4图论/最小生成树/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/","excerpt":"","text":"总纲领给定一个图,求出连通所有点的最小(权值和)生成树 prim算法 邻接矩阵图 - 朴素 n方 从一个部落扩展到整个地球 每次选取最短的边 (集合外点到集合内点) - 堆优化 克鲁斯卡尔 只需要存储边信息即可 将边大小从小到大排序 维护并查集 遍历到一条边如果为连接则连接 将结果添加到集合 证明: 如果不选这条边 那么最终生成的树里把这条边加上则会形成一个换 选这条边不会使总结果更差 PrimAcWing 858. Prim算法求最小生成树难度：简单 题目描述给定一个n个点m条边的无向图，图中可能存在重边和自环，边权可能为负数。 求最小生成树的树边权重之和，如果最小生成树不存在则输出impossible。 给定一张边带权的无向图G&#x3D;(V, E)，其中V表示图中点的集合，E表示图中边的集合，n&#x3D;|V|，m&#x3D;|E|。 由V中的全部n个顶点和E中n-1条边构成的无向连通子图被称为G的一棵生成树，其中边的权值之和最小的生成树被称为无向图G的最小生成树。 输入格式 第一行包含两个整数n和m。 接下来m行，每行包含三个整数u，v，w，表示点u和点v之间存在一条权值为w的边。 输出格式 共一行，若存在最小生成树，则输出一个整数，表示最小生成树的树边权重之和，如果最小生成树不存在则输出impossible。 数据范围 $1≤n≤500,$$1≤m≤105,$图中涉及边的边权的绝对值均不超过10000。 123456789101112输入样例：4 51 2 11 3 21 4 32 3 23 4 4输出样例：6 Solution123456789101112131415161718192021222324252627282930313233343536373839404142import java.io.*;import java.util.*;public class Main&#123; static final int INF = 0x3f3f3f3f, N = 510; static int[][] g = new int[N][N]; static boolean[] st = new boolean[N]; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in));// BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); String[] s = br.readLine().split(&quot; &quot;); int n = Integer.parseInt(s[0]); int m = Integer.parseInt(s[1]); for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; g[i][j] = i == j ? 0 : INF; &#125; &#125; while (m-- &gt; 0) &#123; s = br.readLine().split(&quot; &quot;); int x = Integer.parseInt(s[0]); int y = Integer.parseInt(s[1]); int z = Integer.parseInt(s[2]); g[x][y] = Math.min(g[x][y],z); &#125; System.out.println(prim(n));// bw.close(); br.close(); &#125; public static int prim(int n) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; int t = -1; for (int j = 1; j &lt;= n; j++) &#123; if (!st[j] &amp;&amp; (t == -1 || ) ) &#125; &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"最小生成树","slug":"最小生成树","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"}]},{"title":"AcWing图论","slug":"算法/4图论/AcWing图论","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-03T04:39:14.333Z","comments":true,"path":"2022/09/02/算法/4图论/AcWing图论/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/AcWing%E5%9B%BE%E8%AE%BA/","excerpt":"","text":"849. dijkstra求最短路1234567891011121314151617181920212223242526n, m = map(int, input().split())N = 510inf = 0x3f3f3f3fg = [[inf] * N for _ in range(N)]dist = [inf] * Nst = [False] * Nfor _ in range(m): x, y, z = map(int, input().split()) g[x][y] = min(g[x][y],z)dist[1] = 0for i in range(n): t = -1 for j in range(1, n + 1): if not st[j] and (t == -1 or dist[t] &gt; dist[j]): t = j st[t] = True for j in range(1, n + 1): dist[j] = min(dist[j], dist[t] + g[t][j]) if (dist[n] &gt; inf / 2): print(-1)else: print(dist[n]) 1129. 热浪1234567891011121314151617181920212223242526272829303132from queue import Queuen, m, start, end = map(int, input().split())inf = 0x3f3f3f3fg = [[] for _ in range(n + 1)]for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] g[y] += [(x,z)] dist = [inf] * (n + 1)dist[start] = 0st = [False] * (n + 1)q = Queue()q.put(start)st[start] = Truewhile not q.empty(): t = q.get() st[t] = False for ne in g[t]: b, w = ne if dist[b] &gt; dist[t] + w: dist[b] = dist[t] + w if not st[b]: q.put(b)print(dist[end]) 1128. 信使12345678910111213141516171819202122232425262728293031323334353637383940414243444546# spfafrom queue import QueueN = 110inf = 0x3f3f3f3fn, m = map(int, input().split())g = [[] for _ in range(N)]dist = [inf] * Nst = [False] * Ndist[1] = 0for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] g[y] += [(x,z)] q = Queue()q.put(1)st[1] = Truewhile not q.empty(): t = q.get() st[t] = False for b, w in g[t]: if dist[b] &gt; dist[t] + w: dist[b] = dist[t] + w if not st[b]: q.put(b) flag = Falseres = -1for i in range(1, n + 1): if dist[i] == inf: flag = True break res = max(dist[i], res)if flag: print(-1)else: print(res) 12345678910111213141516171819202122232425262728293031323334353637383940414243# dijkstra 堆优化import heapqn, m = map(int, input().split())inf = 0x3f3f3f3fdist = [inf] * (n + 1)dist[1] = 0g = [[] for _ in range(n + 1)]st = [False] * (n + 1)for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y, z)] g[y] += [(x, z)]q = []heapq.heappush(q, (0, 1))while q: d, t = heapq.heappop(q) if st[t]: continue st[t] = True for ne in g[t]: b, w = ne if dist[b] &gt; dist[t] + w: dist[b] = dist[t] + w heapq.heappush(q, (dist[b], b))flag = Falseres = -1for i in range(1, n + 1): if dist[i] == inf: flag = True break res = max(res, dist[i])if flag: print(-1)else: print(res) 新年好 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162from queue import QueueN, M, INF = 50010, 200010, 0x3f3f3f3fg = [[] for _ in range(N)]dist = [[INF] * N for _ in range(6)]vis = [False] * 6n, m = map(int, input().split())source = [1] + list(map(int, input().split()))ans = INF# u: 第u个位置, start: 起点的下标 因为dist[start]才是对应最短路, 全局变量存储最值def dfs(u, start, distance): global ans if u == 6: ans = min(ans, distance) return for i in range(1, 6): ne = source[i] if not vis[i]: vis[i] = True dfs(u + 1, i, dist[start][ne] + distance) vis[i] = Falsedef spfa(start, d): global N d[start] = 0 q = Queue() q.put(start) st = [False] * N st[start] = True while not q.empty(): t = q.get() st[t] = False for b, w in g[t]: if d[b] &gt; d[t] + w: d[b] = d[t] + w if not st[b]: q.put(b) st[b] = Truefor _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y, z)] g[y] += [(x, z)]for i in range(6): spfa(source[i], dist[i])dfs(1, 0, 0)print(ans) 340. 通信线路12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 二分找答案x, 性质 大于x的 变为1,(check最短路dist[n] 是否 &gt; k , 大于k说明 超过k条大于x的, x 要 变大l = mid + 1才能让 最短路小于等于k ) 小于等于x的变为0 from collections import dequen, m, k = map(int, input().split())g = [[] for _ in range(n + 1)]for _ in range(m): a, b, w = map(int, input().split()) g[a] += [(b,w)] g[b] += [(a,w)] def check(x) -&gt; bool: global n, k st = [False] * (n + 1) q = deque([(0, 1)]) dist = [0x3f3f3f3f] * (n + 1) dist[1] = 0 while q: w, t = q.popleft() # print(w, t) if st[t]: continue st[t] = True for b, w in g[t]: v = 0 if w &gt; x: v = 1 if dist[b] &gt; dist[t] + v: dist[b] = dist[t] + v if v == 1: q.append((dist[b],b)) else: q.appendleft((dist[b],b)) # print(x, dist) return dist[n] &gt; k l, r = 0, int(1e6)while l &lt;= r: mid = l + r &gt;&gt; 1 if check(mid): l = mid + 1 else: r = mid - 1if l == int(1e6 + 1): print(-1)else: print(l) 342. 道路与航线家谱树12345678910111213141516171819202122232425262728293031from collections import dequen = int(input())g = [[] for _ in range(n + 1)]d = [0] * (n + 1)for i in range(1, n + 1): g[i] = list(map(int, input().split())) g[i].pop() for e in g[i]: d[e] += 1 q = deque([])for i in range(1, n + 1): if d[i] == 0: q.append(i) while q: t = q.popleft() print(t, end=&quot; &quot;) for ne in g[t]: d[ne] -= 1 if d[ne] == 0: q.append(ne)","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"最短路","slug":"最短路","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF/"}]},{"title":"搜索AcWing题目","slug":"算法/2搜索/AcWing","date":"2022-09-02T03:51:56.000Z","updated":"2022-12-17T09:53:40.959Z","comments":true,"path":"2022/09/02/算法/2搜索/AcWing/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/AcWing/","excerpt":"","text":"树的重心题目描述给定一颗树，树中包含n个结点（编号1~n）和n-1条无向边。 请你找到树的重心，并输出将重心删除后，剩余各个连通块中点数的最大值。 重心定义：重心是指树中的一个结点，如果将这个点删除后，剩余各个连通块中点数的最大值最小，那么这个节点被称为树的重心。 Solution12345678910111213141516171819202122232425262728293031323334353637383940414243444546public class Main &#123; static int n; static int INF = 0x3f3f3f3f; static int res = INF; static boolean[] st; static List&lt;Integer&gt;[] g; public static void main(String[] args) throws IOException &#123;// Solution solution = new Solution();// solution.solution(); BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); n = Integer.parseInt(br.readLine()); g = new List[n+1]; st = new boolean[n+1]; for (int i = 0; i &lt;= n; i++) &#123; g[i] = new ArrayList&lt;Integer&gt;(); &#125; String[] s; for(int i = 1; i &lt;= n - 1; i++)&#123; s = br.readLine().split(&quot; &quot;); int a = Integer.parseInt(s[0]); int b = Integer.parseInt(s[1]); // 建树 g[a].add(b); g[b].add(a); &#125; dfs(1); System.out.println(res); &#125; // 以u为根的子树中的结点数量 public static int dfs(int root) &#123; st[root] = true; int cur = 0; int sum = 0; for (int node : g[root]) &#123; if (!st[node])&#123; int num = dfs(node); sum += num; cur = Math.max(num,cur); &#125; &#125; cur = Math.max(n-sum-1,cur); res = Math.min(cur,res); return sum+1; &#125;&#125; 1097. 池塘计数农夫约翰有一片 $N*M$ 的矩形土地。 最近，由于降雨的原因，部分土地被水淹没了。 现在用一个字符矩阵来表示他的土地。 每个单元格内，如果包含雨水，则用”W”表示，如果不含雨水，则用”.”表示。 现在，约翰想知道他的土地中形成了多少片池塘。 每组相连的积水单元格集合可以看作是一片池塘。 每个单元格视为与其上、下、左、右、左上、右上、左下、右下八个邻近单元格相连。 请你输出共有多少片池塘，即矩阵中共有多少片相连的”W”块。 输入格式第一行包含两个整数 N 和M。 接下来 N 行，每行包含 M 个字符，字符为”W”或”.”，用以表示矩形土地的积水状况，字符之间没有空格。 输出格式输出一个整数，表示池塘数目。 数据范围1 \\le N,M \\le 1000 输入样例：123456789101110 12W........WW..WWW.....WWW....WW...WW..........WW..........W....W......W...W.W.....WW.W.W.W.....W..W.W......W...W.......W. 输出样例：13 Solution123456789101112131415161718192021222324252627282930313233from collections import dequefrom typing import Tuplen, m = map(int,input().split())# 定义一维 在用for创建二维g = [list(input()) for _ in range(n)]st = [[False] * m for _ in range(n)]def bfs(sx,sy): queue = deque([(sx,sy)]) st[sx][sy] = True while queue: x, y = queue.popleft() for i in range(x-1,x+2): for j in range(y-1,y+2): if i == x and j == y: continue if i &lt; 0 or i &gt;= n or j &lt; 0 or j &gt;= m: continue if g[i][j] == &quot;.&quot; or st[i][j]: continue queue.append((i,j)) st[i][j] = Truedef main(): cnt = 0 for i in range(n): for j in range(m): if not st[i][j] and g[i][j] == &#x27;W&#x27;: bfs(i,j) cnt += 1 print(cnt)main() 1098. 城堡问题1234567891011121314151617 1 2 3 4 5 6 7 #############################1 # | # | # | | # #####---#####---#---#####---#2 # # | # # # # # #---#####---#####---#####---#3 # | | # # # # # #---#########---#####---#---#4 # # | | | | # # ############################# (图 1) # = Wall | = No wall - = No wall 方向：上北下南左西右东。 图1是一个城堡的地形图。 请你编写一个程序，计算城堡一共有多少房间，最大的房间有多大。 城堡被分割成 m∗n个方格区域，每个方格区域可以有0~4面墙。 注意：墙体厚度忽略不计。 输入格式第一行包含两个整数 m 和 n，分别表示城堡南北方向的长度和东西方向的长度。 接下来 m 行，每行包含 n 个整数，每个整数都表示平面图对应位置的方块的墙的特征。 每个方块中墙的特征由数字 P 来描述，我们用1表示西墙，2表示北墙，4表示东墙，8表示南墙，P 为该方块包含墙的数字之和。 例如，如果一个方块的 P 为3，则 3 &#x3D; 1 + 2，该方块包含西墙和北墙。 城堡的内墙被计算两次，方块(1,1)的南墙同时也是方块(2,1)的北墙。 输入的数据保证城堡至少有两个房间。 输出格式共两行，第一行输出房间总数，第二行输出最大房间的面积（方块数）。 数据范围1≤m,n≤501≤m,n≤50,0≤P≤150≤P≤15 输入样例：123454 7 11 6 11 6 3 10 6 7 9 6 13 5 15 5 1 10 12 7 13 7 5 13 11 10 8 10 12 13 输出样例：1259 Solution123456789101112131415161718192021222324252627282930313233343536373839404142434445from collections import dequefrom typing import Tuplen, m = map(int, input().split())# 定义一维 在用for创建二维g = [list(map(int, input().split())) for _ in range(n)]st = [[False] * m for _ in range(n)]dx, dy = [0, -1, 0, 1], [-1, 0, 1, 0]def bfs(sx, sy) -&gt; int: queue = deque([(sx, sy)]) st[sx][sy] = True v = 1 while queue: x, y = queue.popleft() for i in range(4): a, b = x + dx[i], y + dy[i] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= m: continue if st[a][b]: continue if g[x][y] &gt;&gt; i &amp; 1: continue v += 1 print(a,b) queue.append((a, b)) st[a][b] = True return vdef main(): cnt = 0 max_v = 0 for i in range(n): for j in range(m): if not st[i][j]: t = bfs(i, j) max_v = max(t, max_v) cnt += 1 print(cnt) print(max_v)main() 1106.山峰山谷FGD小朋友特别喜欢爬山，在爬山的时候他就在研究山峰和山谷。 为了能够对旅程有一个安排，他想知道山峰和山谷的数量。 给定一个地图，为FGD想要旅行的区域，地图被分为 n×n 的网格，每个格子 (i,j)的高度 w(i,j) 是给定的。 若两个格子有公共顶点，那么它们就是相邻的格子，如与 (i,j) 相邻的格子有(i−1,j−1),(i−1,j),(i−1,j+1),(i,j−1),(i,j+1),(i+1,j−1),(i+1,j),(i+1,j+1)(i−1,j−1),(i−1,j),(i−1,j+1),(i,j−1),(i,j+1),(i+1,j−1),(i+1,j),(i+1,j+1)。 我们定义一个格子的集合 S 为山峰（山谷）当且仅当： S 的所有格子都有相同的高度。 S 的所有格子都连通。 对于 s 属于 S，与 s 相邻的 s′ 不属于 S，都有 ws&gt;ws（山峰），或者 ws&lt;ws′（山谷）。 如果周围不存在相邻区域，则同时将其视为山峰和山谷。 你的任务是，对于给定的地图，求出山峰和山谷的数量，如果所有格子都有相同的高度，那么整个地图即是山峰，又是山谷。 输入格式第一行包含一个正整数 nn，表示地图的大小。 接下来一个 n×n 的矩阵，表示地图上每个格子的高度 ww。 输出格式共一行，包含两个整数，表示山峰和山谷的数量。 数据范围1≤n≤10001≤n≤1000,0≤w≤1090≤w≤109 输入样例1：12345658 8 8 7 77 7 8 8 77 7 7 7 77 8 8 7 87 8 8 8 8 输出样例1：12 1 输入样例2：12345655 7 8 3 15 5 7 6 66 6 6 2 85 7 2 5 87 1 0 1 7 输出样例2：13 3 样例解释样例1： 样例2： Solution1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from collections import dequefrom typing import Tuplen = int(input())# n, m = map(int, input().split())# 定义一维 在用for创建二维g = [list(map(int, input().split())) for _ in range(n)]st = [[False] * n for _ in range(n)]l_ans = 0h_ans = 0def bfs(sx, sy): global h_ans, l_ans queue = deque([(sx, sy)]) st[sx][sy] = True l, h = 0 ,0 while queue: x, y = queue.popleft() for i in range(x-1,x+2): for j in range(y-1,y+2): if i == x and j == y: continue if i &lt; 0 or i &gt;= n or j &lt; 0 or j &gt;= n: continue if g[i][j] &gt; g[x][y]: h += 1 continue if g[i][j] &lt; g[x][y]: l += 1 continue if st[i][j]: continue queue.append((i,j)) st[i][j] = True if l == 0 and h == 0: l_ans += 1 h_ans += 1 print(&quot;既是山峰又是山谷&quot;) elif l == 0: l_ans += 1 print(&quot;没有比自己矮的 所以是山谷&quot;) elif h == 0: h_ans += 1 print(&quot;没有比自己高的 所以是山峰&quot;) print(l_ans,h_ans) print() 提纲 走出迷宫的最小移动次数 迷宫问题(BFS记录路径 抓住那头牛 (一维最短路) 844. 走出迷宫的最小移动次数 最短路 给定一个 n×m 的二维整数数组，用来表示一个迷宫，数组中只包含 0 或 1，其中 0 表示可以走的路，1 表示不可通过的墙壁。 最初，有一个人位于左上角 (1,1)处，已知该人每次可以向上、下、左、右任意一个方向移动一个位置。 请问，该人从左上角移动至右下角 (n,m) 处，至少需要移动多少次。 数据保证 (1,1) 处和 (n,m) 处的数字为 0，且一定至少存在一条通路。 输入格式第一行包含两个整数 n 和 m。 接下来 n 行，每行包含 m 个整数（0 或 1），表示完整的二维数组迷宫。 输出格式输出一个整数，表示从左上角移动至右下角的最少移动次数。 数据范围1≤n,m≤100 输入样例：1234565 50 1 0 0 00 1 0 1 00 0 0 0 00 1 1 1 00 0 0 1 0 输出样例：18 代码123456789101112131415161718192021222324252627282930313233343536373839import java.util.*;class Main&#123; static int n,m; static int[][] g; static int[][] d; // 起点到ij的最短距离 static int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); m = in.nextInT(); g = new int[n][m]; d = new int[n][m]; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; g[i][j] = in.nextInt(); d[i][j] = -1; &#125; &#125; bfs(); System.out(dp[n-1][m-1]); &#125; static public void bfs() &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); d[0][0] = 0; q.offer(new int[]&#123;0,0&#125;); while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int a = t[0] + dx[i], b = t[0] + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) continue; if (g[a][b] == 1 || d[a][b] != -1) continue; q.offer(new int[]&#123;a,b&#125;); d[a][b] = d[t[0]][t[1]] + 1; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435from collections import dequen, m = map(int, input().split())g = [list(map(int, input().split())) for _ in range(n)]st = [[False] * m for _ in range(n)]dx = [-1, 0, 1, 0]dy = [0, -1, 0, 1]def bfs(): cnt = 0 q = deque([(0, 0)]) st[0][0] = True while q: sz = len(q) cnt += 1 for _ in range(sz): x, y = q.popleft() for i in range(4): a = x + dx[i] b = y + dy[i] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= m: continue if st[a][b]: continue if g[a][b] == 1: continue if a == n - 1 and b == m - 1: print(cnt) return q.append((a, b)) st[a][b] = Truebfs() 1076. 迷宫问题(BFS记录路径12345678910111213int maze[5][5] = &#123;0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,&#125;; 它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 数据保证至少存在一条从左上角走到右下角的路径。 输入格式第一行包含整数 n。 接下来 nn 行，每行包含 nn 个整数 0 或 1，表示迷宫。 输出格式输出从左上角到右下角的最短路线，如果答案不唯一，输出任意一条路径均可。 按顺序，每行输出一个路径中经过的单元格的坐标，左上角坐标为 (0,0)，右下角坐标为 (n−1,n−1)。 数据范围0≤n≤10000≤n≤1000 输入样例：12345650 1 0 0 00 1 0 1 00 0 0 0 00 1 1 1 00 0 0 1 0 输出样例：1234567890 01 02 02 12 22 32 43 44 4 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.*;class Main&#123; static int n; static int[][] g; static int[][][] d; // 起点到ij的最短距离 static int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); g = new int[n][n]; d = new int[n][n][3]; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; g[i][j] = in.nextInt(); d[i][j][0] = -1; &#125; &#125; bfs(); int[] end = d[0][0]; while (true) &#123; System.out.println(end[1] + &quot; &quot; + end[2]); if (end[1] == n-1 &amp;&amp; end[2] == n-1) break; end = d[end[1]][end[2]]; &#125; &#125; static public void bfs() &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); d[n-1][n-1][0] = 0; q.offer(new int[]&#123;n-1,n-1&#125;); while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int a = t[0] + dx[i], b = t[1] + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= n) continue; if (g[a][b] == 1 || d[a][b][0] != -1) continue; q.offer(new int[]&#123;a,b&#125;); d[a][b][0] = d[t[0]][t[1]][0] + 1; d[a][b][1] = t[0]; d[a][b][2] = t[1]; &#125; &#125; &#125;&#125; 1100. 抓住那头牛 (一维最短路)农夫知道一头牛的位置，想要抓住它。 农夫和牛都位于数轴上，农夫起始位于点 N，牛位于点 K。 农夫有两种移动方式： 从 X 移动到 X−1或 X+1，每次移动花费一分钟 从 X移动到 2∗X，每次移动花费一分钟 假设牛没有意识到农夫的行动，站在原地不动。 农夫最少要花多少时间才能抓住牛？ 输入格式共一行，包含两个整数N和K。 输出格式输出一个整数，表示抓到牛所花费的最少时间。 数据范围0≤N,K≤105 输入样例：15 17 输出样例：14 代码1234567891011121314151617181920212223242526272829303132333435363738// 想象成 0-(K)2*10的五次方个点 每个点有三种走法public class Main&#123; static int n, k,N; static int[] d; static int[] dx = &#123;-1,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); k = in.nextInt(); N = Math.max(k,n) * 2 + 10; d = new int[N]; Arrays.fill(d,-1); d[n] = 0; bfs(); System.out.println(d[k]); &#125; public static void bfs() &#123; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); q.offer(n); while (!q.isEmpty()) &#123; int cur = q.poll(); for (int i = 0; i &lt; 3; i++) &#123; int a = cur; if (i != 2) &#123; a += dx[i]; &#125;else &#123; a *= 2; &#125; if (a &lt; 0 || a &gt;= N ) continue; if (d[a] != -1) continue; d[a] = d[cur] + 1; q.offer(a); &#125; &#125; &#125;&#125; 绅士风度的牛农民 John 有很多牛，他想交易其中一头被 Don 称为 The Knight 的牛。 这头牛有一个独一无二的超能力，在农场里像 Knight 一样地跳（就是我们熟悉的象棋中马的走法）。 虽然这头神奇的牛不能跳到树上和石头上，但是它可以在牧场上随意跳，我们把牧场用一个 x，yx，y 的坐标图来表示。 这头神奇的牛像其它牛一样喜欢吃草，给你一张地图，上面标注了 The Knight 的开始位置，树、灌木、石头以及其它障碍的位置，除此之外还有一捆草。 现在你的任务是，确定 The Knight 要想吃到草，至少需要跳多少次。 The Knight 的位置用 K 来标记，障碍的位置用 * 来标记，草的位置用 H 来标记。 这里有一个地图的例子： 123456789101112131411 | . . . . . . . . . .10 | . . . . * . . . . . 9 | . . . . . . . . . . 8 | . . . * . * . . . . 7 | . . . . . . . * . . 6 | . . * . . * . . . H 5 | * . . . . . . . . . 4 | . . . * . . . * . . 3 | . K . . . . . . . . 2 | . . . * . . . . . * 1 | . . * . . . . * . . 0 ---------------------- 1 0 1 2 3 4 5 6 7 8 9 0 The Knight 可以按照下图中的 A,B,C,D…A,B,C,D… 这条路径用 55 次跳到草的地方（有可能其它路线的长度也是 55）： 123456789101112131411 | . . . . . . . . . .10 | . . . . * . . . . . 9 | . . . . . . . . . . 8 | . . . * . * . . . . 7 | . . . . . . . * . . 6 | . . * . . * . . . F&lt; 5 | * . B . . . . . . . 4 | . . . * C . . * E . 3 | .&gt;A . . . . D . . . 2 | . . . * . . . . . * 1 | . . * . . . . * . . 0 ---------------------- 1 0 1 2 3 4 5 6 7 8 9 0 注意： 数据保证一定有解。 输入格式第 1 行： 两个数，表示农场的列数 CC 和行数 RR。 第 2..R+12..R+1 行: 每行一个由 CC 个字符组成的字符串，共同描绘出牧场地图。 输出格式一个整数，表示跳跃的最小次数。 数据范围1≤R,C≤1501≤R,C≤150 输入样例：1234567891011121314151610 11..............*..................*.*...........*....*..*...H*............*...*...K...........*.....*..*....*..15 4 0 0 输出样例：15 123456789101112131415161718192021222324252627282930313233343536from collections import dequem, n = map(int, input().split())g = [input() for _ in range(n)]st = [[False] * m for _ in range(n)]dx, dy = [-2, -2, -1, 1, 2, 2, 1, -1], [1,-1,-2,-2,-1,1,2,2]def bfs(sx,sy) -&gt; int: q = deque([(sx,sy)]) st[sx][sy] = True ans = 0 while q: ans += 1 sz = len(q) for j in range(sz): x, y = q.popleft() for i in range(8): a, b = x + dx[i], y + dy[i] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= m: continue if st[a][b]: continue if g[a][b] == &#x27;*&#x27;: continue if g[a][b] == &#x27;H&#x27;: return ans st[a][b] = True q.append((a,b)) return -1; for i in range(n): for j in range(m): if g[i][j] == &#x27;K&#x27;: print(bfs(i,j)) 845. 八数板在一个 3×33×3 的网格中，1∼81∼8 这 88 个数字和一个 x 恰好不重不漏地分布在这 3×33×3 的网格中。 例如： 1231 2 3x 4 67 5 8 在游戏过程中，可以把 x 与其上、下、左、右四个方向之一的数字交换（如果存在）。 我们的目的是通过交换，使得网格变为如下排列（称为正确排列）： 1231 2 34 5 67 8 x 例如，示例中图形就可以通过让 x 先后与右、下、右三个方向的数字交换成功得到正确排列。 交换过程如下： 1231 2 3 1 2 3 1 2 3 1 2 3x 4 6 4 x 6 4 5 6 4 5 67 5 8 7 5 8 7 x 8 7 8 x 现在，给你一个初始网格，请你求出得到正确排列至少需要进行多少次交换。 输入格式输入占一行，将 3×33×3 的初始网格描绘出来。 例如，如果初始网格如下所示： 1231 2 3 x 4 6 7 5 8 则输入为：1 2 3 x 4 6 7 5 8 输出格式输出占一行，包含一个整数，表示最少交换次数。 如果不存在解决方案，则输出 −1−1。 输入样例：12 3 4 1 5 x 7 6 8 输出样例119 1107. 魔板1112. 迷宫123456789101112131415161718192021222324252627282930import syssys.setrecursionlimit(500000)T = int(input())def dfs(xa,ya,xb,yb,g,n,st)-&gt;bool: if g[xa][ya] == &#x27;#&#x27;: return False st[xa][ya] = True if xa == xb and ya == yb: return True dx, dy = [-1,0,1,0], [0,-1,0,1] for i in range(4): a, b = xa + dx[i], ya + dy[i] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= n: continue if st[a][b]: continue if dfs(a,b,xb,yb,g,n,st): return True return False while T: n = int(input()) g = [input() for _ in range(n)] xa, ya, xb, yb = map(int, input().split()) st = [[False] * n for _ in range(n)] if dfs(xa,ya,xb,yb,g,n,st): print(&quot;YES&quot;) else: print(&quot;NO&quot;) T -= 1 红与黑有一间长方形的房子，地上铺了红色、黑色两种颜色的正方形瓷砖。 你站在其中一块黑色的瓷砖上，只能向相邻（上下左右四个方向）的黑色瓷砖移动。 请写一个程序，计算你总共能够到达多少块黑色的瓷砖。 输入格式输入包括多个数据集合。 每个数据集合的第一行是两个整数 WW 和 HH，分别表示 xx 方向和 yy 方向瓷砖的数量。 在接下来的 HH 行中，每行包括 WW 个字符。每个字符表示一块瓷砖的颜色，规则如下 1）‘.’：黑色的瓷砖；2）‘#’：红色的瓷砖；3）‘@’：黑色的瓷砖，并且你站在这块瓷砖上。该字符在每个数据集合中唯一出现一次。 当在一行中读入的是两个零时，表示输入结束。 输出格式对每个数据集合，分别输出一行，显示你从初始位置出发能到达的瓷砖数(记数时包括初始位置的瓷砖)。 数据范围1≤W,H≤201≤W,H≤20 输入样例：12345678910116 9 ....#. .....# ...... ...... ...... ...... ...... #@...# .#..#. 0 0 输出样例：145 1234567891011121314151617181920212223242526272829303132333435import syssys.setrecursionlimit(500000)while True: m, n = map(int, input().split()) if m == 0 and n == 0: break g = [input() for _ in range(n)] st = [[False] * m for _ in range(n)] def dfs(sx,sy) -&gt; int: global n, m st[sx][sy] = True dx, dy = [-1, 0, 1, 0], [0, -1, 0, 1] res = 1 for i in range(4): a = sx + dx[i] b = sy + dy[i] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= m: continue if st[a][b]: continue if g[a][b] == &#x27;#&#x27;: continue res += dfs(a,b) return res for i in range(n): for j in range(m): if g[i][j] == &#x27;@&#x27;: print(dfs(i,j)) 1116. 马走日马在中国象棋以日字形规则移动。 请编写一段程序，给定 n∗m 大小的棋盘，以及马的初始位置 (x，y)(x，y)，要求不能重复经过棋盘上的同一个点，计算马可以有多少途径遍历棋盘上的所有点。 输入格式第一行为整数 T，表示测试数据组数。 每一组测试数据包含一行，为四个整数，分别为棋盘的大小以及初始位置坐标 n,m,x,yn,m,x,y。 输出格式每组测试数据包含一行，为一个整数，表示马能遍历棋盘的途径总数，若无法遍历棋盘上的所有点则输出 0。 数据范围1≤T≤91≤T≤9,1≤m,n≤91≤m,n≤9,1≤n×m≤281≤n×m≤28,0≤x≤n−10≤x≤n−1,0≤y≤m−10≤y≤m−1 输入样例：1215 4 0 0 输出样例：132 代码123456789101112131415161718192021222324252627282930313233343536373839404142#include&lt;cstring&gt;#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;const int N = 10;bool st[N][N];int n, m;int ans;int dx[] = &#123;-2, -2, -1, 1, 2, 2, 1, -1&#125;;int dy[] = &#123;1, -1, -2, -2, -1, 1, 2, 2&#125;;void dfs(int sx, int sy, int cnt) &#123; if (cnt == n * m) &#123; ans ++; return ; &#125; st[sx][sy] = true; for (int i = 0; i &lt; 8; i ++) &#123; int a = sx +dx[i], b = sy + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) continue; if (st[a][b]) continue; dfs(a,b,cnt+1); &#125; st[sx][sy] = false; &#125;int main() &#123; int T; cin &gt;&gt; T; while (T --) &#123; ans = 0; int x, y; cin &gt;&gt; n &gt;&gt; m &gt;&gt; x &gt;&gt; y; dfs(x,y,1); printf(&quot;%d\\n&quot;, ans); &#125; return 0;&#125; 单词接龙12// 单词尾部处理 分为互质组","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"2搜索","slug":"算法/2搜索","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"搜索","slug":"搜索","permalink":"https://gouguoqiang.github.io/tags/%E6%90%9C%E7%B4%A2/"}]},{"title":"图论方法论","slug":"算法/4图论/方法论图论","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-23T05:21:01.886Z","comments":true,"path":"2022/09/02/算法/4图论/方法论图论/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%96%B9%E6%B3%95%E8%AE%BA%E5%9B%BE%E8%AE%BA/","excerpt":"","text":"最短路 朴素dijkstra证明dijkstra 每一步都最近 第一次到达的点离起点最近 Algorithm12345678910111213141. dist = [inf] * n + 1, dist[start] = 02. S: 当前已确定最短距离的点 用boolean数组存, for i in range(1,n + 1) t &lt;- 不在S中的距离起点最近的点 # 每轮迭代确定一个点的最短路径 t = -1 for i in range(1, n + 1): if (!st[i] &amp;&amp; (t == -1 || dist[i] &lt; dist[t])): t = i #确定dist[t]是最小距离, 不会被更新 st[t] = true 3. 用t更新其他所有点的距离 for j in range(n): dist[j] = min(dist[j], dist[t] + g[t][j]) Example123456789101112131415161718192021222324252627n, m = map(int, input().split())N = 510inf = 0x3f3f3f3fg = [[inf] * N for _ in range(N)]dist = [inf] * Nst = [False] * Nfor _ in range(m): x, y, z = map(int, input().split()) g[x][y] = min(g[x][y], z)dist[1] = 0for i in range(n): t = -1 for j in range(1, n + 1): if not st[j] and (t == -1 or dist[t] &gt; dist[j]): t = j st[t] = True for j in range(1, n + 1): dist[j] = min(dist[j], dist[t] + g[t][j])if (dist[n] &gt; inf / 2): print(-1)else: print(dist[n]) 堆优化dijkstra1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123; int INF = Integer.MAX_VALUE / 2; public int networkDelayTime(int[][] times, int n, int k) &#123; //给的是边加边权重,n个网络结点 int[] dis = dij(times,n,k); int ans = Integer.MIN_VALUE; for (int i = 1; i &lt;= n ; i++) &#123; if (dis[i] == INF) &#123; return -1; &#125; ans = Math.max(dis[i],ans); &#125; return ans; &#125; int[] dist; List&lt;int[]&gt;[] g; boolean[] st; public int[] dij(int[][] edges, int n, int start) &#123; g = new List[n+1]; st = new boolean[n+1]; for (int i = 0; i &lt;= n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[0]].add(new int[]&#123;edge[1],edge[2]&#125;); &#125; dist = new int[n+1]; // g是 n+1 Arrays.fill(dist, INF); Queue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a,b)-&gt;&#123; return a[1] - b[1]; &#125;); dist[start] = 0; pq.offer(new int[]&#123;start,0&#125;); while (!pq.isEmpty()) &#123; int[] cur = pq.poll(); int t = cur[0]; int distance = cur[1]; if (st[t]) continue; st[t] = true; for (int[] next : g[t]) &#123; int j = next[0]; int d = distance + next[1]; if (dist[j] &gt; d) &#123; dist[j] = d; pq.offer(new int[]&#123;j,d&#125;); &#125; &#125; &#125; return dist; &#125;&#125; 123456789101112131415161718192021222324252627282930313233import heapqn, m = map(int, input().split())inf = 0x3f3f3f3fg = [[] for _ in range(n + 1)]for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] st = [False] * (n + 1)dist = [inf] * (n + 1)dist[1] = 0q = []heapq.heappush(q,(0,1)) # 默认第一个元素进行从小到大排序, 所以存储距离while q: w, t = heapq.heappop(q) if st[t]: continue st[t] = True for ne in g[t]: j, d = ne if dist[j] &gt; dist[t] + d: dist[j] = dist[t] + d heapq.heappush(q,(dist[t] + d,j)) if dist[n] == inf: print(-1)else: print(dist[n]) Bellman-ford算法Algorithm1234567891011121314bellman核心思想: 建图方式随便建只要能遍历到所有边for n: for n 循环的意义不超过n条边到达b最短路的距离, 所以如果第n次循环还能更新就代表存在负环(一条通路有n条边那么有n+1个点,一定有一个重复的点, 继续更新说明这个环的权值和为负,即负环) #spfa 对这步优化 只有dist[a] 变小 dist[b]才能变小 用队列存一个变小的a for 所有边 a b w(a-&gt;b 的距离为w) m条边 dist[b] = min(dist[b],dist[a] + w) # n-1次后能保证 dist[b] &lt;= dist[a] + w复杂度 n * m Example给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出从 11 号点到 nn 号点的最多经过 kk 条边的最短距离，如果无法从 11 号点走到 nn 号点，输出 impossible。 注意：图中可能 存在负权回路 。 输入格式第一行包含三个整数 n,m,k。 接下来 m 行，每行包含三个整数 x,y,z，表示存在一条从点 x 到点 y 的有向边，边长为 z。 点的编号为 1∼n。 输出格式输出一个整数，表示从 1 号点到 n 号点的最多经过 k 条边的最短距离。 如果不存在满足条件的路径，则输出 impossible。 数据范围1≤n,k≤500,1≤m≤10000,1≤x,y≤n，任意边长的绝对值不超过 10000。 输入样例：12343 3 11 2 12 3 11 3 3 输出样例：13 12345678910111213141516171819inf = 0x3f3f3f3fn, m, k = map(int, input().split())edge = [list(map(int, input().split())) for _ in range(m)]dist = [inf] * (n+1)dist[1] = 0for i in range(k): cpy = dist.copy() for j in range(m): a, b, w = edge[j] dist[b] = min(dist[b], cpy[a] + w)if dist[n] &gt; inf / 2: print(&quot;impossible&quot;)else: print(dist[n]) SPFA头绪: 给定一个 n 个点 m 条边的有向图，图中可能存在重边和自环， 边权可能为负数。 请你求出 1号点到 n 号点的最短距离，如果无法从 1 号点走到 n 号点，则输出 impossible。 数据保证不存在负权回路。 输入格式第一行包含整数 n 和 m。 接下来 m 行每行包含三个整数 x,y,z，表示存在一条从点 x 到点 y的有向边，边长为 z。 输出格式输出一个整数，表示 1 号点到 n 号点的最短距离。 如果路径不存在，则输出 impossible。 数据范围1 &lt; n,m &lt; 10^5,图中涉及边长绝对值均不超过 10000。 输入样例：12343 31 2 52 3 -31 3 4 输出样例：12 123456789101112131415161718192021222324252627282930from queue import Queuen, m = map(int, input().split())inf = 0x3f3f3f3fdist = [inf] * (n + 1)g = [[] for _ in range(n + 1)]for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] dist[1] = 0st = [False] * (n + 1)q = Queue()q.put(1)st[1] = Truewhile not q.empty(): t = q.get() st[t] = False for ne in g[t]: b, w = ne if dist[b] &gt; dist[t] + w: dist[b] = dist[t] + w if not st[b]: q.put(b) if dist[n] &gt; inf / 2: print(&quot;impossible&quot;)else: print(dist[n]) floyd算法 多源汇最短路 1234567891011121314151617基于动态规划 d[k,i,j]i到j只经过1-k这些点的最短距离 选第k个点和不选第k个点初始化： for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == j) d[i][j] = 0; else d[i][j] = INF;// 算法结束后，d[a][b]表示a到b的最短距离void floyd()&#123; for (int k = 1; k &lt;= n; k ++ ) for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]);&#125; 总纲领适用场景: 多远多汇最短路, n3方的复杂度内求出任意两个点之间的最短路(不包含负环) 最短路 传递闭包 找最小环 恰好经过K条边的最短路 流程: 123456int INF 0x3f3f3f3f;d(i,j) = i == j ? 0 : INF;for (k = 1-n) for (i = 1-n) for (j = 1 - n) d(i,j) = min(d(i,j),d(i,k)+d(k,j)); 证明: 基于dp d[k,i,j] 集合 : 所有从i出发,最终走到j,且中间只经过结点编号不超过k的所有路径 属性 路径长度的最小值 状态计算: 集合划分 分为不包含结点k路径和所有包含k节点的路径,取这两个集合最小值 d(k,i,j) &#x3D; min(d(k-1,i,j),d[k-1,i,k] + d(k-1,k,j)); 优化 去掉k维的正确性(证明等效性,有一个 0 的初始化可以忽略) AcWing 854. Floyd求最短路难度：简单 题目描述给定一个n个点m条边的有向图，图中可能存在重边和自环，边权可能为负数。 再给定k个询问，每个询问包含两个整数x和y，表示查询从点x到点y的最短距离，如果路径不存在，则输出“impossible”。 数据保证图中不存在负权回路。 输入格式 第一行包含三个整数n，m，k 接下来m行，每行包含三个整数x，y，z，表示存在一条从点x到点y的有向边，边长为z。 接下来k行，每行包含两个整数x，y，表示询问点x到点y的最短距离。 输出格式 共k行，每行输出一个整数，表示询问的结果，若询问两点间不存在路径，则输出“impossible”。 数据范围 $1≤n≤200,$$1≤k≤n^2$$1≤m≤20000,$图中涉及边长绝对值均不超过10000。 123456789101112输入样例：3 3 21 2 12 3 21 3 12 11 3输出样例：impossible1 Solution12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 0x3f3f3f3f 在 Int_MAX / 3 - MAX / 2之间 当做正无穷处理 import java.io.*;import java.util.*;public class Main&#123; static final int INF = 0x3f3f3f3f, N = 210; static int[][] g = new int[N][N]; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); String[] s = br.readLine().split(&quot; &quot;); int n = Integer.parseInt(s[0]); int m = Integer.parseInt(s[1]); int k = Integer.parseInt(s[2]); for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; g[i][j] = i == j ? 0 : INF; &#125; &#125; while (m-- &gt; 0) &#123; s = br.readLine().split(&quot; &quot;); int x = Integer.parseInt(s[0]); int y = Integer.parseInt(s[1]); int z = Integer.parseInt(s[2]); g[x][y] = Math.min(g[x][y],z); &#125; floyd(n); while (k-- &gt; 0) &#123; s = br.readLine().split(&quot; &quot;); int x = Integer.parseInt(s[0]); int y = Integer.parseInt(s[1]); if (g[x][y] &gt; INF / 2) bw.write(&quot;impossible\\n&quot;); else bw.write(g[x][y]+&quot;\\n&quot;); &#125; bw.close(); br.close(); &#125; public static void floyd(int n) &#123; for (int k = 1; k &lt;= n; k++) for (int i = 1; i &lt;= n; i++) for (int j = 1; j &lt;= n; j++) g[i][j] = Math.min(g[i][j],g[i][k]+g[k][j]); &#125;&#125; 最短路中级(与其他算法结合)最小生成树二分图","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"二分图","slug":"二分图","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86%E5%9B%BE/"}]},{"title":"dp力扣","slug":"算法/5dp/LeetCodeDP","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:34:03.620Z","comments":true,"path":"2022/09/02/算法/5dp/LeetCodeDP/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/LeetCodeDP/","excerpt":"","text":"10. 正则表达式匹配给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 &#39;.&#39; 和 &#39;*&#39; 的正则表达式匹配。 &#39;.&#39; 匹配任意单个字符 &#39;*&#39; 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。 示例 1： 123输入：s = &quot;aa&quot;, p = &quot;a&quot;输出：false解释：&quot;a&quot; 无法匹配 &quot;aa&quot; 整个字符串。 示例 2: 123输入：s = &quot;aa&quot;, p = &quot;a*&quot;输出：true解释：因为 &#x27;*&#x27; 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 &#x27;a&#x27;。因此，字符串 &quot;aa&quot; 可被视为 &#x27;a&#x27; 重复了一次。 示例 3： 123输入：s = &quot;ab&quot;, p = &quot;.*&quot;输出：true解释：&quot;.*&quot; 表示可匹配零个或多个（&#x27;*&#x27;）任意字符（&#x27;.&#x27;）。 提示： 1 &lt;= s.length &lt;= 20 1 &lt;= p.length &lt;= 30 s 只包含从 a-z 的小写字母。 p 只包含从 a-z 的小写字母，以及字符 . 和 *。 保证每次出现字符 * 时，前面都匹配到有效的字符 Solution1234567891011121314151617181920212223class Solution: def isMatch(self, s: str, p: str) -&gt; bool: n, m = len(s), len(p) s = &quot; &quot; + s p = &quot; &quot; + p f = [[False] * (m+1) for _ in range(n+1)] f[0][0] = True for i in range(0, n + 1): for j in range(0, m + 1): if i == 0 and j == 0: f[i][j] = True if j + 1 &lt;= m and p[j+1] == &#x27;*&#x27;: continue if i &gt; 0 and j &gt; 0 and (p[j] == &#x27;.&#x27; or p[j] == s[i]): f[i][j] = f[i-1][j-1] if p[j] == &#x27;*&#x27; and j &gt; 1: f[i][j] = f[i][j-2] or f[i-1][j] and (s[i] == p[j-1] or p[j-1] == &#x27;.&#x27;) print(i,j,f[i][j]) # f[i][j] = f[i][j-2] or f[i-1][j-2] and (s[i]与p[j-1]匹配) or f[i-2][j-2] and(s[i-1:i]与p[j-1]匹配) # # f[i-1][j] = f[i-1][j-2] or f[i-2][j-2] and (s[i-1]与p[j-1]匹配) # 发现每一项都比上面的少一个(s[i]与p[j-1]匹配) = x左边加上 and x就能替代 return f[n][m] 44. 通配符匹配Solution1234567891011121314151617181920212223class Solution &#123; public boolean isMatch(String s, String p) &#123; int n = s.length(); int m = p.length(); s = &quot; &quot; + s; p = &quot; &quot; + p; boolean[][] f = new boolean[n+1][m+1]; for (int i = 0; i &lt;= n; i++) &#123; for (int j = 0; j &lt;= m; j++) &#123; char a = s.charAt(i), b = p.charAt(j); if (i == 0 &amp;&amp; j == 0) f[i][j] = true; if ((i &gt; 0 &amp;&amp; j &gt; 0) &amp;&amp; (b == &#x27;?&#x27; || b == a)) f[i][j] = f[i-1][j-1]; if (b == &#x27;*&#x27;) &#123; // f[i][j] = f[i][j-1] || f[i-1][j-1] || f[i-2][j-1] .. // f[i-1][j] = f[i-1][j-1] || f[i-2][j-1] if (i &gt; 0) f[i][j] = f[i][j] || f[i-1][j]; if (j &gt; 0) f[i][j] = f[i][j] || f[i][j-1]; &#125; &#125; &#125; return f[n][m]; &#125;&#125; 573.机器人的运动范围地上有一个m行n列的方格，从坐标 [0,0] 到坐标 [m-1,n-1] 。一个机器人从坐标 [0, 0] 的格子开始移动，它每次可以向左、右、上、下移动一格（不能移动到方格外），也不能进入行坐标和列坐标的数位之和大于k的格子。例如，当k为18时，机器人能够进入方格 [35, 37] ，因为3+5+3+7&#x3D;18。但它不能进入方格 [35, 38]，因为3+5+3+8&#x3D;19。请问该机器人能够到达多少个格子？ 示例 1： 12输入：m = 2, n = 3, k = 1输出：3 示例 2： 12输入：m = 3, n = 1, k = 0输出：1 提示： 1 &lt;= n,m &lt;= 100 0 &lt;= k &lt;= 20 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; int res; public int movingCount(int m, int n, int k) &#123; res = 0; //bfs,dfs(递归)四个方向 走回头路 坐标 boolean[][] vis = new boolean[m][n]; dfs(m,n,0,0,k,vis); return res; &#125; private void dfs(int m, int n, int i, int j, int k,boolean[][] vis)&#123; if(i&lt;0 || j&lt;0 || i==m || j==n) return ; if(!checkIj(i,j,k)) return ; if(vis[i][j]) return; //干脆不走回头路 res++; vis[i][j] = true; //缩小了规模 总有一天会跳出 dfs(m,n,i+1,j,k,vis); dfs(m,n,i,j+1,k,vis); //下面这两步回溯会帮你干 // dfs(m,n,i-1,j,k,vis); // dfs(m,n,i,j-1,k,vis); &#125; //判断下标的数位之和 private boolean checkIj(int i, int j, int k)&#123; int comp = 0; while(i != 0 || j != 0)&#123; comp += i%10; comp += j%10; j /= 10; i /= 10; &#125; return comp &lt;= k; &#125;&#125; 329. 矩阵中的最长递增路径 1234567891011121314151617181920212223242526272829class Solution &#123; static final int[][] dirs = new int[][]&#123;&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,-1&#125;,&#123;0,1&#125;&#125;; public int longestIncreasingPath(int[][] matrix) &#123; // dfs 记忆化 int n = matrix.length; int m = matrix[0].length; int ans = 1; int[][] memo = new int[n][m]; for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; m; j++) &#123; ans = Math.max(ans,dfs(matrix,memo,i,j)); &#125; return ans; &#125; private int dfs(int[][] matrix, int[][] memo, int row, int col) &#123; if (memo[row][col] != 0) &#123; return memo[row][col]; &#125; memo[row][col]++; int n = matrix.length; int m = matrix[0].length; for (int[] dir : dirs) &#123; int newRow = row + dir[0], newCol = col + dir[1]; if (newRow &lt; 0 || newRow == n || newCol &lt; 0 || newCol == m || matrix[newRow][newCol] &lt;= matrix[row][col]) continue; memo[row][col] = Math.max(memo[row][col],dfs(matrix,memo,newRow,newCol) + 1); &#125; return memo[row][col]; &#125;&#125; 514. 自由之路电子游戏“辐射4”中，任务 “通向自由” 要求玩家到达名为 “Freedom Trail Ring” 的金属表盘，并使用表盘拼写特定关键词才能开门。 给定一个字符串 ring ，表示刻在外环上的编码；给定另一个字符串 key ，表示需要拼写的关键词。您需要算出能够拼写关键词中所有字符的最少步数。 最初，ring 的第一个字符与 12:00 方向对齐。您需要顺时针或逆时针旋转 ring 以使 key 的一个字符在 12:00 方向对齐，然后按下中心按钮，以此逐个拼写完 key 中的所有字符。 旋转 ring 拼出 key 字符 key[i] 的阶段中： 您可以将 ring 顺时针或逆时针旋转 一个位置 ，计为1步。旋转的最终目的是将字符串 ring 的一个字符与 12:00 方向对齐，并且这个字符必须等于字符 key[i] 。如果字符 key[i] 已经对齐到12:00方向，您需要按下中心按钮进行拼写，这也将算作 1 步。按完之后，您可以开始拼写 key 的下一个字符（下一阶段）, 直至完成所有拼写。 示例 1： 输入: ring &#x3D; “godding”, key &#x3D; “gd”输出: 4解释: 对于 key 的第一个字符 ‘g’，已经在正确的位置, 我们只需要1步来拼写这个字符。 对于 key 的第二个字符 ‘d’，我们需要逆时针旋转 ring “godding” 2步使它变成 “ddinggo”。 当然, 我们还需要1步进行拼写。 因此最终的输出是 4。示例 2: 输入: ring &#x3D; “godding”, key &#x3D; “godding”输出: 13 提示： 1 &lt;&#x3D; ring.length, key.length &lt;&#x3D; 100ring 和 key 只包含小写英文字母保证 字符串 key 一定可以由字符串 ring 旋转拼出 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/freedom-trail著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; int[][] memo; Map&lt;Character,List&lt;Integer&gt;&gt; map; public int findRotateSteps(String ring, String key) &#123; //逆转顺转 每一步都最短 是不行的,比如DI 需要知道所有代价 map = new HashMap&lt;&gt;(); int n = ring.length(); for (int i = 0; i &lt; n; i++) &#123; char c = ring.charAt(i); if (map.containsKey(c)) &#123; map.get(c).add(i); &#125; else &#123; map.put(c,new ArrayList&lt;&gt;()); map.get(c).add(i); &#125; &#125; memo = new int[ring.length()][key.length()]; return dp(ring,key,0,0); &#125; //i 当前12点 j 需要输入的字符位置 返回 包括j之后的最优 public int dp(String ring, String key, int i, int j) &#123; if (j == key.length()) return 0; if (memo[i][j] != 0) return memo[i][j]; int best = Integer.MAX_VALUE; char c = key.charAt(j); for (int index : map.get(c)) &#123; int next = dp(ring,key,index,j+1); //处理循环,因为next 固定 只需要找到最小的dealt 分支是由for循环完成的 int delta = (int)Math.abs(index - i); delta = Math.min(delta,ring.length() - delta); best = Math.min(best,delta + 1 + next); &#125; memo[i][j] = best; return best; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"力扣","slug":"力扣","permalink":"https://gouguoqiang.github.io/tags/%E5%8A%9B%E6%89%A3/"}]},{"title":"dp-区间dp","slug":"算法/5dp/区间DP","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-26T01:25:30.471Z","comments":true,"path":"2022/09/02/算法/5dp/区间DP/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E5%8C%BA%E9%97%B4DP/","excerpt":"","text":"方法论区间DP模板 合并石子 设有 NN 堆石子排成一排，其编号为 1，2，3，…，N1，2，3，…，N。 每堆石子有一定的质量，可以用一个整数来描述，现在要将这 NN 堆石子合并成为一堆。 每次只能合并相邻的两堆，合并的代价为这两堆石子的质量之和，合并后与这两堆石子相邻的石子将和新堆相邻，合并时由于选择的顺序不同，合并的总代价也不相同。 例如有 44 堆石子分别为 1 3 5 2， 我们可以先合并 1、21、2 堆，代价为 44，得到 4 5 2， 又合并 1，21，2 堆，代价为 99，得到 9 2 ，再合并得到 1111，总代价为 4+9+11&#x3D;244+9+11&#x3D;24； 如果第二步是先合并 2，32，3 堆，则代价为 77，得到 4 7，最后一次合并代价为 1111，总代价为 4+7+11&#x3D;224+7+11&#x3D;22。 问题是：找出一种合理的方法，使总的代价最小，输出最小代价。 输入格式第一行一个数 NN 表示石子的堆数 NN。 第二行 NN 个数，表示每堆石子的质量(均不超过 10001000)。 输出格式输出一个整数，表示最小代价。 数据范围1≤N≤3001≤N≤300 输入样例：1241 3 5 2 输出样例：122 代码123456789101112131415161718192021222324252627282930import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int N = n + 10; int[] w = new int[N]; int[] s = new int[N]; int[][] f = new int[N][N]; for (int i = 1; i &lt;= n ; i++) &#123; w[i] = in.nextInt(); &#125; for (int i = 1; i &lt;= n; i++) s[i] = s[i-1] + w[i]; for (int len = 1; len &lt;= n; len++) &#123; for (int l == 1; i + len - 1 &lt;= n; l++) &#123; int r = i + len - 1; if (len == 1) f[l][r] = 0; else &#123; for (int k = l; k &lt; r; k++) &#123; f[l][r] = Math.max(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); &#125; &#125; &#125; &#125; System.out.println(f[1][n]) &#125;&#125; 环形石子合并 可以首尾n的四次方枚举缺口 优化 环形DP 123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.util.*;class Main&#123; static final int INF = Integer.MAX_VALUE / 2; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int N = 2 * n + 10; int[] w = new int[N]; int[] s = new int[N]; int[][] f = new int[N][N]; int[][] g = new int[N][N]; for (int i = 1; i &lt;= n ; i++) &#123; w[i] = in.nextInt(); w[i+n] = w[i]; &#125; for (int i = 1; i &lt;= 2 * n; i++) s[i] = s[i-1] + w[i]; for (int i = 0; i &lt; N; i++) &#123; Arrays.fill(f[i],-INF); Arrays.fill(g[i],INF); &#125; for (int len = 1; len &lt;= n; len++) &#123; for (int l == 1; i + len - 1 &lt;= 2 * n; l++) &#123; int r = i + len - 1; if (len == 1) f[l][r] = 0; else &#123; for (int k = l; k &lt; r; k++) &#123; f[l][r] = Math.max(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); g[l][r] = Math.min(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); &#125; &#125; &#125; &#125; int maxv = -INF, minv = INF; for (int i = 1; i &lt;= n; i++) &#123; maxv = Math.max(f[i][i+n-1]); minv = Math.min(g[i][i+n-1]); &#125; System.out.println(minv); System.out.println(maxv); &#125;&#125; leetcode最长回文子串1234567891011121314151617181920212223242526272829class Solution &#123; public String longestPalindrome(String s) &#123; int n = s.length(); int N = n + 10; boolean[][] f = new boolean[N][N]; int max = 0; int ll = 1; int rr = 1; for (int len = 1; len &lt;= n; len++) &#123; for (int l = 1; l + len - 1 &lt;= n; l++) &#123; int r = l + len -1; if (len == 1) f[l][r] = true; else if (len == 2) &#123; if (s.charAt(l-1) == s.charAt(r-1)) f[l][r] = true; &#125; else &#123; f[l][r] = f[l+1][r-1] &amp;&amp; s.charAt(l-1) == s.charAt(r-1); &#125; if (f[l][r] &amp;&amp; (r - l + 1 &gt; max)) &#123; max = r - l + 1; ll = l; rr = r; &#125; &#125; &#125; return s.substring(ll-1,rr); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"区间dp","slug":"区间dp","permalink":"https://gouguoqiang.github.io/tags/%E5%8C%BA%E9%97%B4dp/"}]},{"title":"dp-状态机","slug":"算法/5dp/状态机","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:07:33.614Z","comments":true,"path":"2022/09/02/算法/5dp/状态机/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E7%8A%B6%E6%80%81%E6%9C%BA/","excerpt":"","text":"123456789import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); &#125;&#125; 状态机模型抢家劫舍为例 阿福是一名经验丰富的大盗。趁着月黑风高，阿福打算今晚洗劫一条街上的店铺。 这条街上一共有 N 家店铺，每家店中都有一些现金。 阿福事先调查得知，只有当他同时洗劫了两家相邻的店铺时，街上的报警系统才会启动，然后警察就会蜂拥而至。 作为一向谨慎作案的大盗，阿福不愿意冒着被警察追捕的风险行窃。 他想知道，在不惊动警察的情况下，他今晚最多可以得到多少现金？ 输入格式输入的第一行是一个整数 T，表示一共有 T 组数据。 接下来的每组数据，第一行是一个整数 N ，表示一共有 N 家店铺。 第二行是 N 个被空格分开的正整数，表示每一家店铺中的现金数量。 每家店铺中的现金数量均不超过1000。 输出格式对于每组数据，输出一行。 该行包含一个整数，表示阿福在不惊动警察的情况下可以得到的现金数量。 数据范围$1 \\le T \\le 50$,$1 \\le N \\le 10^5$ 输入样例：12345231 8 2410 7 6 14 输出样例：12824 样例解释对于第一组样例，阿福选择第2家店铺行窃，获得的现金数量为8。 对于第二组样例，阿福选择第1和4家店铺行窃，获得的现金数量为10+14&#x3D;24。 代码12345678910111213141516171819import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); for (int i = 0; i &lt; n; i++) &#123; int m = nextInt(); int[][] dp = new int[m+10][2]; // 0 不偷i家 for (int j = 1; j &lt;= m; j++) &#123; int v = in.nextInt(); dp[j][0] = Math.max(dp[j-1][0],dp[j-1][1]); dp[j][1] = Math.max(dp[j-1][0] + v,dp[j-1][1]); &#125; System.out(Math.max(dp[m][0],dp[m][1])); &#125; &#125;&#125; 1234567891011class Solution &#123; public int rob(int[] nums) &#123; int n = nums.length; int[][] dp = new int[n+10][2]; for (int i = 1; i &lt;= n; i++) &#123; dp[i][0] = Math.max(dp[i-1][0],dp[i-1][1]); dp[i][1] = Math.max(dp[i-1][1],dp[i-1][0] + nums[i-1]); &#125; return Math.max(dp[n][0],dp[n][1]); &#125;&#125; 股票状态机 股票买卖IV &#x2F; leetcode 188123456789101112131415161718192021222324252627class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int maxProfit(int k, int[] prices) &#123; int n = prices.length; int[][][] dp = new int[n+10][k+10][2]; // 买入算半次交易 卖出 为一次完整交易 // 入口为手中无货 for (int i = 0; i &lt; n+10; i++) &#123; for (int j = 0; j &lt; k+10; j++) &#123; dp[i][j][0] = -INF; dp[i][j][1] = -INF; &#125; &#125; for (int i = 0; i &lt; n; i++) dp[i][0][0] = 0; for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= k; j++) &#123; dp[i][j][1] = Math.max(dp[i-1][j][1],dp[i-1][j-1][0] - prices[i-1]); dp[i][j][0] = Math.max(dp[i-1][j][0],dp[i-1][j][1] + prices[i-1]); // 买入则交易次数+1 &#125; &#125; int res = 0; for (int i = 1; i &lt;= k; i++) res = Math.max(dp[n][i][0],res); return res; &#125;&#125; 股票买卖含冷冻期 &#x2F; 309 12345678910111213141516171819class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int maxProfit(int[] prices) &#123; int n = prices.length; // 手中有货 手中无货的第 1天 手中无货的 &gt;=2天 // 1 0 2 int[][] dp = new int[n+10][3]; dp[0][1] = dp[0][0] = -INF; dp[0][2] = 0; for (int i = 1; i &lt;= n; i++) &#123; dp[i][1] = Math.max(dp[i-1][2] - prices[i-1],dp[i-1][1]); dp[i][2] = Math.max(dp[i-1][2],dp[i-1][0]); dp[i][0] = dp[i-1][1] + prices[i-1]; &#125; int res = 0; res = Math.max(dp[n][0],dp[n][2]); return res &lt; 0 ? 0 : res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"状态机","slug":"状态机","permalink":"https://gouguoqiang.github.io/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/"}]},{"title":"dp方法论","slug":"算法/5dp/方法论","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:34:23.344Z","comments":true,"path":"2022/09/02/算法/5dp/方法论/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"一. 二维数组dp1234567891011121314151617单线程f[i][j] 所有从起点走到i,j的集合双线程f[i1][i2][k] j1 = k - i1, j2 = k - i2k 表示 横纵坐标之和 同时也能表示步数, 限制相同步数, 即当且仅当 i1 = i2 时 才能走到同一格:轨迹才能重合其他时都不会重合 for k for i1 for i2 两条路径 右下 x 右下 t = g[i1][j1] i1 != i2 : t+ = g[i2][j2] # k - 1 两个都退1步 同进退 f[k][i1][i2] = max(k-1,i1,i2,| i1 -1,i2,| i2 -1,| i1 - 1,i2 -1) 二. 最长上升子序列一种 f[i]: 以i为尾的最长子序列长度 贪心二分优化: f[i] 长度为i的的上升子序列最大潜力(最小)末尾 Example1234567891011121314151617181920a = list(map(int, input().split()))n = len(a)f = [0] * ncnt = 0for i in range(n): # 非上升子序列的最小覆盖个数 以贪心为思想:保证个数的潜力 大于等于i的最小值(对于单调的而言是第一个大于等于他的) # 找到直接替换(加入一个已有的序列,并改变尾部) # 二分思想求最长上升子序列: 每个尾部尽可能小找到大于i的左边界 将它换了 cnt = max(cnt,l) l = 0 r = cnt - 1 while l &lt;= r: mid = (l + r) // 2 if f[mid] &gt;= a[i]: r = mid - 1 else: l = mid + 1 f[l] = a[i] if l &gt;= cnt: cnt += 1print(cnt) 三. 最长公共子序列四. 背包恰好装满: f &#x3D; [- inf] * (m+1) 01背包Alogorithm123456集合: 从前i件物品选 背包容量为j的所有装配方案的集合属性: 最大价值集合划分:f[i][j] 01背包: 选第i件物品 | 不选第i件物品 f[i-1][j-v[i]] + w[i] | f[i-1][j]状态计算: = max() Example123456789101112131415161718192021222324n, m = map(int, input().split())f = [[0] * (m + 1) for _ in range(n + 1)]for i in range(1, n + 1): v, w = map(int, input().split()) for j in range(m + 1): f[i][j] = f[i-1][j] if j &gt;= v: f[i][j] = max(f[i][j], f[i-1][j-v] + w)print(f[n][m])# 滚动数组优化 只需要一维# 优化: f[j] 背包容量为j时,从前i件物品选n, m = map(int, input().split())f = [0] * (m+1)for i in range(n): v, w = map(int, input().split()) # 正序会错 会导致f[j-v] 是已经算出来的用的是第i层的 而不是i-1层的 for j in range(m, v-1, -1): f[j] = max(f[j], f[j-v] + w)print(f[m]) ​ 完全背包Algorithm12345与01背包不同的是 包含a[i] 的之后 还是要从前i件选划分的区别: f[i][j] 选0件a[i] | 选1件. | 选 2件 | .. 选无穷件 f[i-1][j] | f[i][j-v] 可以从前往后遍历了在表示下这: f[i][j-v] 从数学角度 Example极大独立集 货币系统二 枚举ai : 完全背包方案数 12345678910111213141516171819T = int(input())for _ in range(T): n = int(input()) a = list(map(int, input().split())) a.sort() m = a[n-1] f = [0] * (m + 1) f[0] = 1 res = 0 for i in range(n): if f[a[i]] == 0: res += 1 for j in range(a[i], m + 1): f[j] += f[j - a[i]] print(res) 多重背包Algorithm12345678910前两步一致划分: 0件a[i] | 1 | 2 | .. | s[i] 件 a[i] # 优化 二进制s = 10230,1 2,3,4, ... 10231, 2, 4, 8, .. 512 能拼出来 每个组只能选一个s = 2001, 2, 4, 8, 16, 32, 64(最大的小于等于), 73 Example有 $N$ 种物品和一个容量是 $V$ 的背包。 第 $i$ 种物品最多有 $s_i$ 件，每件体积是 $v_i$，价值是 $w_i$。 求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 输入格式第一行两个整数，$N，V$，用空格隔开，分别表示物品种数和背包容积。 接下来有 $N$ 行，每行三个整数 $v_i, w_i, s_i$，用空格隔开，分别表示第 $i$ 种物品的体积、价值和数量。 输出格式输出一个整数，表示最大价值。 数据范围$0 \\lt N \\le 1000$$0 \\lt V \\le 2000$$0 \\lt v_i, w_i, s_i \\le 2000$ 提示：本题考查多重背包的二进制优化方法。 输入样例123454 51 2 32 4 13 4 34 5 2 输出样例：110 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748n, m = map(int, input().split())f = [[0] * (m + 1) for _ in range(n + 1)]for i in range(1, n + 1): v, w, s = map(int, input().split()) for j in range(1, m + 1): for k in range(0, s+1): if k * v &lt;= j: f[i][j] = max(f[i][j], f[i-1][j- k * v] + k * w)print(f[n][m])# n件物品 -&gt; n * logs 向上取整N = 25000n, m = map(int, input().split())f = [0] * (m + 1)v = [0] * Nw = [0] * Ncnt = 0for i in range(n): a, b, s = map(int, input().split()) k = 1 while(k &lt;= s): cnt += 1 v[cnt] = a * k w[cnt] = b * k s -= k k *= 2 if s &gt; 0: cnt += 1 v[cnt] = a * s w[cnt] = b * sn = cntfor i in range(1, n+1): for j in range(m, v[i] - 1, -1): f[j] = max(f[j], f[j-v[i]] + w[i]) print(f[m]) 分组背包Algorithm12345集合: 从前i组 容量j每组只能选一个集合划分: 不从第i组选 | 选i组第1个 | 选i组第k个 用的是上一层 可优化为倒着的 f[i-1][j] | f[i-1][j-z[i][k]] + w[i][k] Example有 $N$ 组物品和一个容量是 $V$ 的背包。 每组物品有若干个，同一组内的物品最多只能选一个。每件物品的体积是 $v_{ij}$，价值是 $w_{ij}$，其中 $i$ 是组号，$j$ 是组内编号。 求解将哪些物品装入背包，可使物品总体积不超过背包容量，且总价值最大。 输出最大价值。 输入格式第一行有两个整数 $N，V$，用空格隔开，分别表示物品组数和背包容量。 接下来有 $N$ 组数据： 每组数据第一行有一个整数 $S_i$，表示第 $i$ 个物品组的物品数量； 每组数据接下来有 $S_i$ 行，每行有两个整数 $v_{ij}, w_{ij}$，用空格隔开，分别表示第 $i$ 个物品组的第 $j$ 个物品的体积和价值； 输出格式输出一个整数，表示最大价值。 数据范围$0 \\lt N, V \\le 100$$0 \\lt S_i \\le 100$$0 \\lt v_{ij}, w_{ij} \\le 100$ 输入样例123456783 521 22 413 414 5 输出样例：18 123456789101112131415161718192021222324N = 110n, m = map(int, input().split())v = [[0] * N for _ in range(N)]w = [[0] * N for _ in range(N)]s = [0] * Nf = [0] * (m + 1)for i in range(n): s[i] = int(input()) for j in range(s[i]): v[i][j], w[i][j] = map(int, input().split()) for i in range(n): for j in range(m, -1, -1): for k in range(s[i]): if v[i][k] &gt; j: continue f[j] = max(f[j], f[j-v[i][k]] + w[i][k]) print(f[m]) 混合背包 01&#x2F;多重&#x2F;完全Example有 NN 种物品和一个容量是 VV 的背包。 物品一共有三类： 第一类物品只能用1次（01背包）； 第二类物品可以用无限次（完全背包）； 第三类物品最多只能用 sisi 次（多重背包）； 每种体积是 vivi，价值是 wiwi。 求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 输入格式第一行两个整数，N，VN，V，用空格隔开，分别表示物品种数和背包容积。 接下来有 NN 行，每行三个整数 vi,wi,sivi,wi,si，用空格隔开，分别表示第 ii 种物品的体积、价值和数量。 si&#x3D;−1si&#x3D;−1 表示第 ii 种物品只能用1次； si&#x3D;0si&#x3D;0 表示第 ii 种物品可以用无限次； si&gt;0si&gt;0 表示第 ii 种物品可以使用 sisi 次； 输出格式输出一个整数，表示最大价值。 数据范围0&lt;N,V≤10000&lt;N,V≤10000&lt;vi,wi≤10000&lt;vi,wi≤1000−1≤si≤1000−1≤si≤1000 输入样例123454 51 2 -12 4 13 4 04 5 2 输出样例：18 123456789101112131415161718192021T = int(input())while T: n = int(input()) a = list(map(int, input().split())) a.sort() m = a[n - 1] f = [0] * (m + 1) f[0] = 1 res = 0 for i in range(0, n): # 第i个数能否被前i-1个数表示出来 if f[a[i]] == 0: res += 1 for j in range(a[i], m + 1): f[j] += f[j - a[i]] print(res) T -= 1 求最优解具体方案:其实是最短路问题, 符合从哪个状态 转移过来 就输出路径 &#x3D;&gt; 求出一条最短路 Example12345678910111213141516171819202122n, m = map(int, input().split())f = [[0] * (m + 1) for _ in range(n + 2)]v = [0] * (n + 1)w = [0] * (n + 1)for i in range(1, n + 1): v[i], w[i] = map(int, input().split()) for i in range(n, 0, -1): for j in range(1, m + 1): f[i][j] = f[i+1][j] if j &gt;= v[i]: f[i][j] = max(f[i+1][j -v[i]] + w[i], f[i][j])j = m for i in range(1, n + 1): if j &gt;= v[i] and f[i][j] == f[i+1][j-v[i]] + w[i]: print(i, end=&quot; &quot;) j -= v[i] 求最优解方案数&#x3D;&gt; 最短路径的条数 注意求最优解的时候如果两条路都相等两个都要加 12345678910111213141516171819202122232425n, m = map(int, input().split())f = [0] * (m + 1)g = [0] * (m + 1)mod = int(1e9 + 7)maxv = 0g[0] = 1for i in range(n): v, w = map(int, input().split()) for j in range(m, v - 1, -1): t = f[j] f[j] = max(f[j], f[j - v] + w) cnt = 0 if f[j] == t: cnt += g[j] if f[j] == f[j - v] + w: cnt += g[j - v] g[j] = cnt % mod maxv = max(f[j],maxv) res = 0for j in range(m + 1): if f[j] == maxv: res = (res + g[j]) % mod# 方案数的理解: g[j] 恰好装满 而不是不超过 print(res) 五. 状态机模型六. 区间DPAlogorithm12345f[i][j]集合 第i堆石子到j堆石子合并成一堆的所有合并方式j &gt; i划分: 最优一步合并的分界点 k = [i,j-1] (j-1 保证k+1不会出界) for k : f[i][j] = max(f[i][j], f[i][k] + f[k+1][j] + s[j] - s[i-1]) Example1234567891011121314151617181920n = int(input())s = [0] + list(map(int, input().split()))f = [[0] * (n + 1) for _ in range(n + 1)]for i in range(1, n + 1): s[i] += s[i-1] for w in range(2, n + 1): for l in range(1, n - w + 2): # l + w - 1 &lt;= n r = l + w - 1 f[l][r] = int(1e8) for k in range(l,r): f[l][r] = min(f[l][r], f[l][k] + f[k+1][r] + s[r] - s[l-1])print(f[1][n]) 七. 连通性状态压缩DP状态压缩 蒙德里安的梦想12345转换为放行f[i][j] : 第i列放[二进制数枚举行数] 划分: [i-1]二进制枚举f[0][0] = 1 #第0列虚拟列 必须一行不选 所以[0][0] f[1][二进制枚举]计算时 枚举上一步的每种选择,在枚举本步的每种选择 合法: 上一步的每种选择不能有单数不选的情况(预处理), 上一步与本步不能都选(j &amp; k == 0 八. 集合类状态压缩DP韩明顿给定一张 nn 个点的带权无向图，点从 0∼n−10∼n−1 标号，求起点 00 到终点 n−1n−1 的最短 Hamilton 路径。 Hamilton 路径的定义是从 00 到 n−1n−1 不重不漏地经过每个点恰好一次。 输入格式第一行输入整数 nn。 接下来 nn 行每行 nn 个整数，其中第 ii 行第 jj 个整数表示点 ii 到 jj 的距离（记为 a[i,j]a[i,j]）。 对于任意的 x,y,zx,y,z，数据保证 a[x,x]&#x3D;0，a[x,y]&#x3D;a[y,x]a[x,x]&#x3D;0，a[x,y]&#x3D;a[y,x] 并且 a[x,y]+a[y,z]≥a[x,z]a[x,y]+a[y,z]≥a[x,z]。 输出格式输出一个整数，表示最短 Hamilton 路径的长度。 数据范围1≤n≤201≤n≤200≤a[i,j]≤1070≤a[i,j]≤107 输入样例：12345650 2 4 5 12 0 6 5 34 6 0 8 35 5 8 0 51 3 3 5 0 输出样例：118 Solution12345678910111213141516n = int(input())g = [list(map(int, input().split())) for _ in range(n)]f = [[0x3f3f3f3f] * n for _ in range(1 &lt;&lt; n)]f[1][0] = 0for i in range(1 &lt;&lt; n): for j in range(n): if i &gt;&gt; j &amp; 1: for k in range(n): if i - (1 &lt;&lt; j) &gt;&gt; k &amp; 1: f[i][j] = min(f[i][j],f[i - (1 &lt;&lt; j)][k] + g[k][j])print(f[(1 &lt;&lt; n) - 1][n - 1]) 九. 树形DP","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"方法论","slug":"方法论","permalink":"https://gouguoqiang.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"dp-背包","slug":"算法/5dp/背包","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-15T13:18:22.468Z","comments":true,"path":"2022/09/02/算法/5dp/背包/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E8%83%8C%E5%8C%85/","excerpt":"","text":"力扣」上的 0-1 背包问题： 「力扣」第 416 题：分割等和子集（中等）；「力扣」第 474 题：一和零（中等）；「力扣」第 494 题：目标和（中等）；「力扣」第 879 题：盈利计划（困难）；「力扣」上的 完全背包问题： 「力扣」第 322 题：零钱兑换（中等）；「力扣」第 518 题：零钱兑换 II（中等）；「力扣」第 1449 题：数位成本和为目标值的最大数字（困难）。这里要注意鉴别：「力扣」第 377 题，不是「完全背包」问题。 模板初始化的细节问题有的题目要求“恰好装满背包”时的最优解，有的题目则并没有要求必须把背包装满。 如果是第一种问法，要求恰好装满背包，那么在初始化时除了f[0]为0其它f[1..V]均设为-∞，这样就可以保证最终得到的f[N]是一种恰好装满背包的最优解。 如果并没有要求必须把背包装满，而是只希望价格尽量大，初始化时应该将f[0..V]全部设为0。 f[m] 定义为背包体积为m 的最大价值 如果不要求恰好装满 全部初始化为0 f[m] 即为最大值 因为相当于一个偏移量 证明: ​ f[0] &#x3D; 0 &#x3D;&gt; f[w0] &#x3D; v[0]… ​ 设 k 为最大值的体积 f[m-k] &#x3D; 0 &#x3D;&gt; f[m-k+w0] &#x3D; v[0];这样转移过来 如果要求恰好装满的max ​ 除了 dp[0] 为 0 其他为-INF 我这里写清楚了 初始化的问题,具体求解具体应用即可 优化空间后 除了 完全背包可从前往后 ,其他都是从后往前 123for 物品 for 体积 for 决策 01背包12//不选第i个物品和 选一个最后一个物品dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-v[i]] + w[i]); 板子题1234567输入 3 7071 10069 11 2 // 给定背包大小 m,给定 物品个数 n,w[i] v[i] 求背包能装物品的最大值 123456789101112131415import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int[] dp = new int[m+10]; for (int i =0; i &lt; n; i++) &#123; int w = in.nextInt(); int v = in.nextInt(); for (int j = m; j &gt;= w; j--) dp[j] = Math.max(dp[j],dp[j-w]+v); &#125; System.out.println(dp[m]); &#125;&#125; 装箱问题有一个箱子容量为 V，同时有 n 个物品，每个物品有一个体积（正整数）。 要求 n 个物品中，任取若干个装入箱内，使箱子的剩余空间为最小。 输入格式第一行是一个整数 V，表示箱子容量。 第二行是一个整数 n，表示物品数。 接下来 n 行，每行一个正整数（不超过10000），分别表示这 n 个物品的各自体积。 输出格式一个整数，表示箱子剩余空间。 数据范围0&lt;V≤200000&lt;V≤20000,0&lt;n≤300&lt;n≤30 输入样例：123456782468312797 输出样例：10 123456789101112输入 24 // 箱子容量 6 // 物品个数 8 // 物品体积 3 12 7 9 7 // 求任取若干 使箱子剩余空间最小 //思路 体积也看成价值 二维费用的背包问题有 N 件物品和一个容量是 V 的背包，背包能承受的最大重量是 M。 每件物品只能用一次。体积是 vi，重量是 mi，价值是 wi。 求解将哪些物品装入背包，可使物品总体积不超过背包容量，总重量不超过背包可承受的最大重量，且价值总和最大。输出最大价值。 输入格式第一行三个整数，N,V,M，用空格隔开，分别表示物品件数、背包容积和背包可承受的最大重量。 接下来有 N 行，每行三个整数 vi,mi,wi,，用空格隔开，分别表示第 i 件物品的体积、重量和价值。 输出格式输出一个整数，表示最大价值。 数据范围0&lt;N≤100000&lt;V,M≤10000&lt;vi,mi≤10000&lt;wi≤10000 输入样例123454 5 61 2 32 4 43 4 54 5 6 输出样例：18 代码:1f[i][j][k] = Math.max(dp[i-1][j][k],dp[i][j-w1][k-w2] + v) 1234567891011121314151617181920import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int W1 = in.nextInt(); int W2 = in.nextInt(); for (int i = 0; i &lt; n; i++) &#123; int w1 = in.nextInt(); int w2 = in.nextInt(); int v = nextInt(); for (int j = W1; j &gt;= 0; j--) &#123; for (int k = W2; k&gt;= 0; k--) &#123; dp[j][k] = Math.max(dp[i][k],dp[j-w1][k-w2]+v); &#125; &#125; &#125; System.out.println(dp[W1][W2]); &#125;&#125; 数字组合 给定 N 个正整数 A1,A2,…,AN ,从中选出若干个数，使它们的和为 M，求有多少种选择方案。 输入格式第一行包含两个整数 N 和 M。 第二行包含 N 个整数，表示 A1,A2,…,AN。 输出格式包含一个整数，表示可选方案数。 数据范围1≤N≤1001≤N≤100,1≤M≤100001≤M≤10000,1≤Ai≤10001≤Ai≤1000,答案保证在 int 范围内。 输入样例：124 41 1 2 2 输出样例：13 代码1import 给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 LeetCode 416 分割等和子集示例 1： 输入：nums &#x3D; [1,5,11,5]输出：true解释：数组可以分割成 [1, 5, 5] 和 [11] 。示例 2： 输入：nums &#x3D; [1,2,3,5]输出：false解释：数组不能分割成两个元素和相等的子集。 提示： 1 &lt;&#x3D; nums.length &lt;&#x3D; 2001 &lt;&#x3D; nums[i] &lt;&#x3D; 100 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/partition-equal-subset-sum著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 solution123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// 传统用INF的写法class Solution &#123; public boolean canPartition(int[] nums) &#123; //求和 求一半的背包能否装满 int INF = 0x3f3f3f3f; int n = nums.length; int sum = 0; for (int num : nums) &#123; sum += num; &#125; if (sum % 2 == 1 || sum == 0) return false; int weight = sum / 2; int[] dp = new int[weight+1]; for (int i = 1; i &lt;= weight; i++) dp[i] = -INF; for (int i = 1; i &lt;= n; i++) &#123; for (int j = weight; j &gt;= nums[i-1]; j--) &#123; dp[j] = Math.max(dp[j-nums[i-1]] + nums[i-1],dp[j]); &#125; &#125; return dp[weight] &lt; -INF / 2? false : true; &#125;&#125;//这道题有特殊class Solution &#123; public boolean canPartition(int[] nums) &#123; //求和 求一半的背包能否装满 int n = nums.length; int sum = 0; for (int num : nums) &#123; sum += num; &#125; if (sum % 2 == 1 || sum == 0) return false; int weight = sum / 2; int[] dp = new int[weight+1]; for (int i = 1; i &lt;= n; i++) &#123; for (int j = weight; j &gt;= nums[i-1]; j--) &#123; dp[j] = Math.max(dp[j-nums[i-1]] + nums[i-1],dp[j]); &#125; &#125; // 这个结果保证是恰好装满 即从dp[0]转移而来 return dp[weight] == weight; &#125;&#125; 完全背包123456//不选第i个物品和,选一个,选两个,选n个直到不能选dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w]+v,dp[i-1][j-2w]+2v...dp[i-1][j-nw] + nv);dp[i][j-w] = Math.max( dp[i-1][j-w] ,dp[i-1][j-2w]+v ...dp[i-1][j-nw] + (n-1)v);//最终dp[i][j] = Math.max(dp[i-1][j],dp[i][j-w[i]] + v[i]); 零钱兑换I给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins &#x3D; [1, 2, 5], amount &#x3D; 11输出：3解释：11 &#x3D; 5 + 5 + 1示例 2： 输入：coins &#x3D; [2], amount &#x3D; 3输出：-1示例 3： 输入：coins &#x3D; [1], amount &#x3D; 0输出：0 提示： 1 &lt;&#x3D; coins.length &lt;&#x3D; 121 &lt;&#x3D; coins[i] &lt;&#x3D; 231 - 10 &lt;&#x3D; amount &lt;&#x3D; 104 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/coin-change著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 123456789101112131415161718192021class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int coinChange(int[] coins, int amount) &#123; // dp[i] // 刚好装满的最少个数(价值为1) int m = amount; int n = coins.length; int[]dp = new int[amount+10]; Arrays.fill(dp,INF); dp[0] = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = coins[i]; j &lt;= m; j++) &#123; dp[j] = Math.min(dp[j],dp[j-coins[i]]+1); &#125; &#125; return dp[amount] == INF ? -1 : dp[amount]; &#125;&#125; 零钱兑换II买书&#x2F;货币系统&#x2F;零钱兑换II货币系统II多重背包 r &#x3D; j % v; 物品有限s[i]个 12345678910//不选第i个物品,选一个第i个物品,选两个,选到不能选(体积之内 || 个数之内)dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w]+v,dp[i-1][j-2w]+2v...dp[i-1][j-nw] + nv);dp[i][j-w] = Math.max( dp[i-1][j-w] ,dp[i-1][j-2w]+v ...dp[i-1][j-nw] + (n-1)v,dp[i][j-(n+1)w] + v); //在多重背包里会多一项不能直接替换 (在j-w的体积基础上选s[i](n个),所以要-(n+1)w)//最简单的 for(int j = 1; j &lt;= V; j++)&#123; for(int k = 0; k &lt;= s &amp;&amp; j &gt;= k * v; k++)&#123; dp[i][j] = Math.max(dp[i][j], dp[i - 1][j - k * v] + k * w); &#125; &#125; 多重背包I有 N 种物品和一个容量是 V 的背包。 第 i 种物品最多有 si 件，每件体积是 vi，价值是 wi。 求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 输入格式第一行两个整数，N，V用空格隔开，分别表示物品种数和背包容积。 接下来有 NN行，每行三个整数 vi,wi,si，用空格隔开，分别表示第 i种物品的体积、价值和数量。 输出格式输出一个整数，表示最大价值。 数据范围0&lt;N,V≤1000&lt;N,V≤1000&lt;vi,wi,si≤1000&lt;vi,wi,si≤100 输入样例123454 5 // n m1 2 3 //体积、价值和数量2 4 13 4 34 5 2 输出样例：110 代码:12345678910111213import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner.nextInt(); int n = in.nextInt(); int m = in.nextInt(); int[] w = new int[n+10]; int[] v = new int[n+10]; int[] s = new int[n+10]; for (int i = 0; i &lt; n; i++) &#125;&#125; 混合背包板子题最优选法 背包问题求方案数 有 N件物品和一个容量是 V的背包。每件物品只能使用一次。 第 i 件物品的体积是 v_i，价值是 w_i。 求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。 输出 最优选法的方案数。注意答案可能很大，请输出答案模 10^9 + 7 的结果。 输入格式第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。 接下来有 N 行，每行两个整数 v_i, w_i，用空格隔开，分别表示第 i 件物品的体积和价值。 输出格式输出一个整数，表示 方案数 模 10^9 + 7 的结果。 数据范围$0 \\lt N, V \\le 1000$$0\\lt v_i, w_i \\le 1000$ 输入样例123454 51 22 43 44 6 输出样例：12 代码1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.*;class Main&#123; static final int mod = (int)1e9 + 7; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); //恰好装满的最大价值 int[] f = new int[m+10]; Arrays.fill(f,-1010); // 最优解的方案数 int[] g = new int[m+10]; f[0] = 0; g[0] = 1; for (int i = 0; i &lt; n; i++) &#123; int w = in.nextInt(); int v = in.nextInt(); for (int j = m; j &gt;= w; j--) &#123; int maxv = Math.max(f[j],f[j-w] + v); int cnt = 0; if (maxv == f[j]) cnt += g[j]; if (maxv == f[j-w] + v) cnt += g[j-w]; g[j] = cnt % mod; f[j] = maxv; &#125; &#125; int res = 0; for (int i = 1; i &lt;= m; i++) &#123; res = Math.max(res,f[i]); &#125; int cnt = 0; for (int i = 1; i &lt;= m; i++) &#123; if (res == f[i]) cnt = (cnt + g[i]) % mod; &#125; System.out.println(cnt == 0 ? 1 : cnt); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"背包","slug":"背包","permalink":"https://gouguoqiang.github.io/tags/%E8%83%8C%E5%8C%85/"}]},{"title":"数据结构-1单链表","slug":"算法/3数据结构/1单链表","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:06:07.585Z","comments":true,"path":"2022/09/02/算法/3数据结构/1单链表/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/1%E5%8D%95%E9%93%BE%E8%A1%A8/","excerpt":"","text":"AC saber.单链表实现一个单链表，链表初始为空，支持三种操作： 向链表头插入一个数； 删除第 k 个插入的数后面的数； 在第 k 个插入的数后插入一个数。 现在要对该链表进行 M 次操作，进行完所有操作后，从头到尾输出整个链表。 注意:题目中第 k 个插入的数并不是指当前链表的第 k 个数。例如操作过程中一共插入了 n 个数，则按照插入的时间顺序，这 n 个数依次为：第 1 个插入的数，第 2 个插入的数，…第 n 个插入的数。 输入格式第一行包含整数 M，表示操作次数。 接下来 M 行，每行包含一个操作命令，操作命令可能为以下几种： H x，表示向链表头插入一个数 x。 D k，表示删除第 k 个插入的数后面的数（当 k 为 0 时，表示删除头结点）。 I k x，表示在第 k个插入的数后面插入一个数 x（此操作中 k 均大于 0）。 输出格式共一行，将整个链表从头到尾输出。 数据范围1≤M≤100000所有操作保证合法。 输入样例：123456789101110H 9I 1 1D 1D 0H 6I 3 6I 4 5I 4 5I 3 4D 6 输出样例：16 4 6 5 solution123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.io.*;public class Main&#123; // 对输入要求很高 static final int N = 100010; static int[] e = new int[N], ne = new int[N]; // head : 头结点坐标, idx当前该分配的空间 static int head, idx; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); int m = Integer.parseInt(in.readLine()); init(); while (m-- &gt; 0) &#123; String[] s = in.readLine().split(&quot; &quot;); if (s[0].equals(&quot;H&quot;)) &#123; int x = Integer.parseInt(s[1]); addFirst(x); &#125; else if (s[0].equals(&quot;D&quot;)) &#123; int k = Integer.parseInt(s[1]); removeKth(k); &#125; else if (s[0].equals(&quot;I&quot;)) &#123; int k = Integer.parseInt(s[1]); int x = Integer.parseInt(s[2]); addKth(k,x); &#125; &#125; for (int i = head; i != 0; i = ne[i]) &#123; System.out.print(e[i] + &quot; &quot;); &#125; &#125; public static void init() &#123; head = 0; // 标记头结点的下标罢了 idx = 1; &#125; // kth 第k个输入后面进行操作 public static void addFirst(int x) &#123; // 只有这一块有用,其他的对输入输出有高高要求 e[idx] = x; ne[idx] = head; head = idx ++; &#125; public static void addKth(int k, int x) &#123; e[idx] = x; ne[idx] = ne[k]; ne[k] = idx ++; &#125; public static void removeKth(int k) &#123; if (k != 0) &#123; ne[k] = ne[ne[k]]; &#125; else &#123; // 删除头结点 head = ne[head]; &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"单链表","slug":"单链表","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-2双链表","slug":"算法/3数据结构/2双链表","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:06:05.162Z","comments":true,"path":"2022/09/02/算法/3数据结构/2双链表/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/2%E5%8F%8C%E9%93%BE%E8%A1%A8/","excerpt":"","text":"AC saber. 双链表题目描述实现一个双链表，双链表初始为空，支持5种操作： (1) 在最左侧插入一个数； (2) 在最右侧插入一个数； (3) 将第k个插入的数删除； (4) 在第k个插入的数左侧插入一个数； (5) 在第k个插入的数右侧插入一个数 现在要对该链表进行M次操作，进行完所有操作后，从左到右输出整个链表。 注意:题目中第k个插入的数并不是指当前链表的第k个数。例如操作过程中一共插入了n个数，则按照插入的时间顺序，这n个数依次为：第1个插入的数，第2个插入的数，…第n个插入的数。 输入格式 第一行包含整数M，表示操作次数。 接下来M行，每行包含一个操作命令，操作命令可能为以下几种： (1) “L x”，表示在链表的最左端插入数x。 (2) “R x”，表示在链表的最右端插入数x。 (3) “D k”，表示将第k个插入的数删除。 (4) “IL k x”，表示在第k个插入的数左侧插入一个数。 (5) “IR k x”，表示在第k个插入的数右侧插入一个数。 输出格式 共一行，将整个链表从左到右输出。 1234567891011121314151617181920212223数据范围1≤M≤100000所有操作保证合法。输入样例：10R 7D 1L 3IL 2 10D 3IL 2 7L 8R 9IL 4 7IR 2 2输出样例：8 7 7 3 2 9 Solution12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import java.io.*;public class Main&#123; // 对输入要求很高 static final int N = 100010; static int[] e = new int[N], ne = new int[N], pe = new int[N]; static int head, tail, idx; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); int m = Integer.parseInt(in.readLine()); init(); while (m-- &gt; 0) &#123; String[] s = in.readLine().split(&quot; &quot;); if (s[0].equals(&quot;L&quot;)) &#123; int x = Integer.parseInt(s[1]); addFirst(x); &#125; else if (s[0].equals(&quot;R&quot;)) &#123; int x = Integer.parseInt(s[1]); addLast(x); &#125; else if (s[0].equals(&quot;D&quot;)) &#123; int k = Integer.parseInt(s[1]); remove(k); &#125; else if (s[0].equals(&quot;IL&quot;)) &#123; int k = Integer.parseInt(s[1]); int x = Integer.parseInt(s[2]); add(pe[k],x); &#125; else if (s[0].equals(&quot;IR&quot;)) &#123; int k = Integer.parseInt(s[1]); int x = Integer.parseInt(s[2]); add(k,x); &#125; &#125; for (int i = ne[head]; i != tail; i = ne[i]) &#123; System.out.print(e[i] + &quot; &quot;); &#125; &#125; public static void init() &#123; head = 0; tail = 10005; // 用数组的这两个格子存储头尾 // idx指向用于分配的空间 idx = 1; // 方便遍历 ne[head] = tail; pe[tail] = head; &#125; public static void add(int k, int x) &#123; // 插在第k 右侧 //idx.pre = k, i.next = k.next, k.next = i, i.next.pre = i ++; e[idx] = x; pe[idx] = k; ne[idx] = ne[k]; ne[k] = idx; pe[ne[idx]] = idx ++; &#125; public static void addLast(int x) &#123; add(pe[tail],x); &#125; public static void addFirst(int x) &#123; add(0,x); &#125; public static void remove(int k) &#123; // 删除 第k: k.pre.next = k.next, k.next.pre = k.pre; ne[pe[k]] = ne[k]; pe[ne[k]] = pe[k]; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"双链表","slug":"双链表","permalink":"https://gouguoqiang.github.io/tags/%E5%8F%8C%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-4队列","slug":"算法/3数据结构/4队列","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:05:56.745Z","comments":true,"path":"2022/09/02/算法/3数据结构/4队列/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/4%E9%98%9F%E5%88%97/","excerpt":"","text":"AcWing 829. 模拟队列难度：简单 题目描述实现一个队列，队列初始为空，支持四种操作： (1) “push x” – 向队尾插入一个数x； (2) “pop” – 从队头弹出一个数； (3) “empty” – 判断队列是否为空； (4) “query” – 查询队头元素。 现在要对队列进行M个操作，其中的每个操作3和操作4都要输出相应的结果。 输入格式 第一行包含整数M，表示操作次数。 接下来M行，每行包含一个操作命令，操作命令为”push x”，”pop”，”empty”，”query”中的一种。 输出格式 对于每个”empty”和”query”操作都要输出一个查询结果，每个结果占一行。 其中，”empty”操作的查询结果为“YES”或“NO”，”query”操作的查询结果为一个整数，表示队头元素的值。 1234567891011121314151617181920212223242526数据范围1≤M≤100000,1≤x≤109,所有操作保证合法。输入样例：10push 6emptyquerypopemptypush 3push 4popquerypush 6输出样例：NO6YES4 Solution1234567891011121314151617181920212223242526272829303132333435import java.util.*;import java.io.*;class Main&#123; static final int N = 100010; static int[] queue = new int[N]; static int hh = 0, tt = 0; public static void main(String[] args) throws IOException&#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); String[] s = br.readLine().split(&quot; &quot;); int m = Integer.parseInt(s[0]); while(m &gt; 0)&#123; m--; s = br.readLine().split(&quot; &quot;); if(s[0].equals(&quot;push&quot;))&#123; int x = Integer.parseInt(s[1]); queue[tt] = x; tt++; &#125; else if(s[0].equals(&quot;pop&quot;))&#123; hh++; &#125; else if(s[0].equals(&quot;empty&quot;))&#123; if(hh &gt;= tt) bw.write(&quot;YES\\n&quot;); else bw.write(&quot;NO\\n&quot;); &#125; else if(s[0].equals(&quot;query&quot;))&#123; bw.write(queue[hh] + &quot;\\n&quot;); &#125; &#125; bw.close(); br.close(); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"队列","slug":"队列","permalink":"https://gouguoqiang.github.io/tags/%E9%98%9F%E5%88%97/"}]},{"title":"数据结构-3栈","slug":"算法/3数据结构/3栈","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:06:00.603Z","comments":true,"path":"2022/09/02/算法/3数据结构/3栈/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/3%E6%A0%88/","excerpt":"","text":"AC saber. 模拟栈难度：简单 题目描述实现一个栈，栈初始为空，支持四种操作： (1) “push x” – 向栈顶插入一个数x； (2) “pop” – 从栈顶弹出一个数； (3) “empty” – 判断栈是否为空； (4) “query” – 查询栈顶元素。 现在要对栈进行M个操作，其中的每个操作3和操作4都要输出相应的结果。 输入格式 第一行包含整数M，表示操作次数。 接下来M行，每行包含一个操作命令，操作命令为”push x”，”pop”，”empty”，”query”中的一种。 输出格式 对于每个”empty”和”query”操作都要输出一个查询结果，每个结果占一行。 其中，”empty”操作的查询结果为“YES”或“NO”，”query”操作的查询结果为一个整数，表示栈顶元素的值。 12345678910111213141516171819202122232425262728数据范围1≤M≤100000,1≤x≤109所有操作保证合法。输入样例：10push 5querypush 6popquerypopemptypush 4queryempty输出样例：55YES4NO Solution1234567891011121314151617181920212223242526272829303132333435import java.util.*;import java.io.*;class Main&#123; static final int N = 100010; static int[] q = new int[N]; // 表示分配空间的位置 static int hh = 0; public static void main(String[] args) throws IOException&#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); String[] s = br.readLine().split(&quot; &quot;); int m = Integer.parseInt(s[0]); while(m-- &gt; 0)&#123; s = br.readLine().split(&quot; &quot;); if(s[0].equals(&quot;push&quot;))&#123; int x = Integer.parseInt(s[1]); q[hh++] = x; &#125; else if(s[0].equals(&quot;pop&quot;))&#123; hh--; &#125; else if(s[0].equals(&quot;empty&quot;))&#123; if(hh == 0) bw.write(&quot;YES&quot; + &quot;\\n&quot;); else bw.write(&quot;NO&quot; + &quot;\\n&quot;); &#125; else if(s[0].equals(&quot;query&quot;))&#123; bw.write(q[hh - 1] + &quot;\\n&quot;); &#125; &#125; bw.close(); br.close(); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"单链表","slug":"单链表","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"}]},{"title":"数据结构-5单调栈","slug":"算法/3数据结构/5单调栈","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:05:54.237Z","comments":true,"path":"2022/09/02/算法/3数据结构/5单调栈/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/5%E5%8D%95%E8%B0%83%E6%A0%88/","excerpt":"","text":"AcWing 830. 单调栈难度：简单 题目描述给定一个长度为N的整数数列，输出每个数左边第一个比它小的数，如果不存在则输出-1。 输入格式 第一行包含整数N，表示数列长度。 第二行包含N个整数，表示整数数列。 输出格式 共一行，包含N个整数，其中第i个数表示第i个数的左边第一个比它小的数，如果不存在则输出-1。 1234567891011121314数据范围1≤N≤1051≤数列中元素≤109输入样例：53 4 2 7 5输出样例：-1 3 -1 2 2 Solution12345678910111213141516171819202122import java.io.*;public class Main&#123; static final int N = 100010; static int[] stk = new int[N], a = new int[N]; static int hh = 0; public static void main(String[] args) throws IOException &#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); int m = Integer.parseInt(br.readLine()); String[] s = br.readLine().split(&quot; &quot;); for (int i = 0; i &lt; m; i++) &#123; a[i] = Integer.parseInt(s[i]); &#125; for (int i = 0; i &lt; m; i++) &#123; while (hh != 0 &amp;&amp; a[stk[hh-1]] &gt; a[i] ) &#123; hh--; &#125; if (hh == 0) System.out.print(-1 + &quot; &quot;); else System.out.print(a[stk[hh-1]] + &quot; &quot;); stk[hh++] = i; &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"单调栈","slug":"单调栈","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"}]},{"title":"数据结构-AcWing","slug":"算法/3数据结构/AcWing","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-03T14:17:02.093Z","comments":true,"path":"2022/09/02/算法/3数据结构/AcWing/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/AcWing/","excerpt":"","text":"143. 最大异或树 次高位保存不相同的 以此类推 用一个trie 在给定的 $N$ 个整数 $A_1，A_2……A_N$ 中选出两个进行 $xor$（异或）运算，得到的结果最大是多少？ 输入格式第一行输入一个整数 $N$。 第二行输入 $N$ 个整数 $A_1$～$A_N$。 输出格式输出一个整数表示答案。 数据范围$1 \\le N \\le 10^5$,$0 \\le A_i &lt; 2^{31}$ 输入样例：1231 2 3 输出样例：13","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://gouguoqiang.github.io/tags/leetCode/"}]},{"title":"数据结构-6kmp","slug":"算法/3数据结构/6kmp","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-07T14:08:50.883Z","comments":true,"path":"2022/09/02/算法/3数据结构/6kmp/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/6kmp/","excerpt":"","text":"AcWing 831. KMP字符串难度：简单 题目描述给定一个模式串S，以及一个模板串P，所有字符串中只包含大小写英文字母以及阿拉伯数字。 模板串P在模式串S中多次作为子串出现。 求出模板串P在模式串S中所有出现的位置的起始下标。 输入格式 第一行输入整数N，表示字符串P的长度。 第二行输入字符串P。 第三行输入整数M，表示字符串S的长度。 第四行输入字符串S。 输出格式 共一行，输出所有出现位置的起始下标（下标从0开始计数），整数之间用空格隔开。 数据范围 $1≤N≤10^5$ $1≤M≤10^6$ 输入样例： 12343aba5ababa 输出样例： 10 2 Solution 暴力算法怎么做 12345678910S[N], p[M];for(int i = 1; i &lt;= n; i++) &#123; boolean flag = true; for(int j = 1; j &lt;= m; j++) &#123; if(s[i + j - 1] != p[j]) &#123; flag = false; break; &#125; &#125; &#125; KMP 优化 12345678910111213141516171819202122232425262728293031323334import java.util.*;import java.io.*;class Main&#123; public static void main(String[] args) throws IOException&#123; int N = 100010; int M = 1000010; int[] ne = new int[N]; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(System.out)); Integer n = Integer.parseInt(br.readLine()); String sp = &quot; &quot; + br.readLine(); Integer m = Integer.parseInt(br.readLine()); String ss = &quot; &quot; + br.readLine(); char[] p = sp.toCharArray(); char[] s = ss.toCharArray(); for(int i = 2, j = 0; i &lt;= n; i++) &#123; while(j != 0 &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if(p[i] == p[j + 1]) j++; ne[i] = j; &#125; for(int i = 1, j = 0; i &lt;= m; i++) &#123; while(j != 0 &amp;&amp; s[i] != p[j + 1]) j = ne[j]; if(s[i] == p[j + 1]) j++; if(j == n) &#123; bw.write(i - n + &quot; &quot;); j = ne[j]; &#125; &#125; bw.close(); br.close(); &#125;&#125; yxc KMP 匹配过程 求 next 数组 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;using namespace std;const int N = 100010, M = 1000010;int n, m;int ne[N];char s[M], p[N];int main()&#123; cin &gt;&gt; n &gt;&gt; p + 1 &gt;&gt; m &gt;&gt; s + 1; // 求 next 数组过程 for (int i = 2, j = 0; i &lt;= n; i ++ ) &#123; while (j &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if (p[i] == p[j + 1]) j ++ ; ne[i] = j; &#125; // kmp 匹配过程 for (int i = 1, j = 0; i &lt;= m; i ++ ) &#123; while (j &amp;&amp; s[i] != p[j + 1]) j = ne[j]; if (s[i] == p[j + 1]) j ++ ; if (j == n) &#123; printf(&quot;%d &quot;, i - n); j = ne[j]; &#125; &#125; return 0;&#125;// 作者：yxc// 链接：https://www.acwing.com/activity/content/code/content/43108/// 来源：AcWing// 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"kmp","slug":"kmp","permalink":"https://gouguoqiang.github.io/tags/kmp/"}]},{"title":"数据结构-字典树","slug":"算法/3数据结构/7trie","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-24T07:57:24.387Z","comments":true,"path":"2022/09/02/算法/3数据结构/7trie/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/7trie/","excerpt":"","text":"Trie字符串统计维护一个字符串集合，支持两种操作： I x 向集合中插入一个字符串 xx； Q x 询问一个字符串在集合中出现了多少次。 共有 N 个操作，输入的字符串总长度不超过 105，字符串仅包含小写英文字母。 输入格式第一行包含整数 N，表示操作数。 接下来 N 行，每行包含一个操作指令，指令为 I x 或 Q x 中的一种。 输出格式对于每个询问指令 Q x，都要输出一个整数作为结果，表示 x 在集合中出现的次数。 每个结果占一行。 数据范围1≤N≤2∗1041≤N≤2∗104 输入样例：1234565I abcQ abcQ abI abQ ab 输出样例：123101 思路前缀树,字典要内置多叉树(用) 二维数组 N个结点 每个结点 26个分支, 对结点标记是否为单词尾部 实现insert字符串 : 代码一: java类写法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Trie &#123; class TrieNode &#123; boolean val; TrieNode[] children = new TrieNode[26]; &#125; private TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; public void insert(String word) &#123; TrieNode p = root; for (char c : word.toCharArray()) &#123; int i = c - &#x27;a&#x27;; if (p.children[i] == null) p.children[i] = new TrieNode(); p = p.children[i]; &#125; p.val = true; &#125; public boolean search(String word) &#123; TrieNode p = root; for (char c : word.toCharArray()) &#123; int i = c - &#x27;a&#x27;; if (p.children[i] == null) return false; p = p.children[i]; &#125; return p.val; &#125; public boolean startsWith(String prefix) &#123; TrieNode p = root; for (char c : prefix.toCharArray()) &#123; int i = c - &#x27;a&#x27;; if (p.children[i] == null) return false; p = p.children[i]; &#125; return true; &#125;&#125;作者：LFool⚡链接：https://leetcode.cn/problems/sum-of-prefix-scores-of-strings/solutions/1831514/by-lfool-w82u/来源：力扣（LeetCode）著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"字典树","slug":"字典树","permalink":"https://gouguoqiang.github.io/tags/%E5%AD%97%E5%85%B8%E6%A0%91/"}]},{"title":"数据结构-力扣","slug":"算法/3数据结构/LeetCode","date":"2022-09-02T03:51:56.000Z","updated":"2022-12-11T13:51:02.922Z","comments":true,"path":"2022/09/02/算法/3数据结构/LeetCode/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/LeetCode/","excerpt":"","text":"547. 省份数量难度中等873 有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。 省份 是一组直接或间接相连的城市，组内不含其他没有相连的城市。 给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。 返回矩阵中 省份 的数量。 示例 1： 12输入：isConnected = [[1,1,0],[1,1,0],[0,0,1]]输出：2 示例 2： 12输入：isConnected = [[1,0,0],[0,1,0],[0,0,1]]输出：3 提示： 1 &lt;= n &lt;= 200 n == isConnected.length n == isConnected[i].length isConnected[i][j] 为 1 或 0 isConnected[i][i] == 1 isConnected[i][j] == isConnected[j][i] 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public int findCircleNum(int[][] isConnected) &#123; int n = isConnected.length; UF uf = new UF(n); for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (isConnected[i][j] == 1) &#123; uf.union(i,j); &#125; &#125; &#125; return uf.sz; &#125;&#125;class UF &#123; public int[] id; public int sz; // 维护有几个 // 只有根的值 id[i] = i public UF(int n) &#123; id = new int[n]; sz = n; for (int i = 0; i &lt; n; i++) &#123; id[i] = i; &#125; &#125; public void union(int a, int b) &#123; if (! connected(a,b)) sz--; int i = root(a); int j = root(b); id[i] = j; &#125; public boolean connected(int a, int b) &#123; return root(a) == root(b); &#125; public int root(int a) &#123; while (id[a] != a) &#123; id[a] = id[id[a]]; a = id[a]; &#125; return a; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"leetCode","slug":"leetCode","permalink":"https://gouguoqiang.github.io/tags/leetCode/"}]},{"title":"数据结构-方法论","slug":"算法/3数据结构/方法论","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-03T14:32:49.419Z","comments":true,"path":"2022/09/02/算法/3数据结构/方法论/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"Trie树12345678910111213141516171819202122232425262728293031323334353637int son[N][26], cnt[N], idx;// 0号点既是根节点，又是空节点// son[][]存储树中每个节点的子节点// cnt[]存储以每个节点结尾的单词数量// 树是特殊的图 // 插入一个字符串void insert(char *str) // 建立有向边的过程 idx 是有向边的条数&#123; int p = 0; for (int i = 0; str[i]; i ++ ) &#123; int u = str[i] - &#x27;a&#x27;; if (!son[p][u]) son[p][u] = ++ idx; p = son[p][u]; &#125; cnt[p] ++ ; &#125;// 查询字符串出现的次数int query(char *str) // &#123; int p = 0; for (int i = 0; str[i]; i ++ ) &#123; int u = str[i] - &#x27;a&#x27;; if (!son[p][u]) return 0; p = son[p][u]; &#125; return cnt[p];&#125;作者：yxc链接：https://www.acwing.com/blog/content/404/来源：AcWing著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 并查集模板12345def root(i): while i != id[i]: id[i] = id[id[i]] i = id[i] return i KMP给定一个字符串 SS，以及一个模式串 PP，所有字符串中只包含大小写英文字母以及阿拉伯数字。 模式串 PP 在字符串 SS 中多次作为子串出现。 求出模式串 PP 在字符串 SS 中所有出现的位置的起始下标。 输入格式第一行输入整数 NN，表示字符串 PP 的长度。 第二行输入字符串 PP。 第三行输入整数 MM，表示字符串 SS 的长度。 第四行输入字符串 SS。 输出格式共一行，输出所有出现位置的起始下标（下标从 00 开始计数），整数之间用空格隔开。 数据范围1≤N≤1051≤N≤1051≤M≤1061≤M≤106 输入样例：12343aba5ababa 输出样例：10 2","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://gouguoqiang.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"数据结构-树状数组","slug":"算法/3数据结构/树状数组","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-22T10:19:43.816Z","comments":true,"path":"2022/09/02/算法/3数据结构/树状数组/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/","excerpt":"","text":"方法论 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; public int reversePairs(int[] nums) &#123; // 离散化 只记录rank int res = 0; int n = nums.length; int[] tmp = new int[n]; System.arraycopy(nums, 0, tmp, 0, n); // 离散化 Arrays.sort(tmp); for (int i = 0; i &lt; n; ++i) &#123; nums[i] = Arrays.binarySearch(tmp, nums[i]) + 1; &#125; // 树状数组统计逆序对 Bit bit = new Bit(n); for (int i = 0; i &lt; n; i++) &#123; res += bit.query(n) - bit.query(nums[i]); bit.add(nums[i],1); &#125; return res; &#125; class Bit &#123; int[] tree; int n; Bit(int n) &#123; this.n = n; this.tree = new int[n+1]; // 下标0 忽略不用 &#125; public int lowbit(int x) &#123; return x &amp; (-x); &#125; // 更新前缀和 自下向上 // 子节点到父节点 只要+ lowbit 反之 则 - public void add(int x, int v) &#123; for (int i = x; i &lt;= n; i += lowbit(i)) tree[i] += v; &#125; // 比如查询 5 则查询的是比5大的出现的个数 // 查询的是 5的前缀和 public int query(int x) &#123; int ret = 0; for (int i = x; i &gt; 0; i -= lowbit(i)) ret += tree[i]; return ret; &#125; &#125;&#125; https://www.bilibili.com/read/cv9904414?spm_id_from=333.999.0.0","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"堆","slug":"堆","permalink":"https://gouguoqiang.github.io/tags/%E5%A0%86/"}]},{"title":"数据结构-线段树","slug":"算法/3数据结构/线段树","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-22T10:15:05.715Z","comments":true,"path":"2022/09/02/算法/3数据结构/线段树/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E7%BA%BF%E6%AE%B5%E6%A0%91/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"堆","slug":"堆","permalink":"https://gouguoqiang.github.io/tags/%E5%A0%86/"}]},{"title":"容斥原理与博弈论","slug":"算法/6数学知识/容斥原理与博弈论","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:35:07.081Z","comments":true,"path":"2022/09/02/算法/6数学知识/容斥原理与博弈论/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86%E4%B8%8E%E5%8D%9A%E5%BC%88%E8%AE%BA/","excerpt":"","text":"容斥原理&#x2F;&#x2F;代码 二进制枚举 选1个 选两个 组合 能被整除的数给定一个整数 n 和 m 个不同的质数 p1,p2,…,pm 请你求出 1∼n 中能被 p1,p2,…,pm 中的至少一个数整除的整数有多少个。 输入格式第一行包含整数 nn 和 mm。 第二行包含 mm 个质数。 输出格式输出一个整数，表示满足条件的整数的个数。 数据范围1≤m≤161≤m≤16,1≤n,pi≤1091≤n,pi≤109 输入样例：1210 22 3 输出样例：17 代码1234567891011121314151617181920212223242526272829303132333435import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int[] p = new int[m+10]; for (int i = 0; i &lt; m; i++) &#123; p[i] = in.nextInt(); &#125; //枚举每个质数 算集合个数 int res = 0; for (int i = 0; i &lt; 1 &lt;&lt; m; i++) &#123; int t = 1, cnt = 0; for (int j = 0; j &lt; m; j++) &#123; if (((i &gt;&gt; j) &amp; 1) != 0) &#123; cnt++; if ((long)t * p[j] &gt; n) &#123; t = -1; break; &#125; t *= p[j]; &#125; &#125; if (t != -1) &#123; if (cnt % 2 == 0) res -= n / t; else res += n / t; &#125; &#125; Sytem.out.println(res); &#125;&#125; 博弈论先手必胜 先手必败 集合 移棋子游戏有 N个节点的有向无环图，图中某些节点上有棋子，两名玩家交替移动棋子。 玩家每一步可将任意一颗棋子沿一条有向边移动到另一个点，无法移动者输掉游戏。 对于给定的图和棋子初始位置，双方都会采取最优的行动，询问先手必胜还是先手必败。 输入格式第一行，三个整数 N,M,K，N 表示图中节点总数，M 表示图中边的条数，K 表示棋子的个数。 接下来 M 行，每行两个整数 X,Y 表示有一条边从点 X 出发指向点 Y。 接下来一行， K 个空格间隔的整数，表示初始时，棋子所在的节点编号。 节点编号从 1 到 N。 输出格式若先手胜，输出 win，否则输出 lose。 数据范围1≤N≤20001≤N≤2000,1≤M≤60001≤M≤6000,1≤K≤N1≤K≤N 输入样例：123456789106 8 42 12 41 41 54 51 33 53 61 2 4 6 输出样例：1win 代码1234567891011import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int k = in.nextInt(); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"6数学知识","slug":"算法/6数学知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"数学知识","slug":"数学知识","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"name":"容斥原理","slug":"容斥原理","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"}]},{"title":"pyhonAPI刷题笔记","slug":"算法/7刷题笔记/00pythonapi","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:40:15.705Z","comments":true,"path":"2022/09/02/算法/7刷题笔记/00pythonapi/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/00pythonapi/","excerpt":"","text":"API# Queue12345678910111213141516171819202122232425262728293031# SPFAfrom queue import Queuen, m = map(int, input().split())inf = 0x3f3f3f3fdist = [inf] * (n + 1)g = [[] for _ in range(n + 1)]for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] dist[1] = 0st = [False] * (n + 1)q = Queue()q.put(1)st[1] = Truewhile not q.empty(): t = q.get() st[t] = False for ne in g[t]: b, w = ne if dist[b] &gt; dist[t] + w: dist[b] = dist[t] + w if not st[b]: q.put(b) if dist[n] &gt; inf / 2: print(&quot;impossible&quot;)else: print(dist[n]) # heapq12345678910111213141516171819202122232425262728293031323334# dijkstra 堆优化import heapqn, m = map(int, input().split())inf = 0x3f3f3f3fg = [[] for _ in range(n + 1)]for _ in range(m): x, y, z = map(int, input().split()) g[x] += [(y,z)] st = [False] * (n + 1)dist = [inf] * (n + 1)dist[1] = 0q = []heapq.heappush(q,(0,1)) # 默认第一个元素进行从小到大排序, 所以存储距离while q: w, t = heapq.heappop(q) if st[t]: continue st[t] = True for ne in g[t]: j, d = ne if dist[j] &gt; dist[t] + d: dist[j] = dist[t] + d heapq.heappush(q,(dist[t] + d,j)) if dist[n] == inf: print(-1)else: print(dist[n]) # set()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#离散化n, m = map(int, input().split())g = set()o = []q = []for i in range(n): x, c = map(int, input().split()) o += [(x,c)] g.add(x) for j in range(m): l, r = map(int, input().split()) g.add(l) g.add(r) q +=[(l,r)] a = list(g)a.sort()def find(t) -&gt; int: l, r = 0, len(a) - 1 while l &lt;= r: mid = l + (r - l) // 2 if a[mid] &gt;= t: r = mid - 1 else: l = mid + 1 return l lena = len(a) + 1v = [0] * lenafor i in o: x, c = i v[find(x)] += cs = [0] * lenafor i in range(1, lena): s[i] = v[i - 1] + s[i - 1]for i in q: l, r = i print(s[find(r + 1)] - s[find(l)])","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"7刷题笔记","slug":"算法/7刷题笔记","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"python","slug":"python","permalink":"https://gouguoqiang.github.io/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"数据结构-二叉树","slug":"算法/3数据结构/二叉树","date":"2022-09-02T03:51:56.000Z","updated":"2022-11-13T03:29:27.576Z","comments":true,"path":"2022/09/02/算法/3数据结构/二叉树/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"二叉树94. 二叉树的中序遍历12345678910111213141516171819class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); TreeNode p = root; while(p != null || !stack.isEmpty() )&#123; //左根右 while(p!=null)&#123; stack.addLast(p); p = p.left; &#125; TreeNode cur = stack.removeLast(); res.add(cur.val); p = cur.right; &#125; return res; &#125;&#125; 1234567891011121314while(p != null || !stack.isEmpty())&#123; while(p != null )&#123; //res.add(p.val) 前序 stack.addLast(p); p = p.left; //p = p.right &#125; TreeNode cur = stack.removeLast(); //res.add(cur.val);中序 p = cur.right; //p = cur.left;&#125;//Collections.reverse(res);// return res; //后序 左右根 根右左 98 验证二叉搜索树 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //中序遍历 是从小到大 TreeNode p = root; ArrayDeque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); double pre = Double.MAX_VALUE; while(p!=null || !stack.isEmpty())&#123; while(p!=null)&#123; stack.push(p); p = p.left; &#125; TreeNode cur = stack.pop(); if(pre!=Double.MAX_VALUE &amp;&amp; cur.val&lt;=pre) return false; pre = (double)cur.val; p = cur.right; &#125; return true; &#125;&#125;//递归class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //递归 一个子树是否是二叉树 如果自身是left 就要小于父节点之上的 最小值 // right 大于父节点 之上的最大值 // return isValidBSTHelp(root,Long.MIN_VALUE,Long.MAX_VALUE); &#125; private boolean isValidBSTHelp(TreeNode root , long min , long max)&#123; if(root == null ) return true; if(root.val &lt;= min || root.val &gt;= max) return false; boolean l = isValidBSTHelp(root.left,min,root.val); boolean r = isValidBSTHelp(root.right,root.val,max); return l &amp;&amp; r; &#125; &#125; 123456789101112131415161718192021222324252627282930思路错误 丢失了 传递信息 需要将上上层的也记住 而不是只是当前值 最大值最小值得 改变 需要写到一起 class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //递归 一个子树是否是二叉树 如果自身是left 就要小于父节点 // right 大于父节点 // boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l&amp;&amp;r; &#125; private boolean isValidBSTHelpL(TreeNode root , int max)&#123; if(root == null) return true; if(root.val &gt;= max) return false; boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l &amp;&amp; r; &#125; private boolean isValidBSTHelpR(TreeNode root , int min)&#123; if(root == null) return true; if(root.val &lt;= min) return false; boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l &amp;&amp; r; &#125;&#125; 递归 ​ 信息的传递 ,需要的信息 和需要传递的信息 非递归 ​ 两种方式的差别 可以提前跳出 非递归的 对集合的操作配套使用 144. 二叉树的前序遍历12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; //递归栈 Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); TreeNode p = root; // preorder(root,res); while(!stack.isEmpty() || p != null)&#123; //根左右 while(p!=null)&#123; res.add(p.val); stack.addLast(p); p = p.left; &#125; // if(stack.isEmpty())&#123; // break; // &#125;else&#123; TreeNode cur = stack.removeLast(); p = cur.right; // &#125; &#125; //代码简化 只要进入while stack必不为空 return res; &#125; // private void preorder(TreeNode root ,List&lt;Integer&gt; res)&#123; // if(root == null) return ; // res.add(root.val); // preorder(root.left,res); // preorder(root.right,res); // &#125;&#125; 99.恢复搜索二叉树 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public void recoverTree(TreeNode root) &#123; //用中序遍历 如果 遇到大于的则进行处理 栈 // 1 2 3 4 5 6 7 // 相邻或者不相邻 2 1 3 3 2 1 // 处理 遇到第一个错误的时候 err1 = pre err2 = cur 遇到第二个错误 err2 = cur Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); TreeNode err1 = null; TreeNode err2 = null; TreeNode p = root; TreeNode pre = null; // stack.addLast(root); while(p != null || !stack.isEmpty())&#123; while( p != null)&#123; stack.addLast(p); p = p.left; &#125; //少了 判左右 TreeNode cur = stack.removeLast(); if(pre != null &amp;&amp; pre.val &gt;= cur.val)&#123; if(err1 == null)&#123; err1 = pre; err2 = cur; &#125;else&#123; err2 = cur; break; &#125; &#125; pre = cur; p = cur.right; &#125; int temp = err1.val; err1.val = err2.val; err2.val = temp; &#125; &#125; 257.二叉树的所有路径 1234567891011121314151617181920212223class Solution &#123; public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); StringBuilder sb = new StringBuilder(); backtrack(root,res,sb); return res; &#125; private void backtrack(TreeNode root , List&lt;String&gt; res , StringBuilder sb)&#123; if(root == null ) return ; if(root.left == null &amp;&amp; root.right == null )&#123; sb.append(root.val); res.add(sb.toString()); return ; &#125; sb.append(root.val).append(&quot;-&gt;&quot;); backtrack(root.left,res,new StringBuilder(sb)); backtrack(root.right,res,new StringBuilder(sb)); &#125;&#125; 95.不同的二叉搜索树 123456789101112131415161718192021222324252627282930313233class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if(n == 0) return new ArrayList&lt;TreeNode&gt;(); return help(1,n); &#125; private List&lt;TreeNode&gt; help(int start , int end)&#123; List&lt;TreeNode&gt; res = new ArrayList&lt;&gt;(); if(start&gt;end)&#123; res.add(null); return res; &#125; if(start == end)&#123; res.add(new TreeNode(start)); return res; &#125; for(int i = start ; i &lt;= end ; i++)&#123; List&lt;TreeNode&gt; leftt = help(start,i-1); List&lt;TreeNode&gt; rightt = help(i+1,end); for(TreeNode l : leftt)&#123; for(TreeNode r : rightt)&#123; TreeNode root = new TreeNode(i); root.left = l; root.right = r; res.add(root); &#125; &#125; &#125; return res; &#125;&#125; 96. 不同二叉搜索树 1234567891011121314151617181920class Solution &#123; public int numTrees(int n) &#123; int[] dp = new int[n+1]; return help(1,n,dp); &#125; private int help(int start, int end,int[] dp)&#123; int res = 0; if(start &gt;= end ) return 1; if(dp[end-start]!=0) return dp[end-start]; for(int i = start ; i &lt;= end ; i++)&#123; int l = help(start,i-1,dp); int r = help(i+1,end,dp); res += l*r; dp[end-start] = res; &#125; return res; &#125;&#125; 101. 对称的二叉树123456789101112class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; return isSymmetric(root.left,root.right); &#125; public boolean isSymmetric(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null) return true; if (p == null || q == null) return false; if (p.val != q.val) return false; return isSymmetric(p.left,q.right) &amp;&amp; isSymmetric(p.right,q.left); &#125;&#125; 124.二叉树中的最大路径和 12345678910111213141516171819class Solution &#123; int max = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; // 左右子树的 最大路径和 是否包含根节点 //区分的操作 !!! 携带信息带根 结果判断是否带根 //为什么全 :对每个节点都假设存在于最大边 // 则要不左右单边(包含只有自己) 或者 左右相连 postOrder(root); return max; &#125; private int postOrder(TreeNode root)&#123; if(root == null ) return 0; int l = Math.max(postOrder(root.left),0); int r = Math.max(postOrder(root.right),0); max = Math.max(max,l+r+root.val); return root.val + Math.max(l,r); &#125;&#125; 235二叉搜索树的最近公共祖先,236二叉树的~12345678910111213141516171819202122232425262728293031323334class Solution &#123; //运用二叉搜索树性质 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; // if(p.val == q.val ) return p; if(root.val==q.val || root.val == p.val) return root; TreeNode gt = p.val &gt; q.val ? p : q; TreeNode lt = p.val &lt; q.val ? p : q; if(lt.val&lt;root.val &amp;&amp; gt.val &gt; root.val) return root; else if(lt.val &gt; root.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else&#123; return lowestCommonAncestor(root.left,p,q); &#125; &#125;&#125;class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root== null) return null; if(root==p || root==q) return root; //这条好好思考,返回值 TreeNode left = lowestCommonAncestor(root.left,p,q); //pq的位置 TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)&#123; // 在左右两棵子树上 return root; &#125;else if(left!=null)&#123; //只在一颗子树 上 return left; &#125;else&#123; return right; &#125; &#125;&#125; 剑指offer28. 对称的二叉树123456789101112class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; return isSymmetric(root.left,root.right); &#125; public boolean isSymmetric(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null) return true; if (p == null || q == null) return false; if (p.val != q.val) return false; return isSymmetric(p.left,q.right) &amp;&amp; isSymmetric(p.right,q.left); &#125;&#125; 6235. 逐层排序二叉树所需的最少操作数目给你一个 值互不相同 的二叉树的根节点 root 。 在一步操作中，你可以选择 同一层 上任意两个节点，交换这两个节点的值。 返回每一层按 严格递增顺序 排序所需的最少操作数目。 节点的 层数 是该节点和根节点之间的路径的边数。 12// 求一个乱序数组排好序的最小交换次数 : 最大逆序对数?7 6 8 5 两次 2 1 1 0","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"二叉树","slug":"二叉树","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"}]},{"title":"AcWing刷题笔记","slug":"算法/7刷题笔记/AcWing","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:38:54.525Z","comments":true,"path":"2022/09/02/算法/7刷题笔记/AcWing/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/AcWing/","excerpt":"","text":"提高组AcWing 1402. 星空之夜（dfs + 图案hash）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.util.ArrayList;import java.util.List;public class Main &#123; public static int n, m; public static char[][] g; public static List&lt;Double&gt; hash = new ArrayList&lt;&gt;(); public static List&lt;int[]&gt; q = new ArrayList&lt;&gt;(); public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); m = Integer.parseInt(in.readLine()); n = Integer.parseInt(in.readLine()); g = new char[n][m]; for (int i = 0; i &lt; n; i++) &#123; g[i] = in.readLine().toCharArray(); &#125; // 找到一个星图, 将坐标存储, // hash 下标(0+&#x27;a&#x27;) 映射为 double 欧几里得距离 for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; if (g[i][j] == &#x27;1&#x27;) &#123; q.clear(); dfs(i,j); double val = get_hash(); char c = get_char(val); for (int[] p : q) &#123; int x = p[0], y = p[1]; g[x][y] = c; &#125; &#125; &#125; &#125; for (int i = 0; i &lt; n; i++) &#123; System.out.println(g[i]); &#125; &#125; private static char get_char(double val) &#123; int sz = hash.size(); for (int i = 0; i &lt; sz; i++) &#123; if (Math.abs(hash.get(i) - val) &lt; 1e-6) &#123; // 1e-6 return (char)((i) + &#x27;a&#x27;); &#125; &#125; hash.add(val); return (char)((sz) + &#x27;a&#x27;); &#125; public static void dfs(int x, int y) &#123; q.add(new int[]&#123;x,y&#125;); g[x][y] = &#x27;0&#x27;; for (int i = x - 1; i &lt;= x + 1; i++) &#123; for (int j = y - 1; j &lt;= y + 1; j ++) &#123; if (i &gt;= 0 &amp;&amp; i &lt; n &amp;&amp; j &gt;= 0 &amp;&amp; j &lt; m &amp;&amp; g[i][j] == &#x27;1&#x27;) &#123; dfs(i,j); &#125; &#125; &#125; &#125; // 欧几里得距离 两两配对 public static double get_hash() &#123; //connected block double res = 0; int sz = q.size(); for (int i = 0; i &lt; sz; i++) &#123; for (int j = i + 1; j &lt; sz; j++) &#123; res += get_dist(q.get(i),q.get(j)); &#125; &#125; return res; &#125; private static double get_dist(int[] a, int[] b) &#123; int dx = a[0] - b[0]; int dy = a[1] - b[1]; return Math.sqrt(dx * dx + dy * dy); &#125;&#125; AcWing 479. 加分二叉树（寒假每日一题）AcWing 479. 加分二叉树（寒假每日一题） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 区间dp// 因为 要先构造子树 在构造根, 则可以以根的不同划分集合import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.util.Arrays;import java.util.stream.IntStream;public class Main &#123; public static int n; public static int[] w; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); n = Integer.parseInt(in.readLine()); String[] s = in.readLine().split(&quot; &quot;); int[] w = Arrays.stream(s).mapToInt(str -&gt; Integer.parseInt(str)).toArray(); // f[i,j] 区间 i j构成的所有集合 属性 : max 加分 int[][] f = new int[n][n]; int[][] g = new int[n][n]; // g[i][j] 表示 区间i j 字典序 加分最大的 根节点 for (int len = 1; len &lt;= n; len ++) &#123; for (int i = 0; i + len - 1 &lt; n; i++) &#123; int j = i + len - 1; if (len == 1) &#123; f[i][j] = w[i]; g[i][j] = i; &#125; else &#123; for (int k = i; k &lt;= j; k++) &#123; int left = k == i ? 1 : f[i][k-1]; int right = k == j ? 1: f[k+1][j]; int score = w[k] + left * right; if (score &gt; f[i][j]) &#123; f[i][j] = score; g[i][j] = k; &#125; &#125; &#125; &#125; &#125; System.out.println(f[0][n-1]); dfs(0, n - 1, g); &#125; private static void dfs(int l, int r, int[][] g) &#123; if (l &gt; r) return ; int k = g[l][r]; System.out.print((k+1) + &quot; &quot;); dfs(l, k -1,g); dfs(k + 1,r,g); &#125;&#125; \\1414. 牛异或123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// trie树// package WinterImpro;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStreamReader;import java.util.Arrays;import java.util.stream.IntStream;public class Main &#123; private static final int N = 100010, M = 21 * N; public static int n, idx = 0; public static int[] w, id = new int[M]; public static int[][] son = new int[M][2]; public static void main(String[] args) throws IOException &#123; BufferedReader in = new BufferedReader(new InputStreamReader(System.in)); n = Integer.parseInt(in.readLine()); w = new int[n]; // trie树 for (int i = 0; i &lt; n; i++) &#123; w[i] = Integer.parseInt(in.readLine()); &#125; int[] s = new int[n+1]; for (int i = 1; i &lt;= n; i++) &#123; s[i] ^= s[i-1] ^ w[i - 1];// System.out.println(i + &quot; &quot; + s[i]); &#125; int res = -1, a = 0, b = 0; insert(s[0],0); for (int i = 1; i &lt;= n; i++) &#123; int k = query(s[i]); // 找到最大异或值 所在的位置 int t = s[i] ^ s[k]; if (t &gt; res) &#123; res = t; a = k + 1; b = i; &#125; insert(s[i],i); &#125; System.out.println(res + &quot; &quot; + a + &quot; &quot; + b); &#125; public static void insert(int x, int k) &#123; int p = 0; for (int i = 20; i &gt;= 0; i--) &#123; int u = (x &gt;&gt; i) &amp; 1; if (son[p][u] == 0) son[p][u] = ++ idx; p = son[p][u]; &#125; id[p] = k; // id 存储与之前第几个位置 结果最大 &#125; public static int query(int x) &#123; int p = 0; for (int i = 20; i &gt;= 0; i--) &#123; int u = (x &gt;&gt; i) &amp; 1; int _u = 1 - u; if (son[p][_u] != 0) p = son[p][_u]; // 查不同前缀 存在不同前缀则 走不同前缀 else p = son[p][u]; &#125; return id[p]; &#125;&#125; AcWing 122. 糖果传递 数学&#x2F;贪心入门组AcWing 756. 蛇形矩阵12345678910111213141516171819202122232425262728293031import java.util.Arrays;import java.util.Scanner;public class Main &#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int[][] q = new int[n][m]; int[] dx = new int[]&#123;0,1,0,-1&#125;, dy = new int[]&#123;1,0,-1,0&#125;; int d = 0, x = 0, y = 0; for (int i = 1; i &lt;= n * m; i++) &#123; q[x][y] = i; int a = x + dx[d], b = y + dy[d]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m || q[a][b] != 0) &#123; // q[a][b] != 0 边界会发生变化 不等于0 也相当于 边界变化 d = (d + 1) % 4; a = x + dx[d]; b = y + dy[d]; &#125; x = a; y = b; &#125; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; System.out.print(q[i][j] + &quot; &quot;); &#125; System.out.println(); &#125; &#125;&#125; AcWing 680. 剪绳子 二分AcWing 1208. 翻硬币小明正在玩一个“翻硬币”的游戏。 桌上放着排成一排的若干硬币。我们用 * 表示正面，用 o 表示反面（是小写字母，不是零）。 比如，可能情形是：**oo***oooo 如果同时翻转左边的两个硬币，则变为：oooo***oooo 现在小明的问题是：如果已知了初始状态和要达到的目标状态，每次只能同时翻转相邻的两个硬币,那么对特定的局面，最少要翻动多少次呢？ \\1353. 滑雪场设计 (a∗b)modp&#x3D;(amodp∗bmodp)modp","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"7刷题笔记","slug":"算法/7刷题笔记","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"杂记","slug":"杂记","permalink":"https://gouguoqiang.github.io/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"数学方法论","slug":"算法/6数学知识/方法论数学知识","date":"2022-09-02T03:51:56.000Z","updated":"2023-03-01T14:00:02.435Z","comments":true,"path":"2022/09/02/算法/6数学知识/方法论数学知识/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/%E6%96%B9%E6%B3%95%E8%AE%BA%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/","excerpt":"","text":"质数12345# 试除法质数: 只能被1和本身整除暴力思路: 试除法 o(N) 1. 成对出现 所以从2枚举到 &lt;= 根号n即可 i &lt; n / i 1234567891011121314151617181920212223242526272829#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;bool is_prime(int a) &#123; if (a &lt; 2) return false; for (int i = 2; i &lt;= a / i; i++) &#123; if (a % i == 0) return false; &#125; return true;&#125;int main() &#123; int n; cin &gt;&gt; n; for (int i = 0; i &lt; n; i++) &#123; int a; cin &gt;&gt; a; if (is_prime(a)) &#123; cout &lt;&lt; &quot;Yes&quot; &lt;&lt; endl; &#125;else &#123; cout &lt;&lt; &quot;No&quot; &lt;&lt; endl; &#125; &#125; return 0; &#125; 分解质因数分解质因数每个合数都可以写成几个质数相乘的形式，其中每个质数都是这个合数的因数，把一个合数用质因数相乘的形式表示出来，叫做分解质因数 筛质数线性筛: 枚举每个数将他的倍数删掉 埃氏筛法 12345678910111213141516171819202122232425262728293031323334#include&lt;bits/stdc++.h&gt;using namespace std;const int maxn = 1e6 + 10;int primes[maxn],cnt;bool st[maxn]; int n; void get_prime(int n) &#123; for(int i = 2;i &lt;= n;i++) &#123; //当前这个数没有被筛掉说明它是素数 /*埃式筛 if(!st[i]) &#123; primes[cnt++] = n; for(int j = i + i;j &lt;= n;j += i) st[j] = true; &#125; */ //线性筛 if(!st[i]) primes[cnt++] = i; for(int j = 0;primes[j] &lt;= n / i;j++) &#123; st[primes[j] * i] = true; if(i % primes[j] == 0) break; &#125; &#125;&#125; int main() &#123; cin &gt;&gt; n; get_prime(n); cout &lt;&lt; cnt &lt;&lt; endl; return 0;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"6数学知识","slug":"算法/6数学知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://gouguoqiang.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"数学","slug":"数学","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"算法随记","slug":"算法/7刷题笔记/算法随记","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:38:43.109Z","comments":true,"path":"2022/09/02/算法/7刷题笔记/算法随记/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/%E7%AE%97%E6%B3%95%E9%9A%8F%E8%AE%B0/","excerpt":"","text":"公式推导题AcWing 122. 糖果传递【环形纸牌均分问题】 https://www.acwing.com/solution/content/41677/ 奇特的单链表操作方式147. 对链表进行插入排序给定单个链表的头 head ，使用 插入排序 对链表进行排序，并返回 排序后链表的头 。 插入排序 算法的步骤: 插入排序是迭代的，每次只移动一个元素，直到所有元素可以形成一个有序的输出列表。每次迭代中，插入排序只从输入数据中移除一个待排序的元素，找到它在序列中适当的位置，并将其插入。重复直到所有输入数据插入完为止。下面是插入排序算法的一个图形示例。部分排序的列表(黑色)最初只包含列表中的第一个元素。每次迭代时，从输入数据中删除一个元素(红色)，并就地插入已排序的列表中。 对链表进行插入排序。 示例 1： 输入: head &#x3D; [4,2,1,3]输出: [1,2,3,4]示例 2： 输入: head &#x3D; [-1,5,3,4,0]输出: [-1,0,3,4,5] 提示： 列表中的节点数在 [1, 5000]范围内-5000 &lt;&#x3D; Node.val &lt;&#x3D; 5000 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/insertion-sort-list著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 方法一 直观模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445//方法一 直观模拟class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //分割成单结点,去除链表的影响 所有都是单节点操作 //不分割可能会成环 if (head == null) return null; ListNode newHead = head; head = head.next; ListNode p = newHead; newHead.next = null; ListNode q = head; // 梳理下思路 //遍历输入结点,找输入结点在新链表的位置,小于等于新结点的头结点 直接头插 大于则往后遍历, // 直到 p.next.val &gt; q.val // 如果 p.next == null 直接尾插 while (q != null) &#123; //头插 ListNode t = q.next; q.next = null; if (q.val &lt; newHead.val) &#123; //头插法 q.next = newHead; newHead = q; q = t; p = newHead; continue; &#125; while (p == null || q.val &gt; p.val) &#123; if (p.next == null || p.next.val &gt; q.val ) &#123; break; &#125; p = p.next; &#125; // 插入 ListNode temp = p.next; p.next = q; q.next = temp; q = t; p = newHead; &#125; return newHead; &#125;&#125; 方法二12345678910111213141516171819202122232425class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //在一条环上处理 ListNode dummy = new ListNode(-1,head); ListNode newLast = dummy.next; ListNode cur = newLast.next; while (cur != null) &#123; if (cur.val &gt;= newLast.val) &#123; newLast = cur; &#125;else &#123; //从头找到p.next.val &gt; cur.val //因为last.val &gt; cur.val 所以.next不会为空的 ListNode p = dummy; while (p.next.val &lt;= cur.val) &#123; p = p.next; &#125; newLast.next = cur.next; cur.next = p.next; p.next = cur; &#125; cur = newLast.next; &#125; return dummy.next; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"7刷题笔记","slug":"算法/7刷题笔记","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"杂记","slug":"杂记","permalink":"https://gouguoqiang.github.io/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"力扣刷题笔记","slug":"算法/7刷题笔记/leetcode周赛","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-19T04:39:13.166Z","comments":true,"path":"2022/09/02/算法/7刷题笔记/leetcode周赛/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/leetcode%E5%91%A8%E8%B5%9B/","excerpt":"","text":"12.18 || 324场周赛统计相似字符串对的数目1234567891011121314151617181920212223class Solution &#123; public int similarPairs(String[] words) &#123; int n = words.length; int[] map = new int[n]; for (int i = 0; i &lt; n; i ++) &#123; String s = words[i]; for (int j = 0; j &lt; s.length(); j ++) &#123; int a = s.charAt(j) - &#x27;a&#x27;; if ((map[i] &amp; (1 &lt;&lt; a)) == 0) &#123; map[i] += (1 &lt;&lt; a); &#125; &#125; &#125; for (int i = 0; i &lt; n; i++) System.out.println(map[i]); int ans = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = i + 1; j &lt; n; j ++) &#123; if (map[i] == map[j]) ans ++; &#125; &#125; return ans; &#125;&#125; 使用质因数之和替换后可以取到的最小值1234567891011121314151617181920212223242526class Solution &#123; public int smallestValue(int n) &#123; int ans = divide(n); if (ans == n) return ans; return smallestValue(ans); &#125; public int divide(int x) &#123; int ans = 0; for (int i = 2; i &lt;= x / i; i ++ ) if (x % i == 0) &#123; int s = 0; while (x % i == 0) &#123; x /= i; s ++; &#125; ans += i * s; &#125; if (x &gt; 1) ans += x; return ans; &#125;&#125; \\6267. 添加边使所有节点度数都为偶数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104class Solution &#123; public boolean isPossible(int n, List&lt;List&lt;Integer&gt;&gt; edges) &#123; Set&lt;Integer&gt;[] g = new Set[n + 1]; for (int i = 1; i &lt;= n; i ++) &#123; g[i] = new HashSet&lt;&gt;(); &#125; int[] d = new int[n + 1]; for (List&lt;Integer&gt; edge: edges) &#123; int x = edge.get(0), y = edge.get(1); g[x].add(y); g[y].add(x); g[x].add(x); g[y].add(y); d[x] ++; d[y] ++; &#125; List&lt;Integer&gt; ji = new ArrayList&lt;&gt;(); for (int i = 0; i &lt;= n; i++) &#123; // System.out.println(i + &quot; &quot; + d[i]); if (d[i] % 2 == 1) &#123; ji.add(i); &#125; &#125; // System.out.println(ji); int sz = ji.size(); // System.out.println(sz); if (sz == 0) return true; if (sz &gt; 4 || sz % 2 == 1) return false; if (sz == 2) &#123; int x = ji.get(0), y = ji.get(1); if (!g[x].contains(y)) return true; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[x].contains(i) &amp;&amp; !g[y].contains(i)) return true; &#125; return false; &#125; int x = ji.get(0), y = ji.get(1); int a = ji.get(2), b = ji.get(3); int flag = 0; if (!g[x].contains(y) &amp;&amp; !g[a].contains(b)) return true; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[x].contains(i) &amp;&amp; !g[y].contains(i)) &#123; flag ++; break; &#125; &#125; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[a].contains(i) &amp;&amp; !g[b].contains(i)) &#123; flag ++; break; &#125; &#125; if (flag == 2) return true; x = ji.get(0); y = ji.get(2); a = ji.get(1); b = ji.get(3); flag = 0; if (!g[x].contains(y) &amp;&amp; !g[a].contains(b)) return true; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[x].contains(i) &amp;&amp; !g[y].contains(i)) &#123; flag ++; break; &#125; &#125; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[a].contains(i) &amp;&amp; !g[b].contains(i)) &#123; flag ++; break; &#125; &#125; if (flag == 2) return true; x = ji.get(0); y = ji.get(3); a = ji.get(1); b = ji.get(2); flag = 0; if (!g[x].contains(y) &amp;&amp; !g[a].contains(b)) return true; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[x].contains(i) &amp;&amp; !g[y].contains(i)) &#123; flag ++; break; &#125; &#125; for (int i = 1; i &lt;= n; i ++) &#123; if (!g[a].contains(i) &amp;&amp; !g[b].contains(i)) &#123; flag ++; break; &#125; &#125; if (flag == 2) return true; return false; &#125;&#125; 12.11连续内存分配 计数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Allocator &#123; int[] a; int s; int m; public Allocator(int n) &#123; m = n; a = new int[n]; // map = new HashMap&lt;&gt;(); &#125; public int allocate(int size, int mID) &#123; for(int i = 0; i &lt; m; i++) &#123; if (a[i] == 0) &#123; int cnt = -1; for (int j = i; j &lt;= m; j++) &#123; cnt ++; if (j == m || a[j] != 0 ) &#123; // if (j == m) cnt --; if (cnt &gt;= size) &#123; for (int k = i; k &lt; i + size; k ++) &#123; a[k] = mID; &#125; return i; &#125; i = j; break; &#125; &#125; &#125; &#125; return -1; &#125; public int free(int mID) &#123; int res = 0; for (int i = 0; i &lt; m; i++) &#123; if (a[i] == mID) &#123; res ++; a[i] = 0; &#125; &#125; return res; &#125;&#125;/** * Your Allocator object will be instantiated and called as such: * Allocator obj = new Allocator(n); * int param_1 = obj.allocate(size,mID); * int param_2 = obj.free(mID); */ 2022.11.20&#x2F; 第320场周赛\\6243. 到达首都的最少油耗给你一棵 n 个节点的树（一个无向、连通、无环图），每个节点表示一个城市，编号从 0 到 n - 1 ，且恰好有 n - 1 条路。0 是首都。给你一个二维整数数组 roads ，其中 roads[i] = [ai, bi] ，表示城市 ai 和 bi 之间有一条 双向路 。 每个城市里有一个代表，他们都要去首都参加一个会议。 每座城市里有一辆车。给你一个整数 seats 表示每辆车里面座位的数目。 城市里的代表可以选择乘坐所在城市的车，或者乘坐其他城市的车。相邻城市之间一辆车的油耗是一升汽油。 请你返回到达首都最少需要多少升汽油。 示例 1： 1234567输入：roads = [[0,1],[0,2],[0,3]], seats = 5输出：3解释：- 代表 1 直接到达首都，消耗 1 升汽油。- 代表 2 直接到达首都，消耗 1 升汽油。- 代表 3 直接到达首都，消耗 1 升汽油。最少消耗 3 升汽油。 示例 2： 1234567891011输入：roads = [[3,1],[3,2],[1,0],[0,4],[0,5],[4,6]], seats = 2输出：7解释：- 代表 2 到达城市 3 ，消耗 1 升汽油。- 代表 2 和代表 3 一起到达城市 1 ，消耗 1 升汽油。- 代表 2 和代表 3 一起到达首都，消耗 1 升汽油。- 代表 1 直接到达首都，消耗 1 升汽油。- 代表 5 直接到达首都，消耗 1 升汽油。- 代表 6 到达城市 4 ，消耗 1 升汽油。- 代表 4 和代表 6 一起到达首都，消耗 1 升汽油。最少消耗 7 升汽油。 示例 3： 123输入：roads = [], seats = 1输出：0解释：没有代表需要从别的城市到达首都。 提示： 1 &lt;= n &lt;= 105 roads.length == n - 1 roads[i].length == 2 0 &lt;= ai, bi &lt; n ai != bi roads 表示一棵合法的树。 1 &lt;= seats &lt;= 105 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123; int[][] g; boolean[] st; int[] dist; int INF = Integer.MAX_VALUE / 2; public long minimumFuelCost(int[][] roads, int seats) &#123; // 尽量做一辆车 每个点到达 首都的最短路 前seats个结点只耗这一条路径的油 进行标记 // 记录每个点走的pre 对每一个点遍历下路径 // dj后 从最远的开始遍历 遍历到前seate个点移出set不在计算油耗 int n = roads.length() + 1; g = new int[n+1][n+1]; for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; g[i][j] = g[j][i] = i == j ? 0 : INF; &#125; &#125; for (int[] t : roads) &#123; g[t[0]][t[1]] = 1; g[t[1]][t[0]] = 1; &#125; dijkstra(n,k); &#125; public void dijkstra(int n, int k) &#123; dist = new int[n+1]; st = new boolean[n+1]; Arrays.fill(dist,INF); dist[k] = 0; for (int i = 1; i &lt;= n; i++) &#123; int t = -1; for (int j = 1; j &lt;= n; j++) &#123; if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) &#123; t = j; &#125; &#125; st[t] = true; for (int j = 1; j &lt;= n; j++) &#123; dist[j] = Math.min(dist[j],dist[t] + g[t][j]); &#125; &#125; &#125;&#125; \\6244. 完美分割的方案数给你一个字符串 s ，每个字符是数字 &#39;1&#39; 到 &#39;9&#39; ，再给你两个整数 k 和 minLength 。 如果对 s 的分割满足以下条件，那么我们认为它是一个 完美 分割： s 被分成 k 段互不相交的子字符串。 每个子字符串长度都 至少 为 minLength 。 每个子字符串的第一个字符都是一个 质数 数字，最后一个字符都是一个 非质数 数字。质数数字为 &#39;2&#39; ，&#39;3&#39; ，&#39;5&#39; 和 &#39;7&#39; ，剩下的都是非质数数字。 请你返回 s 的 完美 分割数目。由于答案可能很大，请返回答案对 109 + 7 取余 后的结果。 一个 子字符串 是字符串中一段连续字符串序列。 示例 1： 123456输入：s = &quot;23542185131&quot;, k = 3, minLength = 2输出：3解释：存在 3 种完美分割方案：&quot;2354 | 218 | 5131&quot;&quot;2354 | 21851 | 31&quot;&quot;2354218 | 51 | 31&quot; 示例 2： 123输入：s = &quot;23542185131&quot;, k = 3, minLength = 3输出：1解释：存在一种完美分割方案：&quot;2354 | 218 | 5131&quot; 。 示例 3： 123输入：s = &quot;3312958&quot;, k = 3, minLength = 1输出：1解释：存在一种完美分割方案：&quot;331 | 29 | 58&quot; 。 提示： 1 &lt;= k, minLength &lt;= s.length &lt;= 1000 s 每个字符都为数字 &#39;1&#39; 到 &#39;9&#39; 之一。 123456789class Solution &#123; public int beautifulPartitions(String s, int k, int minLength) &#123; // 记忆化搜索 前i个字符分成一组后 剩下的 分成k-1组 // f[i][j] 前i个字符分成j组的方案数 // 区间dp int n = s.length(); &#125;&#125; 2022.11.13&#x2F; 第319场周赛\\6235. 逐层排序二叉树所需的最少操作数目给你一个 值互不相同 的二叉树的根节点 root 。 在一步操作中，你可以选择 同一层 上任意两个节点，交换这两个节点的值。 返回每一层按 严格递增顺序 排序所需的最少操作数目。 节点的 层数 是该节点和根节点之间的路径的边数。 示例 1 ： 12345678输入：root = [1,4,3,7,6,8,5,null,null,null,null,9,null,10]输出：3解释：- 交换 4 和 3 。第 2 层变为 [3,4] 。- 交换 7 和 5 。第 3 层变为 [5,6,8,7] 。- 交换 8 和 7 。第 3 层变为 [5,6,7,8] 。共计用了 3 步操作，所以返回 3 。可以证明 3 是需要的最少操作数目。 示例 2 ： 12345678输入：root = [1,3,2,7,6,5,4]输出：3解释：- 交换 3 和 2 。第 2 层变为 [2,3] 。 - 交换 7 和 4 。第 3 层变为 [4,6,5,7] 。 - 交换 6 和 5 。第 3 层变为 [4,5,6,7] 。共计用了 3 步操作，所以返回 3 。 可以证明 3 是需要的最少操作数目。 示例 3 ： 123输入：root = [1,2,3,4,5,6]输出：0解释：每一层已经按递增顺序排序，所以返回 0 。 提示： 树中节点的数目在范围 [1, 105] 。 1 &lt;= Node.val &lt;= 105 树中的所有值 互不相同 。 Solution： 12345class Solution &#123; public int minimumOperations(TreeNode root) &#123; // todo: minimum number of swaps required to sort an Array &#125;&#125; \\6236. 不重叠回文子字符串的最大数目给你一个字符串 s 和一个 正 整数 k 。 从字符串 s 中选出一组满足下述条件且 不重叠 的子字符串： 每个子字符串的长度 至少 为 k 。 每个子字符串是一个 回文串 。 返回最优方案中能选择的子字符串的 最大 数目。 子字符串 是字符串中一个连续的字符序列。 示例 1 ： 1234输入：s = &quot;abaccdbbd&quot;, k = 3输出：2解释：可以选择 s = &quot;abaccdbbd&quot; 中斜体加粗的子字符串。&quot;aba&quot; 和 &quot;dbbd&quot; 都是回文，且长度至少为 k = 3 。可以证明，无法选出两个以上的有效子字符串。 示例 2 ： 123输入：s = &quot;adbcda&quot;, k = 2输出：0解释：字符串中不存在长度至少为 2 的回文子字符串。 提示： 1 &lt;= k &lt;= s.length &lt;= 2000 s 仅由小写英文字母组成 Solution： 1234567class Solution &#123; public int maxPalindromes(String s, int k) &#123; // 求出各种分割方案的(贪心: 一个回文串的长度尽可能短,满足条件即分割 // 贪心的证明: // 最大满足条件的子串个数 &#125;&#125; 周赛 318\\2460. 对数组执行操作题目描述 给你一个下标从 0 开始的数组 nums ，数组大小为 n ，且由 非负 整数组成。 你需要对数组执行 n - 1 步操作，其中第 i 步操作（从 0 开始计数）要求对 nums 中第 i 个元素执行下述指令： 如果 nums[i] == nums[i + 1] ，则 nums[i] 的值变成原来的 2 倍，nums[i + 1] 的值变成 0 。否则，跳过这步操作。 在执行完 全部 操作后，将所有 0 移动 到数组的 末尾 。 123456789101112131415161718192021222324252627282930# 模拟题class Solution: def applyOperations(self, a: List[int]) -&gt; List[int]: n = len(a) for i in range(n-1): if a[i] == a[i+1]: a[i] *= 2 a[i+1] = 0 # 学下下面这两句的语法 b = [x for x in a if x &gt; 0] # 访问数组元素 满足条件才赋值 b += [0] * (n - len(b)) #扩容 return b#降低空间复杂度class Solution: def applyOperations(self, a: List[int]) -&gt; List[int]: n = len(a) j = 0 for i in range(0,n-1): if a[i] == a[i+1]: a[i] *= 2 a[i+1] = 0 if a[i] != 0: a[j] = a[i] j += 1 if a[n-1] != 0: a[j] = a[n-1] j += 1 for i in range(j,n): a[i] = 0 return a \\2461. 长度为 K 子数组中的最大和给你一个整数数组 nums 和一个整数 k 。请你从 nums 中满足下述条件的全部子数组中找出最大子数组和： 子数组的长度是 k，且 子数组中的所有元素 各不相同 。 返回满足题面要求的最大子数组和。如果不存在子数组满足这些条件，返回 0 。 子数组 是数组中一段连续非空的元素序列。 123456789101112131415161718class Solution: def maximumSubarraySum(self, nums: List[int], k: int) -&gt; int: ans = 0 cnt = Counter(nums[:k-1]) #hash 频次表 s = sum(nums[:k-1]) # [:k-1]是左闭右开 for i, j in zip(nums[k-1:],nums): cnt[i] += 1 s += i if len(cnt) == k: ans = max(ans,s) cnt[j] -= 1 if cnt[j] == 0: del cnt[j] s -= j return ans \\2462. 雇佣 K 位工人的总代价12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123; public long totalCost(int[] costs, int k, int candidates) &#123; //维护两个堆 long ans = 0; int n = costs.length; PriorityQueue&lt;Integer&gt; pre = new PriorityQueue&lt;&gt;(candidates); PriorityQueue&lt;Integer&gt; suf = new PriorityQueue&lt;&gt;(candidates); if (candidates * 2 &lt; n) &#123; for (int i = 0; i &lt; candidates; i++) &#123; pre.offer(costs[i]); suf.offer(costs[n-i-1]); &#125; for (int i = candidates, j = n - candidates - 1; i &lt;= j &amp;&amp; k &gt; 0; k--) &#123; if(pre.peek() &lt;= suf.peek()) &#123; ans += pre.poll(); pre.offer(costs[i++]); &#125; else &#123; ans += suf.poll(); suf.offer(costs[j--]); &#125; &#125; // 这一步和 下面else 那一步 python可以简化为一步 // python的堆可以直接拼接 然后在排序即可 while (k-- &gt; 0) &#123; if (pre.isEmpty()) &#123; ans += suf.poll(); &#125; else if (suf.isEmpty()) &#123; ans += pre.poll(); &#125;else if(pre.peek() &lt;= suf.peek()) &#123; ans += pre.poll(); &#125; else &#123; ans += suf.poll(); &#125; &#125; &#125; else &#123; Arrays.sort(costs); for (int i = 0; i &lt; k; i++) &#123; ans += costs[i]; &#125; &#125; // i &gt; j 刚好重叠 对剩下的元素进行排序 || 一开始就大于= n return ans; &#125;&#125; 12345678910111213141516171819202122232425from heapq import heapify, heapreplaceclass Solution: def totalCost(self, costs: List[int], k: int, candidates: int) -&gt; int: ans = 0 n = len(costs) if candidates * 2 &lt; n: pre = costs[:candidates] heapify(pre) suf = costs[-candidates:] heapify(suf) i, j = candidates, n - candidates - 1 while k and i &lt;= j: if pre[0] &lt;= suf[0]: # 返回栈顶并将costs[i]交换为栈顶 比直接pop效率高 ans += heapreplace(pre,costs[i]) i += 1 else: ans += heapreplace(suf,costs[j]) j -= 1 k -= 1 costs = pre + suf costs.sort() return ans + sum(costs[:k]) \\2463. 最小移动总距离X 轴上有一些机器人和工厂。给你一个整数数组 robot ，其中 robot[i] 是第 i 个机器人的位置。再给你一个二维整数数组 factory ，其中 factory[j] = [positionj, limitj] ，表示第 j 个工厂的位置在 positionj ，且第 j 个工厂最多可以修理 limitj 个机器人。 每个机器人所在的位置 互不相同 。每个工厂所在的位置也 互不相同 。注意一个机器人可能一开始跟一个工厂在 相同的位置 。 所有机器人一开始都是坏的，他们会沿着设定的方向一直移动。设定的方向要么是 X 轴的正方向，要么是 X 轴的负方向。当一个机器人经过一个没达到上限的工厂时，这个工厂会维修这个机器人，且机器人停止移动。 任何时刻，你都可以设置 部分 机器人的移动方向。你的目标是最小化所有机器人总的移动距离。 请你返回所有机器人移动的最小总距离。测试数据保证所有机器人都可以被维修。 12345678910111213141516171819class Solution: def minimumTotalDistance(self, robot: List[int], factory: List[List[int]]) -&gt; int: robot.sort() factory.sort(key=lambda a:a[0]) # 前i个工厂处理j个工厂的最短步数 n = len(factory) m = len(robot) f = [0] + [inf] * m # 前i个工厂修复j个机器人的最小步数 for pos, limit in factory: # 倒着填前i个工厂维修(m-&gt;1]个机器人的属性 for j in range(m,0,-1): t = 0 # 划分集合 修理1个修理2个修理3个到极限, 为什么没有不修: 压缩后直接从上一个状态转移过来了 f[j] = f[j]会增加代码的复杂度 for k in range(1,min(j,limit) + 1): t += abs(pos - robot[j-k]) f[j] = min(f[j],f[j-k]+t) return f[-1] 周赛317\\2456. 最流行的视频创作者给你两个字符串数组 creators 和 ids ，和一个整数数组 views ，所有数组的长度都是 n 。平台上第 i 个视频者是 creator[i] ，视频分配的 id 是 ids[i] ，且播放量为 views[i] 。 视频创作者的 流行度 是该创作者的 所有 视频的播放量的 总和 。请找出流行度 最高 创作者以及该创作者播放量 最大 的视频的 id 。 如果存在多个创作者流行度都最高，则需要找出所有符合条件的创作者。 如果某个创作者存在多个播放量最高的视频，则只需要找出字典序最小的 id 。 返回一个二维字符串数组 answer ，其中 answer[i] = [creatori, idi] 表示 creatori 的流行度 最高 且其最流行的视频 id 是 idi ，可以按任何顺序返回该结果。 12345678910111213141516171819202122class Solution: def mostPopularCreator(self, creators: List[str], ids: List[str], views: List[int]) -&gt; List[List[str]]: # 统计作者对应的播放量和 播放量最大的id # 记录一个全局最大 m = &#123;&#125; # name : [view_sum, max_view, id] max_view_sum = 0 for name, i, view in zip(creators, ids, views): if name in m: t = m[name] t[0] += view if view &gt; t[1] or view == t[1] and i &lt; t[2]: t[1], t[2] = view, i else: m[name] = [view, view, i] max_view_sum = max(max_view_sum, m[name][0]) ans = [] for name ,(view_sum, _, i) in m.items(): if view_sum == max_view_sum: ans.append([name,i]) return ans 周赛98替换一个数字后的最大差值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Solution &#123; public int minMaxDifference(int num) &#123; // 将最高位换成9 换成0 相减 Integer max = h1(num); Integer min = h2(num); return max - min; &#125; private Integer h2(Integer x) &#123; // 最高位肯定不是0 char[] s = x.toString().toCharArray(); int n = s.length; char start = s[0]; for (int i = 0; i &lt; n; i++) &#123; if (s[i] == start) &#123; s[i] = &#x27;0&#x27;; &#125; &#125; StringBuilder sb = new StringBuilder(); for (char a : s) &#123; sb.append(a); &#125; return Integer.parseInt(sb.toString()); &#125; public int h1(Integer x) &#123; char[] s = x.toString().toCharArray(); int n = s.length; // 最高位不一定不是9 char start = &#x27;a&#x27;; for (int i = 0; i &lt; n; i++) &#123; if (s[i] == &#x27;9&#x27;) continue; else &#123; start = s[i]; break; &#125; &#125; if (start == &#x27;a&#x27;) return x; for (int i = 0; i &lt; n; i++) &#123; if (s[i] == start) &#123; s[i] = &#x27;9&#x27;; &#125; &#125; StringBuilder sb = new StringBuilder(); for (char a : s) &#123; sb.append(a); &#125; return Integer.parseInt(sb.toString()); &#125;&#125; \\6360. 最小无法得到的或值\\6361. 修改两个元素的最小分数\\6358. 更新数组后处理求和查询不会做","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"7刷题笔记","slug":"算法/7刷题笔记","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"力扣","slug":"力扣","permalink":"https://gouguoqiang.github.io/tags/%E5%8A%9B%E6%89%A3/"},{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"杂记","slug":"杂记","permalink":"https://gouguoqiang.github.io/tags/%E6%9D%82%E8%AE%B0/"}]},{"title":"根据零神rating刷题","slug":"算法/7刷题笔记/zeroRating","date":"2022-09-02T03:51:56.000Z","updated":"2023-02-28T10:15:15.915Z","comments":true,"path":"2022/09/02/算法/7刷题笔记/zeroRating/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/zeroRating/","excerpt":"","text":"2023. 2.15日 1607- 1709\\1864. 构成交替字符串需要的最小交换次数 1600给你一个二进制字符串 s ，现需要将其转化为一个 交替字符串 。请你计算并返回转化所需的 最小 字符交换次数，如果无法完成转化，返回 -1 。 交替字符串 是指：相邻字符之间不存在相等情况的字符串。例如，字符串 &quot;010&quot; 和 &quot;1010&quot; 属于交替字符串，但 &quot;0100&quot; 不是。 任意两个字符都可以进行交换，不必相邻 。 示例 1： 123输入：s = &quot;111000&quot;输出：1解释：交换位置 1 和 4：&quot;111000&quot; -&gt; &quot;101010&quot; ，字符串变为交替字符串。 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public int minSwaps(String s) &#123; // 每一次操作减少两个 必须是偶数 且01数量够 char[] carr = s.toCharArray(); int n = carr.length; // 统计两种开头 位置不对的数量 int start0 = 0; // 010101 i % 2 == 1 int start1 = 0; // 101010 i % 2 == 1 == 0 start0 ++; int cnt0 = 0; int cnt1 = 0; for (int i = 1; i &lt;= n; i++) &#123; if (carr[i-1] == &#x27;0&#x27;) &#123; cnt0 ++; &#125; else &#123; cnt1 ++; &#125; if (i % 2 == 1) &#123; if (carr[i-1] == &#x27;1&#x27;) &#123; start0 ++; &#125; else &#123; // System.out.println(&quot;-&quot;); start1 ++; &#125; &#125; else &#123; if (carr[i-1] == &#x27;1&#x27;) &#123; // System.out.println(&quot;==&quot;); start1 ++; &#125; else &#123; start0 ++; &#125; &#125; &#125; if (n % 2 == 0 &amp;&amp; (cnt0 != cnt1)) return -1; if (n % 2 == 1 &amp;&amp; Math.abs(cnt0 - cnt1) != 1) return -1; // System.out.println(start0 + &quot; &quot; + start1); if (start0 % 2 != 0 &amp;&amp; start1 % 2 != 0) return -1; if (start0 % 2 != 0) return start1 / 2; if (start1 % 2 != 0) return start0 / 2; return Math.min(start0 / 2, start1 / 2); &#125;&#125; \\962. 最大宽度坡 1607 #状态机分析给定一个整数数组 A，坡是元组 (i, j)，其中 i &lt; j 且 A[i] &lt;= A[j]。这样的坡的宽度为 j - i。 找出 A 中的坡的最大宽度，如果不存在，返回 0 。 示例 1： 1234输入：[6,0,8,2,1,5]输出：4解释：最大宽度的坡为 (i, j) = (1, 5): A[1] = 0 且 A[5] = 5. 示例 2： 1234输入：[9,8,1,0,1,9,4,0,4,1]输出：7解释：最大宽度的坡为 (i, j) = (2, 9): A[2] = 1 且 A[9] = 1. 提示： 2 &lt;= A.length &lt;= 50000 0 &lt;= A[i] &lt;= 50000 思路一 二分优化查找12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public int maxWidthRamp(int[] nums) &#123; // 优化: 找出第一个比ai小的 aj res = max(res,i - j) int n = nums.length; int[] q = new int[n]; int tail = 0; int res = 0; for (int i = 0; i &lt; n; i++) &#123; // 单调队列 存储下标 单调性: 6 0 (8 , 0 不需要的8 0 ) if (i == 0) q[tail++] = i; else &#123; int l = b_search(nums[i],0,tail-1,q,nums); if (l != 0x3f3f3f3f) res = Math.max(res,i - q[l]); if (nums[q[tail-1]] &gt; nums[i]) &#123; q[tail++] = i; &#125; &#125; &#125; return res; &#125; public int b_search(int target, int i, int j, int[] q, int[] nums) &#123; //找到&lt;= a[i] 的左边界; // 从大到小 5 4 3 2 1 小于等于2 int l = i, r = j; while (l &lt;= r) &#123; int mid = l + (r - l) / 2; if (nums[q[mid]] &lt;= target) &#123; r = mid - 1; &#125; else &#123; l = mid + 1; &#125; &#125; if (l &gt; j) return 0x3f3f3f3f; return l; &#125;&#125; 思路二 排序后 股票买卖1思路三 单调栈 贪心\\829. 连续整数求和 1694 #设x 将未知变量表示出来 增加条件给定一个正整数 n，返回 连续正整数满足所有数字之和为 n 的组数 。 示****例 1: 123输入: n = 5输出: 2解释: 5 = 2 + 3，共有两组连续整数([5],[2,3])求和后为 5。 示例 2: 123输入: n = 9输出: 3解释: 9 = 4 + 5 = 2 + 3 + 4 示例 3: 123输入: n = 15输出: 4解释: 15 = 8 + 7 = 4 + 5 + 6 = 1 + 2 + 3 + 4 + 5 提示: 1 &lt;= n &lt;= 109 思路: 设x 将未知变量表示出来123456789101112131415161718192021222324252627282930class Solution &#123; public int consecutiveNumbersSum(int n) &#123; // 数学推导 int count = 0; for (int w = 1; w * w &lt;= 2 * n + 1; w++) &#123; if (bsearch(n,w)) &#123; count++; &#125; &#125; return count; &#125; // long w public boolean bsearch(int n, int w) &#123; long nn = (long)n; int l = 0, r = n; while (l &lt;= r) &#123; int m = l + (r - l) / 2; // 这里的 w转为long是为了 防止溢出 如果 int 是先以int算在转为long long sum = (m + m + w - 1) * (long)w; if (sum == 2 * nn) &#123; return true; &#125; else if (sum &gt; 2 * nn) &#123; r = m - 1; &#125; else &#123; l = m + 1; &#125; &#125; return false; &#125;&#125; \\870. 优势洗牌 1648 # 不能改变顺序的 将idx按从小到排序给定两个大小相等的数组 nums1 和 nums2，nums1 相对于 nums2 的优势可以用满足 nums1[i] &gt; nums2[i] 的索引 i 的数目来描述。 返回 nums1 的任意排列，使其相对于 nums2 的优势最大化。 示例 1： 12输入：nums1 = [2,7,11,15], nums2 = [1,10,4,11]输出：[2,11,7,15] 示例 2： 12输入：nums1 = [12,24,8,32], nums2 = [13,25,32,11]输出：[24,32,8,12] 提示： 1 &lt;= nums1.length &lt;= 105 nums2.length == nums1.length 0 &lt;= nums1[i], nums2[i] &lt;= 109 12345678910111213141516171819202122232425class Solution &#123; public int[] advantageCount(int[] a, int[] nums2) &#123; // 田忌赛马 nums2 不变 贪心 对nums2每张牌 选取大于他的最小值 // 都排序后 对于 每个首项 1 &gt; 2 则 优势加一 并将结果保存 // 否则 1的这张牌不能作为 2的任意优势牌 放在从后往前的right int n = a.length; int[] ans = new int[n]; Integer[] idx = new Integer[n]; for (int i = 0; i &lt; n; i++) &#123; idx[i] = i; &#125; Arrays.sort(a); Arrays.sort(idx, (i, j) -&gt; nums2[i] - nums2[j]); int l = 0, r = n - 1; for (int i = 0; i &lt; n; i++) &#123; if (a[i] &gt; nums2[idx[l]]) ans[idx[l++]] = a[i]; else &#123; ans[idx[r--]] = a[i]; &#125; &#125; return ans; &#125;&#125; \\2196. 根据描述创建二叉树 1643给你一个二维整数数组 descriptions ，其中 descriptions[i] = [parenti, childi, isLefti] 表示 parenti 是 childi 在 二叉树 中的 父节点，二叉树中各节点的值 互不相同 。此外： 如果 isLefti == 1 ，那么 childi 就是 parenti 的左子节点。 如果 isLefti == 0 ，那么 childi 就是 parenti 的右子节点。 请你根据 descriptions 的描述来构造二叉树并返回其 根节点 。 测试用例会保证可以构造出 有效 的二叉树。 示例 1： 1234输入：descriptions = [[20,15,1],[20,17,0],[50,20,1],[50,80,0],[80,19,1]]输出：[50,20,80,15,17,19]解释：根节点是值为 50 的节点，因为它没有父节点。结果二叉树如上图所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode() &#123;&#125; * TreeNode(int val) &#123; this.val = val; &#125; * TreeNode(int val, TreeNode left, TreeNode right) &#123; * this.val = val; * this.left = left; * this.right = right; * &#125; * &#125; */class Solution &#123; public TreeNode createBinaryTree(int[][] descriptions) &#123; TreeNode root = new TreeNode(); Map&lt;Integer,TreeNode&gt; map = new HashMap(); int n = descriptions.length; int[] isRoot = new int[100001]; for (int[] node : descriptions) &#123; int p = node[0]; int c = node[1]; int isL = node[2]; TreeNode parent = map.get(p) == null ? new TreeNode(p): map.get(p); TreeNode child = map.get(c) == null ? new TreeNode(c): map.get(c); isRoot[c] = 1; map.put(p,parent); map.put(c,child); if (isL == 1) &#123; parent.left = child; &#125; else &#123; parent.right = child; &#125; &#125; for (Integer val : map.keySet()) &#123; if (isRoot[val] == 0) &#123; return map.get(val); &#125; &#125; return null; &#125;&#125; \\792. 匹配子序列的单词数 1695 #多单词子序列匹配给定字符串 s 和字符串数组 words, 返回 words[i] 中是s的子序列的单词个数 。 字符串的 子序列 是从原始字符串中生成的新字符串，可以从中删去一些字符(可以是none)，而不改变其余字符的相对顺序。 例如， “ace” 是 “abcde” 的子序列。 示例 1: 123输入: s = &quot;abcde&quot;, words = [&quot;a&quot;,&quot;bb&quot;,&quot;acd&quot;,&quot;ace&quot;]输出: 3解释: 有三个是 s 的子序列的单词: &quot;a&quot;, &quot;acd&quot;, &quot;ace&quot;。 Example 2: 12输入: s = &quot;dsahjpjauf&quot;, words = [&quot;ahjpjau&quot;,&quot;ja&quot;,&quot;ahbwzgqnuk&quot;,&quot;tnmlanowax&quot;]输出: 2 提示: 1 &lt;= s.length &lt;= 5 * 104 1 &lt;= words.length &lt;= 5000 1 &lt;= words[i].length &lt;= 50 words[i]和 s 都只由小写字母组成。 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public int numMatchingSubseq(String s, String[] words) &#123; // 多个匹配 遍历s 没字符 去判断每个word能不能匹配 // 5 * 1e4 * 5e3 还需将 words[i]分组优化 // System.out.println(5e3); int n = words.length; int ans = 0; Deque&lt;String&gt;[] w = new Deque[26]; for (int i = 0; i &lt; 26; i++) &#123; w[i] = new ArrayDeque&lt;&gt;(); &#125; for (int i = 0; i &lt; n; i++) &#123; w[words[i].charAt(0) - &#x27;a&#x27;].addLast(words[i]); &#125; for (int i = 0; i &lt; s.length(); i++) &#123; char c = s.charAt(i); Deque&lt;String&gt; q = w[c - &#x27;a&#x27;]; if (q.isEmpty()) continue; int sz = q.size(); for (int j = 0; j &lt; sz; j++) &#123; String ss = q.removeFirst().substring(1); if (ss.length() == 0) &#123; ans ++; continue; &#125; else &#123; w[ss.charAt(0) - &#x27;a&#x27;].addLast(ss); &#125; &#125; &#125; return ans; &#125;&#125; \\979. 在二叉树中分配硬币 1709 ##边贡献问题 1答案要加上子树的每天边的贡献值, 累加起来即可 1234567891011121314151617181920class Solution &#123; int[] ans = new int[3]; public int distributeCoins(TreeNode root) &#123; // 思路 边贡献: 对每个顶点计算 使该子树的除了root每个结点都为1的 负载量 = (子树硬币数 - 当前子树结点数 ) 后序遍历 // 算法合理性 对于一个 只有三个节点的子树 可以单独算每条边的贡献度 // 类似dp具有 return dfs(root)[2]; &#125; // 算负载值 public int[] dfs(TreeNode root) &#123; if (root == null) return new int[]&#123;0,0,0&#125;; int[] l = dfs(root.left); int[] r = dfs(root.right); int x = l[0] + r[0] + 1; int y = l[1] + r[1] + root.val; return new int[]&#123;x,y,(int)Math.abs(x - y) + l[2] + r[2]&#125;; &#125;&#125; 2023&#x2F;2&#x2F;19 1653-1680\\1155. 掷骰子的N种方法 1653 #分组背包这里有 n 个一样的骰子，每个骰子上都有 k 个面，分别标号为 1 到 k 。 给定三个整数 n , k 和 target ，返回可能的方式(从总共 kn 种方式中)滚动骰子的数量，使正面朝上的数字之和等于 target 。 答案可能很大，你需要对 109 + 7 取模 。 示例 1： 1234输入：n = 1, k = 6, target = 3输出：1解释：你扔一个有6张脸的骰子。得到3的和只有一种方法。 示例 2： 1234输入：n = 2, k = 6, target = 7输出：6解释：你扔两个骰子，每个骰子有6个面。得到7的和有6种方法1+6 2+5 3+4 4+3 5+2 6+1。 示例 3： 123输入：n = 30, k = 30, target = 500输出：222616187解释：返回的结果必须是对 109 + 7 取模。 提示： 1 &lt;= n, k &lt;= 30 1 &lt;= target &lt;= 1000 12345678910111213141516171819202122232425262728293031class Solution &#123; public int ans = 0; public int numRollsToTarget(int n, int k, int target) &#123; // n dice k faces dfs 超时吧 剪枝 怎么剪 ❌ // 分组背包 的方案数 // f[j] 前i组物品 选 1..6 装满容量恰好j的最大值 // g[j] 方案数 用1个即可吧 int mod = (int)1e9 + 7; int[] f = new int[target+1]; f[0] = 1; for (int i = 0; i &lt; n; i++) &#123; for (int j = target; j &gt;= 0; j--) &#123; f[j] = 0; // 至少选一个 把不选的去掉 for(int s = 1; s &lt;= k; s++) &#123; if (j &lt; s) continue; f[j] = (f[j] + f[j - s]) % mod; // System.out.println(s + &quot; &quot; + j + &quot; &quot; + f[j]); &#125; &#125; // System.out.println(i + &quot; &quot; + &quot; &quot; + f[target]); &#125; return f[target]; &#125;&#125; \\1339. 分裂二叉树的最大乘积 1674123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for a binary tree node. * public class TreeNode &#123; * int val; * TreeNode left; * TreeNode right; * TreeNode() &#123;&#125; * TreeNode(int val) &#123; this.val = val; &#125; * TreeNode(int val, TreeNode left, TreeNode right) &#123; * this.val = val; * this.left = left; * this.right = right; * &#125; * &#125; */class Solution &#123; public int sum; public int mod = (int)1e9 + 7; public int maxProduct(TreeNode root) &#123; // sum - kroot sum List&lt;Integer&gt; l = new ArrayList&lt;&gt;(); dfs(root,l); long ans = 0; int sz = l.size(); for (int i = 0; i &lt; sz; i++) &#123; // 居然不能再 max里 取模 ans = Math.max((sum - l.get(i)) * (long)l.get(i), ans); // System.out.println(sum + &quot; &quot; + l.get(i) + &quot; &quot; + ans); &#125; return (int)(ans % mod); &#125; public int dfs(TreeNode root, List&lt;Integer&gt; l) &#123; if (root == null) return 0; int res = dfs(root.left,l) + dfs(root.right,l) + root.val; l.add(res); sum = res; return res; &#125;&#125; \\1254. 统计封闭岛屿的数目1658123456789101112131415161718192021222324252627282930313233class Solution &#123; public int n, m; public int closedIsland(int[][] grid) &#123; // closed island 没有越界的陆地 // int ans = 0; n = grid.length; m = grid[0].length; for (int i = 0; i &lt; n; i++) &#123; for(int j = 0; j &lt; m; j++) &#123; if (grid[i][j] == 0) &#123; if (dfs(i,j,grid)) ans++; &#125; &#125; &#125; return ans; &#125; public boolean dfs(int x, int y, int[][] grid) &#123; grid[x][y] = 1; int[] dx = new int[]&#123;-1,0,1,0&#125;, dy = new int[]&#123;0,1,0,-1&#125;; boolean res = true; for (int i = 0; i &lt; 4; i++) &#123; int a = x + dx[i], b = y + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) &#123; res = false; continue; &#125; if (grid[a][b] == 1) continue; res &amp;= dfs(a,b,grid); &#125; return res; &#125;&#125; \\2457. 美丽整数的最小增量 1680 #整数进位操作给你两个正整数 n 和 target 。 如果某个整数每一位上的数字相加小于或等于 target ，则认为这个整数是一个 美丽整数 。 找出并返回满足 n + x 是 美丽整数 的最小非负整数 x 。生成的输入保证总可以使 n 变成一个美丽整数。 123456789101112131415161718192021class Solution &#123; public long makeIntegerBeautiful(long n, int target) &#123; // 从后往前 置0 long tail = 1; while (true) &#123; // 数学题 进位操作 tail 每 ＊ 10 long m = n + (tail - (n % tail)) % tail; long t = m; int cur = 0; while(t != 0) &#123; cur += t % 10; t /= 10; &#125; if (cur &lt;= target) return m - n; tail *= 10; &#125; &#125;&#125; 2023&#x2F;2&#x2F;20 1709\\2140. 解决智力问题 1709 ##线性dp与状态dp的区分好题给你一个下标从 0 开始的二维整数数组 questions ，其中 questions[i] = [pointsi, brainpoweri] 。 这个数组表示一场考试里的一系列题目，你需要 按顺序 （也就是从问题 0 开始依次解决），针对每个问题选择 解决 或者 跳过 操作。解决问题 i 将让你 获得 pointsi 的分数，但是你将 无法 解决接下来的 brainpoweri 个问题（即只能跳过接下来的 brainpoweri 个问题）。如果你跳过问题 i ，你可以对下一个问题决定使用哪种操作。 比方说，给你 1questions = [[3, 2], [4, 3], [4, 4], [2, 5]] ： 如果问题 0 被解决了， 那么你可以获得 3 分，但你不能解决问题 1 和 2 。 如果你跳过问题 0 ，且解决问题 1 ，你将获得 4 分但是不能解决问题 2 和 3 。 请你返回这场考试里你能获得的 最高 分数。 示例 1： 1234567输入：questions = [[3,2],[4,3],[4,4],[2,5]]输出：5解释：解决问题 0 和 3 得到最高分。- 解决问题 0 ：获得 3 分，但接下来 2 个问题都不能解决。- 不能解决问题 1 和 2- 解决问题 3 ：获得 2 分总得分为：3 + 2 = 5 。没有别的办法获得 5 分或者多于 5 分。 示例 2： 12345678输入：questions = [[1,1],[2,2],[3,3],[4,4],[5,5]]输出：7解释：解决问题 1 和 4 得到最高分。- 跳过问题 0- 解决问题 1 ：获得 2 分，但接下来 2 个问题都不能解决。- 不能解决问题 2 和 3- 解决问题 4 ：获得 5 分总得分为：2 + 5 = 7 。没有别的办法获得 7 分或者多于 7 分。 提示： 1 &lt;= questions.length &lt;= 105 questions[i].length == 2 1 &lt;= pointsi, brainpoweri &lt;= 105 1234567891011121314151617181920212223242526272829// 超时线性dpclass Solution &#123; public long mostPoints(int[][] questions) &#123; // 状态机 或者 线性dp // f[i] 从0 - i 选择获得第i个问题的分数&gt; 的所有选法的集合 // 属性 max // f[i] = 前面能结尾的所有的max或者都不选 + 第i个分数 n方 10的10次方不符合 // 从后往前考虑 f[i] 表示选第i个题 从后往前的选法 int n = questions.length; int[] f = new int[n]; f[0] = questions[0][0]; int ans = f[0]; for (int i = 1; i &lt; n; i++) &#123; for (int j = 0; j &lt;= i; j++) &#123; // 要优化 int d = i - j; if (questions[j][1] &gt;= d) &#123; f[i] = Math.max(f[i], questions[i][0]); continue; &#125; f[i] = Math.max(f[i],f[j] + questions[i][0]); &#125; ans = Math.max(ans,f[i]); // System.out.println(i + &quot; &quot; + f[i]); &#125; return ans; &#125;&#125; 123456789101112131415161718192021222324// 正解状态机划分清除状态class Solution &#123; public long mostPoints(int[][] questions) &#123; // 状态机划分 将 选i 与不选i划分开来 // f[i] 表示 从i到最后的里的所有选法 的 max // 选第i个题 与不选第i个题的划分 int n = questions.length; long[] f = new long[n+1]; // f[n-1] = questions[n-1][0]; long ans = 0; for (int i = n-1; i &gt;= 0; i--) &#123; f[i] = questions[i][0]; int next = i + questions[i][1] + 1; // 线性选法不一定是next 而是合条件的最大值 状态机的定义则必定是next f[i] = Math.max(f[i+1], f[Math.min(n,next)] + f[i]); ans = Math.max(ans,f[i]); // System.out.println(i + &quot; &quot; + f[i]); &#125; return ans; &#125;&#125; \\901. 股票价格跨度 1708设计一个算法收集某些股票的每日报价，并返回该股票当日价格的 跨度 。 当日股票价格的 跨度 被定义为股票价格小于或等于今天价格的最大连续日数（从今天开始往回数，包括今天）。 例如，如果未来 7 天股票的价格是 [100,80,60,70,60,75,85]，那么股票跨度将是 [1,1,1,2,1,4,6] 。 实现 StockSpanner 类： StockSpanner() 初始化类对象。 int next(int price) 给出今天的股价 price ，返回该股票当日价格的 跨度 。 示例： 123456789101112131415输入：[&quot;StockSpanner&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;, &quot;next&quot;][[], [100], [80], [60], [70], [60], [75], [85]]输出：[null, 1, 1, 1, 2, 1, 4, 6]解释：StockSpanner stockSpanner = new StockSpanner();stockSpanner.next(100); // 返回 1stockSpanner.next(80); // 返回 1stockSpanner.next(60); // 返回 1stockSpanner.next(70); // 返回 2stockSpanner.next(60); // 返回 1stockSpanner.next(75); // 返回 4 ，因为截至今天的最后 4 个股价 (包括今天的股价 75) 都小于或等于今天的股价。stockSpanner.next(85); // 返回 6 提示： 1 &lt;= price &lt;= 105 最多调用 next 方法 104 次 12345678910111213141516171819202122232425262728class StockSpanner &#123; // 对 第k个来说 求右往左第一个大于本身的坐标 Deque&lt;Integer&gt; stack; int idx; List&lt;Integer&gt; arr; public StockSpanner() &#123; stack = new ArrayDeque&lt;&gt;(); idx = 0; arr = new ArrayList&lt;&gt;(); &#125; public int next(int price) &#123; arr.add(price); while(!stack.isEmpty() &amp;&amp; arr.get(stack.peekLast()) &lt;= price)&#123; stack.removeLast(); &#125; int ans = stack.isEmpty() ? idx + 1 : idx - stack.peekLast(); stack.addLast(idx); idx++; return ans; &#125;&#125;/** * Your StockSpanner object will be instantiated and called as such: * StockSpanner obj = new StockSpanner(); * int param_1 = obj.next(price); */ 2023&#x2F;2&#x2F;21 1700_1710圆和矩阵是否重叠 1708 1234567891011class Solution &#123; public boolean checkOverlap(int r, int a, int b, int x1, int y1, int x2, int y2) &#123; if (d(a,b,x1,y1) &lt;= r || d(a,b,x1,y2) &lt;= r || d(a,b,x2,y1) &lt;= r || d(a,b,x2,y2) &lt;= r) return true; if (a &gt;= x1 - r &amp;&amp; a &lt;= x2 + r &amp;&amp; b &gt;= y1 &amp;&amp; b &lt;= y2) return true; if (a &gt;= x1 &amp;&amp; a &lt;= x2 &amp;&amp; b &gt;= y1 - r &amp;&amp; b &lt;= y2 + r) return true; return false; &#125; public int d (int x, int y, int a, int b) &#123; return (int)Math.sqrt((x-a) * (x-a) + (y-b) * (y-b)); &#125;&#125; \\826. 安排工作以达到最大收益你有 n 个工作和 m 个工人。给定三个数组： difficulty, profit 和 worker ，其中: difficulty[i] 表示第 i 个工作的难度，profit[i] 表示第 i 个工作的收益。 worker[i] 是第 i 个工人的能力，即该工人只能完成难度小于等于 worker[i] 的工作。 每个工人 最多 只能安排 一个 工作，但是一个工作可以 完成多次 。 举个例子，如果 3 个工人都尝试完成一份报酬为 $1 的同样工作，那么总收益为 $3 。如果一个工人不能完成任何工作，他的收益为 $0 。 返回 在把工人分配到工作岗位后，我们所能获得的最大利润 。 12345678910111213141516171819202122232425import java.awt.Point;class Solution &#123; public int maxProfitAssignment(int[] d, int[] p, int[] w) &#123; // 思路 排序好一遍遍历 // 工人按能力排序 // 工作按难度排序 双指针+ 记录以前的best收益即可: int n = d.length; Point[] jobs = new Point[n]; for (int i = 0; i &lt; n; i++) &#123; jobs[i] = new Point(d[i],p[i]); &#125; Arrays.sort(w); Arrays.sort(jobs, (a, b) -&gt; a.x - b.x); int i = 0, best = 0, ans = 0; for (int a : w) &#123; while (i &lt; n &amp;&amp; a &gt;= jobs[i].x) &#123; best = Math.max(best, jobs[i++].y); &#125; ans += best; &#125; return ans; &#125;&#125; \\2100. 适合打劫银行的日子你和一群强盗准备打劫银行。给你一个下标从 0 开始的整数数组 security ，其中 security[i] 是第 i 天执勤警卫的数量。日子从 0 开始编号。同时给你一个整数 time 。 如果第 i 天满足以下所有条件，我们称它为一个适合打劫银行的日子： 第 i 天前和后都分别至少有 time 天。 第 i 天前连续 time 天警卫数目都是非递增的。 第 i 天后连续 time 天警卫数目都是非递减的。 更正式的，第 i 天是一个合适打劫银行的日子当且仅当：security[i - time] &gt;= security[i - time + 1] &gt;= ... &gt;= security[i] &lt;= ... &lt;= security[i + time - 1] &lt;= security[i + time]. 请你返回一个数组，包含 所有 适合打劫银行的日子（下标从 0 开始）。返回的日子可以 任意 顺序排列。 示例 1： 123456输入：security = [5,3,3,3,5,6,2], time = 2输出：[2,3]解释：第 2 天，我们有 security[0] &gt;= security[1] &gt;= security[2] &lt;= security[3] &lt;= security[4] 。第 3 天，我们有 security[1] &gt;= security[2] &gt;= security[3] &lt;= security[4] &lt;= security[5] 。没有其他日子符合这个条件，所以日子 2 和 3 是适合打劫银行的日子。 示例 2： 1234输入：security = [1,1,1,1,1], time = 0输出：[0,1,2,3,4]解释：因为 time 等于 0 ，所以每一天都是适合打劫银行的日子，所以返回每一天。 示例 3： 12345输入：security = [1,2,3,4,5,6], time = 2输出：[]解释：没有任何一天的前 2 天警卫数目是非递增的。所以没有适合打劫银行的日子，返回空数组。 提示： 1 &lt;= security.length &lt;= 105 0 &lt;= security[i], time &lt;= 105 123456789101112131415161718192021222324class Solution &#123; public List&lt;Integer&gt; goodDaysToRobBank(int[] s, int t) &#123; // 对天k 判断前time天 和后time天 // 预处理优化: time固定 从前往后 从后往前记录 int n = s.length; int[] f = new int[n]; // 记录以i为尾满足条件的连续天数 int[] g = new int[n]; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); f[0] = 1; for (int i = 1; i &lt; n; i++) &#123; f[i] = s[i - 1] &gt;= s[i] ? f[i - 1] + 1: 1; &#125; g[n-1] = 1; for (int i = n - 2; i &gt;= 0; i--) &#123; g[i] = s[i+1] &gt;= s[i] ? g[i + 1] + 1: 1; &#125; for (int i = 0; i &lt; n; i++) &#123; if(f[i] &gt; t &amp;&amp; g[i] &gt; t) ans.add(i); &#125; return ans; &#125;&#125; \\1054. 距离相等的条形码在一个仓库里，有一排条形码，其中第 i 个条形码为 barcodes[i]。 请你重新排列这些条形码，使其中任意两个相邻的条形码不能相等。 你可以返回任何满足该要求的答案，此题保证存在答案。 12345678910111213141516171819202122232425262728293031323334353637class Solution &#123; public int[] rearrangeBarcodes(int[] b) &#123; // 频次贪心 安排 int n = b.length; Map&lt;Integer,Integer&gt; map = new HashMap(); for (int a: b) &#123; map.put(a,map.getOrDefault(a,0) + 1); &#125; PriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a, c) -&gt; &#123; return c[0] - a[0]; &#125;); for (int key: map.keySet()) &#123; pq.offer(new int[]&#123;map.get(key),key&#125;); &#125; for (int i = 0; i &lt; n; i++) &#123; int[] cur = pq.poll(); b[i++] = cur[1]; cur[0] --; boolean flag = false; int[] next = null; if (!pq.isEmpty()) &#123; next = pq.poll(); b[i] = next[1]; next[0] --; flag = true; &#125; if (cur[0] &gt; 0) pq.offer(cur); if (flag &amp;&amp; next[0] &gt; 0) pq.offer(next); &#125; return b; &#125; &#125; 2023&#x2F;2&#x2F;22 1710_1720\\2017. 网格游戏 1718 问题抽象给你一个下标从 0 开始的二维数组 grid ，数组大小为 2 x n ，其中 grid[r][c] 表示矩阵中 (r, c) 位置上的点数。现在有两个机器人正在矩阵上参与一场游戏。 两个机器人初始位置都是 (0, 0) ，目标位置是 (1, n-1) 。每个机器人只会 向右 ((r, c) 到 (r, c + 1)) 或 向下 ((r, c) 到 (r + 1, c)) 。 游戏开始，第一个 机器人从 (0, 0) 移动到 (1, n-1) ，并收集路径上单元格的全部点数。对于路径上所有单元格 (r, c) ，途经后 grid[r][c] 会重置为 0 。然后，第二个 机器人从 (0, 0) 移动到 (1, n-1) ，同样收集路径上单元的全部点数。注意，它们的路径可能会存在相交的部分。 第一个 机器人想要打击竞争对手，使 第二个 机器人收集到的点数 最小化 。与此相对，第二个 机器人想要 最大化 自己收集到的点数。两个机器人都发挥出自己的 最佳水平 的前提下，返回 第二个 机器人收集到的 点数 。 示例 1： 12345输入：grid = [[2,5,4],[1,5,1]]输出：4解释：第一个机器人的最佳路径如红色所示，第二个机器人的最佳路径如蓝色所示。第一个机器人访问过的单元格将会重置为 0 。第二个机器人将会收集到 0 + 0 + 4 + 0 = 4 个点。 示例 2： 12345输入：grid = [[3,3,1],[8,5,2]]输出：4解释：第一个机器人的最佳路径如红色所示，第二个机器人的最佳路径如蓝色所示。 第一个机器人访问过的单元格将会重置为 0 。第二个机器人将会收集到 0 + 3 + 1 + 0 = 4 个点。 提示： grid.length == 2 n == grid[r].length 1 &lt;= n &lt;= 5 * 104 1 &lt;= grid[r][c] &lt;= 105 1234567891011121314151617class Solution &#123; public long gridGame(int[][] grid) &#123; // 2 * n int n = grid[0].length; long[] s1 = new long[n+1]; long[] s2 = new long[n+1]; for (int i = 1; i &lt;= n; i++) &#123; s1[i] = s1[i-1] + grid[0][i-1]; s2[i] = s2[i-1] + grid[1][i-1]; &#125; long ans = Long.MAX_VALUE; for(int i = 0; i &lt; n; i++) &#123; ans = Math.min(ans,Math.max(s2[i],s1[n] - s1[i+1])); &#125; return ans; &#125;&#125; \\1239. 串联字符串的最大长度 1719 dfs子集123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 比较暴力了class Solution &#123; int ans = 0; public int maxLength(List&lt;String&gt; arr) &#123; int n = arr.size(); int[] mark = new int[26]; dfs(0,arr,mark,n, 0); return ans; &#125; public void dfs(int u, List&lt;String&gt; arr, int[] mark, int n, int len) &#123; if (u == n) &#123; ans = Math.max(ans, len); return ; &#125; // dfs剪枝 Set&lt;Character&gt; set = new HashSet&lt;&gt;(); char[] s = arr.get(u).toCharArray(); boolean flag = false; for(char a: s) &#123; if (mark[a - &#x27;a&#x27;] &gt; 0) &#123; flag = true; break; &#125; if (set.contains(a)) &#123; flag = true; break; &#125; set.add(a); &#125; // 剪枝 if (flag) &#123; dfs(u+1,arr,mark,n,len); return ; &#125; for (char a: s) &#123; mark[a - &#x27;a&#x27;] ++; &#125; dfs(u+1,arr,mark,n,len+s.length); for (char a: s) &#123; mark[a - &#x27;a&#x27;] --; &#125; // 不选第i dfs(u+1, arr, mark, n, len); &#125;&#125; 2023&#x2F;2&#x2F;23 1710_1720\\1567. 乘积为正数的最长子数组长度 1710123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public int getMaxLen(int[] nums) &#123; // 对第 k个数而言 如果为正 正max[k-1] ) // 为负 找负min[k-1] int n = nums.length; int[] f = new int[n]; // f[i] 以第i个为尾的 为正的长度 int[] g = new int[n]; // 为负的长度 int ans = 0; if (nums[0] &gt; 0) &#123; f[0] = 1; ans ++; &#125; else if (nums[0] &lt; 0) &#123; g[0] = 1; &#125; for (int i = 1; i &lt; n; i++) &#123; if (nums[i] == 0) continue; if (nums[i] &gt; 0) &#123; if (f[i-1] != 0) &#123; f[i] = f[i-1] + 1; &#125; else &#123; f[i] = 1; &#125; if(g[i-1] != 0) &#123; g[i] = g[i-1] + 1; &#125; &#125; else &#123; if (f[i-1] != 0) &#123; g[i] = f[i-1] + 1; &#125; else &#123; g[i] = 1; &#125; if(g[i-1] != 0) &#123; f[i] = g[i-1] + 1; &#125; &#125; ans = Math.max(ans, f[i]); &#125; return ans; &#125;&#125; \\2359. 找到离给定两个节点最近的节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123; final int INF = 0x3f3f3f3f; List&lt;Integer&gt;[] g; int n; public int closestMeetingNode(int[] edges, int n1, int n2) &#123; // 对两个点 dijkstra n = edges.length; g = new List[n]; for (int i = 0; i &lt; n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int i = 0; i &lt; n; i++) &#123; if (edges[i] &lt; 0) continue ; g[i].add(edges[i]); &#125; int[] d1 = dijkstra(n1); int[] d2 = dijkstra(n2); int than = INF; int ans = -1; for (int i = 0; i &lt; n; i++) &#123; int t = Math.max(d1[i],d2[i]); if (t &lt; than) &#123; ans = i; than = t; &#125; &#125; return ans; &#125; public int[] dijkstra(int start) &#123; int[] d = new int[n]; boolean[] st = new boolean[n]; Arrays.fill(d,INF); d[start] = 0; PriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a,b)-&gt; a[0] - b[0]); pq.offer(new int[]&#123;0,start&#125;); while (!pq.isEmpty()) &#123; int[] min = pq.poll(); int t = min[1]; if (st[t]) continue; st[t] = true; for (int ne: g[t]) &#123; if (d[t] + 1 &lt; d[ne]) &#123; d[ne] = d[t] + 1; pq.offer(new int[]&#123;d[ne], ne&#125;); &#125; &#125; &#125; return d; &#125;&#125; 2023&#x2F;2&#x2F;24 1720_1730\\1220. 统计元音字母序列的数目123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int countVowelPermutation(int n) &#123; //记忆化搜索 dp fa[i] 第i个以a为尾的个数 // a -&gt; e // e -&gt; a i // i -&gt; a e o u // o -&gt; i u // u -&gt; a int mod = (int)1e9 + 7; long[] fa = new long[n]; long[] fe = new long[n]; long[] fi = new long[n]; long[] fo = new long[n]; long[] fu = new long[n]; for (int i = 0; i &lt; n; i++) &#123; if (i == 0) &#123; fa[i] = 1; fe[i] = 1; fi[i] = 1; fo[i] = 1; fu[i] = 1; &#125; else &#123; fa[i] = (fe[i-1] + fi[i-1] + fu[i-1]) % mod; fe[i] = (fi[i-1] + fa[i-1]) % mod; fi[i] = (fe[i-1] + fo[i-1]) % mod; fo[i] = (fi[i-1]) % mod; fu[i] = (fo[i-1] + fi[i-1]) % mod; &#125; // System.out.println(i + &quot; &quot; + fa[i] + &quot; &quot;+ fe[i] +&quot; &quot; + fi[i] + &quot; &quot; + fo[i] + &quot; &quot;+ fu[i]); &#125; return (int)((fa[n-1] + fe[n-1] + fi[n-1] + fo[n-1] + fu[n-1]) % mod); &#125;&#125; \\2070. 每一个查询的最大美丽值给你一个二维整数数组 items ，其中 items[i] = [pricei, beautyi] 分别表示每一个物品的 价格 和 美丽值 。 同时给你一个下标从 0 开始的整数数组 queries 。对于每个查询 queries[j] ，你想求出价格小于等于 queries[j] 的物品中，最大的美丽值 是多少。如果不存在符合条件的物品，那么查询的结果为 0 。 请你返回一个长度与 queries 相同的数组 answer，其中 answer[j]是第 j 个查询的答案。 12345678910111213141516171819202122232425262728class Solution &#123; public int[] maximumBeauty(int[][] items, int[] q) &#123; // 类似安排工人 多了一个下标映射 int n = q.length; int m = items.length; Integer[] idx = new Integer[n]; for (int i = 0; i &lt; n; i++) &#123; idx[i] = i; &#125; Arrays.sort(idx, (a,b) -&gt; q[a] - q[b]); int[] ans = new int[n]; Arrays.sort(items, (a,b) -&gt; a[0] - b[0]); int best = 0, j = 0; for (int i = 0; i &lt; n; i++) &#123; int query = q[idx[i]]; while(j &lt; m &amp;&amp; query &gt;= items[j][0]) &#123; best = Math.max(best, items[j][1]); j++; &#125; ans[idx[i]] = best; &#125; return ans; &#125;&#125; \\1359. 有效的快递序列数目给你 n 笔订单，每笔订单都需要快递服务。 请你统计所有有效的 收件&#x2F;配送 序列的数目，确保第 i 个物品的配送服务 delivery(i) 总是在其收件服务 pickup(i) 之后。 由于答案可能很大，请返回答案对 10^9 + 7 取余的结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public int countOrders(int n) &#123; // p1 (n-1个位置[1, n-1] = x) d1([x+1, n]) // n - 2 同样的 // 15 + 14 + 13 + 12 + 11 10 9 8 7 6 5 4 3 2 1 // 60 + 15 + 45 int mod = (int)1e9 + 7; int t = 2 * n; long last = 1; while (t != 0) &#123; int an = 0; for (int i = 1; i &lt; t; i++) &#123; an += t - i; &#125; if (an != 0) last =(last * an) % mod; t -= 2; &#125; return (int)(last % mod); &#125;&#125;//优化class Solution &#123; public int countOrders(int n) &#123; // p1 (n-1个位置[1, n-1] = x) d1([x+1, n]) // 15 + 14 + 13 + 12 + 11 10 9 8 7 6 5 4 3 2 1 // 60 + 15 + 45 int mod = (int)1e9 + 7; int t = 2; long last = 1; int an = 0; while (t &lt;= 2 * n) &#123; an += (2 * (t - 1) -1); last =(last * an) % mod; // System.out.println(t + &quot; &quot; + last + &quot; &quot; + an); t += 2; &#125; return (int)(last % mod); &#125;&#125; \\1011. 在 D 天内送达包裹的能力传送带上的包裹必须在 days 天内从一个港口运送到另一个港口。 传送带上的第 i 个包裹的重量为 weights[i]。每一天，我们都会按给出重量（weights）的顺序往传送带上装载包裹。我们装载的重量不会超过船的最大运载重量。 返回能在 days 天内将传送带上的所有包裹送达的船的最低运载能力。 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123; public int shipWithinDays(int[] w, int d) &#123; // 设 为 x (0 , sum) &#123;&#125; 二分 int sum = 0; int max = 0; for (int a: w) &#123; max = Math.max(max,a); sum += a; &#125; int l = max, r = sum; // 求 a &gt; d 的左边界 while (l &lt;= r) &#123; int mid = l + (r - l) / 2; // r 越小 装完的天数越多 多了一层映射 if (check(w,mid,d)) &#123; l = mid + 1; &#125; else &#123; r = mid - 1; &#125; &#125; return l; &#125; public boolean check(int[] w, int x, int d) &#123; int n = w.length; int ans = 1; int t = 0; // System.out.println(x); // 滑动窗口 for (int i = 0; i &lt; n; i++) &#123; t += w[i]; if (t &gt; x) &#123; ans++; t = 0; i --; &#125; // System.out.print(i + &quot; &quot; + t + &quot; -- &quot;); &#125; // System.out.println(x + &quot; &quot; + ans); return ans &gt; d; &#125;&#125; \\2416. 字符串的前缀分数和1234567891011121314151617181920212223242526272829303132333435363738394041424344// 每个结点都有26个子节点class Solution &#123; public int[] sumPrefixScores(String[] words) &#123; // 1000 * 1000 // 排序加 字典树 // 对每一个查询 统计 cnt int n = words.length; int[] ans = new int[n]; TreeNode root = new TreeNode(); for (String s: words) &#123; insert(root,s); &#125; for (int i = 0; i &lt; n; i++) &#123; ans[i] = query(root,words[i]); &#125; return ans; &#125; class TreeNode &#123; int cnt; TreeNode[] children = new TreeNode[26]; &#125; public void insert(TreeNode p, String s) &#123; for (char c: s.toCharArray()) &#123; int son = c - &#x27;a&#x27;; if (p.children[son] == null) &#123; p.children[son] = new TreeNode(); &#125; p = p.children[son]; p.cnt ++; &#125; &#125; public int query(TreeNode p, String s) &#123; int ans = 0; for (char c: s.toCharArray()) &#123; int son = c - &#x27;a&#x27;; ans += p.children[son].cnt; p = p.children[son]; &#125; return ans; &#125;&#125; 2023&#x2F;2&#x2F;25\\1593. 拆分字符串使唯一子字符串的数目最大 1739 #dfs拆分给你一个字符串 s ，请你拆分该字符串，并返回拆分后唯一子字符串的最大数目。 字符串 s 拆分后可以得到若干 非空子字符串 ，这些子字符串连接后应当能够还原为原字符串。但是拆分出来的每个子字符串都必须是 唯一的 。 注意：子字符串 是字符串中的一个连续字符序列。 示例 1： 123输入：s = &quot;ababccc&quot;输出：5解释：一种最大拆分方法为 [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;ab&#x27;, &#x27;c&#x27;, &#x27;cc&#x27;] 。像 [&#x27;a&#x27;, &#x27;b&#x27;, &#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;cc&#x27;] 这样拆分不满足题目要求，因为其中的 &#x27;a&#x27; 和 &#x27;b&#x27; 都出现了不止一次。 示例 2： 123输入：s = &quot;aba&quot;输出：2解释：一种最大拆分方法为 [&#x27;a&#x27;, &#x27;ba&#x27;] 。 示例 3： 123输入：s = &quot;aa&quot;输出：1解释：无法进一步拆分字符串。 提示： 1 &lt;= s.length &lt;= 16 s 仅包含小写英文字母 1234567891011121314151617181920212223242526272829303132333435class Solution &#123; Set&lt;String&gt; set = new HashSet&lt;&gt;(); int ans = 0; public int maxUniqueSplit(String s) &#123; // dfs 搜索顺序: 对位置u 拆分还是不拆分 已有 char[] carr = s.toCharArray(); dfs(0,0,new StringBuilder(),carr); return ans; &#125; public void dfs(int u, int k, StringBuilder sb, char[] carr) &#123; // if (k &gt;= carr.length) &#123; // ans = k; // return ; // &#125; if (u &gt;= carr.length) &#123; ans = Math.max(k,ans); return ; &#125; sb.append(carr[u]); if (!set.contains(sb.toString())) &#123; // 拆分 set.add(sb.toString()); dfs(u+1,k+1,new StringBuilder(),carr); set.remove(sb.toString()); &#125; // sb.deleteCharAt(sb.length()-1); // //不拆分 // sb.append(carr[u]); dfs(u+1,k,sb,carr); sb.deleteCharAt(sb.length()-1); &#125;&#125; 2023&#x2F;2&#x2F;28 1800_1810给你一个整数数组 arr ，你一开始在数组的第一个元素处（下标为 0）。 每一步，你可以从下标 i 跳到下标 i + 1 、i - 1 或者 j ： i + 1 需满足：i + 1 &lt; arr.length i - 1 需满足：i - 1 &gt;= 0 j 需满足：arr[i] == arr[j] 且 i != j 请你返回到达数组最后一个元素的下标处所需的 最少操作次数 。 注意：任何时候你都不能跳到数组外面。 \\1345. 跳跃游戏 IV 1809#BFS优化123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123; public int minJumps(int[] arr) &#123; //BFS 第一次访问同数跳时加入队列后就不在访问这些元素了 // 因为显然在之后的都是走回头路了 总会有比更短的 int n = arr.length; int INF = 0x3f3f3f3f; int[] dist = new int[n]; Arrays.fill(dist,INF); Map&lt;Integer,List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; n; i++) &#123; if (map.get(arr[i]) == null) &#123; map.put(arr[i],new ArrayList&lt;&gt;()); &#125; map.get(arr[i]).add(i); &#125; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); q.offer(0); dist[0] = 0; while (!q.isEmpty()) &#123; int i = q.poll(); int curd = dist[i]; if (i - 1 &gt;= 0 &amp;&amp; dist[i-1] == INF) &#123; q.offer(i-1); dist[i-1] = curd + 1; &#125; if (i + 1 &lt; n &amp;&amp; dist[i+1] == INF) &#123; q.offer(i+1); dist[i+1] = curd + 1; &#125; if (map.get(arr[i]) != null) &#123; for (int u: map.get(arr[i])) &#123; if (dist[u] == INF) &#123; q.offer(u); dist[u] = curd + 1; &#125; &#125; map.remove(arr[i]); &#125; &#125; return dist[n-1]; &#125;&#125; \\1519. 子树中标签相同的节点数 1809# dfs存储信息给你一棵树（即，一个连通的无环无向图），这棵树由编号从 0 到 n - 1 的 n 个节点组成，且恰好有 n - 1 条 edges 。树的根节点为节点 0 ，树上的每一个节点都有一个标签，也就是字符串 labels 中的一个小写字符（编号为 i 的 节点的标签就是 labels[i] ） 边数组 edges 以 edges[i] = [ai, bi] 的形式给出，该格式表示节点 ai 和 bi 之间存在一条边。 返回一个大小为 n 的数组，其中 ans[i] 表示第 i 个节点的子树中与节点 i 标签相同的节点数。 树 T 中的子树是由 T 中的某个节点及其所有后代节点组成的树。 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; int[] ans; List&lt;Integer&gt;[] g; public int[] countSubTrees(int n, int[][] edges, String labels) &#123; // 子树的概念: 隔离父边 g = new List[n]; for (int i = 0; i &lt; n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge: edges) &#123; int a = edge[0]; int b = edge[1]; g[a].add(b); g[b].add(a); &#125; ans = new int[n]; dfs(0,-1,n,labels); return ans; &#125; // dfs 统计信息 public int[] dfs(int cur, int p, int n, String labels) &#123; // 恰好n-1edges 所以能过 int[] hash = new int[26]; char c = labels.charAt(cur); for (int node: g[cur]) &#123; if (node != p) &#123; int[] a = dfs(node,cur,n,labels); for (int i = 0; i &lt; 26; i++) &#123; hash[i] += a[i]; &#125; &#125; &#125; ans[cur] += ++hash[c -&#x27;a&#x27;]; return hash; &#125;&#125; \\2302. 统计得分小于 K 的子数组数目一个数字的 分数 定义为数组之和 乘以 数组的长度。 比方说，[1, 2, 3, 4, 5] 的分数为 (1 + 2 + 3 + 4 + 5) * 5 = 75 。 给你一个正整数数组 nums 和一个整数 k ，请你返回 nums 中分数 严格小于 k 的 非空整数子数组数目。 子数组 是数组中的一个连续元素序列。 1234567891011121314class Solution &#123; public long countSubarrays(int[] nums, long k) &#123; // 1e5, 滑动窗口 long sum = 0, ans = 0; for (int i = 0, j = 0; i &lt; nums.length; i++) &#123; sum += nums[i]; while (sum * (i - j + 1) &gt;= k) &#123; sum -= nums[j++]; &#125; ans += (i - j + 1); &#125; return ans; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"6基础知识","slug":"算法/6基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"suibi","slug":"0suibi","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:47:50.088Z","comments":true,"path":"2022/09/01/0suibi/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/0suibi/","excerpt":"","text":"Java类型转换一定都要转换运算之后一定要转化 Java中 a+&#x3D;b和a&#x3D;a+b有什么区别？+&#x3D; 是java中的一个运算符，而不是两个，所以在运算时 会进行自动类型转换 Serializable接口为什么需要定义serialVersionUID常量验证加载的类与序列化对象是否兼容 序列化运行时使用一个称为 serialVersionUID 的版本号与每个可序列化类相关联，该序列号在反序列化过程中用于验证序列化对象的发送者和接收者是否为该对象加载了与序列化兼容的类。如果接收者加载的该对象的类的 serialVersionUID 与对应的发送者的类的版本号不同，则反序列化将会导致 InvalidClassException HashMap扩容初始化如果不指定则为null 添加之后初始化数组为16 每次put会检查元素总量超过阈值 0.75乘当前数组大小扩容为两倍 如果链表长度大于8 数组长度小于64则会进行数组扩容 两个都满足 则会变为红黑树 扩容方法是用一个新数组代替了原来的数组 将原数据迁移到新数组中使用尾插法 框架谈谈 spring IOCSpring提供了一个IOC容器 ,用于管理对象的创建与对象之间的依赖关系,有各种子接口和实现类以AnnotationApplicationContext为例 有三类后置处理器以提供干涉对象创建的过程 有一些具体的实现类来完成Spring的功能 比如各种Aware接口 生命周期接口 等等 Bean定义,BeanFactoy,Bean IOC容器作用 有准备对象信息阶段,内置了defaultListableBeanFactory 他的内部有各种Map作为各种信息的池,IOC容器来读取我们告诉他的信息生成Bean定义信息(有机会更改Bean信息的生成)存到map里以备后用,Sping内部有一些默认的Bean也会自动的加载进来 创建对象等等 对于mybatis中#和$绑定参数的区别总结＃{}将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：order by #{id}，如果传入的值是111，那么解析成sql时的值为order by “111”， 如果传入的值是id，则解析成的sql为order by “id”。 ${}将传入的数据直接显示生成在sql中。如：order by ${id}，如果传入的值是111，那么解析成sql时的值为order by 111， 如果传入的值是id，则解析成的sql为order by id。 Spring Boot自动装配原理 从@SpringBootApplication说起,内含了@EnableAutoConfiguration注解,其内含了@import()引入了一个选择器 选择器导入组件SpringFactoriesLoader.loadFactoryNanme;去类路径下找META-INF&#x2F;spring.factory文件SPI机制找到@EnableAutoConfiguration全类名对应的所有配置的值 SprignBoot写好的全场景的自动配置类全部导入进来,配置类就是给容器放组件Bean SPI机制其实就是根据Servlet厂商（服务提供商）提供要求的一个接口， 在固定的目录（META-INF&#x2F;services）放上以接口全类名 为命名的文件， 文件中放入接口的实现的全类名，该类由我们自己实现，按照这种约定的方式（即SPI规范），服务提供商会调用文件中实现类的方法， 从而完成扩展。 SPI演示案例： 假设我们自己是服务提供商： 现在要求的一个接口 IUserDao 1.在固定的目录放上接口的文件名 2.文件中放入实现类（该实现类由你实现）： 一行一个实现类。 3.通过java.util.ServiceLoader提供的ServiceLoader就可以完成SPI的实现类加载 ​ public class App { public static void main(String[] args) { ServiceLoader daos &#x3D; ServiceLoader.load(IUserDao.class); for (IUserDao dao : daos) { dao.save(); } } Mysql谈谈MVCC Mysql在读已提交和可重复读隔离级别下都实现了MVCC机制 可重复读 实际上就是一个copyOrwrite 写的是副本数据,写完在提交,读的是旧数据,所以写的可能是脏写,所以加CAS 读操作会有一个版本号,写的时候必须要和持有的版本号一致, 这个读的是最新数据而不是快照数据用来做更新 读已提交读的是最新数据 查询事务需要事务吗 具体分析 (别的情况会更改数据) Undo日志版本链通俗: 每次修改都会记录一次undo日志,行记录有两个隐藏字段 一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚日志,串联起来形成一个历史记录版本链 可重复读级别 快照读(生成当前事务的一致性视图)该视图在事务结束前不会变化 读已提交 则每次查询重新生成 DATA_TRX_ID 事务ID 记录最近更新这条行记录的事务 ID，大小为 6 个字节 DATA_ROLL_PTR 回滚指针 表示指向该行回滚段（rollback segment）的指针，大小为 7 个字节，InnoDB 便是通过这个指针找到之前版本的数据。该行记录上所有旧版本，在 undo 中都通过链表的形式组织。 图片解释 roll_point指向 insert undo(即一条delete id &#x3D; 1 的指令) 操作系统项目作者 | 张建飞 阿里巴巴高级技术专家 了解我的人都知道，我一直在致力于应用架构和代码复杂度的治理。 这两天在看零售通商品域的代码。面对零售通如此复杂的业务场景，如何在架构和代码层面进行应对，是一个新课题。针对该命题，我进行了比较细致的思考和研究。结合实际的业务场景，我沉淀了一套“如何写复杂业务代码”的方法论，在此分享给大家。 我相信，同样的方法论可以复制到大部分复杂业务场景。 一个复杂业务的处理过程业务背景简单的介绍下业务背景，零售通是给线下小店供货的 B2B 模式，我们希望通过数字化重构传统供应链渠道，提升供应链效率，为新零售助力。阿里在中间是一个平台角色，提供的是 Bsbc 中的 service 的功能。 商品力是零售通的核心所在，一个商品在零售通的生命周期如下图所示： 在上图中红框标识的是一个运营操作的“上架”动作，这是非常关键的业务操作。上架之后，商品就能在零售通上面对小店进行销售了。因为上架操作非常关键，所以也是商品域中最复杂的业务之一，涉及很多的数据校验和关联操作。 针对上架，一个简化的业务流程如下所示： 过程分解像这么复杂的业务，我想应该没有人会写在一个 service 方法中吧。一个类解决不了，那就分治吧。 说实话，能想到分而治之的工程师，已经做的不错了，至少比没有分治思维要好很多。我也见过复杂程度相当的业务，连分解都没有，就是一堆方法和类的堆砌。 不过，这里存在一个问题：即很多同学过度的依赖工具或是辅助手段来实现分解。比如在我们的商品域中，类似的分解手段至少有 3 套以上，有自制的流程引擎，有依赖于数据库配置的流程处理： 本质上来讲，这些辅助手段做的都是一个 pipeline 的处理流程，没有其它。因此，我建议此处最好保持 KISS（Keep It Simple and Stupid），即最好是什么工具都不要用，次之是用一个极简的 Pipeline 模式，最差是使用像流程引擎这样的重方法。 除非你的应用有极强的流程可视化和编排的诉求，否则我非常不推荐使用流程引擎等工具。第一，它会引入额外的复杂度，特别是那些需要持久化状态的流程引擎；第二，它会割裂代码，导致阅读代码的不顺畅。大胆断言一下，全天下估计 80% 对流程引擎的使用都是得不偿失的。 回到商品上架的问题，这里问题核心是工具吗？是设计模式带来的代码灵活性吗？显然不是，问题的核心应该是如何分解问题和抽象问题，知道金字塔原理的应该知道，此处，我们可以使用结构化分解将问题解构成一个有层级的金字塔结构： 按照这种分解写的代码，就像一本书，目录和内容清晰明了。 以商品上架为例，程序的入口是一个上架命令（OnSaleCommand）, 它由三个阶段（Phase）组成。 每个 Phase 又可以拆解成多个步骤（Step），以 OnSaleProcessPhase 为例，它是由一系列 Step 组成的： 看到了吗，这就是商品上架这个复杂业务的业务流程。需要流程引擎吗？不需要；需要设计模式支撑吗？也不需要。对于这种业务流程的表达，简单朴素的组合方法模式（Composed Method）是再合适不过的了。 因此，在做过程分解的时候，我建议工程师不要把太多精力放在工具上，放在设计模式带来的灵活性上。而是应该多花时间在对问题分析，结构化分解，最后通过合理的抽象，形成合适的阶段（Phase）和步骤（Step）上。 过程分解后的两个问题的确，使用过程分解之后的代码，已经比以前的代码更清晰、更容易维护了。不过，还有两个问题值得我们去关注一下： 1、领域知识被割裂肢解什么叫被肢解？因为我们到目前为止做的都是过程化拆解，导致没有一个聚合领域知识的地方。每个 Use Case 的代码只关心自己的处理流程，知识没有沉淀。 相同的业务逻辑会在多个 Use Case 中被重复实现，导致代码重复度高，即使有复用，最多也就是抽取一个 util，代码对业务语义的表达能力很弱，从而影响代码的可读性和可理解性。 2、代码的业务表达能力缺失试想下，在过程式的代码中，所做的事情无外乎就是取数据 – 做计算 – 存数据，在这种情况下，要如何通过代码显性化的表达我们的业务呢？ 说实话，很难做到，因为我们缺失了模型，以及模型之间的关系。脱离模型的业务表达，是缺少韵律和灵魂的。 举个例子，在上架过程中，有一个校验是检查库存的，其中对于组合品（CombineBackOffer）其库存的处理会和普通品不一样。原来的代码是这么写的： 然而，如果我们在系统中引入领域模型之后，其代码会简化为如下： 有没有发现，使用模型的表达要清晰易懂很多，而且也不需要做关于组合品的判断了，因为我们在系统中引入了更加贴近现实的对象模型（CombineBackOffer 继承 BackOffer），通过对象的多态可以消除我们代码中的大部分的 if-else。 过程分解+对象模型通过上面的案例，我们可以看到有过程分解要好于没有分解，过程分解+对象模型要好于仅仅是过程分解。对于商品上架这个 case，如果采用过程分解+对象模型的方式，最终我们会得到一个如下的系统结构： 写复杂业务的方法论通过上面案例的讲解，我想说，我已经交代了复杂业务代码要怎么写：即自上而下的结构化分解+自下而上的面向对象分析。 接下来，让我们把上面的案例进行进一步的提炼，形成一个可落地的方法论，从而可以泛化到更多的复杂业务场景。 上下结合所谓上下结合，是指我们要结合自上而下的过程分解和自下而上的对象建模，螺旋式的构建我们的应用系统。这是一个动态的过程，两个步骤可以交替进行、也可以同时进行。 这两个步骤是相辅相成的，上面的分析可以帮助我们更好的理清模型之间的关系，而下面的模型表达可以提升我们代码的复用度和业务语义表达能力。 其过程如下图所示： 使用这种上下结合的方式，我们就有可能在面对任何复杂的业务场景，都能写出干净整洁、易维护的代码。 能力下沉一般来说实践 DDD 有两个过程： 套概念阶段了解了一些 DDD 的概念，然后在代码中“使用”Aggregation Root，Bounded Context，Repository 等等这些概念。更进一步，也会使用一定的分层策略。然而这种做法一般对复杂度的治理并没有多大作用。 融会贯通阶段术语已经不再重要，理解 DDD 的本质是统一语言、边界划分和面向对象分析的方法。 大体上而言，我大概是在 1.7 的阶段，因为有一个问题一直在困扰我，就是哪些能力应该放在 Domain 层，是不是按照传统的做法，将所有的业务都收拢到 Domain 上，这样做合理吗？说实话，这个问题我一直没有想清楚。 因为在现实业务中，很多的功能都是用例特有的（Use case specific）的，如果“盲目”的使用 Domain 收拢业务并不见得能带来多大的益处。相反，这种收拢会导致 Domain 层的膨胀过厚，不够纯粹，反而会影响复用性和表达能力。 鉴于此，我最近的思考是我们应该采用能力下沉的策略。 所谓的能力下沉，是指我们不强求一次就能设计出 Domain 的能力，也不需要强制要求把所有的业务功能都放到 Domain 层，而是采用实用主义的态度，即只对那些需要在多个场景中需要被复用的能力进行抽象下沉，而不需要复用的，就暂时放在 App 层的 Use Case 里就好了。 注：Use Case 是《架构整洁之道》里面的术语，简单理解就是响应一个 Request 的处理过程。 通过实践，我发现这种循序渐进的能力下沉策略，应该是一种更符合实际、更敏捷的方法。因为我们承认模型不是一次性设计出来的，而是迭代演化出来的。**下沉的过程如下图所示，假设两个 use case 中，我们发现 uc1 的 step3 和 uc2 的 step1 有类似的功能，我们就可以考虑让其下沉到 Domain 层，从而增加代码的复用性。 指导下沉有两个关键指标：代码的复用性和内聚性。 复用性是告诉我们 When（什么时候该下沉了），即有重复代码的时候。内聚性是告诉我们 How（要下沉到哪里），功能有没有内聚到恰当的实体上，有没有放到合适的层次上（因为 Domain 层的能力也是有两个层次的，一个是 Domain Service 这是相对比较粗的粒度，另一个是 Domain 的 Model 这个是最细粒度的复用）。 比如，在我们的商品域，经常需要判断一个商品是不是最小单位，是不是中包商品。像这种能力就非常有必要直接挂载在 Model 上。 之前，因为老系统中没有领域模型，没有 CSPU 这个实体。你会发现像判断单品是否为最小单位的逻辑是以 StringUtils.equals(code, baseCode) 的形式散落在代码的各个角落。这种代码的可理解性是可想而知的，至少我在第一眼看到这个代码的时候，是完全不知道什么意思。 业务技术要怎么做写到这里，我想顺便回答一下很多业务技术同学的困惑，也是我之前的困惑：即业务技术到底是在做业务，还是做技术？业务技术的技术性体现在哪里？ 通过上面的案例，我们可以看到业务所面临的复杂性并不亚于底层技术，要想写好业务代码也不是一件容易的事情。业务技术和底层技术人员唯一的区别是他们所面临的问题域不一样。 业务技术面对的问题域变化更多、面对的人更加庞杂。而底层技术面对的问题域更加稳定、但对技术的要求更加深。比如，如果你需要去开发 Pandora，你就要对 Classloader 有更加深入的了解才行。 但是，不管是业务技术还是底层技术人员，有一些思维和能力都是共通的。比如，分解问题的能力，抽象思维，结构化思维等等。 用我的话说就是：“做不好业务开发的，也做不好技术底层开发，反之亦然。业务开发一点都不简单，只是我们很多人把它做“简单”了 因此，如果从变化的角度来看，业务技术的难度一点不逊色于底层技术，其面临的挑战甚至更大。因此，我想对广大的从事业务技术开发的同学说：沉下心来，夯实自己的基础技术能力、OO 能力、建模能力… 不断提升抽象思维、结构化思维、思辨思维… 持续学习精进，写好代码。我们可以在业务技术岗做的很“技术”","categories":[{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/categories/suibi/"}],"tags":[{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/tags/suibi/"}]},{"title":"Mysql","slug":"11Mysql","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T12:48:45.243Z","comments":true,"path":"2022/09/01/11Mysql/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/11Mysql/","excerpt":"","text":"刷一点题吧 from -&gt; join -&gt; on-&gt; where -&gt; group by -&gt; having -&gt; select -&gt; distnect -&gt; order by -&gt; limit having 条件表达式 需要注意having和where的用法区别：1.having只能用在group by之后，对分组后的结果进行筛选(即使用having的前提条件是分组)。2.where肯定在group by 之前，即也在having之前。 零、select顺序12345678select [distinct] 字段或表达式列表from 表名 join(left join, right join) on 连接条件where 筛选条件group by 字段列表having 字段列表order by 字段列表limit &lt;m,n&gt;-- 说服我干活哦 1234567891011121314151617181920212223241. FROM子句2. WHERE子句3. GROUP BY子句 4. HAVING子句5. SELECT子句6. ORDER BY子句7. LIMIT子句/* ########################################################################################执行顺序的说明：(1) from 子句组装来自不同数据源的数据； (2) where 子句基于指定的条件对记录行进行筛选； (3) group by 子句将数据划分为多个分组； (4) 使用聚集函数进行计算； (5) 使用 having 子句筛选分组； (6) 计算所有的表达式； (7) select 字段；(8) 使用 order by 对结果集进行排序。需要去理解下关于字段别名的使用：(1) WHERE子句不能使用字段别名。(2) 从 GROUP BY 子句开始，后面的所有子句可以使用字段别名。######################################################################################## */ 一、索引1. B+Tree索引 找到键值所在的页再在内存找到具体的数据, 聚蔟索引有完整数据, 非聚蔟(联合索引)没有完整数据, 有时候会回表 双链表范围查询, 可以有磁盘预读: 不是严格按需 而是会把附近的也读上,相邻的结点也能预加载 操作: 对每一层进行二分, 插入删除会破坏平衡, 最好自增主键 与红黑树的比较 数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I&#x2F;O 就能完全载入一个节点, 树高低IO就低 因为不再需要进行全表扫描, 只需要对树进行搜索即可，所以查找速度快很多 2. Hash索引只支持精确查找,无法用于排序与分组, InnoDB有自适应哈希索引,当某个索引值应用特别频繁, 会在B+tree索引之上在创建一个hash索引 3. 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。 查找条件使用 MATCH AGAINST，而不是普通的 WHERE。 全文索引使用倒排索引实现，它记录着关键词到其所在文档的映射。 InnoDB 存储引擎在 MySQL 5.6.4 版本中也开始支持全文索引。 4. 空间数据索引MyISAM 存储引擎支持空间数据索引（R-Tree），可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 必须使用 GIS 相关的函数来维护数据。 数据库为什么使用B+树而不是B树 B+树的叶子节点存储了所有的数据，非叶子节点中存储的是比较关键字。而B树所有的节点都会存储数据。B+树的叶子节点之间存在一个指针连接，B树不存在指针连接。B+树这种设计结构能带来什么好处呢？B+树所有的数据都存储在叶子节点，那么顺着叶子节点从左往右即可完成对数据的遍历，极大了简化了排序操作。这也是mysql设计索引是采用B+树的原因，不仅仅能方便查找，而且有助于排序，在mysql的索引中叶子节点之间数双向链表可正反遍历，更加灵活； B树只适合随机检索，而B+树同时支持随机检索和顺序检索； B+树空间利用率更高，可减少I&#x2F;O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I&#x2F;O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素； B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 索引优化1. 独立的列不能是表达式一部分, 或函数的参数 2. 多列索引在需要使用多个列作为条件进行查询时，使用多列索引比使用多个单列索引性能更好。例如下面的语句中，最好把 actor_id 和 film_id 设置为多列索引。 12SELECT film_id, actor_ id FROM sakila.film_actorWHERE actor_id = 1 AND film_id = 1; 3. 索引列的顺序让选择性最强的索引列放在前面。 索引的选择性是指：不重复的索引值和记录总数的比值。最大值为 1，此时每个记录都有唯一的索引与其对应。选择性越高，每个记录的区分度越高，查询效率也越高。 例如下面显示的结果中 customer_id 的选择性比 staff_id 更高，因此最好把 customer_id 列放在多列索引的前面。 1234SELECT COUNT(DISTINCT staff_id)/COUNT(*) AS staff_id_selectivity,COUNT(DISTINCT customer_id)/COUNT(*) AS customer_id_selectivity,COUNT(*)FROM payment; 123 staff_id_selectivity: 0.0001customer_id_selectivity: 0.0373 COUNT(*): 16049 4. 前缀索引对于 BLOB、TEXT 和 VARCHAR 类型的列，必须使用前缀索引，只索引开始的部分字符。 前缀长度的选取需要根据索引选择性来确定。 5. 覆盖索引索引包含所有需要查询的字段的值。 具有以下优点： 索引通常远小于数据行的大小，只读取索引能大大减少数据访问量。 一些存储引擎（例如 MyISAM）在内存中只缓存索引，而数据依赖于操作系统来缓存。因此，只访问索引可以不使用系统调用（通常比较费时）。 对于 InnoDB 引擎，若辅助索引能够覆盖查询，则无需访问主索引。 索引的优点 大大减少了服务器需要扫描的数据行数。 帮助服务器避免进行排序和分组，以及避免创建临时表（B+Tree 索引是有序的，可以用于 ORDER BY 和 GROUP BY 操作。临时表主要是在排序和分组过程中创建，不需要排序和分组，也就不需要创建临时表）。 将随机 I&#x2F;O 变为顺序 I&#x2F;O（B+Tree 索引是有序的，会将相邻的数据都存储在一起）。 索引的使用条件 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效； 对于中到大型的表，索引就非常有效； 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术。 二、查询性能优化使用 Explain 进行分析Explain 用来分析 SELECT 查询语句，开发人员可以通过分析 Explain 结果来优化查询语句。 比较重要的字段有： select_type : 查询类型，有简单查询、联合查询、子查询等 key : 使用的索引 rows : 扫描的行数 优化数据访问1. 减少请求的数据量 只返回必要的列：最好不要使用 SELECT * 语句。 只返回必要的行：使用 LIMIT 语句来限制返回的数据。 缓存重复查询的数据：使用缓存可以避免在数据库中进行查询，特别在要查询的数据经常被重复查询时，缓存带来的查询性能提升将会是非常明显的。 2. 减少服务器端扫描的行数最有效的方式是使用索引来覆盖查询。 重构查询方式1. 切分大查询一个大查询如果一次性执行的话，可能一次锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞很多小的但重要的查询。 1DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH); 12345rows_affected = 0do &#123; rows_affected = do_query( &quot;DELETE FROM messages WHERE create &lt; DATE_SUB(NOW(), INTERVAL 3 MONTH) LIMIT 10000&quot;)&#125; while rows_affected &gt; 0 2. 分解大连接查询将一个大连接查询分解成对每一个表进行一次单表查询，然后在应用程序中进行关联，这样做的好处有： 让缓存更高效。对于连接查询，如果其中一个表发生变化，那么整个查询缓存就无法使用。而分解后的多个查询，即使其中一个表发生变化，对其它表的查询缓存依然可以使用。 分解成多个单表查询，这些单表查询的缓存结果更可能被其它查询使用到，从而减少冗余记录的查询。 减少锁竞争； 在应用层进行连接，可以更容易对数据库进行拆分，从而更容易做到高性能和可伸缩。 查询本身效率也可能会有所提升。例如下面的例子中，使用 IN() 代替连接查询，可以让 MySQL 按照 ID 顺序进行查询，这可能比随机的连接要更高效。 1234SELECT * FROM tagJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag=&#x27;mysql&#x27;; 123SELECT * FROM tag WHERE tag=&#x27;mysql&#x27;;SELECT * FROM tag_post WHERE tag_id=1234;SELECT * FROM post WHERE post.id IN (123,456,567,9098,8904); 索引算法有哪些？索引算法有 BTree算法和Hash算法 BTree算法 BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在&#x3D;,&gt;,&gt;&#x3D;,&lt;,&lt;&#x3D;和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： 1234-- 只要它的查询条件是一个不以通配符开头的常量select * from user where name like &#x27;jack%&#x27;;-- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如：select * from user where name like &#x27;%jack&#x27;; Hash算法 Hash算法只能用于对等比较，例如&#x3D;,&lt;&#x3D;&gt;（相当于&#x3D;）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到叶子节点这样多次IO访问，所以检索效率远高于BTree索引。 @$创建索引的原则？索引设计的原则？索引虽好，但也不是无限制的使用，最好符合以下几个原则 为常作为查询条件的字段建立索引，where子句中的列，或者连接子句中指定的列 为经常需要排序、分组操作的字段建立索引 更新频繁字段不适合创建索引 不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 对于定义为text、image和bit的数据类型的列不要建立索引 最左前缀原则，就是最左边的优先。指的是联合索引中，优先走最左边列的索引。对于多个字段的联合索引，如 index(a,b,c) 联合索引，则相当于创建了 a 单列索引，(a,b)联合索引，和(a,b,c)联合索引（但并不是建立了多个索引树）。mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &#x3D; 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长 什么情况使用了索引，查询还是慢 索引全表扫描 索引过滤性不好 频繁回表的开销 MySQL使用自增主键的好处 自增主键按顺序存放，增删数据速度快，对于检索非常有利； 数字型，占用空间小，易排序； 使用整形才可以使用AUTO_INCREAMENT，不用担心主键重复问题。 @$什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据与索引放到了一块，索引结构的叶子节点存储了行数据，找到索引也就找到了数据 非聚簇索引：将数据与索引分开存储，索引结构的叶子节点存储的是行数据的地址 聚簇索引的优点 数据访问更快。聚族索引将索引和数据保存在同一个B+树中，因此从聚族索引中获取数据通常比非聚族索引中查找更快。 当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。 使用覆盖索引扫描的查询可以直接使用节点中的主键值。 聚簇索引的缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列作为主键。 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新。 通过辅助索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。 几个概念 对于普通索引，如 name 字段，则需要根据 name 字段的索引树（非聚簇索引）找到叶子节点对应的主键，然后再通过主键去主键索引树查询一遍，才可以得到要找的记录，这就叫回表查询。先定位主键值，再定位行记录，它的性能较扫描一遍索引树的效率更低 InnoDB的行锁是建立在索引的基础之上的，行锁锁的是索引，不是数据，所以提高并发写的能力要在查询字段添加索引 主索引和辅助索引：主索引就是主键索引，辅助索引就是根据业务需要，自己设置的普通的非主键的索引。这个在Myisam里面区别不大，但是在Innodb的时候差别很大 聚簇索引：Innodb的主索引采用的是聚簇索引，一个表只能有1个聚簇索引，因为表数据存储的物理位置是唯一的。聚簇索引的value存的就是真实的数据，不是数据的地址。主索引树里面包含了真实的数据。key是主键值，value值就是data，key值按照B+树的规则分散排布的叶子节点。 非聚簇索引：Myisam的主索引和辅助索引都采用的是非聚簇索引，索引和表数据是分离的，索引的value值存储的是行数据的地址。 Innodb的索引：主索引采用聚簇索引，叶子节点的value值，直接存储的真实的数据。辅助索引是非聚簇索引，value值指向主索引的位置。所以在Innodb中，根据辅助索引查询值需要遍历2次B+树，同时主键的长度越短越好，越短辅助索引的value值就越小。Innodb中根据主键进行范围查询，会特别快。 Myisam的索引：主索引和辅助索引都是非聚簇索引 B+树：不管是什么索引，在mysql中的数据结构都是B+树的结构，可以充分利用数据块，来减少IO查询的次数，提升查询的效率。一个数据块data里面，存储了很多个相邻key的value值，所有的非叶子节点都不存储数据，都是指针。 mysql采用B+树的优点：IO读取次数少（每次都是页读取），范围查找更快捷（相邻页之间有指针） 联合索引是什么？组合索引是什么？MySQL可以使用多个字段组合建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 @$联合索引数据结构和实现原理，使用联合索引是怎么进行查询的假设，我们对(a,b)字段建立索引，那么入下图所示 如上图所示他们是按照a来进行排序，在a相等的情况下，才按b来排序。 因此，我们可以看到a是有序的1，1，2，2，3，3。而b是一种全局无序，局部相对有序状态！什么意思呢？ 从全局来看，b的值为1，2，1，4，1，2，是无序的，因此直接执行b = 2这种查询条件没有办法利用索引。 从局部来看，当a的值确定的时候，b是有序的。例如a &#x3D; 1时，b值为1，2是有序的状态。当a&#x3D;2时候，b的值为1,4也是有序状态。因此，你执行a = 1 and b = 2是a,b字段能用到索引的。而你执行a &gt; 1 and b = 2时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段用不上索引。 综上所示，最左匹配原则，在遇到范围查询的时候，就会停止匹配。 @$什么是最左前缀原则？什么是最左匹配原则？为什么需要注意联合索引中的顺序？ 最左前缀原则，就是最左边的优先。指的是联合索引中，优先走最左边列的索引。对于多个字段的联合索引，如 index(a,b,c) 联合索引，则相当于创建了 a 单列索引，(a,b)联合索引，和(a,b,c)联合索引（但并不是建立了多个索引树）。mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &#x3D; 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 &#x3D;和in可以乱序，比如a &#x3D; 1 and b &#x3D; 2 and c &#x3D; 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 如果建立的索引顺序是 (a,b) 那么直接采用 where b &#x3D; 5 这种查询条件是无法利用到索引的，这一条最能体现最左匹配的特性。 三、存储引擎InnoDB是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。 实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。 详情见事务与锁 幻读是因为MVCC记录在行上 对增加删除无法约束, 那么在select加上范围锁即可, 具体实现是访问过的索引加锁和前面的间隙也加锁, 防止有别的事务对这些数据进行增加删除 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。 todo MyISAM设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。 提供了大量的特性，包括压缩表、空间数据索引等。 不支持事务。 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。 可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。 如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。 比较 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。 外键：InnoDB 支持外键。 备份：InnoDB 支持在线热备份。 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。 其它特性：MyISAM 支持压缩表和空间数据索引。 四、数据类型整型TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT 分别使用 8, 16, 24, 32, 64 位存储空间，一般情况下越小的列越好。 INT(11) 中的数字只是规定了交互工具显示字符的个数，对于存储和计算来说是没有意义的。 浮点数FLOAT 和 DOUBLE 为浮点类型，DECIMAL 为高精度小数类型。CPU 原生支持浮点运算，但是不支持 DECIMAl 类型的计算，因此 DECIMAL 的计算比浮点类型需要更高的代价。 FLOAT、DOUBLE 和 DECIMAL 都可以指定列宽，例如 DECIMAL(18, 9) 表示总共 18 位，取 9 位存储小数部分，剩下 9 位存储整数部分。 字符串主要有 CHAR 和 VARCHAR 两种类型，一种是定长的，一种是变长的。 VARCHAR 这种变长类型能够节省空间，因为只需要存储必要的内容。但是在执行 UPDATE 时可能会使行变得比原来长，当超出一个页所能容纳的大小时，就要执行额外的操作。MyISAM 会将行拆成不同的片段存储，而 InnoDB 则需要分裂页来使行放进页内。 在进行存储和检索时，会保留 VARCHAR 末尾的空格，而会删除 CHAR 末尾的空格。 时间和日期MySQL 提供了两种相似的日期时间类型：DATETIME 和 TIMESTAMP。 1. DATETIME能够保存从 1000 年到 9999 年的日期和时间，精度为秒，使用 8 字节的存储空间。 它与时区无关。 默认情况下，MySQL 以一种可排序的、无歧义的格式显示 DATETIME 值，例如“2008-01-16 22:37:08”，这是 ANSI 标准定义的日期和时间表示方法。 2. TIMESTAMP和 UNIX 时间戳相同，保存从 1970 年 1 月 1 日午夜（格林威治时间）以来的秒数，使用 4 个字节，只能表示从 1970 年到 2038 年。 它和时区有关，也就是说一个时间戳在不同的时区所代表的具体时间是不同的。 MySQL 提供了 FROM_UNIXTIME() 函数把 UNIX 时间戳转换为日期，并提供了 UNIX_TIMESTAMP() 函数把日期转换为 UNIX 时间戳。 默认情况下，如果插入时没有指定 TIMESTAMP 列的值，会将这个值设置为当前时间。 应该尽量使用 TIMESTAMP，因为它比 DATETIME 空间效率更高。 五、切分水平切分水平切分又称为 Sharding，它是将同一个表中的记录拆分到多个结构相同的表中。 当一个表的数据不断增多时，Sharding 是必然的选择，它可以将数据分布到集群的不同节点上，从而缓存单个数据库的压力。 垂直切分垂直切分是将一张表按列切分成多个表，通常是按照列的关系密集程度进行切分，也可以利用垂直切分将经常被使用的列和不经常被使用的列切分到不同的表中。 在数据库的层面使用垂直切分将按数据库中表的密集程度部署到不同的库中，例如将原来的电商数据库垂直切分成商品数据库、用户数据库等。 Sharding 策略 哈希取模：hash(key) % N； 范围：可以是 ID 范围也可以是时间范围； 映射表：使用单独的一个数据库来存储映射关系。 Sharding 存在的问题1. 事务问题使用分布式事务来解决，比如 XA 接口。 2. 连接可以将原来的连接分解成多个单表查询，然后在用户程序中进行连接。 3. ID 唯一性 使用全局唯一 ID（GUID） 为每个分片指定一个 ID 范围 分布式 ID 生成器 (如 Twitter 的 Snowflake 算法) 主从复制主要涉及三个线程：binlog 线程、I&#x2F;O 线程和 SQL 线程。 binlog 线程 ：负责将主服务器上的数据更改写入二进制日志（Binary log）中。 I&#x2F;O 线程 ：负责从主服务器上读取二进制日志，并写入从服务器的中继日志（Relay log）。 SQL 线程 ：负责读取中继日志，解析出主服务器已经执行的数据更改并在从服务器中重放（Replay）。 todo日志 读写分离主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。 读写分离能提高性能的原因在于： 主从服务器负责各自的读和写，极大程度缓解了锁的争用； 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销； 增加冗余，提高可用性。 读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。 参考资料 BaronScbwartz, PeterZaitsev, VadimTkacbenko, 等. 高性能 MySQL[M]. 电子工业出版社, 2013. 姜承尧. MySQL 技术内幕: InnoDB 存储引擎 [M]. 机械工业出版社, 2011. 20+ 条 MySQL 性能优化的最佳经验 服务端指南 数据存储篇 | MySQL（09） 分库与分表带来的分布式困境与应对之策 How to create unique row ID in sharded databases? SQL Azure Federation – Introduction MySQL 索引背后的数据结构及算法原理 MySQL 性能优化神器 Explain 使用分析 How Sharding Works 大众点评订单系统分库分表实践 [B + 树]( 六、数据库优化数据库结构优化一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意： 冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 @$大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？1. 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I&#x2F;O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据库中间加了一个代理层。分片逻辑统一维护在中间件服务中。 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 MySQL的主从复制原理以及流程主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行，从而使得从数据库的数据与主数据库保持一致。 主从复制的作用 高可用和故障切换：主数据库出现问题，可以切换到从数据库。 负载均衡：可以进行数据库层面的读写分离。 数据备份：可以在从数据库上进行日常备份。 复制过程 Binary log：主数据库的二进制日志 Relay log：从数据库的中继日志 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I&#x2F;O Thread，该线程在master打开一个普通连接，将这些事件写入到中继日志中。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 读写分离有哪些解决方案？未完成读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。主从复制要求slave不能写只能读 方案一 利用中间件来做代理，使用mysql-proxy代理，负责对数据库的请求识别出读还是写，并分发到不同的数据库中。 优点：直接实现读写分离和负载均衡，不用修改代码，数据库和应用程序弱耦合，master和slave用一样的帐号，mysql官方不建议实际生产中使用 缺点：降低性能， 不支持事务，代理存在性能瓶颈和可靠性风险增加。 方案二 使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。 如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert&#x2F;update&#x2F;delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。 不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。 方案三 使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务 缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需要进行特殊处理 七、SQL查询的基本原理条SQL查询语句是如何执行的？ 连接：应用服务器与数据库服务器建立一个连接 获得请求SQL：数据库进程拿到请求sql 查询缓存：如果查询命中缓存则直接返回结果 语法解析和预处理： 首先通过mysql关键字将语句解析，会生成一个内部解析树，mysql解析器将对其解析，查看是否是有错误的关键字，关键字顺序是否正确；预处理器则是根据mysql的规则进行进一步的检查，检查mysql语句是否合法，如库表是否存在，字段是否存在，字段之间是否模棱两可等等，预处理器也会验证权限。 查询优化器：sql语句在优化器中转换成执行计划，一条sql语句可以有多种方式查询，最后返回的结果肯定是相同，但是不同的查询方式效果不同，优化器的作用就是：选择一种合适的执行计划。mysql是基于成本的优化器，他将预测执行此计划的成本，并选择成本最小的那条 执行计划，执行SQL：在解析和优化后，MySQL将生成查询对应的执行计划，由执行计划调用存储引擎的API来执行查询 将结果返回给客户端 关掉连接，释放资源 一条更新语句的执行流程又是怎样的112-- 如果要将 ID=2 这一行的值加 1mysql&gt; update T set c=c+1 where ID=2; 你执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是 更新流程2还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。 在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 补充: redo是存储在磁盘上的 是顺序写 提前分配好一块区域追加写速度很快(Kafka,mq等就是顺序写) 直接往磁盘(账本)上写是随机写,修改这条数据和修改那条数据,所以很慢 ​ binlog前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 八、事务与锁@$事务的四大特性(ACID)介绍一下?关系性数据库需要遵循ACID规则，具体内容如下： 特性 说明 原子性 Atomic 事务是最小的执行单位，不允许分割。事务包含的所有操作要么全部成功，要么全部失败回滚。 一致性 Consistency 事务执行之前和执行之后都必须处于一致性状态。符合现实举例：拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 隔离性 Isolation 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间是相互隔离的。数据库规定了多种事务隔离级别，不同的隔离级别对应不同的干扰程度。隔离级别越高，数据一致性越好，但并发性越差。 持久性 Durability 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下，也不会丢失提交事务的操作。 @$什么是脏读？不可重复读？幻读？ 脏读(Dirty Read)：一个事务读取到另外一个事务未提交的数据。举例：一个事务1读取了被另一个事务2修改但还未提交的数据。由于某种异常事务2回滚，则事务1读取的是无效数据。 不可重复读(Non-repeatable read)：一个事务读取同一条记录2次，得到的结果不一致。这可能是两次查询过程中间，另一个事务更新了这条记录。 幻读(Phantom Read)：幻读发生在两个完全相同的查询，得到的结果不一致。这可能是两次查询过程中间，另一个事务增加或者减少了行记录。 不可重复度和幻读区别 不可重复读的重点是修改，幻读的重点在于新增或者删除。 这里需要注意的是：MySQL 默认采用的 REPEATABLE_READ隔离级别，Oracle 默认采用的 READ_COMMITTED隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是**READ-COMMITTED(读取已提交)**，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重复读）并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。 @$MySQL数据库可重复读隔离级别是怎么实现的，MVCC并发版本控制原理实际上就是一个copyOrwrite 写的是副本数据,写完在提交,读的是旧数据,所以写的可能是脏写,所以加CAS 读操作会有一个版本号(储存在行),写的时候必须要和持有的版本号一致,负责重查 事务一查询 version1 500 事务二 修改为 1000 version2 事务一 在查询的数据上+300 提交时发现 version不是1 则重新查询 ​ 查询version 2 1000 ​ +300 version 是2 则提交 变为1300 MySQL可重复读是通过MVCC实现的 MVCC(Multi Version Concurrency Control的简称)，代表多版本并发控制。与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control)。MVCC最大的优势：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能 锁对MySQL的锁了解吗当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。 隔离级别与锁的关系在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，事务执行完毕后才释放共享锁。 在SERIALIZABLE级别下，是限制性最强的隔离级别，该级别下锁定整个范围的键，并一直持有锁，直到事务完成。 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认采用行级锁 行级锁，表级锁和页级锁对比 行级锁 行级锁是MySQL中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：锁定粒度最小，对当前操作的行记录加锁，发生锁冲突的概率最低，并发度也最高；加锁开销大，加锁慢；会出现死锁； 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：锁定粒度大，对当前操作的整张表加锁，发出锁冲突的概率最高，并发度最低；加锁开销小，加锁快；不会出现死锁； 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 SQL优化@$如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到？或者说怎么才可以知道这条语句运行很慢的原因？对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。而执行计划，就是显示数据库引擎对于SQL语句执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 执行计划包含的信息 id 由一组数字组成。表示一个查询中各个子查询的执行顺序; id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为null时，表示一个合并结果集的操作的执行id为null，常出现在包含union等查询语句中。 select_type 每个子查询的查询类型，一些常见的查询类型。 id select_type description 1 SIMPLE 不包含任何子查询或union查询 2 PRIMARY 包含子查询时最外层查询就显示为 PRIMARY 3 SUBQUERY 在select或where子句中出现的子查询 4 DERIVED from字句中出现的子查询 5 UNION union连接的两个select查询，第一个查询是dervied派生表，除了第一个表外，第二个以后的表select_type都是union。 6 UNION RESULT 包含union的结果集，在union和union all语句中，因为它不需要参与查询，所以id字段为null 7 dependent subquery 与dependent union类似，表示这个subquery的查询要受到外部表查询的影响。 8 dependent union 与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响 table 显示的查询表名，如果查询使用了别名，那么这里显示的是别名。 type访问类型(非常重要，可以看到有没有走索引) 依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，unique_subquery，index_subquery，range，index_merge，index，ALL。 除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引。 类型 描述 system 表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index。 const 使用唯一索引或者主键，返回记录是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描。 eq_ref 出现在要连接多个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_ref。 ref 像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见于普通索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。 fulltext 全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引。 ref_or_null 与ref方法类似，只是增加了null值的比较。实际用的不多。 unique_subquery 用于where中的in形式子查询，子查询返回不重复值唯一值。 index_subquery 用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。 range 索引范围扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。 index_merge 表示查询使用了两个以上的索引，最后取交集或者并集。常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取多个索引，性能可能都不如range。 index 索引全表扫描。把索引从头到尾扫一遍，常见于使用索引列就可以处理，不需要读取数据文件的查询、可以使用索引排序或者分组的查询。 ALL 全表扫描数据文件 possible_keys 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。 key_length 索引长度 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 这里是执行计划中估算的扫描行数，不是精确值。 extra 的信息非常丰富，常见的有： Using index 使用覆盖索引 Using where 使用了where子句来过滤结果集 Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。 Using temporary 使用了临时表 SQL优化的目标可以参考阿里开发手册 123456【推荐】SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。说明：1） consts 单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。2） ref 指的是使用普通的索引（normal index）。3） range 对索引进行范围检索。反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描是小巫见大巫。 @$SQL的生命周期？一条SQL查询语句是如何执行的？MySQL总体架构—&gt;SQL执行流程—&gt;语句执行顺序MySQL 的逻辑架构图 12-- 比如，你有个最简单的表，表里只有一个 ID 字段，在执行下面这个查询语句时：mysql&gt; select * from T where ID=10; MySQL的框架有几个组件, 各是什么作用? Server层和存储引擎层各是什么作用？ 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 SQL执行流程，SQL的生命周期？ 连接器 第一步，客户端与数据库server层的连接器进行连接。连接器负责跟客户端建立连接、获取权限、维持和管理连接。 查询缓存 连接建立完成后，会判断查询缓存是否开启，如果已经开启，会判断sql是select还是update&#x2F;insert&#x2F;delete，对于select，尝试去查询缓存，如果命中缓存直接返回数据给客户端， 如果缓存没有命中，或者没有开启缓存， 会进入到下一步分析器。 分析器 分析器进行词法分析和语法分析，分析器先会做“词法分析”，分析SQL中的字符串分别是什么，校验数据库表和字段是否存在，然后进行语法分析，判断SQL是否满足MySQL语法。 优化器 优化器对sql执行计划分析，得到最终执行计划，得到优化后的执行计划交给执行器。 优化器是在表里面有多个索引的时候，决定使用哪个索引，或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如果有权限，执行器调用存储引擎api执行sql，得到响应结果， 将结果返回给客户端，如果缓存是开启状态， 会更新缓存。 详细逻辑架构图 连接：应用服务器与数据库服务器建立一个连接 获得请求SQL：数据库进程拿到请求sql 查询缓存：如果查询命中缓存则直接返回结果 语法解析和预处理： 首先通过mysql关键字将语句解析，会生成一个内部解析树，mysql解析器将对其解析，查看是否是有错误的关键字，关键字顺序是否正确；预处理器则是根据mysql的规则进行进一步的检查，检查mysql语句是否合法，如库表是否存在，字段是否存在，字段之间是否模棱两可等等，预处理器也会验证权限。 查询优化器：sql语句在优化器中转换成执行计划，一条sql语句可以有多种方式查询，最后返回的结果肯定是相同，但是不同的查询方式效果不同，优化器的作用就是：选择一种合适的执行计划。mysql是基于成本的优化器，他将预测执行此计划的成本，并选择成本最小的那条 执行计划，执行SQL：在解析和优化后，MySQL将生成查询对应的执行计划，由执行计划调用存储引擎的API来执行查询 将结果返回给客户端 关掉连接，释放资源 一条更新语句的执行流程又是怎样的呢？12-- 如果要将 ID=2 这一行的值加 1mysql&gt; update T set c=c+1 where ID=2; 你执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。 在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 重要的日志模块：binlog 前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 常用SQL查询语句优化方法 不要使用select * from t，用具体的字段列表代替“*”，使用星号会降低查询效率，如果数据库字段改变，可能出现不可预知隐患。 应尽量避免在where子句中使用!&#x3D;或&lt;&gt;操作符，避免在where子句中字段进行null值判断，存储引擎将放弃使用索引而进行全表扫描。 避免使用左模糊，左模糊查询将导致全表扫描。 IN语句查询时包含的值不应过多，否则将导致全表扫描。 为经常作为查询条件的字段，经常需要排序、分组操作的字段建立索引。 在使用联合索引字段作为条件时，应遵循最左前缀原则。 OR前后两个条件都要有索引，整个SQL才会使用索引，只要有一个条件没索引整个SQL就不会使用索引。 尽量用union all代替union，union需要将结果集合并后再进行唯一性过滤操作，这就会涉及到排序，增加大量的CPU运算，加大资源消耗及延迟。 九、Explain工具介绍使用EXPLAIN关键字可以模拟优化器执行SQL语句，分析你的查询语句或是结构的性能瓶颈 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是 执行这条SQL 注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中 Explain分析示例 示例表 1234567891011121314151617181920212223242526272829303132333435363738394041424344CREATE TABLE ggq_tulin;USE ggq_tulin;DROP TABLE IF EXISTS `actor`;CREATE TABLE `actor` (`id` INT(11) NOT NULL,`name` VARCHAR(45) DEFAULT NULL,`update_time` DATETIME DEFAULT NULL,PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8;SHOW CREATE DATABASE ggq_tulin;INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,&#x27;a&#x27;,&#x27;2017-12-22 15:27:18&#x27;), (2,&#x27;b&#x27;,&#x27;2017-12-22 15:27:18&#x27;), (3,&#x27;c&#x27;,&#x27;2017-12-22 15:27:18&#x27;);SELECT * FROM actor;DROP TABLE IF EXISTS `film`;CREATE TABLE `film` (`id` INT(11) NOT NULL AUTO_INCREMENT,`name` VARCHAR(10) DEFAULT NULL,PRIMARY KEY (`id`),KEY `idx_name` (`name`)) ENGINE=INNODB DEFAULT CHARSET=utf8;SHOW DATABASES;USE ggq_tuling;INSERT INTO `film` (`id`, `name`) VALUES (3,&#x27;film0&#x27;),(1,&#x27;film1&#x27;),(2,&#x27;film2&#x27;);SELECT * FROM film;DROP TABLE IF EXISTS `film_actor`;CREATE TABLE `film_actor` (`id` INT(11) NOT NULL,`film_id` INT(11) NOT NULL,`actor_id` INT(11) NOT NULL,`remark` VARCHAR(255) DEFAULT NULL,PRIMARY KEY (`id`),KEY `idx_film_actor_id` (`film_id`,`actor_id`)) ENGINE=INNODB DEFAULT CHARSET=utf8;INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); 1explain select * from actor; 在查询中的每个表会输出一行，如果有两个表通过 join 连接查询，那么会输出两行 explain 两个变种 1紧随其后通过 show warnings 命令可 以得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows * filtered&#x2F;100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的 表）。 123explain select * from film where id = 1; show warnings; explain中的列接下来我们将展示 explain 中每个列的信息。 \\1. id列不一定唯一 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。 id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 \\2. select_type列select_type 表示对应行是简单还是复杂的查询。 1）simple：简单查询。查询不包含子查询和union 11 mysql&gt; explain select * from film where id = 2; 2）primary：复杂查询中最外层的 select 3）subquery：包含在 select 中的子查询（不在 from 子句中） 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表(原来没有的表)中，也称为派生(衍生)表（derived的英文含 义） 用这个例子来了解 primary、subquery 和 derived 类型 11 mysql&gt; set session optimizer_switch=&#x27;derived_merge=off&#x27;; #关闭mysql5.7新特性对衍生表的合并优化 1232 mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der;1 mysql&gt; set session optimizer_switch=&#x27;derived_merge=on&#x27;; #还原默认配置 5）union：在 union 中的第二个和随后的 select 11 mysql&gt; explain select 1 union all select 1; \\3. table列这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id&#x3D;N 的查询，于是先执行 id&#x3D;N 的查 询。 当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 \\4. type列这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 效率依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说，得保证查询达到range级别，最好达到ref 很少: NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可 以单独查找索引来完成，不需要在执行时访问表 11 mysql&gt; explain select min(id) from film; const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是 const的特例，表里只有一条元组匹配时为system 1231 mysql&gt; explain extended select * from (select * from film where id = 1) tmp;1 mysql&gt; show warnings; eq_ref：primary key 或 unique key 索引的所有部分被连接使用&#x2F;唯一索引 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 11 mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id; ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会 找到多个符合条件的行。 \\1. 简单 select 查询，name是普通索引（非唯一索引） 11 mysql&gt; explain select * from film where name = &#x27;film1&#x27;; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 11 mysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.fi lm_id; range：范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;&#x3D; 等操作中。使用一个索引来检索给定范围的行。 11 mysql&gt; explain select * from actor where id &gt; 1; index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些。 1 mysql&gt; explain select * from film; ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。 11 mysql&gt; explain select * from actor; \\5. possible_keys列这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引 对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提 高查询性能，然后用 explain 查看效果。 \\6. key列这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 \\7. key_len列这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通 过结果中的key_len&#x3D;4可推断出查询使用了第一个列：film_id列来执行索引查找。 11 mysql&gt; explain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串，char(n)和varchar(n)，5.0.3以后版本中，n均代表字符数，而不是字节数，如果是utf-8，一个数字 或字母占1个字节，一个汉字占3个字节 char(n)：如果存汉字长度就是 3n 字节 varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为 varchar是变长字符串 数值类型 tinyint：1字节 smallint：2字节 int：4字节 bigint：8字节 时间类型 date：3字节 timestamp：4字节 datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索 引。 \\8. ref列这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） \\9. rows列这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 \\10. Extra列这一列展示的是额外信息。常见的重要值如下： 1）Using index：使用覆盖索引 覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中 获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个 查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值 11 mysql&gt; explain select film_id from film_actor where film_id = 1; 2）Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖 11 mysql&gt; explain select * from actor where name = &#x27;a&#x27;; 3）Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围； 11 mysql&gt; explain select * from film_actor where film_id &gt; 1; 4）Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索 引来优化。 \\1. actor.name没有索引，此时创建了张临时表来distinct 11 mysql&gt; explain select distinct name from actor; \\2. film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表 11 mysql&gt; explain select distinct name from film; 5）Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一 般也是要考虑使用索引来优化的。 \\1. actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 11 mysql&gt; explain select * from actor order by name; \\2. film.name建立了idx_name索引,此时查询时extra是using index 11 mysql&gt; explain select * from film order by name; 6）Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 11 mysql&gt; explain select min(id) from film; 索引最佳实践12345678910111213141516171819202122232425262728291 示例表：2 CREATE TABLE `employees` (3 `id` int(11) NOT NULL AUTO_INCREMENT,4 `name` varchar(24) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;姓名&#x27;,5 `age` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;年龄&#x27;,6 `position` varchar(20) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;职位&#x27;,7 `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;入职时间&#x27;,8 PRIMARY KEY (`id`),9 KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE10 ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COMMENT=&#x27;员工记录表&#x27;;1112 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;LiLei&#x27;,22,&#x27;manager&#x27;,NOW());13 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;HanMeimei&#x27;,23,&#x27;dev&#x27;,NOW());14 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;Lucy&#x27;,23,&#x27;dev&#x27;,NOW()); 1.全值匹配 123451 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27;;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22 AND position =&#x27;manager&#x27;; 2.最左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 12345678910111 EXPLAIN SELECT * FROM employees WHERE name = &#x27;Bill&#x27; and age = 31;2 EXPLAIN SELECT * FROM employees WHERE age = 30 AND position = &#x27;dev&#x27;;3 EXPLAIN SELECT * FROM employees WHERE position = &#x27;manager&#x27;;3.不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描1 EXPLAIN SELECT * FROM employees WHERE name = &#x27;LiLei&#x27;;2 EXPLAIN SELECT * FROM employees WHERE left(name,3) = &#x27;LiLei&#x27;; 给hire_time增加一个普通索引： 1231 ALTER TABLE `employees` ADD INDEX `idx_hire_time` (`hire_time`) USING BTREE ;1 EXPLAIN select * from employees where date(hire_time) =&#x27;2018‐09‐30&#x27;; 转化为日期范围查询，有可能会走索引： 1231 EXPLAIN select * from employees where hire_time &gt;=&#x27;2018‐09‐30 00:00:00&#x27; and hire_time &lt;=&#x27;2018‐09‐30 23:59:59&#x27;; 还原最初索引状态 11 ALTER TABLE `employees` DROP INDEX `idx_hire_time`; 4.存储引擎不能使用索引中范围条件右边的列 1231 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22 AND position =&#x27;manager&#x27;;2 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age &gt; 22 AND position =&#x27;manager&#x27;; 5.尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少 select * 语句 12345671 EXPLAIN SELECT name,age FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 23 AND position=&#x27;manager&#x27;;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 23 AND position =&#x27;manager&#x27;; 6.mysql在使用不等于（！&#x3D;或者&lt;&gt;），not in ，not exists 的时候无法使用索引会导致全表扫描 &lt; 小于、 &gt; 大于、 &lt;&#x3D;、&gt;&#x3D; 这些，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引 11 EXPLAIN SELECT * FROM employees WHERE name != &#x27;LiLei&#x27;; 7.is null,is not null 一般情况下也无法使用索引 11 EXPLAIN SELECT * FROM employees WHERE name is null 8.like以通配符开头（’$abc…’）mysql索引失效会变成全表扫描操作 1231 EXPLAIN SELECT * FROM employees WHERE name like &#x27;%Lei&#x27;1 EXPLAIN SELECT * FROM employees WHERE name like &#x27;Lei%&#x27; 问题：解决like’%字符串%’索引不被使用的方法？ a）使用覆盖索引，查询字段必须是建立覆盖索引字段 11 EXPLAIN SELECT name,age,position FROM employees WHERE name like &#x27;%Lei%&#x27;; b）如果不能使用覆盖索引则可能需要借助搜索引擎 9.字符串不加单引号索引失效 1231 EXPLAIN SELECT * FROM employees WHERE name = &#x27;1000&#x27;;2 EXPLAIN SELECT * FROM employees WHERE name = 1000; 10.少用or或in，用它查询时，mysql不一定使用索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评 估是否使用索引，详见范围查询优化 1 EXPLAIN SELECT * FROM employees WHERE name &#x3D; ‘LiLei’ or name &#x3D; ‘HanMeimei’; 11.范围查询优化 给年龄添加单值索引 1231 ALTER TABLE `employees` ADD INDEX `idx_age` (`age`) USING BTREE ;1 explain select * from employees where age &gt;=1 and age &lt;=2000; 没走索引原因：mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引。比如这个例子，可能是 由于单次数据量查询过大导致优化器最终选择不走索引 优化方法：可以将大的范围拆分成多个小范围 1231 explain select * from employees where age &gt;=1 and age &lt;=1000;2 explain select * from employees where age &gt;=1001 and age &lt;=2000; 还原最初索引状态 11 ALTER TABLE `employees` DROP INDEX `idx_age`; 索引使用总结： like KK%相当于&#x3D;常量，%KK和%KK% 相当于范围 1231 ‐‐ mysql5.7关闭ONLY_FULL_GROUP_BY报错2 select version(), @@sql_mode;SET sql_mode=(SELECT REPLACE(@@sql_mode,&#x27;ONLY_FULL_GROUP_BY&#x27;,&#x27;&#x27;)); 文档：02-VIP-Explain详解与索引最佳实践 1 http://note.youdao.com/noteshare?id=59d7a574ef9a905e3bb0982bbe33e74d&amp;sub=83A39BAAADD14B8F99E1DCEFFB 7642CA 十、随笔范式名词解析什么是候选键：能够完全决定所有属性的那个属性或者属性组是候选键什么是非主属性：不包含在任何候选键中的属性是非主属性什么是完全函数依赖：如果x决定y那么x的任何一个真子集x&#96;都不能决定y 就叫做 SQL语句分类DDL：数据定义语句【create 表，库…】 DML：数据操作语句【增加 insert，修改update，删除delete】 DQL：数据查询语句【select】 DCL：数据控制语句【管理数据库：比如用户权限 grant revoke】","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"ES","slug":"14ES","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:24:58.716Z","comments":true,"path":"2022/09/01/14ES/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/14ES/","excerpt":"","text":"1-今日内容 初识 ElasticSearch 安装 ElasticSearch ElasticSearch 核心概念 操作 ElasticSearch ElasticSearch JavaAPI 2-初识ElasticSearch2.2-倒排索引倒排索引：将文档进行分词，形成词条和id的对应关系即为反向索引。 以唐诗为例，所处包含“前”的诗句 正向索引：由《静夜思》–&gt;窗前明月光—&gt;“前”字 反向索引：“前”字–&gt;窗前明月光–&gt;《静夜思》 反向索引的实现就是对诗句进行分词，分成单个的词，由词推据，即为反向索引 “床前明月光”–&gt; 分词 将一段文本按照一定的规则，拆分为不同的词条（term） 2.3-ES存储和查询的原理index（索引）：相当于mysql的库 映射：相当于mysql 的表结构 **document(文档)**：相当于mysql的表中的数据 数据库查询存在的问题： 性能低：使用模糊查询，左边有通配符，不会走索引，会全表扫描，性能低 功能弱：如果以”华为手机“作为条件，查询不出来数据 Es使用倒排索引，对title 进行分词 使用“手机”作为关键字查询 生成的倒排索引中，词条会排序，形成一颗树形结构，提升词条的查询速度 待学习 使用“华为手机”作为关键字查询 华为：1,3 手机：1,2,3 2.4-ES概念详解•ElasticSearch是一个基于Lucene的搜索服务器 •是一个分布式、高扩展、高实时的搜索与数据分析引擎 •基于RESTful web接口 •Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎 •官网：https://www.elastic.co/ 应用场景 •搜索：海量数据的查询 •日志数据分析 •实时数据分析 3-安装ElasticSearch3.1-ES安装 参见ElasticSearch-ES安装.md 查看elastic是否启动 1ps -ef|grep elastic 3.2-ES辅助工具安装 参见ElasticSearch-ES安装.md 后台启动 1nohup ../bin/kibana &amp; 4-ElasticSearch核心概念索引（index） ElasticSearch存储数据的地方，可以理解成关系型数据库中的数据库概念。 映射（mapping） mapping定义了每个字段的类型、字段所使用的分词器等。相当于关系型数据库中的表结构。 文档（document） Elasticsearch中的最小数据单元，常以json格式显示。一个document相当于关系型数据库中的一行数据。 倒排索引 一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，对应一个包含它的文档id列表。 类型（type） 一种type就像一类表。如用户表、角色表等。在Elasticsearch7.X默认type为_doc 12345\\- ES 5.x中一个index可以有多种type。 \\- ES 6.x中一个index只能有一种type。 \\- ES 7.x以后，将逐步移除type这个概念，现在的操作已经不再使用，默认_doc 5-脚本操作ES5.1-RESTful风格介绍1.ST（Representational State Transfer），表述性状态转移，是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是RESTful。就是一种定义接口的规范。 2.基于HTTP。 3.使用XML格式定义或JSON格式定义。 4.每一个URI代表1种资源。 5.客户端使用GET、POST、PUT、DELETE 4个表示操作方式的动词对服务端资源进行操作： GET：用来获取资源 POST：用来新建资源（也可以用于更新资源） PUT：用来更新资源 DELETE：用来删除资源 ​ 5.2-操作索引​ PUT 1http://ip:端口/索引名称 查询 123GET http://ip:端口/索引名称 # 查询单个索引信息GET http://ip:端口/索引名称1,索引名称2... # 查询多个索引信息GET http://ip:端口/_all # 查询所有索引信息 •删除索引 1DELETE http://ip:端口/索引名称 •关闭、打开索引 12POST http://ip:端口/索引名称/_close POST http://ip:端口/索引名称/_open 5.3-ES数据类型​ 1. **简单数据类型** 字符串 聚合：相当于mysql 中的sum（求和） 123text：会分词，不支持聚合keyword：不会分词，将全部内容作为一个词条，支持聚合 数值 布尔：boolean 二进制：binary 范围类型 1integer_range, float_range, long_range, double_range, date_range 日期:date 复杂数据类型 •数组：[ ] Nested: nested (for arrays of JSON objects 数组类型的JSON对象) •对象：{ } Object: object(for single JSON objects 单个JSON对象) 5.4-操作映射​ 123456789101112131415161718PUT personGET person#添加映射PUT /person/_mapping&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125; &#125;&#125; ​ #创建索引并添加映射 1234567891011121314151617 #创建索引并添加映射 PUT /person1&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;&#125;GET person1/_mapping 添加字段 123456789101112#添加字段PUT /person1/_mapping&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125;&#125; 5.5-操作文档​ •添加文档，指定id 12345678910POST /person1/_doc/2&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京&quot;&#125;GET /person1/_doc/1 •添加文档，不指定id 12345678910#添加文档，不指定idPOST /person1/_doc/&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京&quot;&#125;#查询所有文档GET /person1/_search 12#删除指定id文档DELETE /person1/_doc/1 6-分词器6.1分词器-介绍•IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包 •是一个基于Maven构建的项目 •具有60万字&#x2F;秒的高速处理能力 •支持用户词典扩展定义 •下载地址：https://github.com/medcl/elasticsearch-analysis-ik/archive/v7.4.0.zip 安装包在资料文件夹中提供 6.2-ik分词器安装 参见 ik分词器安装.md 执行如下命令时如果出现 打包失败（501码）将maven镜像换成阿里云的 1mvn package /opt/apache-maven-3.1.1/conf/setting.xml 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; 6.3-ik分词器使用 IK分词器有两种分词模式：ik_max_word和ik_smart模式。 1、ik_max_word 会将文本做最细粒度的拆分，比如会将“乒乓球明年总冠军”拆分为“乒乓球、乒乓、球、明年、总冠军、冠军。 123456#方式一ik_max_wordGET /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;乒乓球明年总冠军&quot;&#125; ik_max_word分词器执行如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;乒乓球&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;乒乓&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;球&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;明年&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;总冠军&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;冠军&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125; ]&#125; 2、ik_smart 会做最粗粒度的拆分，比如会将“乒乓球明年总冠军”拆分为乒乓球、明年、总冠军。 123456#方式二ik_smartGET /_analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;乒乓球明年总冠军&quot;&#125; ik_smart分词器执行如下： 1234567891011121314151617181920212223242526&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;乒乓球&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;明年&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;总冠军&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125; ]&#125; 由此可见 使用ik_smart可以将文本”text”: “乒乓球明年总冠军”分成了【乒乓球】【明年】【总冠军】 这样看的话，这样的分词效果达到了我们的要求。 ​ 6.4使用IK分词器-查询文档•词条查询：term ​ 词条查询不会分析查询条件，只有当词条和查询字符串完全匹配时才匹配搜索 •全文查询：match ​ 全文查询会分析查询条件，先将查询条件进行分词，然后查询，求并集 1.创建索引，添加映射，并指定分词器为ik分词器 1234567891011121314PUT person2&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125;&#125; 2.添加文档 123456789101112131415161718192021POST /person2/_doc/1&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京海淀区&quot;&#125;POST /person2/_doc/2&#123; &quot;name&quot;:&quot;李四&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京朝阳区&quot;&#125;POST /person2/_doc/3&#123; &quot;name&quot;:&quot;王五&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京昌平区&quot;&#125; 3.查询映射 1GET person2 4.查看分词效果 123456GET _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;北京海淀&quot;&#125; 5.词条查询：term 查询person2中匹配到”北京”两字的词条 12345678910GET /person2/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;address&quot;: &#123; &quot;value&quot;: &quot;北京&quot; &#125; &#125; &#125;&#125; 6.全文查询：match ​ 全文查询会分析查询条件，先将查询条件进行分词，然后查询，求并集 12345678GET /person2/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;:&quot;北京昌平&quot; &#125; &#125;&#125; 7-ElasticSearch JavaApi-​ 7.1SpringBoot整合ES①搭建SpringBoot工程 ②引入ElasticSearch相关坐标 12345678910111213141516&lt;!--引入es的坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; ③测试 ElasticSearchConfig 123456789101112131415161718192021222324252627282930313233343536@Configuration@ConfigurationProperties(prefix=&quot;elasticsearch&quot;)public class ElasticSearchConfig &#123; private String host; private int port; public String getHost() &#123; return host; &#125; public void setHost(String host) &#123; this.host = host; &#125; public int getPort() &#123; return port; &#125; public void setPort(int port) &#123; this.port = port; &#125; @Bean public RestHighLevelClient client()&#123; return new RestHighLevelClient(RestClient.builder( new HttpHost(host,port,&quot;http&quot;) )); &#125;&#125; ElasticsearchDay01ApplicationTests 注意：使用@Autowired注入RestHighLevelClient 如果报红线，则是因为配置类所在的包和测试类所在的包，包名不一致造成的 123456789101112131415@SpringBootTestclass ElasticsearchDay01ApplicationTests &#123; @Autowired RestHighLevelClient client; /** * 测试 */ @Test void contextLoads() &#123; System.out.println(client); &#125;&#125; 7.2-创建索引 1.添加索引 1234567891011121314151617/** * 添加索引 * @throws IOException */ @Test public void addIndex() throws IOException &#123; //1.使用client获取操作索引对象 IndicesClient indices = client.indices(); //2.具体操作获取返回值 //2.1 设置索引名称 CreateIndexRequest createIndexRequest=new CreateIndexRequest(&quot;itheima&quot;); CreateIndexResponse createIndexResponse = indices.create(createIndexRequest, RequestOptions.DEFAULT); //3.根据返回值判断结果 System.out.println(createIndexResponse.isAcknowledged()); &#125; 2.添加索引，并添加映射 1234567891011121314151617181920212223242526272829303132/** * 添加索引，并添加映射 */ @Test public void addIndexAndMapping() throws IOException &#123; //1.使用client获取操作索引对象 IndicesClient indices = client.indices(); //2.具体操作获取返回值 //2.具体操作，获取返回值 CreateIndexRequest createIndexRequest = new CreateIndexRequest(&quot;itcast&quot;); //2.1 设置mappings String mapping = &quot;&#123;\\n&quot; + &quot; \\&quot;properties\\&quot; : &#123;\\n&quot; + &quot; \\&quot;address\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;text\\&quot;,\\n&quot; + &quot; \\&quot;analyzer\\&quot; : \\&quot;ik_max_word\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;age\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;long\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;name\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;keyword\\&quot;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;&quot;; createIndexRequest.mapping(mapping,XContentType.JSON); CreateIndexResponse createIndexResponse = indices.create(createIndexRequest, RequestOptions.DEFAULT); //3.根据返回值判断结果 System.out.println(createIndexResponse.isAcknowledged()); &#125; ​ 7.3-查询、删除、判断索引 查询索引 1234567891011121314151617181920 /** * 查询索引 */@Testpublic void queryIndex() throws IOException &#123; IndicesClient indices = client.indices(); GetIndexRequest getRequest=new GetIndexRequest(&quot;itcast&quot;); GetIndexResponse response = indices.get(getRequest, RequestOptions.DEFAULT); Map&lt;String, MappingMetaData&gt; mappings = response.getMappings(); //iter 提示foreach for (String key : mappings.keySet()) &#123; System.out.println(key+&quot;===&quot;+mappings.get(key).getSourceAsMap()); &#125;&#125; 删除索引 1234567891011/** * 删除索引 */ @Test public void deleteIndex() throws IOException &#123; IndicesClient indices = client.indices(); DeleteIndexRequest deleteRequest=new DeleteIndexRequest(&quot;itheima&quot;); AcknowledgedResponse delete = indices.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(delete.isAcknowledged()); &#125; 索引是否存在 123456789101112131415/** * 索引是否存在 */ @Test public void existIndex() throws IOException &#123; IndicesClient indices = client.indices(); GetIndexRequest getIndexRequest=new GetIndexRequest(&quot;itheima&quot;); boolean exists = indices.exists(getIndexRequest, RequestOptions.DEFAULT); System.out.println(exists); &#125; 7.4-添加文档 1.添加文档,使用map作为数据 12345678910@Test public void addDoc1() throws IOException &#123; Map&lt;String, Object&gt; map=new HashMap&lt;&gt;(); map.put(&quot;name&quot;,&quot;张三&quot;); map.put(&quot;age&quot;,&quot;18&quot;); map.put(&quot;address&quot;,&quot;北京二环&quot;); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(&quot;1&quot;).source(map); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId()); &#125; 2.添加文档,使用对象作为数据 123456789101112@Testpublic void addDoc2() throws IOException &#123; Person person=new Person(); person.setId(&quot;2&quot;); person.setName(&quot;李四&quot;); person.setAge(20); person.setAddress(&quot;北京三环&quot;); String data = JSON.toJSONString(person); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(person.getId()).source(data,XContentType.JSON); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId());&#125; ​ 7.5-修改、查询、删除文档 1.修改文档：添加文档时，如果id存在则修改，id不存在则添加 123456789101112131415161718/** * 修改文档：添加文档时，如果id存在则修改，id不存在则添加 */ @Testpublic void UpdateDoc() throws IOException &#123; Person person=new Person(); person.setId(&quot;2&quot;); person.setName(&quot;李四&quot;); person.setAge(20); person.setAddress(&quot;北京三环车王&quot;); String data = JSON.toJSONString(person); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(person.getId()).source(data,XContentType.JSON); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId());&#125; 2.根据id查询文档 123456789101112/** * 根据id查询文档 */@Testpublic void getDoc() throws IOException &#123; //设置查询的索引、文档 GetRequest indexRequest=new GetRequest(&quot;itcast&quot;,&quot;2&quot;); GetResponse response = client.get(indexRequest, RequestOptions.DEFAULT); System.out.println(response.getSourceAsString());&#125; 3.根据id删除文档 123456789101112/** * 根据id删除文档 */ @Test public void delDoc() throws IOException &#123; //设置要删除的索引、文档 DeleteRequest deleteRequest=new DeleteRequest(&quot;itcast&quot;,&quot;1&quot;); DeleteResponse response = client.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(response.getId()); &#125; ​","categories":[{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/categories/ES/"}],"tags":[{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/tags/ES/"}]},{"title":"MQ","slug":"13MQ","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:07:30.020Z","comments":true,"path":"2022/09/01/13MQ/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/13MQ/","excerpt":"","text":"0. 学习目标 能够说出什么是消息中间件 能够安装RabbitMQ 能够编写RabbitMQ的入门程序 能够说出RabbitMQ的5种模式特征 能够使用Spring整合RabbitMQ 1. 消息中间件概述1.1. 什么是消息中间件MQ全称为Message Queue，消息队列是应用程序和应用程序之间的通信方法。 1、任务异步处理 将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。提高了应用程序的响应时间。 原文链接：https://blog.csdn.net/qq_41519304/article/details/120432790 2、应用程序解耦合 MQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。 3、削峰填谷 1.4. RabbitMQRabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。 RabbitMQ官方地址：http://www.rabbitmq.com/ RabbitMQ提供了6种模式：简单模式，work模式，Publish&#x2F;Subscribe发布与订阅模式，Routing路由模式，Topics主题模式，RPC远程调用模式（远程调用，不太算MQ；暂不作介绍）； 官网对应模式介绍：https://www.rabbitmq.com/getstarted.html 3. RabbitMQ入门3.1. 搭建示例工程3.2. 编写生产者编写消息生产者com.itheima.rabbitmq.simple.Producer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.itheima.rabbitmq.simple;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Producer &#123; static final String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] args) throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(&quot;localhost&quot;); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(&quot;/itcast&quot;); //连接用户名；默认为guest connectionFactory.setUsername(&quot;heima&quot;); //连接密码；默认为guest connectionFactory.setPassword(&quot;heima&quot;); //创建连接 Connection connection = connectionFactory.newConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 要发送的信息 String message = &quot;你好；小兔子！&quot;; /** * 参数1：交换机名称，如果没有指定则使用默认Default Exchage * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;已发送消息：&quot; + message); // 关闭资源 channel.close(); connection.close(); &#125;&#125; 在执行上述的消息发送之后；可以登录rabbitMQ的管理控制台，可以发现队列和其消息： 3.3. 编写消费者抽取创建connection的工具类com.itheima.rabbitmq.util.ConnectionUtil； 123456789101112131415161718192021222324252627package com.itheima.rabbitmq.util;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class ConnectionUtil &#123; public static Connection getConnection() throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(&quot;localhost&quot;); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(&quot;/itcast&quot;); //连接用户名；默认为guest connectionFactory.setUsername(&quot;heima&quot;); //连接密码；默认为guest connectionFactory.setPassword(&quot;heima&quot;); //创建连接 return connectionFactory.newConnection(); &#125;&#125; 编写消息的消费者com.itheima.rabbitmq.simple.Consumer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.itheima.rabbitmq.simple;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer &#123; public static void main(String[] args) throws Exception &#123; Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel)&#123; @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); &#125; &#125;; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, true, consumer); //不关闭资源，应该一直监听消息 //channel.close(); //connection.close(); &#125;&#125; 3.4. 小结上述的入门案例中中其实使用的是如下的简单模式： 在上图的模型中，有以下概念： P：生产者，也就是要发送消息的程序 C：消费者：消息的接受者，会一直等待消息到来。 queue：消息队列，图中红色部分。类似一个邮箱，可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。 4. AMQP 概念 说明 连接Connection 一个网络连接，比如TCP&#x2F;IP套接字连接。 会话Session 端点之间的命名对话。在一个会话上下文中，保证“恰好传递一次”。 信道Channel 多路复用连接中的一条独立的双向数据流通道。为会话提供物理传输介质。 客户端Client AMQP连接或者会话的发起者。AMQP是非对称的，客户端生产和消费消息，服务器存储和路由这些消息。 服务节点Broker 消息中间件的服务节点；一般情况下可以将一个RabbitMQ Broker看作一台RabbitMQ 服务器。 端点 AMQP对话的任意一方。一个AMQP连接包括两个端点（一个是客户端，一个是服务器）。 消费者Consumer 一个从消息队列里请求消息的客户端程序。 生产者Producer 一个向交换机发布消息的客户端应用程序。 可靠性传输消息的可靠投递1、定义 confirm 确认模式return退回模式rabbitMQ整个消息投递的路径为：pruducer —&gt;rabbitMQ broker —-&gt;exchange——&gt;queue —-&gt; consumer 消息从producer到exchange则会返回一个confirmCallback。消息从exchange–&gt;queue投递失败则会返回一个returnCallback。我们将利用这两个callback控制消息的可靠性投递。 我们都知道，消息从生产端到消费端消费要经过3个步骤： 生产端发送消息到RabbitMQ； RabbitMQ发送消息到消费端； 消费端消费这条消息； 这3个步骤中的每一步都有可能导致消息丢失 生产端可靠性投递生产端可靠性投递，即生产端要确保将消息正确投递到RabbitMQ中。生产端投递的消息丢失的原因有很多，比如消息在网络传输的过程中发生网络故障消息丢失，或者消息投递到RabbitMQ时RabbitMQ挂了，那消息也可能丢失，而我们根本不知道发生了什么。针对以上情况，RabbitMQ本身提供了一些机制。 事务消息机制事务消息机制由于会严重降低性能，所以一般不采用这种方法，我就不介绍了，而采用另一种轻量级的解决方案——confirm消息确认机制。 confirm消息确认机制什么是confirm消息确认机制？顾名思义，就是生产端投递的消息一旦投递到RabbitMQ后，RabbitMQ就会发送一个确认消息给生产端，让生产端知道我已经收到消息了，否则这条消息就可能已经丢失了，需要生产端重新发送消息了。 通过下面这句代码来开启确认模式： 1channel.confirmSelect();// 开启发送方确认模式 然后异步监听确认和未确认的消息： 123456789101112131415channel.addConfirmListener(new ConfirmListener() &#123; //消息正确到达broker @Override public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println(&quot;已收到消息&quot;); //做一些其他处理 &#125; //RabbitMQ因为自身内部错误导致消息丢失，就会发送一条nack消息 @Override public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println(&quot;未确认消息，标识：&quot; + deliveryTag); //做一些其他处理，比如消息重发等 &#125;&#125;); 这样就可以让生产端感知到消息是否投递到RabbitMQ中了，当然这样还不够，稍后我会说一下极端情况。 消息持久化RabbitMQ收到消息后将这个消息暂时存在了内存中，那这就会有个问题，如果RabbitMQ挂了，那重启后数据就丢失了，所以相关的数据应该持久化到硬盘中 message消息到达RabbitMQ后先是到exchange交换机中，然后路由给queue队列，最后发送给消费端。 所有需要给exchange、queue和message都进行持久化： exchange持久化： 12//第三个参数true表示这个exchange持久化channel.exchangeDeclare(EXCHANGE_NAME, &quot;direct&quot;, true); queue持久化： 12//第二个参数true表示这个queue持久化channel.queueDeclare(QUEUE_NAME, true, false, false, null); message持久化： 12//第三个参数MessageProperties.PERSISTENT_TEXT_PLAIN表示这条消息持久化channel.basicPublish(EXCHANGE_NAME, ROUTING_KEY, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); 这样，如果RabbitMQ收到消息后挂了，重启后会自行恢复消息。 到此，RabbitMQ提供的几种机制都介绍完了，但这样还不足以保证消息可靠性投递RabbitMQ中，上面我也提到了会有极端情况，比如RabbitMQ收到消息还没来得及将消息持久化到硬盘时，RabbitMQ挂了，这样消息还是丢失了，或者RabbitMQ在发送确认消息给生产端的过程中，由于网络故障而导致生产端没有收到确认消息，这样生产端就不知道RabbitMQ到底有没有收到消息，就不好做接下来的处理。 所以除了RabbitMQ提供的一些机制外，我们自己也要做一些消息补偿机制，以应对一些极端情况。接下来我就介绍其中的一种解决方案——消息入库。 消息入库消息入库，顾名思义就是将要发送的消息保存到数据库中。 首先发送消息前先将消息保存到数据库中，有一个状态字段status&#x3D;0，表示生产端将消息发送给了RabbitMQ但还没收到确认；在生产端收到确认后将status设为1，表示RabbitMQ已收到消息。这里有可能会出现上面说的两种情况，所以生产端这边开一个定时器，定时检索消息表，将status&#x3D;0并且超过固定时间后（可能消息刚发出去还没来得及确认这边定时器刚好检索到这条status&#x3D;0的消息，所以给个时间）还没收到确认的消息取出重发（第二种情况下这里会造成消息重复，消费者端要做幂等性），可能重发还会失败，所以可以做一个最大重发次数，超过就做另外的处理。 这样消息就可以可靠性投递到RabbitMQ中了，而生产端也可以感知到了。 消费端消息不丢失既然已经可以让生产端100%可靠性投递到RabbitMQ了，那接下来就改看看消费端的了，如何让消费端不丢失消息。 默认情况下，以下3种情况会导致消息丢失： 在RabbitMQ将消息发出后，消费端还没接收到消息之前，发生网络故障，消费端与RabbitMQ断开连接，此时消息会丢失； 在RabbitMQ将消息发出后，消费端还没接收到消息之前，消费端挂了，此时消息会丢失； 消费端正确接收到消息，但在处理消息的过程中发生异常或宕机了，消息也会丢失。 其实，上述3中情况导致消息丢失归根结底是因为RabbitMQ的自动ack机制，即默认RabbitMQ在消息发出后就立即将这条消息删除，而不管消费端是否接收到，是否处理完，导致消费端消息丢失时RabbitMQ自己又没有这条消息了。 所以就需要将自动ack机制改为手动ack机制。 消费端手动确认消息： 1234567891011DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; try &#123; //接收到消息，做处理 //手动确认 channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); &#125; catch (Exception e) &#123; //出错处理，这里可以让消息重回队列重新发送或直接丢弃消息 &#125;&#125;;//第二个参数autoAck设为false表示关闭自动确认机制，需手动确认channel.basicConsume(QUEUE_NAME, false, deliverCallback, consumerTag -&gt; &#123;&#125;); 这样，当autoAck参数置为false，对于RabbitMQ服务端而言，队列中的消息分成了两个部分：一部分是等待投递给消费端的消息；一部分是已经投递给消费端，但是还没有收到消费端确认信号的消息。如果RabbitMQ一直没有收到消费端的确认信号，并且消费此消息的消费端已经断开连接或宕机（RabbitMQ会自己感知到），则RabbitMQ会安排该消息重新进入队列（放在队列头部），等待投递给下一个消费者，当然也有能还是原来的那个消费端，当然消费端也需要确保幂等性。 简单实践官网get start 要先将RabbitMQ 已安装并运行在标准端口( 5672 )上的localhost上。如果您使用不同的主机、端口或凭据，则需要调整连接设置。 其他操作没有实际意义就不写了,主要是使用Java客户端与服务端进行连接 6.2 发送支付状态(1)集成RabbitMQ 修改支付微服务，集成RabbitMQ，添加如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 这里我们建议在后台手动创建队列，并绑定队列。如果使用程序创建队列，可以按照如下方式实现。 修改application.yml，配置支付队列和交换机信息，代码如下： 123456789#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order routing: key: queue.order 创建队列以及交换机并让队列和交换机绑定，修改com.changgou.WeixinPayApplication,添加如下代码： 1234567891011121314151617181920212223242526/*** * 创建DirectExchange交换机 * @return */@Beanpublic DirectExchange basicExchange()&#123; return new DirectExchange(env.getProperty(&quot;mq.pay.exchange.order&quot;), true,false);&#125;/*** * 创建队列 * @return */@Bean(name = &quot;queueOrder&quot;)public Queue queueOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.order&quot;), true);&#125;/**** * 队列绑定到交换机上 * @return */@Beanpublic Binding basicBinding()&#123; return BindingBuilder.bind(queueOrder()).to(basicExchange()).with(env.getProperty(&quot;mq.pay.routing.key&quot;));&#125; 6.2.2 发送MQ消息修改回调方法，在接到支付信息后，立即将支付信息发送给RabbitMQ，代码如下： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Value(&quot;$&#123;mq.pay.exchange.order&#125;&quot;)private String exchange;@Value(&quot;$&#123;mq.pay.queue.order&#125;&quot;)private String queue;@Value(&quot;$&#123;mq.pay.routing.key&#125;&quot;)private String routing;@Autowiredprivate WeixinPayService weixinPayService;@Autowiredprivate RabbitTemplate rabbitTemplate;/*** * 支付回调 * @param request * @return */@RequestMapping(value = &quot;/notify/url&quot;)public String notifyUrl(HttpServletRequest request)&#123; InputStream inStream; try &#123; //读取支付回调数据 inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); // 将支付回调数据转换成xml字符串 String result = new String(outSteam.toByteArray(), &quot;utf-8&quot;); //将xml字符串转换成Map结构 Map&lt;String, String&gt; map = WXPayUtil.xmlToMap(result); //将消息发送给RabbitMQ rabbitTemplate.convertAndSend(exchange,routing, JSON.toJSONString(map)); //响应数据设置 Map respMap = new HashMap(); respMap.put(&quot;return_code&quot;,&quot;SUCCESS&quot;); respMap.put(&quot;return_msg&quot;,&quot;OK&quot;); return WXPayUtil.mapToXml(respMap); &#125; catch (Exception e) &#123; e.printStackTrace(); //记录错误日志 &#125; return null;&#125; 6.3 监听MQ消息处理订单在订单微服务中，我们需要监听MQ支付状态消息，并实现订单数据操作。 6.3.1 集成RabbitMQ在订单微服务中，先集成RabbitMQ，再监听队列消息。 在pom.xml中引入如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml中配置rabbitmq配置，代码如下： 在application.yml中配置队列名字，代码如下： 12345#位置支付交换机和队列mq: pay: queue: order: queue.order 6.3.2 监听消息修改订单在订单微服务于中创建com.changgou.order.consumer.OrderPayMessageListener，并在该类中consumeMessage方法，用于监听消息，并根据支付状态处理订单，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041@Component@RabbitListener(queues = &#123;&quot;$&#123;mq.pay.queue.order&#125;&quot;&#125;)public class OrderPayMessageListener &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private OrderService orderService; /*** * 接收消息 */ @RabbitHandler public void consumeMessage(String msg)&#123; //将数据转成Map Map&lt;String,String&gt; result = JSON.parseObject(msg,Map.class); //return_code=SUCCESS String return_code = result.get(&quot;return_code&quot;); //业务结果 String result_code = result.get(&quot;result_code&quot;); //业务结果 result_code=SUCCESS/FAIL，修改订单状态 if(return_code.equalsIgnoreCase(&quot;success&quot;) )&#123; //获取订单号 String outtradeno = result.get(&quot;out_trade_no&quot;); //业务结果 if(result_code.equalsIgnoreCase(&quot;success&quot;))&#123; if(outtradeno!=null)&#123; //修改订单状态 out_trade_no orderService.updateStatus(outtradeno,result.get(&quot;transaction_id&quot;)); &#125; &#125;else&#123; //订单删除 orderService.deleteOrder(outtradeno); &#125; &#125; &#125;&#125; 7 定时处理订单状态7.1 业务分析在现实场景中，可能会出现这么种情况，就是用户支付后，有可能畅购服务网络不通或者服务器挂了，此时会导致回调地址无法接收到用户支付状态，这时候我们需要取微信服务器查询。所以我们之前订单信息的ID存入到了Redis队列，主要用于解决这种网络不可达造成支付状态无法回调获取的问题。 实现思路如下： 123451.每次下单，都将订单存入到Reids List队列中2.定时每5秒检查一次Redis 队列中是否有数据，如果有，则再去查询微信服务器支付状态3.如果已支付，则修改订单状态4.如果没有支付，是等待支付，则再将订单存入到Redis队列中，等会再次检查5.如果是支付失败，直接删除订单信息并修改订单状态 消息中间件&amp;RabbitMQ面试什么是RabbitMQ？RabbitMQ是一款开源的，Erlang编写的，基于AMQP协议的消息中间件 为什么使用MQ？MQ的优点 异步处理 - 相比于传统的串行、并行方式，提高了系统的吞吐量。 应用解耦 - 系统间通过消息通信，不用关心其他系统的处理。 流量削锋 - 可以通过消息队列长度控制请求量，可以缓解短时间内的高并发请求。 消息通讯 - 消息队列一般都内置了高效的通信机制，因此也可以用在纯消息通讯上。比如实现点对点消息队列，或者聊天室等。 日志处理 - 解决大量日志传输。(todo) @$你们公司生产环境用的是什么消息中间件？这个首先你可以说下你们公司选用的是什么消息中间件，比如用的是RabbitMQ，然后可以初步给一些你对不同MQ中间件技术的选型分析。 举个例子：比如说ActiveMQ是老牌的消息中间件，国内很多公司过去运用的还是非常广泛的，功能很强大。 但是问题在于ActiveMQ没法支撑互联网公司的高并发、高负载以及高吞吐的复杂场景，现在在国内互联网公司落地较少。而且使用较多的是一些传统企业，用ActiveMQ做异步调用和系统解耦。 然后你可以说说RabbitMQ，他的好处在于可以支撑高并发、高吞吐量、性能很高，同时有非常完善便捷的后台管理界面可以使用。 另外，他还支持集群化、高可用部署架构、消息高可靠支持，功能较为完善。 而且经过调研，国内各大互联网公司落地RabbitMQ集群支撑自身业务的case较多，国内各种中小型互联网公司使用RabbitMQ的实践也比较多。 除此之外，RabbitMQ的开源社区很活跃，较高频率的版本迭代，来修复发现的bug以及进行各种优化，因此综合考虑过后，公司采取了RabbitMQ。 但是RabbitMQ也有一点缺陷，就是他自身是基于erlang语言开发的，所以导致较为难以分析里面的源码，也较难进行深层次的源码定制和改造，需要较为扎实的erlang语言功底。 然后可以聊聊RocketMQ，是阿里开源的，经过阿里生产环境的超高并发、高吞吐的考验，性能卓越，同时还支持分布式事务等特殊场景。 而且RocketMQ是基于Java语言开发的，适合深入阅读源码，有需要可以站在源码层面解决线上问题，包括源码的二次开发和改造。 另外就是Kafka。Kafka提供的消息中间件的功能明显较少一些，相对上述几款MQ中间件要少很多。 但是Kafka的优势在于专为超高吞吐量的实时日志采集、实时数据同步、实时数据计算等场景。 因此Kafka在大数据领域中配合实时计算技术（比如Spark Streaming、Storm、Flink）使用的较多。但是在传统的MQ中间件使用场景中较少采用。 ActiveMQ、RabbitMQ、RocketMQ、Kafka有什么优缺点？ ActiveMQ RabbitMQ RocketMQ Kafka ZeroMQ 单机吞吐量 比RabbitMQ低 2.6w&#x2F;s（消息做持久化） 11.6w&#x2F;s 17.3w&#x2F;s 29w&#x2F;s 开发语言 Java Erlang Java Scala&#x2F;Java C 主要维护者 Apache Mozilla&#x2F;Spring Alibaba Apache iMatix，创始人已去世 成熟度 成熟 成熟 开源版本不够成熟 比较成熟 只有C、PHP等版本成熟 订阅形式 点对点(p2p)、广播（发布-订阅） 提供了4种：direct, topic ,Headers和fanout。fanout就是广播模式 基于topic&#x2F;messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 基于topic以及按照topic进行正则匹配的发布订阅模式 点对点(p2p) 持久化 支持少量堆积 支持少量堆积 支持大量堆积 支持大量堆积 不支持 顺序消息 不支持 不支持 支持 支持 不支持 性能稳定性 好 好 一般 较差 很好 集群方式 支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 支持简单集群，’复制’模式，对高级集群模式支持不好。 常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master 天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave 不支持 管理界面 一般 较好 一般 无 无 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 @$MQ 有哪些常见问题？如何解决这些问题？MQ 的常见问题有： 消息的顺序问题 消息的重复问题 消息的顺序问题 消息有序指的是可以按照消息的发送顺序来消费。 假如生产者产生了 2 条消息：M1、M2，假定 M1 发送到 S1，M2 发送到 S2，如果要保证 M1 先于 M2 被消费，怎么做？ 解决方案： 保证生产者 - MQServer - 消费者是一对一对一的关系 RabbitMQ：拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 缺陷： 并行度就会成为消息系统的瓶颈（吞吐量不够） 更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。通过合理的设计或者将问题分解来规避。 不关注顺序的应用实际大量存在 队列无序并不意味着消息无序，所以从业务层面来保证消息的顺序而不仅仅是依赖于消息系统，是一种更合理的方式。 其他解决方案 方案一：消费端增加消息记录表，暂存不满足业务条件的消息，并采用定时器进行补偿处理，补偿超次进行预警；（该方案对技术营运友好，目前DMS正在使用，同样该方案可以用来解决重复消费问题） 方案二：消费端对不满足业务条件的消息不进行确认，多次消费失败进入死信队列，监听死信队列进行补偿，补偿超次或失败进行预警； 方案三：采用RocketMQ顺序消费机制；（不建议使用，会降低系统吞吐量） todo 消息记录表 消息的重复问题 造成消息重复的根本原因是：网络不可达。 所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑需要保持幂等性。只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。保证每条消息都有唯一编号和添加一张日志表来记录已经处理成功的消息的 ID，如果新到的消息 ID 已经在日志表中，那么就不再处理这条消息。 @$消息积压怎么处理消息积压的原因 消息积压的直接原因，一定是系统中某个部分出现了性能问题，来不及处理上游发送的消息，才会导致消息积压。 如果日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题才不至于影响业务。 消息积压的处理 排查消息积压原因的方法：能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。 大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。 如果监控到消费变慢了，你需要检查你的消费实例，分析一下是什么原因导致消费变慢。优先检查一下日志是否有大量的消费错误，如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程是不是卡在什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。 @$如何保证RabbitMQ消息的可靠传输？消息丢失怎么办？消息不可靠的情况可能是消息丢失，劫持等原因； 丢失又分为：生产者丢失消息、消息列表丢失消息、消费者丢失消息； 生产者丢失消息：从生产者弄丢数据这个角度来看，RabbitMQ提供transaction和confirm模式来确保生产者不丢消息； transaction机制就是说：发送消息前，开启事务（channel.txSelect()），然后发送消息，如果发送过程中出现什么异常，事务就会回滚（channel.txRollback()），如果发送成功则提交事务（channel.txCommit()）。然而，这种方式有个缺点：吞吐量下降； confirm模式用的居多：一旦channel进入confirm模式，所有在该信道上发布的消息都将会被指派一个唯一的ID（从1开始），一旦消息被投递到所有匹配的队列之后，rabbitMQ就会发送一个ACK给生产者（包含消息的唯一ID），这就使得生产者知道消息已经正确到达目的队列了；如果rabbitMQ没能处理该消息，则会发送一个Nack消息给你，生产者可以进行重试操作。 消息队列丢数据：消息持久化。 处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。 这个持久化配置可以和confirm机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个Ack信号。 这样，如果消息持久化磁盘之前，rabbitMQ阵亡了，那么生产者收不到Ack信号，生产者会自动重发。 那么如何持久化呢？ 这里顺便说一下吧，其实也很容易，就下面两步 将queue的持久化标识durable设置为true，则代表是一个持久的队列 发送消息的时候将deliveryMode&#x3D;2 这样设置以后，即使rabbitMQ挂了，重启后也能恢复数据 消费者丢失消息：消费者丢数据一般是因为采用了自动确认消息模式，改为手动确认消息即可！ 消费者在收到消息之后，处理消息之前，会自动回复RabbitMQ已收到消息； 如果这时处理消息失败，就会丢失该消息； 解决方案：处理消息成功后，手动回复确认消息。 todo 手动回复确认消息 @$RabbitMQ 常见工作模式和应用场景 交换机todo 一、简单模式原理：一个生产者，一个消费者。生产者将消息发送到队列，消费者监听消息队列，如果队列中有消息，就进行消费，消费后消息从队列中删除 场景：聊天；有一个oa系统，用户通过接收手机验证码进行注册，页面上点击获取验证码后，将验证码放到消息队列，然后短信服务从队列中获取到验证码，并发送给用户。 二、工作模式原理：一个生产者，多个消费者，一条消息只能被一个消费者消费。生产者将消息发送到消息队列，多个消费者同时监听一个队列，谁先抢到消息谁负责消费。这样就形成了资源竞争，谁的资源空闲大，争抢到的可能性就大。 场景：红包；有一个电商平台，有两个订单服务，用户下单的时候，任意一个订单服务消费用户的下单请求生成订单即可。不用两个订单服务同时消费用户的下单请求。 三、发布订阅模式原理：一个生产者，多个消费者，每个消费者都可以收到相同的消息。生产者将消息发送到交换机，交换机类型是fanout，不同的队列注册到交换机上，不同的消费者监听不同的队列，所有消费者都会收到消息。 场景：邮件群发，群聊天，广播(广告)；有一个商城，我们新添加一个商品后，可能同时需要去更新缓存和数据库。 四、路由模式原理：生产者将消息发送给交换机，消息携带具体的routingkey。交换机类型是direct，交换机匹配与之绑定的队列的routingkey，分发到不同的队列上。 场景：还是一样，有一个商城，新添加了一个商品，实时性不是很高，只需要添加到数据库即可，不用刷新缓存。 五、主题模式原理：路由模式的一种，交换机类型是topic，路由功能添加了模糊匹配。星号（*）代表1个单词，#号（#）代表一个或多个单词。 场景：还是一样，有一个商城，新添加了一个商品，实时性不是很高，只需要添加到数据库即可，数据库包含了主数据库mysql1和从数据库mysql2的内容，不用刷新缓存。 六、RPC1、首先客户端发送一个reply_to和corrention_id的请求，发布到RPC队列中； 2、服务器端处理这个请求，并把处理结果发布到一个回调Queue,此Queue的名称应当与reply_to的名称一致 3、客户端从回调Queue中得到先前corrention_id设定的值的处理结果。如果碰到和先前不一样的corrention_id的值，将会忽略而不是抛出异常。 @$如何保证高可用的？RabbitMQ 的集群RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的?，没人生产用单机模式 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 类似 nginx vip(queue) 只是大家可以直接找到queue被连接也能提供服务,而不是消费端直连vip 镜像集群模式：这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"OS","slug":"15os","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-25T03:41:26.117Z","comments":true,"path":"2022/09/01/15os/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/15os/","excerpt":"","text":"笔试题: AMAT（平均内存访问时间）&#x3D; 命中时间 + 未命中率 * 未命中惩罚 一级缓存，10ns，50% 二级缓存，50ns，90% 主存储器，60ns，100% res &#x3D; 10 + 0.5*(50 + 0.1 * (600)) &#x3D; 65ns 第一章 操作系统概述(从软件工程角度)abstractWhat’s OS按软件工程的观点分析OS的结构操作系统的发展，类型及特征现代操作系统体系结构基础知识 1.1What’s OS1.User&#x2F;Computer Interface (用户角度)OS是用户使用计算机系统的接口，为用户提供了方便的工作环境2.Virtual Machine（程序员）建立在硬件上的虚拟机器,为应用软件提供了许多比计算机硬件功能更强或没有的功能3.Resource Manager(OS开发者1）负责分配，回收，以及控制系统中的各种软硬件资源 4.Job Organizer（OS开发者2）工作流程的组织者，负责协调各个应用软件的运行次序 JVM是建立在OS之上的虚拟机：可对比理解 软件工程 需求分析→系统设计→编码实现→产品测试 1.2 OS的系统需求软件系统的系统需求指人们从外部对系统提出的诸多期望包括三种类型 提供的服务 OS提供服务需要满足的限制条件 OS具有适应某些变化的能力 第一类系统需求是后两类系统需求赖以存在的基础，称为功能性需求，后两者为非功能性需求 功能性需求计算机用户需要的用户命令，由OS实现的所有用户命令构成的集合，被称为用户接口或者命令接口引用软件需要的系统调用，由OS实现的所有系统调用的集合被称为程序接口或应用编程接口 Interface(用户接口)表示形式：字符，菜单，图形形式使用方式：脱机（off-line处理时不能改变作业步)&#x2F;联机（on-line可随时改变） System Call应用软件在运行过程可以引用的系统服务常用 POSIX.1 WIN32 API OS对硬件平台的依赖Timer I&#x2F;O Interrupts DMA or Channel Privileged Instructions (特权指令） Memory Protection Mechanism… … 基本概念：Job（作业） 用户一次上机过程中要求计算机为其所做工作的集合；作业中的每项相对独立的工作称为作业步 通常，一组命令来描述作业；其中每个命令定义为一个作业步-…… 基本概念：Thread&amp;Process Thread是指程序的一次相对独立的运行过程，在现代OS中，线程是系统调度的最小单位。 Process是指，系统分配资源的基本对象；在现代OS中，进程仅仅是系统中拥有资源的最小实体；不过在传统OS中进程同时也是系统调度的最小单位 基本概念 Virtual Memory&amp;File 虚拟存储简单的说就是进程的逻辑地址空间；是现代OS对计算机系统中多级物理存储体系进行高度抽象的结果 文件，简单的说就是命名了的字节流；它是现代OS对计算机系统种类繁多的外设设备进行高度抽象的结果 1.3 OS的演变，类型及特点串行处理简单批处理系统1.4 OS体系结构需求分析→系统设计→编码实现→产品测试系统设计→软件体系结构设计→软件部件设计 一种常见的OS总体结构风格 大多数现代OS总体结构包含两类子系统 ：用户接口子系统（提供命令接口），和基础平台子系统（提供系统调用） 两者关系单项性，具体来说用户接口子系统在实现各种用户命令时能够引用基础平台子系统提供的各种系统调用，但基础平台子系统在实现各种系统调用不会引用用户命令 OS基础平台子系统结构风格 分层 特征：按层实现一组概念及其相关的基本属性 ，上层只依赖直接下层 分级 类似分层 按级 ~ 只依赖以下各级 级~ 分块 按块实现~ ~ ，所有各块均可任意引用其他各块提供的概念及属性 双模式user model 和kernel model 第一章 概述(从了解已有功能角度)基本特征1. 并发并发是指宏观上在一段时间内能同时运行多个程序，而并行则指同一时刻能运行多个指令。 并行需要硬件支持，如多流水线、多核处理器或者分布式计算系统。 操作系统通过引入进程和线程，使得程序能够并发运行。 2. 共享共享是指系统中的资源可以被多个并发进程共同使用。 有两种共享方式：互斥共享和同时共享。 互斥共享的资源称为临界资源，例如打印机等，在同一时刻只允许一个进程访问，需要用同步机制来实现互斥访问。 3. 虚拟虚拟技术把一个物理实体转换为多个逻辑实体。 主要有两种虚拟技术：时（时间）分复用技术和空（空间）分复用技术。 多个进程能在同一个处理器上并发执行使用了时分复用技术，让每个进程轮流占用处理器，每次只执行一小个时间片并快速切换。 虚拟内存使用了空分复用技术，它将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，地址空间的页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页置换到内存中。 4. 异步异步指进程不是一次性执行完毕，而是走走停停，以不可知的速度向前推进。 基本功能1. 进程管理进程控制、进程同步、进程通信、死锁处理、处理机调度等。 2. 内存管理内存分配、地址映射、内存保护与共享、虚拟内存等。 3. 文件管理文件存储空间的管理、目录管理、文件读写管理和保护等。 4. 设备管理完成用户的 I&#x2F;O 请求，方便用户使用各种设备，并提高设备的利用率。 主要包括缓冲管理、设备分配、设备处理、虛拟设备等。 系统调用如果一个进程在用户态需要使用内核态的功能，就进行系统调用从而陷入内核，由操作系统代为完成。 Linux 的系统调用主要有以下这些： Task Commands 进程控制 fork(); exit(); wait(); 进程通信 pipe(); shmget(); mmap(); 文件操作 open(); read(); write(); 设备操作 ioctl(); read(); write(); 信息维护 getpid(); alarm(); sleep(); 安全 chmod(); umask(); chown(); 宏内核和微内核1. 宏内核宏内核是将操作系统功能作为一个紧密结合的整体放到内核。 由于各模块共享信息，因此有很高的性能。 2. 微内核由于操作系统不断复杂，因此将一部分操作系统功能移出内核，从而降低内核的复杂性。移出的部分根据分层的原则划分成若干服务，相互独立。 在微内核结构下，操作系统被划分成小的、定义良好的模块，只有微内核这一个模块运行在内核态，其余模块运行在用户态。 因为需要频繁地在用户态和核心态之间进行切换，所以会有一定的性能损失。 中断分类1. 外中断由 CPU 执行指令以外的事件引起，如 I&#x2F;O 完成中断，表示设备输入&#x2F;输出处理已经完成，处理器能够发送下一个输入&#x2F;输出请求。此外还有时钟中断、控制台中断等。 2. 异常由 CPU 执行指令的内部事件引起，如非法操作码、地址越界、算术溢出等。 3. 陷入在用户程序中使用系统调用。 第二章 进程和调度abstract基础：进程描述及控制实现：互斥与同步避免：死锁与饥饿解决：几个经典问题关于：进程通信策略：进程调度 2.1 进程描述和控制进程状态：执行或非执行new（ready suspengd） ready Running exit （blocked suspengd） Bolcked使用两个队列 等待相同资源（例如锁）的在一队 多个不同的队列 Swapping（交换技术）将内存中暂时不能运行的进程，或暂时不用的数据和程序，Swapping-out到外存，以腾出足够的内存空间，把已具备运行条件的进程或进程所需要的数据和程序，Swapping-in内存 程序暂停的原因Swapping用户要求定时任务父进程要求其他系统原因（例如系统怀疑进程会引起问题，会让他暂停） 处理机可处理的一个是新创建的进程或者换入一个以前挂起的进程 进程描述OS如何感知进程，控制进程及其所用的系统资源 OS控制体系 关于进程和资源的当前状态信息 表是为了每个操作系统管理的实体构建的 存储表 I&#x2F;O 表 文件表 程序表存储表分配内存给程序分配二级存储给程序（内存外的所有可访问数据存储器）共享存储区域的访问的保护属性管理虚拟内存所需要的信息 I&#x2F;O表I&#x2F;O设备可选还是已分配I&#x2F;O活动的状态内存中被用作起点或目标的位置 进程表进程在哪为了管理进程而需要的属性 - 进程ID - 进程状态 - 进程所在存储位置进程该过程包括一组要执行的程序 - 数据位置 - 任何定义的常数 - 堆栈进程控制块（PCB） - 属性集合(进程映像) - 程序、数据、堆栈和属性的集合。 2_1_09 PCB OS内核功能资源管理功能进程管理：进程创建，终止，调度，状态转换，同步和通信，管理PCB存储管理：为进程分配空间，对换，段&#x2F;页管理I&#x2F;O管理 ：缓存管理，为进程分配I&#x2F;O通道和设备通过原语 线程 执行状态（正在运行、准备就绪等） 未运行时要保存线程上下文 有一个执行堆栈 每个线程局部变量的静态存储 访问其进程的内存和资源 一个进程的所有线程都共享进程内存和资源线程的好处 创建新线程所需的时间比创建进程所需的时间少 终止线程的时间比终止进程的时间短 在同一进程内的两个线程之间切换的时间更短 由于同一进程中的线程共享内存和文件，因此可以在不调用内核的情况下相互通信。 挂起进程涉及挂起进程的所有线程 -因为所有线程共享相同的地址空间进程的终止，终止进程中的所有线程 多线程操作系统支持在单个进程中执行多个线程。MS-DOS支持单线程。UNIX支持多个用户进程，但每个进程只支持一个线程。Windows 2000、Solaris、Linux、Mach和Os&#x2F;2支持多线程。 线程状态关键状态 Running Ready Blocked改变线程状态的相关操作 派生， 派生其他线程 阻塞 解除阻塞 完成 用户级线程所有线程被应用管理核心不能感知存在描述此类线程的数据结构及控制此类线程的原语在核外子系统中实现 二合一方法Solaris大量线程被创建在用户空间大批调度和同步在用户线程空间 进程与线程1. 进程进程是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 2. 线程线程是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 3. 区别Ⅰ 拥有资源 进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。 Ⅱ 调度 线程是独立调度的基本单位，在同一进程中，线程的切换不会引起进程切换，从一个进程中的线程切换到另一个进程中的线程时，会引起进程切换。 Ⅲ 系统开销 由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、I&#x2F;O 设备等，所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程 CPU 环境的保存及新调度进程 CPU 环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。 Ⅳ 通信方面 线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。 进程状态的切换 就绪状态（ready）：等待被调度 运行状态（running） 阻塞状态（waiting）：等待资源 应该注意以下内容： 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。 进程调度算法不同环境的调度算法目标不同，因此需要针对不同环境来讨论调度算法。 1. 批处理系统批处理系统没有太多的用户操作，在该系统中，调度算法目标是保证吞吐量和周转时间（从提交到终止的时间）。 1.1 先来先服务 first-come first-serverd（FCFS） 非抢占式的调度算法，按照请求的顺序进行调度。 有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。 1.2 短作业优先 shortest job first（SJF） 非抢占式的调度算法，按估计运行时间最短的顺序进行调度。 长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。 1.3 最短剩余时间优先 shortest remaining time next（SRTN） 最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。 2. 交互式系统交互式系统有大量的用户交互操作，在该系统中调度算法的目标是快速地进行响应。 2.1 时间片轮转 将所有就绪进程按 FCFS 的原则排成一个队列，每次调度时，把 CPU 时间分配给队首进程，该进程可以执行一个时间片。当时间片用完时，由计时器发出时钟中断，调度程序便停止该进程的执行，并将它送往就绪队列的末尾，同时继续把 CPU 时间分配给队首的进程。 时间片轮转算法的效率和时间片的大小有很大关系： 因为进程切换都要保存进程的信息并且载入新进程的信息，如果时间片太小，会导致进程切换得太频繁，在进程切换上就会花过多时间。 而如果时间片过长，那么实时性就不能得到保证。 2.2 优先级调度 为每个进程分配一个优先级，按优先级进行调度。 为了防止低优先级的进程永远等不到调度，可以随着时间的推移增加等待进程的优先级。 2.3 多级反馈队列 一个进程需要执行 100 个时间片，如果采用时间片轮转调度算法，那么需要交换 100 次。 多级队列是为这种需要连续执行多个时间片的进程考虑，它设置了多个队列，每个队列时间片大小都不同，例如 1,2,4,8,..。进程在第一个队列没执行完，就会被移到下一个队列。这种方式下，之前的进程只需要交换 7 次。 每个队列优先权也不同，最上面的优先权最高。因此只有上一个队列没有进程在排队，才能调度当前队列上的进程。 可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。 3. 实时系统实时系统要求一个请求在一个确定时间内得到响应。 分为硬实时和软实时，前者必须满足绝对的截止时间，后者可以容忍一定的超时。 2.3 并发 互斥与同步 死锁与饥饿 目标 解释什么是并发，同步，互斥，死锁，饥饿,临界区 掌握互斥的要求 掌握互斥的方法：软件方法&amp;硬件支持—Semaphores，Monitors，Message Passing 区别掌握信号量的类型和意义 掌握生产者消费者，读写者，哲学家问题 理解死锁的情况，死锁的预防，死锁的避免，死锁侦查，一旦检测出死锁的策略，银行家算法安全情况和不安全情况 2.3.1 并发控制并发设计的问题 进程间交流 共享&#x2F;竞争资源 多进程的同步 处理器时间的分配 并发性的困难 全局资源共享 资源分配的管理 编程错误难以定位 操作系统问题跟踪活动进程:PCB． 分配和释放资源 处理器时间:调度 记忆:虚拟内存 文件I &#x2F; O设备 保护数据和资源 进程的结果必须独立于其他并发进程的执行 相互作用过程(理解)进程之间不知道彼此——竞争 ​ 互斥、死锁、饥饿 进程可以间接感知彼此——共享合作 ​ 互斥，死锁，饥饿，数据一致性(Data coherence) 进程直接感知彼此 消息传递 死锁,饥饿 进程间的合作通过沟通消息传递 互斥不是控制要求。 ．可能出现死锁每个进程都在等待来自另一个进程的消息。 ．可能会挨饿两个进程互相发送消息，而另一个进程等待消息。 2.3.3 互斥条件与解决方案互斥的要求 空则让进 忙则等待 有限等待 让权等待 互斥的方法(Approaches of Mutual Exclusion) 软件方法 内存级别,保证内存 Access to the same location in main memory are serializedby some sort of memory arbiter (内存仲裁器待学习) 硬件支持 中断禁用(OS中断待学习) 进程运行到调用操作系统服务或中断为止。 禁用中断保证了互斥。 这种方法的代价很高。 多处理 在一个处理器上禁用中断不能保证互斥。 信号量 称为信号量的特殊变量用于发送信号。 如果一个进程正在等待一个信号，那么它将被阻塞，直到该信号被发送。 等待和信号操作不能中断。 Queue用于保存等待信号量的进程。 信号量信号量是一个具有整数值的变量s。可以初始化为非负数。Wait和Signal是原语(是系统调用,原子的，不能被中断，每个例程可以被视为不可分割的步骤)。 可理解为资源数,门口多把钥匙,都能进 管程 队列+互斥区 信号量容易出错(wait和signal操作的顺序很重要) 消息传递 send(地址,消息) 和 recive(地址,消息) 可以是send和recive都可以是非阻塞的 生产者消费者 一个缓冲区同时只能只有一个人访问(互斥) 不能在空的取,不能在满的加(同步) 循环使用缓冲区 Java实现 &#96;&#96;&#96;java&#x2F;&#x2F;同步方式: Object的wait notify, park,unpark(再复习下),Lock.Condition接口的await()和signal()&#x2F;&#x2F;BlockQueue方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187// synchronized 实现互斥,wait,notifyAll()实现同步 public class Cache &#123; private final static int MAX_SIZE = 10; private int cacheSize = 0; public Cache()&#123; cacheSize = 0; &#125; public Cache(int size)&#123; cacheSize = size; &#125; //分析过程 一个生产者进来 不大于直接生产 小于则让出 public void produce()&#123; synchronized (this)&#123; while (cacheSize &gt;= MAX_SIZE )&#123; try &#123; System.out.println(&quot;缓存已满，生产者需要等待&quot;); wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize++; System.out.println(&quot;生产了一个产品。当前产品数量为&quot;+ cacheSize); notify(); &#125; &#125; public void consume()&#123; synchronized (this)&#123; while(cacheSize &lt;= 0)&#123; try &#123; System.out.println(&quot;缓存为空，消费者需要等待&quot;); wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize--; System.out.println(&quot;消费了一个产品。当前产品数量为&quot;+ cacheSize); notify(); &#125; &#125; &#125;​ // Java内部信号量 //public class Cache &#123; // private int cacheSize = 0; // // public Semaphore mutex; // public Semaphore empty; //保证了容器空的时候（empty的信号量&lt;=0), 消费者等待 // public Semaphore full; //保证了容器满的时候（full的信号量 &lt;= 0），生产者等待 // // public Cache(int size) &#123; // mutex = new Semaphore(1); //二进制信号量，表示互斥锁 // empty = new Semaphore(size); // full = new Semaphore(0); // &#125; // // public int getCacheSize()throws InterruptedException&#123; // return cacheSize; // &#125; // // public void produce() throws InterruptedException&#123; // empty.acquire(); // 消耗一个空位 // mutex.acquire(); // cacheSize++; // System.out.println(&quot;生产了一个产品， 当前产品数为&quot; + cacheSize); // mutex.release(); // full.release(); // 增加了一个产品 // // // &#125; // // public void consume() throws InterruptedException&#123; // full.acquire(); // 消耗了一个产品 // mutex.acquire(); // cacheSize--; // System.out.println(&quot;消费了一个产品， 当前产品数为&quot; + cacheSize); // mutex.release(); // empty.release(); // 增加了一个空位 // // &#125; //&#125;​ //管程 public class Cache &#123; private final static int MAX_SIZE = 10; private int cacheSize = 0; private Lock lock; private Condition notFull; private Condition notEmpty; public Cache()&#123; cacheSize = 0; this.lock = new ReentrantLock(); this.notFull = lock.newCondition(); this.notEmpty = lock.newCondition(); &#125; public Cache(int size)&#123; cacheSize = size; this.lock = new ReentrantLock(); this.notFull = lock.newCondition(); this.notEmpty = lock.newCondition(); &#125; //分析过程 一个生产者进来 不大于直接生产 小于则让出 public void produce()&#123; lock.lock(); while (cacheSize == MAX_SIZE )&#123; try &#123; System.out.println(&quot;缓存已满，生产者需要等待&quot;); notFull.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize++; System.out.println(&quot;生产了一个产品。当前产品数量为&quot;+ cacheSize); notEmpty.signal(); lock.unlock(); &#125; public void consume()&#123; lock.lock(); while (cacheSize == 0 )&#123; try &#123; System.out.println(&quot;缓存为空，消费者需要等待&quot;); notEmpty.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize--; System.out.println(&quot;消费了一个产品。当前产品数量为&quot;+ cacheSize); notFull.signal(); lock.unlock(); &#125; &#125; //值得一提的信号量自己实现 public class Semaphore&#123; private volatile int permit; public Semaphore(int permit)&#123; this.permit = permit; &#125; // public synchronized void acquire() throws InterruptedException &#123; // 获取许可 while (permit == 0)&#123; // 无法获取许可 //等待 wait(); &#125; //执行permit--操作 permit--; &#125; public synchronized void release()&#123; //释放许可 //执行permit++ 操作 permit++; while (permit &gt; 0)&#123; //唤醒 notifyAll(); &#125; &#125; &#125; //消息传递实现 // // 创建两个队列为邮箱 一个存放&quot;有东西的箱子&quot; 让消费者取,取完将空箱子放到第二个队列, //生产者从空箱子队列取箱子装东西放到 第一个队列 // 生产者阻塞:没有空箱子(队列2为空),消费者阻塞:没有东西取(队列1为空) 进程通信1. 管道管道是通过调用 pipe 函数创建的，fd[0] 用于读，fd[1] 用于写。 12#include &lt;unistd.h&gt;int pipe(int fd[2]); 它具有以下限制： 只支持半双工通信（单向交替传输）； 只能在父子进程或者兄弟进程中使用。 2. FIFO也称为命名管道，去除了管道只能在父子进程中使用的限制。 123#include &lt;sys/stat.h&gt;int mkfifo(const char *path, mode_t mode);int mkfifoat(int fd, const char *path, mode_t mode); FIFO 常用于客户-服务器应用程序中，FIFO 用作汇聚点，在客户进程和服务器进程之间传递数据。 3. 消息队列相比于 FIFO，消息队列具有以下优点： 消息队列可以独立于读写进程存在，从而避免了 FIFO 中同步管道的打开和关闭时可能产生的困难； 避免了 FIFO 的同步阻塞问题，不需要进程自己提供同步方法； 读进程可以根据消息类型有选择地接收消息，而不像 FIFO 那样只能默认地接收。 4. 信号量它是一个计数器，用于为多个进程提供对共享数据对象的访问。 5. 共享存储允许多个进程共享一个给定的存储区。因为数据不需要在进程之间复制，所以这是最快的一种 IPC。 需要使用信号量用来同步对共享存储的访问。 多个进程可以将同一个文件映射到它们的地址空间从而实现共享内存。另外 XSI 共享内存不是使用文件，而是使用内存的匿名段。 6. 套接字与其它通信机制不同的是，它可用于不同机器间的进程通信。 2.4 死锁概述 死锁概念 条件 避免 预防 第三章 内存管理第四章IO设备管理RAIDRAID的基本特性由两部分组成:磁盘阵列(一组磁盘)可以并行工作，磁盘阵列管理软件磁盘阵列一组数据管理软件在逻辑上连续交叉分布，存储在磁盘阵列中到磁盘上。优点:磁盘阵列管理软件可以并行处理一组数据的单个或多个数据访问请求。磁盘阵列管理软件还负责校验存储相关信息。受益:当磁盘阵列出现磁盘故障时，磁盘阵列管理软件可以恢复磁盘上的数据。磁盘阵列管理软件屏蔽了磁盘阵列的物理细节，操作系统的其他成分不知道磁盘阵列的存在;在他们看来，逻辑盘的容量过大。 Linux五种IO模型Java NIO,AIO 依赖于epoll IO有两个阶段 等待数据 (写入socket直到socket可读) 和将数据从内核复制到用户空间复制完成返回成功指示 1. 阻塞IO进程阻塞于两个阶段 直到最后返回成功指示 进程阻塞(CPU空转了一直被动等待IO数据准备好) 2. 非阻塞IO自己主动问询准备好了吗recvfrom指令 3. IO多路复用select异步阻塞IO 由select帮线程轮询socket 但是进程阻塞于select直到多个socket其中一个可读 select使用了Reactor设计模式 由select帮线程轮询socket 可读后在通知用户线程 异步: 进程走走停停 以不可知的速度执行, 不用等待结果,等回调在拿到结果 4. (信号驱动)第一阶段非阻塞IO5. 真正的异步IO两个阶段都不需要阻塞 linux中的aio_read select、poll、epoll的区别？ select, poll, epoll 都是I&#x2F;O多路复用的具体的实现，之所以有这三个存在，其实是他们出现是有先后顺序的。 https://blog.csdn.net/nanxiaotao/article/details/90612404 https://www.cnblogs.com/aspirant/p/9166944.html https://www.zhihu.com/question/32163005 select 它仅仅知道了，有I&#x2F;O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。 单个进程可监视的fd_set(监听的端口个数)数量被限制：32位机默认是1024个，64位机默认是2048。 pollpoll本质上和select没有区别，采用链表的方式替换原有fd_set数据结构,而使其没有连接数的限制。 epoll epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I&#x2F;O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数。即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 epoll通过内核和用户空间共享一块内存来实现的。select和poll都是内核需要将消息传递到用户空间，都需要内核拷贝动作 epoll有EPOLLLT和EPOLLET两种触发模式。(暂时不去记，有个印象，大致是什么样就可以) linux进程管理进程类型 前台进程 占用终端shell 在Linux系统中执行某些操作时候，有时需要将当前任务暂停调至后台好执行其他命令；有时又得将后台暂停的任务调至前台重新运行。这些操作可使用 jobs、bg 和 fg 三个命令以及 Ctrl + z 快捷键来完成。 按下ctr+z，将当前运行的程序放入后台挂起，此时你就可以执行其他任务了。jobs 命令，显示后台被挂起的所有进程bg N 使第N个序号的任务在后台运行fg N 使第N个序号的任务在前台运行 ctr+c 退出前台进程 注：默认bg,fg不带N时表示对最后一个进程操作!例如：假如我在使用 vim a.js遍历一个文件，此时我可以使用ctrl+z将vim放入后台；再调用less config.cfg查看文件，暂时找到需要的东西后再使用ctrl+z将less查看放入后台； jobs查看所有的后台任务。记下刚才挂起的vim序列号为1，再通过fg 1将其从后台重新放入前台进行编辑。 常用指令netstat从整体上看，netstat的输出结果可以分为两个部分： 一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。 另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。 Proto显示连接使用的协议 RefCnt表示连接到本套接口上的进程号 Types显示套接口的类型 State显示套接口当前的状态 Path表示连接到套接口的其它进程使用的路径名 -a 显示所有连接和侦听端口，默认不显示LISTEN相关-b 显示在创建每个连接或侦听端口时涉及的可执行程序。在某些情况下，已知可执行程序承载多个独立的组件，这些情况下，显示创建连接或侦听端口时涉及的组件序列。在此情况下，可执行程序的名称位于底部 [] 中，它调用的组件位于顶部，直至达到 TCP&#x2F;IP。注意，此选项可能很耗时，并且在你没有足够权限时可能失败。-e 显示以太网统计信息。此选项可以与 -s 选项结合使用。 -i 显示网络接口列表-n 以数字形式显示地址和端口号。-o 显示拥有的与每个连接关联的进程 ID。-p proto 显示 proto 指定的协议的连接；proto可以是下列任何一个: TCP、UDP、TCPv6 或 UDPv6。如果与 -s选项一起用来显示每个协议的统计信息，proto 可以是下列任何一个:IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 或 UDPv6。-q 显示所有连接、侦听端口和绑定的非侦听 TCP 端口。绑定的非侦听端口不一定与活动连接相关联。-r 显示路由表。-s 显示每个协议的统计信息。默认情况下，显示 IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 和 UDPv6 的统计信息;-p 选项可用于指定默认的子网。interval 重新显示选定的统计信息，各个显示间暂停的间隔秒数。按 CTRL+C 停止重新显示统计信息。如果省略，则 netstat 将打印当前的配置信息一次。-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-c 每隔一个固定时间，执行该netstat命令，netstat 将每隔一秒输出网络信息。 psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD","categories":[{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/tags/OS/"}]},{"title":"Net","slug":"16net","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T08:54:43.145Z","comments":true,"path":"2022/09/01/16net/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/16net/","excerpt":"","text":"第一章 计算机网络概述四层协议，五层协议和七层协议的关系如下： TCP&#x2F;IP是一个四层: 应用层、运输层、网际层和网络接口层。五层协议的体系结构主要包括：应用层、运输层、网络层，数据链路层和物理层。OSI七层协议模型主要包括是：应用层（Application）、表示层（Presentation）、会话层（Session）、运输层（Transport）、网络层（Network）、数据链路层（Data Link）、物理层（Physical）。 第二章 数据链路层(帧)实际场景中，⽹络并不是⼀个整体，⽐如你家和我家就不属于⼀个⽹络，所以数据不仅可以在同⼀个⽹络中设备间 进⾏传输，也可以跨⽹络进⾏传输 ⼀旦数据需要跨⽹络传输，就需要有⼀个设备同时在两个⽹络当中，这个设备⼀般是路由器，路由器可以通过路由 表计算出下⼀个要去的 IP 地址 那问题来了，路由器怎么知道这个 IP 地址是哪个设备的呢 于是，就需要有⼀个专⻔的层来标识⽹络中的设备，让数据在⼀个链路中传输，这就是数据链路层（Data Link Layer），它主要为⽹络层提供链路级别传输的服务 每⼀台设备的⽹卡都会有⼀个 MAC 地址，它就是⽤来唯⼀标识设备的。路由器计算出了下⼀个⽬的地 IP 地址，再 通过 ARP 协议找到该⽬的地的 MAC 地址，这样就知道这个 IP 地址是哪个设备的了 管理相邻结点的数据通信 差错检测 奇偶校验码 循环冗余校验码CRC r为g(x)最高阶 奇偶校验码就是CRC-1MTU MTU 路径MTU 帧也不是无限大的 MTU即为最大传输单元 一般为1500 路径MTU由路径中的最小MTU决定 以太网协议(Ethernet应用于数据链路层) 完成相邻设备的数据帧传输 MAC地址 以太网协议数据格式 MAC地址表 以太网协议应用 第三章 网络层传输层可能⼤家刚接触的时候，会认为它负责将数据从⼀个设备传输到另⼀个设备，事实上它并不负责。 实际场景中的⽹络环节是错综复杂的，中间有各种各样的线路和分叉路⼝，如果⼀个设备的数据要传输给另⼀个设 备，就需要在各种各样的路径和节点进⾏选择，⽽传输层的设计理念是简单、⾼效、专注，如果传输层还负责这⼀ 块功能就有点违背设计原则了。 也就是说，我们不希望传输层协议处理太多的事情，只需要服务好应⽤即可，让其作为应⽤间数据传输的媒介，帮助实现应⽤到应⽤的通信，⽽实际的传输功能就交给下⼀层，也就是⽹络层（Internet Layer） 决定数据在网络中的路径 解决跨设备的数据传输 ⽹络层最常使⽤的是 IP 协议（Internet Protocol），IP 协议会将传输层的报⽂作为数据部分，再加上 IP 包头组装 成 IP 报⽂，如果 IP 报⽂⼤⼩超过 MTU（以太⽹中⼀般为 1500 字节）就会再次进⾏分⽚，得到⼀个即将发送到⽹ 络的 IP 报⽂ ⽹络层负责将数据从⼀个设备传输到另⼀个设备，世界上那么多设备，⼜该如何找到对⽅呢？因此，⽹络层需要有 区分设备的编号。 我们⼀般⽤ IP 地址给设备进⾏编号，对于 IPv4 协议， IP 地址共 32 位，分成了四段，每段是 8 位。只有⼀个单纯 的 IP 地址虽然做到了区分设备，但是寻址起来就特别麻烦，全世界那么多台设备，难道⼀个⼀个去匹配？这显然 不科学 因此，需要将 IP 地址分成两种意义： ​ ⼀个是⽹络号，负责标识该 IP 地址是属于哪个⼦⽹的； ​ ⼀个是主机号，负责标识同⼀⼦⽹下的不同主机 怎么分的呢？这需要配合⼦⽹掩码才能算出 IP 地址 的⽹络号和主机号。那么在寻址的过程中，先匹配到相同的⽹络号，才会去找对应的主机。 除了寻址能⼒， IP 协议还有另⼀个重要的能⼒就是路由。实际场景中，两台设备并不是⽤⼀条⽹线连接起来的， ⽽是通过很多⽹关、路由器、交换机等众多⽹络设备连接起来的，那么就会形成很多条⽹络的路径，因此当数据包 到达⼀个⽹络节点，就需要通过算法决定下⼀步⾛哪条路径。 所以，IP 协议的寻址作⽤是告诉我们去往下⼀个⽬的地该朝哪个⽅向⾛，路由则是根据「下⼀个⽬的地」选择路 径。寻址更像在导航，路由更像在操作⽅向盘 主要内容IP协议的作用封装复杂的实际网络化为一个虚拟互联的网络供上层使用 IP头部 版本：占4位，指的是IP协议的 版本，通信双方的版本必须一 致，当前主流版本是4，即IPv4， 也有IPv6 首部位长度：占4位，最大数值 为15，表示的是IP首部长度， 单位是“32位字”（4个字节）， 也即是IP首部最大长度为60字 节 总长度：占16位，最大数值为 65535，表示的是IP数据报总长 度（IP首部+IP数据） 标识 标志 片偏移 TTL：占8位，表明IP数据报文 在网络中的寿命，每经过一个 设备，TTL减1，当TTL&#x3D;0时， 网络设备必须丢弃该报文 协议：占8位，表明IP数据所携 带的具体数据是什么协议的 （如：TCP、UDP等） 首部校验和：占16位，校验IP 首部是否有出错 路由表 IP协议的转发流程 数据帧每一跳的MAC地址都在变化 IP数据报每一跳的IP地址始终不变 ARP协议与RARP协议 由这个协议完成 路由器不知道MAC地址 划分子网 无分类编址CIDR NAT技术 网络地址转换NAT(Network Address Translation) NAT技术用于多个主机通过一个公有IP访问互联网的私有网络中 NAT减缓了IP地址的消耗，但是增加了网络通信的复杂度 ICMP 网际控制报文协议（Internet Control Message Protocol） ICMP协议可以报告错误信息或者异常情况 ICMP头部两种报文 差错报告与询问 第四章 传输层概述应⽤层的数据包会传给传输层，传输层（Transport Layer）是为应⽤层提供⽹络⽀持的 TCP 的全称叫传输层控制协议（Transmission Control Protocol），⼤部分应⽤使⽤的正是 TCP 传输层协议，⽐ 如 HTTP 应⽤层协议。TCP 相⽐ UDP 多了很多特性，⽐如流量控制、超时重传、拥塞控制等，这些都是为了保证 数据包能可靠地传输给对⽅ UDP 就相对很简单，简单到只负责发送数据包，不保证数据包是否能抵达对⽅，但它实时性相对更好，传输效率 也⾼。当然，UDP 也可以实现可靠传输，把 TCP 的特性在应⽤层上实现就可以，不过要实现⼀个商⽤的可靠 UDP 传输协议，也不是⼀件简单的事情 应⽤需要传输的数据可能会⾮常⼤，如果直接传输就不好控制，因此当传输层的数据包⼤⼩超过 MSS（TCP 最⼤ 报⽂段⻓度） ，就要将数据包分块，这样即使中途有⼀个分块丢失或损坏了，只需要᯿新这⼀个分块，⽽不⽤᯿新 发送整个数据包。在 TCP 协议中，我们把每个分块称为⼀个 TCP 段（TCP Segment） 当设备作为接收⽅时，传输层则要负责把数据包传给应⽤，但是⼀台设备上可能会有很多应⽤在接收或者传输数 据，因此需要⽤⼀个编号将应⽤区分开来，这个编号就是端⼝。 ⽐如 80 端⼝通常是 Web 服务器⽤的，22 端⼝通常是远程登录服务器⽤的。⽽对于浏览器（客户端）中的每个标 签栏都是⼀个独⽴的进程，操作系统会为这些进程分配临时的端⼝号。 由于传输层的报⽂中会携带端⼝号，因此接收⽅可以识别出该报⽂是发送给哪个应⽤ TCP报文字段序列号：在建⽴连接时由计算机⽣成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送⼀次数据，就 「累加」⼀次该「数据字节数」的⼤⼩。⽤来解决⽹络包乱序问题。 确认应答号：指下⼀次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数 据都已经被正常接收。⽤来解决不丢包的问题 TCP连接连接: ⽤于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括Socket、序列号和窗⼝ ⼤⼩称为连接 第五章 应用层概述最上层的，也是我们能直接接触到的就是应⽤层（Application Layer），我们电脑或⼿机使⽤的应⽤软件都是在应 ⽤层实现。那么，当两个不同设备的应⽤需要通信的时候，应⽤就把应⽤数据传给下⼀层，也就是传输层。 所以，应⽤层只需要专注于为⽤户提供应⽤功能，不⽤去关⼼数据是如何传输的，就类似于，我们寄快递的时候， 只需要把包裹交给快递员，由他负责运输快递，我们不需要关⼼快速是如何被运输的。 ⽽且应⽤层是⼯作在操作系统中的⽤户态，传输层及以下则⼯作在内核态 HTTPHTTP 是⼀个在计算机世界⾥专⻔在「两点」之间「传输」⽂字、图⽚、⾳频、视频等「超⽂本」数据的「约定和 规范」 两点(C&#x2F;S S&#x2F;S B&#x2F;S等都可) 常用状态码1xx: 提示信息，表示目前是协议处理的中间状态，还需要后续的操作;2xx: 成功，报文已经收到并被正确处理;3xx: 重定向，资源位置发生变动，需要客户端重新发送请求;4xx: 客户端错误，请求报文有误，服务器无法处理;5xx: 服务器错误，服务器在处理请求时内部发生了错误。 1xx 类状态码属于提示信息，是协议处理中的⼀种中间状态，实际⽤到的⽐较少。 2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。 「200 OK」是最常⻅的成功状态码，表示⼀切正常。如果是⾮ HEAD 请求，服务器返回的响应头都会有 body 数据。 「204 No Content」也是常⻅的成功状态码，与 200 OK 基本相同，但响应头没有 body 数据。 「206 Partial Content」是应⽤于 HTTP 分块下载或断点续传，表示响应返回的 body 数据并不是资源的全部，⽽ 是其中的⼀部分，也是服务器处理成功的状态。 3xx 类状态码表示客户端请求的资源发送了变动，需要客户端⽤新的 URL 重新发送请求获取资源，也就是重定 向。 「301 Moved Permanently」表示永久重定向，说明请求的资源已经不存在了，需改⽤新的 URL 再次访问。 「302 Found」表示临时重定向，说明请求的资源还在，但暂时需要⽤另⼀个 URL 来访问。 301 和 302 都会在响应头⾥使⽤字段 Location ，指明后续要跳转的 URL，浏览器会⾃动᯿定向新的 URL。 「304 Not Modified」不具有跳转的含义，表示资源未修改，重定向已存在的缓冲⽂件，也称缓存重定向，⽤于缓 存控制。 4xx 类状态码表示客户端发送的报⽂有误，服务器⽆法处理，也就是错误码的含义。 「400 Bad Request」表示客户端请求的报⽂有错误，但只是个笼统的错误。 「403 Forbidden」表示服务器禁⽌访问资源，并不是客户端的请求出错。 「404 Not Found」表示请求的资源在服务器上不存在或未找到，所以⽆法提供给客户端。 5xx 类状态码表示客户端请求报⽂正确，但是服务器处理时内部发⽣了错误，属于服务器端的错误码。 「500 Internal Server Error」与 400 类型，是个笼统通⽤的错误码，服务器发⽣了什么错误，我们并不知道。 「501 Not Implemented」表示客户端请求的功能还不⽀持，类似“即将开业，敬请期待”的意思。 「502 Bad Gateway」通常是服务器作为⽹关或代理时返回的错误码，表示服务器⾃身⼯作正常，访问后端服务器 发⽣了错误。 「503 Service Unavailable」表示服务器当前很忙，暂时⽆法响应服务器，类似“⽹络服务正忙，请稍后重试”的意 思 常见字段Host 字段 客户端发送请求时，⽤来指定服务器的域名 Host: www.A.com 服务器在返回数据时，会有 Content-Length 字段，表明本次回应的数据⻓度(字节) Content-Length: 1000 HTTP&#x2F;1.1 版本的默认连接都是持久连接，但为了兼容⽼版本的 HTTP，需要指定 Connection ⾸部字段的值为 Keep-Alive 。 ⼀个可以复⽤的 TCP 连接就建⽴了，直到客户端或服务器主动关闭连接。但是，这不是标准字段 Connection: keep-alive Content-Type 字段⽤于服务器回应时，告诉客户端，本次数据是什么格式 Content-Type: text&#x2F;html; charset&#x3D;utf-8 客户端请求的时候，可以使⽤ Accept 字段声明⾃⼰可以接受哪些数据格式。 Accept: &#x2F; Content-Encoding 字段说明数据的压缩⽅法。表示服务器返回的数据使⽤了什么压缩格式 Content-Encoding: gzip 上⾯表示服务器返回的数据采⽤了 gzip ⽅式压缩，告知客户端需要⽤此⽅式解压。 客户端在请求时，⽤ Accept-Encoding 字段说明⾃⼰可以接受哪些压缩⽅法 Accept-Encoding: gzip, deflate HTTPS 也就是在 HTTP 与 TCP 层之间增加了 SSL&#x2F;TLS 安全传输层，HTTP&#x2F;3 甚⾄把 TCP 层换成了基于 UDP 的 QUIC。 \\1. ⽆状态双刃剑 ⽆状态的好处，因为服务器不会去记忆 HTTP 的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的 负担，能够把更多的 CPU 和内存⽤来对外提供服务。 ⽆状态的坏处，既然服务器没有记忆能⼒，它在完成有关联性的操作时会⾮常麻烦。 例如登录-&gt;添加购物⻋-&gt;下单-&gt;结算-&gt;⽀付，这系列操作都要知道⽤户的身份才⾏。但服务器不知道这些请求是有 关联的，每次都要问⼀遍身份信息。 这样每操作⼀次，都要验证信息，这样的购物体验还能愉快吗？别问，问就是酸爽 用cookie解决 http缓存机制http研究之旅：expires 将 expires的值设置为了 new Date(Date.now()+60000)，有效期就是一分钟，即60秒看看响应头部（response header）： 在 expires设置的 有效期内，http请求不会真的向服务器发起请求，而是直接从缓存里获取数据expires 是通过 设置资源的有效期来控制http的缓存 浏览器缓存机制详解 一、为什么需要缓存在前端开发中，我们主要追求的是性能和用户体验。对于一个网站查看性能最简单的方式就是打开网站的速度。而一个好的缓存策略可以大大提升网站的性能。使得已经下载后的资源被重复利用。减少客户端和服务器之间的请求次数，减少带宽，减少网络负荷。 二、什么是缓存对于web缓存，主要是针对一些web资源（html、 js、图片、数据等），就是介于web服务器和浏览器之间的文件数据副本。当我们第一次打开某一个网页时，浏览器向服务器发起请求，请求所需要的资源。如果我们使用了web缓存，当我们下一次再次访问该网站页面的时候，我们可以根据一些缓存策略，来决定是否直接使用缓存中的一些资源，还是再次向服务端发起请求，从而避免再次向服务器发起请求，减少客户端和服务器之间通信的时延。 三、缓存的作用 减少网络带宽的消耗 降低服务器压力 减少网络延时，加快页面打开速度。 四、浏览器的缓存机制对于浏览器端的缓存来说，规则是在http协议头和html的mate标签中定义的，他们分别从过期机制和校验值来判断是否直接使用该缓存，还是需要从服务器去获取更新的版本。 1.新鲜度(过期机制)：也就是缓存副本的有效期。一个缓存副本必须满足以下条件之一，浏览器才会认为它是有效的，足够新的，才会直接使用缓存。 http协议头中存在过期时间等信息，并且仍在有效期内。 浏览器已经使用过这个缓存副本，并且在一个会话中已经检查过新鲜度。 2.校验值(验证机制)：服务器相应中，在响应头中存在Etag标签，用来验证资源是否更改的标识，如果缓存的标识和服务器的标识相同则无需重新请求资源，如果不相同，则重新发送资源请求。 五、浏览器缓存控制1.html中的mate标签设置缓存1234设置过期时间＜meta http-equiv=&quot;expires&quot; content=&quot;Wed, 20 Jun 2007 22:33:00 GMT&quot;＞ 设置缓存＜meta http-equiv=&quot;Pragma&quot; content=&quot;no-cache&quot;＞ 2.与缓存有关的字段Cache-control:max-age(单位为s),当某一个资源的响应头设置max-age&#x3D;3600， 则表示在1h时间内，服务器的资源发生变化，浏览器都不会想服务器发送该资源的请求，直接使用缓存。并且max-age会覆盖Expires。 如下图所示 Cache-control:s-maxage,s-maxage表示CDN缓存，也就是代理缓存，如果设置s-maxage&#x3D;60,表示60秒内无论cdn服务器的该资源发生怎么样的改变，都不会重新请求，并且s-maxage会覆盖max-age和Expires. Cache-control:public，指定是否是共享缓存，如果设置Cache-control的值设置为public，则表示多个浏览器之间可以共同使用该资源缓存。如果没有指定Cache-control是为private还是public，则默认是public. Cache-control:private，表示该缓存是私有的，不存在用户共享。 Cache-control:no-cache；Cache-control的值设置为no-cache并不代表不缓存，浏览器是缓存的，但是当每一次访问该资源的时候，都要向服务器请求查看资源是否改变，如果改变，则直接重新下载，如果没有改变，则使用缓存。可以在设置完no-cache之后，在设置private，以及设置过期时间为过去的时间。 Cache-control:no-store,表示严格不缓存，每一次资源必须从服务器上重新获取。 Expires:缓存过期时间，Expires&#x3D;max-age + 最后一次请求的时间。Cache-control和Expires相比，Cache-control的优先级更高。Expires需要和Last-modifyed来一起使用。 Last-Modified和if-modified-since:last-modified是响应头上的属性，if-modifyed-since是请求头的数据。该属性值需要cache-control配合使用。当再次向服务器发送请求该资源时，会携带if-modified-since的请求头字段，到服务器比对和last-modified是否相同。如果相同则直接返回304,直接使用缓存，如果不相同，则再次请求新的数据，返回200。 ETag和if-None-Match:这俩个属性其实和last-modified和if-modified-since类似。不过Etag是服务器更加内容产生的hash字符串，并且Etag是响应头内容。if-None-match是请求头的内容。当再次向服务器发送请求某一个资源时，请求头会携带if-None-match属性，到达服务器后，和Etag进行比对。如果相同，则返回304，如果不相同则返回该资源，并且状态码为200。 六、缓存报文头种类和优先级1.Cache-control和Expires比较Cache-control的优先级比Expires的优先级高。 2.Last-Modified和ETag比较Etag的优先级要高于Last-modified，当在请求头中会先进行ETag比较，然后再进行Last-modified比较，如果两者都相等，则直接返回304,直接使用缓存资源。两者比较一下，你可能会觉得两者的功能差不多，但是为什么要在http1.1中新增Etag呢？ Last-modified精确到秒，如果在一秒钟内修改多次文件，则无法准确拿到最新的文件。 如果缓存文件，打开后但是不修改内容，导致Last-modified发生变化，下一次就没有办法使用缓存文件了。 可能存在服务器没有获取准确的修改时间，或者代理服务器时间不一致的情况。 3.Last-Modified&#x2F;Etag和Cache-control&#x2F;Expires比较Cache-control&#x2F;Expries的优先级要比Last-Modified&#x2F;Etag的优先级高，当第二次发送请求时，会首先查看Cache-control&#x2F;Expries是否过期，如果没有过期，则任然使用该资源，如果过期了，则再次向服务器发送请求来请求最新的资源。到达服务器时通过比对Last-modified&#x2F;Etag是否和原来的值相等，来判断资源是否改变，如果没有改变，则返回304。如果改变了，则返回最新的资源，并且状态码为200。 七、有哪些请求不能进行缓存的无法被浏览器缓存的请求 http信息头部cache-control:no-cache , pragma: nocache或者使用cache-control:max-age&#x3D;0。 根据cookie，认证信息决定输入内容的信息是否可以被缓存的。 经过https加密的请求。 post请求无法被缓存。 在http响应头中不存在last-modified&#x2F;Etag和cache-control&#x2F;expires等。 八、使用缓存流程 上面的过程可以分为三个阶段： 本地缓存阶段：如果本地存在缓存，并且通过检查本地资资源的缓存并没有过期，则直接使用本地缓存。 协商缓存阶段：如果在本地存在该资源，但是本地资源已经过期，此时就需要封装http请求，向服务端发送请求，检查是否存在更改资源。如果资源没有更改，则直接返回304，直接在本地使用资源。 缓存失败阶段：如果资源发生了更改，则重新返回最新的资源，并且返回状态码为200。如果此时不存在该资源，则直接返回404。 九、用户行为与缓存的关系用户在浏览器采用一些操作，例如，返回上一阶段，下一阶段，刷新页面，强制刷新等操作，这些对于一些缓存属性的影响是不一样的。下面将进行详细解读。 刷新（仅仅是F5刷新）：此时对于cache-control&#x2F;Expires是不生效的，但是last-modified&#x2F;Etag都生效的，所以此时会向服务器发起请求，用来判断目标文件是否发生变化。 强制刷新(F5刷新+ctrl)：此时对于cache-control&#x2F;expires和last-modified&#x2F;Etag都不生效，此时必须从服务器拿到新数据。 回车或者转向：此时所有的缓存都生效。 十、从缓存角度改善站点 同一个资源保证只有一个稳定的url地址。 css,js,图片资源增加http缓存头，入口html文件不被缓存。 减少对cookie的依赖。 减少对http协议加密的使用。 HTTPSTLS 握手过程，主要目的是为了协商对称加密的密钥，因为在最终的通信链路上使用对称加解密会更快。 我们知道，生成最终通信的对称密钥需要三个随机数： 客户端随机数 服务端随机数 pre-master 前两个是公开的，那么最重要的就是 pre-master，在协商过程中如何保证它不会被窃取。为了保证协商安全性、对端可靠性，那么采用非对称加密的方式会更加合适，因为私钥只有服务端持有。 在 TLS 中采用的非对称加密方式主要有如下两种： RSA ECDHE 而这两种方式的区别，就在于 pre-master 的生成方式不同。 作者：微微笑的蜗牛链接：https://www.jianshu.com/p/11d6eb418780来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 前向保密:pfs保护过去的会话以免在未来秘钥或密码的（泄露）造成的危害。因为对手或恶意方会活跃地干扰，导致长期秘钥（long-term secret keys）或者密码在未来可能被泄露，但是如果使用了pfs，即使秘钥被泄露，过去的加密的通信和会话记录也不会被恢复和解密。 TLS握手:RSA实现对称秘钥交换防止泄露 三次握手后 C: client hello (发送随机数C,TLS版本号,密码套件列表) S: ACK S: server hello (发送随机数S,确认TLS版本号,使用的密码套件(RSA)) S: 服务器使用的证书 S: 服务器Hello完成 C: ACK (校验证书取得公钥) ​ 之前是TCP三次握手 TLS 第⼀次握⼿客户端⾸先会发⼀个「Client Hello」消息，字⾯意思我们也能理解到，这是跟服务器「打招呼」。 消息⾥⾯有客户端使⽤的 TLS 版本号、⽀持的密码套件列表，以及⽣成的随机数（Client Random），这个随机 数会被服务端保留，它是⽣成对称加密密钥的材料之⼀。 TLS 第⼆次握⼿当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否⽀持，和从密码套件列表中选择⼀个密码 套件，以及⽣成随机数（Server Random）。 接着，返回「Server Hello」消息，消息⾥⾯有服务器确认的 TLS 版本号，也给出了随机数（Server Random）， 然后从客户端的密码套件列表选择了⼀个合适的密码套件。 可以看到，服务端选择的密码套件是 “Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256”。 这个密码套件看起来真让⼈头晕，好⼀⼤串，但是其实它是有固定格式和规范的。基本的形式是「密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法」， ⼀般 WITH 单词前⾯有两个单词，第⼀个单词是约定密钥交换的算法， 第⼆个单词是约定证书的验证算法。⽐如刚才的密码套件的意思就是： 由于 WITH 单词只有⼀个 RSA，则说明握⼿时密钥交换算法和签名算法都是使⽤ RSA； 握⼿后的通信使⽤ AES 对称算法，密钥⻓度 128 位，分组模式是 GCM； 摘要算法 SHA384 ⽤于消息认证和产⽣随机数； 就前⾯这两个客户端和服务端相互「打招呼」的过程，客户端和服务端就已确认了 TLS 版本和使⽤的密码套件， ⽽且你可能发现客户端和服务端都会各⾃⽣成⼀个随机数，并且还会把随机数传递给对⽅。 那这个随机数有啥⽤呢？其实这两个随机数是后续作为⽣成「会话密钥」的条件，所谓的会话密钥就是数据传输 时，所使⽤的对称加密密钥。 然后，服务端为了证明⾃⼰的身份，会发送「Server Certificate」给客户端，这个消息⾥含有数字证书。 随后，服务端发了「Server Hello Done」消息，⽬的是告诉客户端，我已经把该给你的东⻄都给你了，本次打招 呼完毕。 客户端进行证书验证 TLS 第三次握⼿客户端验证完证书后，认为可信则继续往下⾛。接着，客户端就会⽣成⼀个新的随机数 (pre-master)，⽤服务器 的 RSA 公钥加密该随机数，通过「Change Cipher Key Exchange」消息传给服务端。 服务端收到后，⽤ RSA 私钥解密，得到客户端发来的随机数 (pre-master)。 ⾄此，客户端和服务端双⽅都共享了三个随机数，分别是 Client Random、Server Random、pre-master。 于是，双⽅根据已经得到的三个随机数，⽣成会话密钥（Master Secret），它是对称密钥，⽤于对后续的 HTTP 请求&#x2F;响应的数据加解密。 ⽣成完会话密钥后，然后客户端发⼀个「Change Cipher Spec」，告诉服务端开始使⽤加密⽅式发送消息。 然后，客户端再发⼀个「Encrypted Handshake Message（Finishd）」消息，把之前所有发送的数据做个摘 要，再⽤会话密钥（master secret）加密⼀下，让服务器做个验证，验证加密通信是否可⽤和之前握⼿信息是否有 被中途篡改过。 可以发现，「Change Cipher Spec」之前传输的 TLS 握⼿数据都是明⽂，之后都是对称密钥加密的密⽂。 TLS 第四次握⼿服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双⽅都 验证加密和解密没问题，那么握⼿正式完成。 最后，就⽤「会话密钥」加解密 HTTP 请求和响应了。 TCP和UDP可以监听同一个端口吗TCP UDP 表述上应该是可以绑定同一个端口吗 答案是 可以 协议的不同也能是不同应用 多个 TCP 服务进程可以绑定同一个端口吗？默认情况下，针对「多个 TCP 服务进程可以绑定同一个端口吗？」这个问题的答案是：如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 注意，如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。 这是因为 0.0.0.0 地址比较特殊，代表任意地址，意味着绑定了 0.0.0.0 地址，相当于把主机上的所有 IP 地址都绑定了。 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？TCP 服务进程需要绑定一个 IP 地址和一个端口，然后就监听在这个地址和端口上，等待客户端连接的到来。 然后在实践中，我们可能会经常碰到一个问题，当 TCP 服务进程重启之后，总是碰到“Address in use”的报错信息，TCP 服务进程不能很快地重启，而是要过一会才能重启成功。 这是为什么呢？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 而等 TIME_WAIT 状态的连接结束后，重启 TCP 服务进程就能成功。 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？ 我们可以在调用 bind 前，对 socket 设置 SO_REUSEADDR 属性，可以解决这个问题。 12int on = 1;setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;on, sizeof(on)); 因为 SO_REUSEADDR 作用是：如果当前启动进程绑定的 IP+PORT 与处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就可以绑定成功。 举个例子，服务端有个监听 0.0.0.0 地址和 8888 端口的 TCP 服务进程。‍ 有个客户端（IP地址：192.168.1.100）已经和服务端（IP 地址：172.19.11.200）建立了 TCP 连接，那么在 TCP 服务进程重启时，服务端会与客户端经历四次挥手，服务端的 TCP 连接会短暂处于 TIME_WAIT 状态： 12客户端地址:端口 服务端地址:端口 TCP 连接状态192.168.1.100:37272 172.19.11.200:8888 TIME_WAIT 如果 TCP 服务进程没有对 socket 设置 SO_REUSEADDR 属性，那么在重启时，由于存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，那么在执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 如果 TCP 服务进程对 socket 设置 SO_REUSEADDR 属性了，那么在重启时，即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 因此，在所有 TCP 服务器程序中，调用 bind 之前最好对 socket 设置 SO_REUSEADDR 属性，这不会产生危害，相反，它会帮助我们在很快时间内重启服务端程序。‍ 前面我提到过这个问题：如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。 这个问题也可以由 SO_REUSEADDR 解决，因为它的另外一个作用是：****绑定的 IP地址 + 端口时，只要 IP 地址不是正好(exactly)相同，那么允许绑定。 比如，0.0.0.0:8888 和192.168.1.100:8888，虽然逻辑意义上前者包含了后者，但是 0.0.0.0 泛指所有本地 IP，而 192.168.1.100 特指某一IP，两者并不是完全相同，所以在对 socket 设置 SO_REUSEADDR 属性后，那么执行 bind() 时候就会绑定成功。 客户端的端口可以重复使用吗？客户端在执行 connect 函数的时候，会在内核里随机选择一个端口，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。 所以，客户端的端口选择的发生在 connect 函数，内核在选择端口的时候，会从 net.ipv4.ip_local_port_range 这个内核参数指定的范围来选取一个端口作为客户端端口。 该参数的默认值是 32768 61000，意味着端口总可用的数量是 61000 - 32768 &#x3D; 28232 个。 当客户端与服务端完成 TCP 连接建立后，我们可以通过 netstat 命令查看 TCP 连接。 123$ netstat -napt协议 源ip地址:端口 目的ip地址：端口 状态tcp 192.168.110.182.64992 117.147.199.51.443 ESTABLISHED 那问题来了，上面客户端已经用了 64992 端口，那么还可以继续使用该端口发起连接吗？ 这个问题，很多同学都会说不可以继续使用该端口了，如果按这个理解的话， 默认情况下客户端可以选择的端口是 28232 个，那么意味着客户端只能最多建立 28232 个 TCP 连接，如果真是这样的话，那么这个客户端并发连接也太少了吧，所以这是错误理解。 正确的理解是，TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。所以如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 比如下面这张图，有 2 个 TCP 连接，左边是客户端，右边是服务端，客户端使用了相同的端口 50004 与两个服务端建立了 TCP 连接。 仔细看，上面这两条 TCP 连接的四元组信息中的「目的 IP 地址」是不同的，一个是 180.101.49.12 ，另外一个是 180.101.49.11。 多个客户端可以 bind 同一个端口吗？ bind 函数虽然常用于服务端网络编程中，但是它也是用于客户端的。 前面我们知道，客户端是在调用 connect 函数的时候，由内核随机选取一个端口作为连接的端口。 而如果我们想自己指定连接的端口，就可以用 bind 函数来实现：客户端先通过 bind 函数绑定一个端口，然后调用 connect 函数就会跳过端口选择的过程了，转而使用 bind 时确定的端口。 针对这个问题：多个客户端可以 bind 同一个端口吗？ 要看多个客户端绑定的 IP + PORT 是否都相同，如果都是相同的，那么在执行 bind() 时候就会出错，错误是“Address already in use”。 如果一个绑定在 192.168.1.100:6666，一个绑定在 192.168.1.200:6666，因为 IP 不相同，所以执行 bind() 的时候，能正常绑定。 所以， 如果多个客户端同时绑定的 IP 地址和端口都是相同的，那么执行 bind() 时候就会出错，错误是“Address already in use”。 一般而言，客户端不建议使用 bind 函数，应该交由 connect 函数来选择端口会比较好，因为客户端的端口通常都没什么意义。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 针对这个问题要看，客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。 但是，因为只要客户端连接的服务器不同，端口资源可以重复使用的。 所以，如果客户端都是与不同的服务器建立连接，即使客户端端口资源只有几万个， 客户端发起百万级连接也是没问题的（当然这个过程还会受限于其他资源，比如文件描述符、内存、CPU 等）。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 前面我们提到，如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。 针对这个问题，也是有解决办法的，那就是打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 举个例子，假设客户端已经与服务器建立了一个 TCP 连接，并且这个状态处于 TIME_WAIT 状态： 12客户端地址:端口 服务端地址:端口 TCP 连接状态192.168.1.100:2222 172.19.11.21:8888 TIME_WAIT 然后客户端又与该服务器（172.19.11.21:8888）发起了连接，在调用 connect 函数时，内核刚好选择了 2222 端口，接着发现已经被相同四元组的连接占用了： 如果没有开启 net.ipv4.tcp_tw_reuse 内核参数，那么内核就会选择下一个端口，然后继续判断，直到找到一个没有被相同四元组的连接使用的端口， 如果端口资源耗尽还是没找到，那么 connect 函数就会返回错误。 如果开启了 net.ipv4.tcp_tw_reuse 内核参数，就会判断该四元组的连接状态是否处于 TIME_WAIT 状态，如果连接处于 TIME_WAIT 状态并且该状态持续的时间超过了 1 秒，那么就会重用该连接，于是就可以使用 2222 端口了，这时 connect 就会返回成功。 再次提醒一次，开启了 net.ipv4.tcp_tw_reuse 内核参数，是客户端（连接发起方） 在调用 connect() 函数时才起作用，所以在服务端开启这个参数是没有效果的。 客户端端口选择的流程总结 至此，我们已经把客户端在执行 connect 函数时，内核选择端口的情况大致说了一遍，为了让大家更明白客户端端口的选择过程，我画了一流程图。 总结 TCP 和 UDP 可以同时绑定相同的端口吗？ 可以的。 TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP&#x2F;UDP，所以可以根据这个信息确定送给哪个模块（TCP&#x2F;UDP）处理，送给 TCP&#x2F;UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此， TCP&#x2F;UDP 各自的端口号也相互独立，互不影响。 多个 TCP 服务进程可以同时绑定同一个端口吗？ 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。 如何解决服务端重启时，报错“Address already in use”的问题？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 要解决这个问题，我们可以对 socket 设置 SO_REUSEADDR 属性。 这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 客户端的端口可以重复使用吗？ 在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。 TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。 所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态。 如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 RSA 握手如果双方使用传统的 RSA 算法进行密钥交换，那么 pre-master 是由客户端生成的一个随机数，然后用服务器公钥加密后发给服务端，服务端用私钥解密得到 pre-master。 双方再根据 client-random + server-random + pre-master 三个参数计算出主密钥 master-key。 流程图如下： RSA.png 但是这种方式会存在安全问题，它不具有前向安全性。那什么是前向安全性？也就是指历史数据的安全性，不会被破解。 假设一个黑客收集了很多历史数据，当他破解服务端私钥后，可以计算出 pre-master，从而根据历史数据中的 client-random + server-random 计算出密钥，然后就可解密所有之前的加密数据。并且，由于私钥是固定的，在后续新的会话中，仍然可以获取 pre-master，继续截获信息。所以 RSA 并不安全。 ECDHE 握手而现在主流的 TLS 握手算法，一般会选择安全性更强的 ECDHE 实现密钥交换，即椭圆曲线算法，相比 RSA 算法来说具有前向安全性。 因为在每次握手过程中，服务端和客户端都重新会生成 ECDHE 算法的参数，也就是一对公私钥，并且是一次一密。即使黑客截获了当前会话，那也只能监听该次通信内容。 流程图如下： ECDHE.png 从图中我们可以看到，在服务端发送 Server Certificate 后，多了一步 Server Key Exchange 的过程。 服务端会生成一个公钥 server-public-key 发给客户端。 客户端在 Client Key Change 时也会生成一个公钥 client-public-key 发给服务端。 最终，pre-master 由服务端的 server-public-key + 客户端的 client-public-key，再根据 ECDHE 算法计算而来。 该算法可以保证两边计算出来的结果是一样的。 ECDHE 原理在了解什么是 ECDHE 之前，首先可先了解下 DH 算法 的原理。 其实很简单，就是利用了模幂运算的特性。 1gᵃᵇ mod p = (gᵃ mod p)ᵇ mod p = (gᵇ mod p)ᵃ mod p 对照上面 ECDHE 握手过程图来说。 客户端在 Client Key Change 这一步的公私钥数据如下： 12私钥：a公钥：A = gᵃ mod p 服务端在 Server Key Change 这一步的公私钥数据如下： 12私钥：b公钥：B = gᵇ mod p 客户端根据服务端传来的公钥 B 和自己的私钥 a，计算出秘钥 k1： 1k1 = Bᵃ mod p = (gᵇ mod p)ᵃ mod p = gᵃᵇ mod p 服务端根据客户端传来的公钥 A 和自己的私钥 b，计算出秘钥 k2： 1k2 = Aᵇ mod p = (gᵃ mod p)ᵇ mod p = gᵃᵇ mod p 那么，由此计算出的 k1 和 k2 是相等的。 那 DHE 又是什么呢？DHE 算法与 DH 原理是相同的，只不过由于 DH 算法一方的公钥是固定的，不具有前向安全性。因此改进成了 DHE，E 代表 ephemeral，短暂的，即每次都生成公私钥。 ECDHE 则是在 DHE 的基础上，将整数域里的离散对数替换成了椭圆曲线的离散对数，使其更难以破解，更加安全。 注意：ECDHE 算法参数 public-key 是不需要加密的。因为即使黑客拿到了公钥参数，也很难计算出 pre-master。 两者区别两种算法的区别主要在于： RSA 私钥是固定的，破解后可以得到所有的算法参数。 ECDHE 是每次重新生成参数，一次一密，更加安全。 因此在 TLS 1.3 中，废除了 RSA 和 DH 算法，使用了更加安全的 ECDHE。 作者：微微笑的蜗牛链接：https://www.jianshu.com/p/11d6eb418780来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/categories/Net/"}],"tags":[{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/tags/Net/"}]},{"title":"Linux","slug":"17linux","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-19T04:42:41.364Z","comments":true,"path":"2022/09/01/17linux/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/17linux/","excerpt":"","text":"内核参数调整**作为Java开发者，必可避免的需要开发或使用一些中间件，对于Java开发的中间件，除了JVM参数必须调整外， 的一些内核参数也必须要调整，这里几个，仅供参考。 ** 无非都是跟磁盘文件IO、网络通信、内存管理、线程数量有关系的，因为我们的中间件系统在运行的时候无非就是跟这些打交道。 ![介绍几个Java大型中间件系统中须调整的Linux内核参数介绍几个Java大型中间件系统中须调整的Linux内核参数] 这个参数有三个值可以选择，0、1、2。 如果值是0的话，在你的中间件系统申请内存的时候，操作系统内核会检查可用内存是否足够，如果足够的话就分配内存给你，如果感觉剩余内存不是太够了，干脆就拒绝你的申请，导致你申请内存失败，进而导致中间件系统异常出错。因此一般需要将这个参数的值调整为1，意思是把所有可用的物理内存都允许分配给你，只要有内存就给你来用，这样可以避免申请内存失败的问题。 比如我们曾经线上环境部署的Redis就因为这个参数是0，导致在save数据快照到磁盘文件的时候，需要申请大内存的时候被拒绝了，进而导致了异常报错。 可以用如下 修改： 1echo &#x27;vm.overcommit_memory=1&#x27; &gt;&gt; /etc/sysctl.conf vm.max_map_count 限制一个进程可以拥有的VMA(虚拟内存区域)的数量 这个参数的值会影响中间件系统可以开启的线程的数量，同样也是非常重要的。 如果这个参数过小，有的时候可能会导致有些中间件无法开启足够的线程，进而导致报错，甚至中间件系统挂掉。 他的默认值是65536，但是这个值有时候是不够的，比如我们大数据团队的生产环境部署的Kafka集群曾经有一次就报出过这个异常，说无法开启足够多的线程，直接导致Kafka宕机了。 可以用如下 修改： 1echo &#x27;vm.max_map_count=655360&#x27; &gt;&gt; /etc/sysctl.conf vm.swappiness 这个参数是用来控制进程的swap行为的，这个简单来说就是操作系统会把一部分磁盘空间作为swap区域，然后如果有的进程现在可能不是太活跃，就会被操作系统把进程调整为睡眠状态，把进程中的数据放入磁盘上的swap区域，然后让这个进程把原来占用的内存空间腾出来，交给其他活跃运行的进程来使用。 如果这个参数的值设置为0，意思就是尽量别把任何一个进程放到磁盘swap区域去，尽量大家都用物理内存。 如果这个参数的值是100，那么意思就是尽量把一些进程给放到磁盘swap区域去，内存腾出来给活跃的进程使用。 默认这个参数的值是60，有点偏高了，可能会导致我们的中间件运行不活跃的时候被迫腾出内存空间然后放磁盘swap区域去。因此通常在生产环境建议把这个参数调整小一些，比如设置为10，尽量用物理内存，别放磁盘swap区域去。 可以用如下命令修改： 1echo &#x27;vm.swappiness=10&#x27; &gt;&gt; /etc/sysctl.conf ulimit 这个是用来控制linux上的最大文件连接数的，默认值可能是1024，一般肯定是不够的，因为你在大量频繁的读写磁盘文件的时候，或者是进行网络通信的时候，都会跟这个参数有关系 对于一个中间件系统而言肯定是不能使用默认值得，如果你采用默认值，很可能在线上会出现如下错误： 1error: too many open files 因此通常建议用如下命令修改这个值： 1echo &#x27;ulimit -n 1000000&#x27; &gt;&gt; /etc/profile 一点小小的总结 中间件系统肯定要开启大量的线程（跟vm.max_map_count有关）。 而且要进行大量的网络通信和磁盘IO（跟ulimit有关）。 然后大量的使用内存（跟vm.swappiness和vm.overcommit_memory有关）。 所以对OS内核参数的调整，往往也就是围绕跟中间件系统运行最相关的一些东西。 linux如何管理内存采用段页式管理 Linux 操作系统是采用段页式内存管理方式： 页式存储管理能有效地提高内存利用率（解决内存碎片），而分段存储管理能反映程序的逻辑结构并有利于段的共享。将这两种存储管理方法结合起来，就形成了段页式存储管理方式。 段页式存储管理方式即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。在段页式系统中，为了实现从逻辑地址到物理地址的转换，系统中需要同时配置段表和页表，利用段表和页表进行从用户地址空间到物理内存空间的映射。 系统为每一个进程建立一张段表，每个分段有一张页表。段表表项中至少包括段号、页表长度和页表始址，页表表项中至少包括页号和块号。在进行地址转换时，首先通过段表查到页表始址，然后通过页表找到页帧号，最终形成物理地址。","categories":[{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://gouguoqiang.github.io/tags/Linux/"}]},{"title":"Nginx","slug":"18Nginx","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-22T02:00:20.157Z","comments":true,"path":"2022/09/01/18Nginx/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/18Nginx/","excerpt":"","text":"总结 高可用场景及解决方案添加Nginx主备模式,用keepalived 来管理虚拟ip,主备Nginx对vip进行切换, 安装keepalived 进程间检测对两台nginx所在的服务器的keepalived进行配置 主备 虚拟ip 以及分组 最小配置 第一台 123456789101112131415161718192021global_defs &#123; router_id lb110&#125;vrrp_instance atguigu &#123; # state MASTER #网卡 interface ens33 # virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.44.200 &#125;&#125; 第二台 123456789101112131415161718global_defs &#123; router_id lb110&#125;vrrp_instance atguigu &#123; state BACKUP interface ens33 virtual_router_id 51 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.44.200 &#125;&#125; 选举方式主机down了 备用机获得vip 接收请求 集群化(上游服务的集群化)nginx对集群实现负载均衡 负载均衡的方式以及实操 负载均衡负载策略 会话维持 upstream 负载均衡流程 限流配置针对同一IP 的qps限制 针对同一IP的线程数限制 漏桶算法 性能测试jmetter 网页输入ip 默认访问80端口,openresty自动返回&#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;html下的index.html 基础使用 默认配置初步讲解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#user root;#工作进程 根据cpu核数 如果分配给一个核启动多个进程反而效率低worker_processes 1;#pid logs/nginx.pid;events &#123; #一个工作进程对应多少连接 worker_connections 1024;&#125;http &#123; #引入其他文件 mime里是其他请求头 文件类型 include mime.types; #mime不包含则 启动该默认类型 字节流 default_type application/octet-stream; #数据零拷贝 sendfile on; #tcp_nopush on; #keepalive_timeout 0; # 反向代理细讲 keepalive_timeout 65; #gzip on; #一个 config可以配置多个主机,一个server就是一个主机 #虚拟主机 vhost server &#123; listen 80; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125; 配置&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service，通过systemctl启动： 我这里使用的是openresty 123456789101112[Unit]Description=openresty - high performance web serverAfter=network.target remote-fs.target nss-lookup.target[Service]Type=forkingExecStart=/usr/local/openresty/bin/openresty -c /usr/local/openresty/nginx/conf/nginx.confExecReload=/usr/local/openresty/bin/openresty -s reloadExecStop=/usr/local/openresty/bin/openresty -s stop[Install]WantedBy=multi-user.target 2.使配置生效 1systemctl daemon-reload 多vhost测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950server &#123; listen 88; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root /www/www; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; server &#123; listen 89; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root /www/www2; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 5.2 nginx的限流nginx提供两种限流的方式： 一是控制速率 二是控制并发连接数 5.2.1 控制速率控制速率的方式之一就是采用漏桶算法。 (1)漏桶算法实现控制速率限流 漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率.示意图如下: (2)nginx的配置 配置示意图如下： 超出则返回503 我当前无法响应你 binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。zone：定义共享内存区来存储访问信息， contentRateLimit:10m 表示一个大小为10M，名字为contentRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。rate 用于设置最大访问速率，rate&#x3D;10r&#x2F;s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r&#x2F;s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求.我们这里设置成2 方便测试。 修改&#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;conf&#x2F;nginx.conf: 12345678910111213141516171819202122232425262728293031323334353637383940user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; #使用限流配置 limit_req zone=contentRateLimit; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试： 重新加载配置文件 123cd /usr/local/openresty/nginx/sbin./nginx -s reload 访问页面：http://192.168.211.132/read_content?id=1 ,连续刷新会直接报错。 (3)处理突发流量 上面例子限制 2r&#x2F;s，如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 burst 参数使用来解决该问题。 例如，如下配置表示： 上图代码如下： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数,当 rate&#x3D;10r&#x2F;s 时，将1s拆成10份，即每100ms可处理1个请求。 此处，**burst&#x3D;4 **，若同时有4个请求到达，Nginx 会处理第一个请求，剩余3个请求将放入队列，然后每隔500ms从队列中获取一个请求进行处理。若请求数大于4，将拒绝处理多余的请求，直接返回503. 不过，单独使用 burst 参数并不实用。假设 burst&#x3D;50 ，rate依然为10r&#x2F;s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。 因此，burst 往往结合 nodelay 一起使用。 例如：如下配置： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; limit_req zone&#x3D;one burst&#x3D;5 nodelay;第一个参数：zone&#x3D;contentRateLimit 设置使用哪个配置区域来做限制，与上面limit_req_zone 里的name对应。 第二个参数：burst&#x3D;5，重点说明一下这个配置，burst爆发的意思，这个配置的意思是设置一个大小为5的缓冲区当有大量请求(爆发)过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。 第三个参数：nodelay，如果设置，超过访问频次而且缓冲区也满了的时候就会直接返回503，如果没有设置，则所有请求会等待排队。实例二 burst缓存处理 我们看到，我们短时间内发送了大量请求，Nginx按照毫秒级精度统计，超出限制的请求直接拒绝。这在实际场景中未免过于苛刻，真实网络环境中请求到来不是匀速的，很可能有请求“突发”的情况，也就是“一股子一股子”的。Nginx考虑到了这种情况，可以通过burst关键字开启对突发请求的缓存处理，而不是直接拒绝。 来看我们的配置： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4; }}我们加入了burst&#x3D;4，意思是每个key(此处是每个IP)最多允许4个突发请求的到来。如果单个IP在10ms内发送6个请求，结果会怎样呢？ 相比实例一成功数增加了4个，这个我们设置的burst数目是一致的。具体处理流程是：1个请求被立即处理，4个请求被放到burst队列里，另外一个请求被拒绝。通过burst参数，我们使得Nginx限流具备了缓存处理突发流量的能力。 但是请注意：burst的作用是让多余的请求可以先放到队列里，慢慢处理。如果不加nodelay参数，队列里的请求不会立即处理，而是按照rate设置的速度，以毫秒级精确的速度慢慢处理。 实例三 nodelay降低排队时间 实例二中我们看到，通过设置burst参数，我们可以允许Nginx缓存处理一定程度的突发，多余的请求可以先放到队列里，慢慢处理，这起到了平滑流量的作用。但是如果队列设置的比较大，请求排队的时间就会比较长，用户角度看来就是RT变长了，这对用户很不友好。 有什么解决办法呢？nodelay参数允许请求在排队的时候就立即被处理，也就是说只要请求能够进入burst队列，就会立即被后台worker处理，请注意，这意味着burst设置了nodelay时，系统瞬间的QPS可能会超过rate设置的阈值。nodelay参数要跟burst一起使用才有作用。 延续实例二的配置，我们加入nodelay选项： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4 nodelay; }}单个IP 10ms内并发发送6个请求，结果如下： 跟实例二相比，请求成功率没变化，但是总体耗时变短了。这怎么解释呢？实例二中，有4个请求被放到burst队列当中，工作进程每隔500ms(rate&#x3D;2r&#x2F;s)取一个请求进行处理，最后一个请求要排队2s才会被处理；实例三中，请求放入队列跟实例二是一样的，但不同的是，队列中的请求同时具有了被处理的资格，所以实例三中的5个请求可以说是同时开始被处理的，花费时间自然变短了。 但是请注意，虽然设置burst和nodelay能够降低突发请求的处理时间，但是长期来看并不会提高吞吐量的上限，长期吞吐量的上限是由rate决定的，因为nodelay只能保证burst的请求被立即处理，但Nginx会限制队列元素释放的速度，就像是限制了令牌桶中令牌产生的速度。 看到这里你可能会问，加入了nodelay参数之后的限速算法，到底算是哪一个“桶”，是漏桶算法还是令牌桶算法？当然还算是漏桶算法。考虑一种情况，令牌桶算法的token为耗尽时会怎么做呢？由于它有一个请求队列，所以会把接下来的请求缓存下来，缓存多少受限于队列大小。但此时缓存这些请求还有意义吗？如果server已经过载，缓存队列越来越长，RT越来越高，即使过了很久请求被处理了，对用户来说也没什么价值了。所以当token不够用时，最明智的做法就是直接拒绝用户的请求，这就成了漏桶算法 令牌桶算法: 2.5.2 令牌桶算法令牌桶算法是比较常见的限流算法之一，大概描述如下：1）所有的请求在处理之前都需要拿到一个可用的令牌才会被处理；2）根据限流大小，设置按照一定的速率往桶里添加令牌；3）桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝；4）请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除；5）令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流 如下图： 这个算法的实现，有很多技术，Guaua是其中之一，redis客户端也有其实现。 123456789101112routes: - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/brand** filters: - StripPrefix=1 - name: RequestRateLimiter #请求数限流 名字不能随便写 ，使用默认的facatory args: key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 redis-rate-limiter.replenishRate是您希望允许用户每秒执行多少请求，而不会丢弃任何请求。这是令牌桶填充的速率 redis-rate-limiter.burstCapacity是指令牌桶的容量，允许在一秒钟内完成的最大请求数,将此值设置为零将阻止所有请求。 超过也许会返回429错误码 too many request 如上表示： 平均每秒允许不超过2个请求，突发不超过4个请求，并且处理突发4个请求的时候，没有延迟，等到完成之后，按照正常的速率处理。 如上两种配置结合就达到了速率稳定，但突然流量也能正常处理的效果。完整配置代码如下： 123456789101112131415161718192021222324252627282930313233343536373839user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试：如下图 在1秒钟之内可以刷新4次，正常处理。 但是超过之后，连续刷新5次，抛出异常。 5.2.2 控制并发量（连接数）ngx_http_limit_conn_module 提供了限制连接数的能力。主要是利用limit_conn_zone和limit_conn两个指令。 利用连接数限制 某一个用户的ip连接的数量来控制流量。 注意：并非所有连接都被计算在内 只有当服务器正在处理请求并且已经读取了整个请求头时，才会计算有效连接。此处忽略测试。 配置语法： 123Syntax: limit_conn zone number;Default: —;Context: http, server, location; (1)配置限制固定连接数 如下，配置如下： limit_conn_zone $binary_remote_addr zone&#x3D;addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10M limit_conn addr 2; 表示 同一个地址只允许连接2次。 上图配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; #根据IP地址来限制，存储内存大小10M limit_conn_zone $binary_remote_addr zone=addr:1m; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #所有以brand开始的请求，访问本地changgou-service-goods微服务 location /brand &#123; limit_conn addr 2; proxy_pass http://192.168.211.1:18081; &#125; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 表示： 123limit_conn_zone $binary_remote_addr zone=addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10Mlimit_conn addr 2; 表示 同一个地址只允许连接2次。 测试： 此时开3个线程，测试的时候会发生异常，开2个就不会有异常 (2)限制每个客户端IP与服务器的连接数，同时限制与虚拟服务器的连接总数。(了解) 如下配置： 12345678910111213limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m; server &#123; listen 80; server_name localhost; charset utf-8; location / &#123; limit_conn perip 10;#单个客户端ip与服务器的连接数． limit_conn perserver 100; ＃限制与服务器的总连接数 root html; index index.html index.htm; &#125;&#125; 总结反向代理流程: proxy_pass http:&#x2F;&#x2F; 常用配置server location root alias","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"web应用前端","slug":"19web","date":"2022-09-01T03:51:56.000Z","updated":"2022-12-02T15:30:41.227Z","comments":true,"path":"2022/09/01/19web/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/19web/","excerpt":"","text":"1标签1.1基础标签大部分本质是span 与div 只不过加了写样式 span不带 回车 div带 回车 段 1 段 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2 回车 等宽字体 带括号与回车 cin cout 斜体 1.2图片 坐标 React数据驱动 node维护本地js库 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677使用bind()函数绑定this取值在JavaScript中，函数里的this指向的是执行时的调用者，而非定义时所在的对象。例如：const person = &#123; name: &quot;yxc&quot;, talk: function() &#123; console.log(this); &#125;&#125;person.talk();const talk = person.talk;talk();运行结果：&#123;name: &#x27;yxc&#x27;, talk: ƒ&#125;Windowbind()函数，可以绑定this的取值。例如：const talk = person.talk.bind(person);箭头函数的简写方式const f = (x) =&gt; &#123; return x * x;&#125;;可以简写为：const f = x =&gt; x * x;箭头函数不重新绑定this的取值例如：const person = &#123; talk: function() &#123; setTimeout(function() &#123; console.log(this); &#125;, 1000); &#125;&#125;;person.talk(); // 输出Windowconst person = &#123; talk: function() &#123; setTimeout(() =&gt; &#123; console.log(this); &#125;, 1000); &#125;&#125;;person.talk(); // 输出 &#123;talk: f&#125;对象的解构例如：const person = &#123; name: &quot;yxc&quot;, age: 18, height: 180,&#125;;const &#123;name : nm, age&#125; = person; // nm是name的别名数组和对象的展开例如：let a = [1, 2, 3];let b = [...a]; // b是a的复制let c = [...a, 4, 5, 6];const a = &#123;name: &quot;yxc&quot;&#125;;const b = &#123;age: 18&#125;;const c = &#123;...a, ...b, height: 180&#125;;Named 与 Default exportsNamed Export：可以export多个，import的时候需要加大括号，名称需要匹配Default Export：最多export一个，import的时候不需要加大括号，可以直接定义别名作者：yxc链接：","categories":[{"name":"前端","slug":"前端","permalink":"https://gouguoqiang.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"前端","slug":"前端","permalink":"https://gouguoqiang.github.io/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"Django","slug":"20Django","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-25T14:21:37.931Z","comments":true,"path":"2022/09/01/20Django/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/20Django/","excerpt":"","text":"网页: http://124.220.192.97:8000/ 项目: https://gitee.com/pro_ggq_915377549/django Python语法输入的处理: 读入行String 对String进行处理 对于全局变量的修改， 如果全局变量是int或者str，那么如果想要在函数中对函数变量进行修改，则需要先在函数内，声明其为global，再进行修改 如果是list或者dict则可以直接修改 608. 差123456a = int(input()) # 读入一行 为字符串 转化为intb = int(input())c = int(input())d = int(input())print(&quot;DIFERENCA = %d&quot; % (a * b - c * d)) 665. 倍数1234567# 输入 6 24 a, b = map(int, input().split(&#x27; &#x27;))if a % b == 0 or b % a == 0: print(&quot;Sao Multiplos&quot;)else: print(&quot;Nao sao Multiplos&quot;) 670. 动物 1234567891011121314151617181920212223242526a = input()b = input()c = input()if a == &quot;vertebrado&quot;: if b == &quot;ave&quot;: if c == &quot;carnivoro&quot;: print(&quot;aguia&quot;) elif c == &quot;onivoro&quot;: print(&quot;pomba&quot;) elif b == &quot;mamifero&quot;: if c == &quot;onivoro&quot;: print(&quot;homem&quot;) elif c == &quot;herbivoro&quot;: print(&quot;vaca&quot;)elif a == &quot;invertebrado&quot;: if b == &quot;inseto&quot;: if c == &quot;hematofago&quot;: print(&quot;pulga&quot;) elif c == &quot;herbivoro&quot;: print(&quot;lagarta&quot;) else: if c == &quot;hematofago&quot;: print(&quot;sanguessuga&quot;) else: print(&quot;minhoca&quot;) 660. 零食1234a = [4.00, 4.50, 5.00, 2.00, 1.50]x, y = map(int, input().split())print(&quot;Total: R$ %.2f&quot; %(a[x-1] * y)) 721. 递增序列读取一系列的整数 XX，对于每个 XX，输出一个 1,2,…,X1,2,…,X 的序列。 输入格式输入文件中包含若干个整数，其中最后一个为 00，其他的均为正整数。 每个整数占一行。 对于输入的正整数，按题目要求作输出处理。 对于最后一行的整数 00，不作任何处理。 输出格式对于每个输入的正整数 XX，输出一个从 11 到 XX 的递增序列，每个序列占一行。 数据范围1≤X≤1001≤X≤100 输入样例：123451030 输出样例：1231 2 3 4 51 2 3 4 5 6 7 8 9 101 2 3 solution12345678while True: x = int(input()) if x == 0: break else: for i in range(1, x + 1): print(i, end = &quot; &quot;) print() 726. 质数一个大于 11 的自然数，如果除了 11 和它自身外，不能被其他自然数整除则称该数为质数。 例如 77 就是一个质数，因为它只能被 11 和 77 整除。 现在，给定你 NN 个大于 11 的自然数，请你依次判断这些数是否是质数。 输入格式第一行包含整数 NN，表示共有 NN 个测试数据。 接下来 NN 行，每行包含一个自然数 XX。 输出格式每个测试用例输出一个结果，每个结果占一行。 如果测试数据是质数，则输出 X is prime，其中 XX 是测试数据。 如果测试数据不是质数，则输出 X is not prime，其中 XX 是测试数据。 数据范围1≤N≤1001≤N≤100,1&lt;X≤1071&lt;X≤107 输入样例：123438517 输出样例：1238 is not prime51 is not prime7 is prime Solution1234# 如果 d | x(d 是 x 的约数) 那么 x/d 也是 x的约数# d | x 则 x/d | x // 因此成对出现 , 因此枚举只需要枚举较小的因数就可以了 不妨设 d &lt;= x / d# d &lt;= 根号x (o(x) -&gt; o(根号x)# 例 d = 12 x = 24 12345678910111213import mathn = int(input())for i in range(n): x = int(input()) for j in range(2, int(math.sqrt(x)) + 1): if x % j == 0: print(x,&quot;is not prime&quot;) break else: print(x,&quot;is prime&quot;) #for的 else 语句: 执行 break 则不执行else 745. 数组的右上半部分Solution123456789101112t = input()s, c = 0, 0for i in range(12): d = list(map(float, input().split())) for j in range(12): if j &gt; i: s += d[j] c += 1if t == &quot;M&quot;: print(&quot;%.1f&quot; % (s/c))else: print(&quot;%.1f&quot; % (s)) 756. 蛇形矩阵Solution1234567891011121314151617181920212223n, m = map(int,input().split())res = [[0 for j in range(m)] for i in range(n)]dx = [0,1,0,-1] dy = [1,0,-1,0]x, y, d = 0, 0, 0for i in range(1, n * m + 1): res[x][y] = i a, b = x + dx[d], y + dy[d] if a &lt; 0 or a &gt;= n or b &lt; 0 or b &gt;= m or res[a][b]: d = (d + 1) % 4 a, b = x + dx[d], y + dy[d] x, y = a, b for i in range(n): for j in range(m): print(res[i][j], end = &quot; &quot;) print() 823. 排列Solution123456789101112131415161718192021222324n = int(input())path = [0 for i in range(n)]used = [False for i in range(n)]def dfs(u): if u == n: for i in range(n): print(path[i] + 1, end = &quot; &quot;) print() else: for i in range(n): if used[i]: continue else: path[u] = i used[i] = True dfs(u+1) path[u] = 0 used[i] = False dfs(0) 搭建环境docker 挂起容器退出: 先 ctrl + p 在 ctrl + q 直接 ctrl + d 是关闭容器退出,tmux退出也是 vimtmux博客: https://cloud.tencent.com/developer/article/1526675?from=15425 ctrl + a + { 复制 选中 } 粘贴 tmux 默认 是b 可以更改设置文件为 a tmux ctrl + a 类似 vim 的ESC模式 ctrl + a + n 切换到下一个 ctrl + a + p 切换到上一个 ctrl + a + c 创建一个新的 Django将ac terminal的Django镜像上传到自己的云服务器scp django_lesson_1_0.tar aliyun:进入自己的云服务并将上传的镜像下载下来ssh aliyundocker load -i django_lesson_1_0.tar生成容器，映射端口docker run -p 20000:22 -p 8000:8000 –name django_server -itd django_lesson:1.0进入容器，并配置用户权限adduser ***usermod -aG sudo ***去阿里云开放端口后回到ac terminal配置免密登陆，以后直接ssh到docker容器中的用户vim .ssh&#x2F;config #添加Host djangossh-copy-id djangoscp .vimrc .bashrc .tmux.conf django: #配置vim和tmux回到容器开始项目(记得在tmux中开始)#创建项目django-admin startproject acapp #创建git仓库git initgit config –global user.name ““git config –global user.email “*****@qq.com” #将git仓库创建好后将公钥复制到git上，然后上传到git上git push –set-upstream origin master #看项目能不能跑python3 manage.py runserver 0.0.0.0:8000 #将提示的东西加入到settings.py中cd acapp&#x2F;acappvim settings.py #第28行 #让git不在显示一些中间文件vim .gitignore **&#x2F;pycachegit add .git commit -m “modify allowed host”开始创建apppython3 manage.py startapp game #将数据更新到Django中python3 manage.py migrate #创建管理员账号python3 manage.py createsuperuser文件结构touch urls.pymkdir tmplates models：存储class、user等views：存储函数的实现urls：存储路由 &#x2F;&#x2F; 以上三个可以是文件 后面都可以变为 文件夹 templates：存储html文件 第一个网页#创建项目django-admin startproject acapp, 会生成一个acapp目录 开始创建apppython3 manage.py startapp game 服务器目录结构 运行项目 python3 manage.py runserver 0.0.0.0:8000 创建一个app 项目系统设计方法论 代码太长了就分模块 , 项目系统设计 menu：菜单页面playground：游戏界面settings：设置界面 在每个文件目录里都分类: 项目文件结构templates目录：管理html文件urls目录：管理路由，即链接与函数的对应关系views目录：管理http函数models目录：管理数据库数据static目录：管理静态文件，比如： css：对象的格式，比如位置、长宽、颜色、背景、字体大小等 js：对象的逻辑，比如对象的创建与销毁、事件函数、移动、变色等 image：图片 audio：声音 …consumers目录：管理websocket函数 本节课用到的素材地址背景图片下载方式：wget –output-document&#x3D;自定义图片名称 图片地址 jquery库： 作者：yxc链接：https://www.acwing.com/file_system/file/content/whole/index/content/3199626/来源：AcWing著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 全局设置 将文件变成文件夹: 底下是python文件 则需要创建 init文件 为什么不写成html 而是写成js渲染: 写成html的话会在后端渲染:就是所有用户请求网页都是后端把网页生成好在把网页返回前端,如果把文件传给用户,就是用户执行js进行渲染,提升用户体验 views&#x2F;index.py只会在web用 返回刚刚写好的html(服务器端渲染,将html的字符串拼接) djanggo render(request,”目录从templates开始写”) urls 路由规则 constructor js 习惯上如果是一个html对象 则在前面+一个 $ 实现简易引擎 基类 AcGameObject 基类中的每个物体 每一帧都要重新画一次,因此创建一个物体的时候要把物体加入到一个全局数组里,之后 调用数组里的对象 每秒钟60次 jQuery : 部署Nginx与对接app docker commit CONTAINER_NAME django_lesson:1.1 # 将容器保存成镜像，将CONTAINER_NAME替换成容器名称docker stop CONTAINER_NAME # 关闭容器docker rm CONTAINER_NAME # 删除容器 #使用保存的镜像重新创建容器 docker run -p 20000:22 -p 8000:8000 -p 8001:80 -p 44301:443 –name django_server -itd django_lesson:1.1 sudo 下 vim高亮 sudo cp .vimrc .tmux.conf &#x2F;root&#x2F;","categories":[{"name":"Python","slug":"Python","permalink":"https://gouguoqiang.github.io/categories/Python/"}],"tags":[{"name":"实践","slug":"实践","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%9E%E8%B7%B5/"}]},{"title":"java","slug":"1java","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T09:23:41.200Z","comments":true,"path":"2022/09/01/1java/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/1java/","excerpt":"","text":"1234byte b = 1;char c = 1;short s = 1;int i = 1; 123456789101112131415161718192021byte b = 1;char c = 1;short s = 1;int i = 1;// 三目，一边为byte另一边为char，结果为int// 其它情况结果为两边中范围大的。适用包装类型i = true? b : c;// intb = true? b : b; // bytes = true? b : s;// short// 表达式，两边为byte,short,char，结果为int型// 其它情况结果为两边中范围大的。适用包装类型i = b + c; // inti = b + b; // inti = b + s; // int// 当 a 为基本数据类型时，a += b，相当于 a = (a) (a + b)// 当 a 为包装类型时， a += b 就是 a = a + bb += s; // 没问题c += i; // 没问题// 常量任君搞，long以上不能越b = (char) 1+ (short) 1+ (int) 1; // 没问题// i = (long) 1 // 错误 类与对象 依赖 use-a 聚合 has-a 继承 is-a 1.域变量与局部变量 域变量会初始化为自动默认值,如果不明确初始化会影响代码的可读性 局部变量不会自动初始化为null 123456789101112131415class Employee &#123; public String name; ...&#125;Emplpyee[] staff = new Employee[3];// 省略不同的名字初始化// 如果需要返回一个可变数据域的拷贝,应该使用cloneEmployee boss = new Employee();System.out.println(boss.name); // nullpublic boolean equals(Employee other) &#123; //访问 int a; System.out.println(a);//报错 在使用时a未被初始化 当然如果不使用a就不会报错 &#125; 首先方法可以访问所调用对象的私有数据(平常使用显而易见). 然后让人奇怪的是一个方法可以访问所属类的所有对象的私有数据 1234567891011121314151617181920212223class Employee &#123; private String name;// ... Employee (String name) &#123; this.name = name; &#125; public boolean equals(Employee other) &#123; //访问 return name.equals(other.name); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Employee boss = new Employee(&quot;boss&quot;); Employee harry = new Employee(&quot;harry&quot;);// System.out.println(boss.name); // 编译器报错 if (boss.equals(harry)) &#123; System.out.println(&quot;Employee的方法可以访问Employee类任何一个对象的私有域&quot;); &#125; &#125;&#125; private 方法内部使用,想删就删(不会被外部使用) final实例域 必须确保在每个构造器执行后这个域的值被设置 123456789101112class Employee &#123; public final String name;//编译器报错 //如果去掉无参构造器则不报错 Employee () &#123; &#125; Employee (String name) &#123; this.name = name; this.name = &quot;不能改&quot;;// 报错: name&#x27; might already have been assigned(分配) to,因为涉及更改引用 &#125;&#125; final关键字只是表示不会在指向别的地方,对象本身可以更改 静态方法不需要构建对象就可以调用 静态代码(只有在装载类的时候被执行) 12345678910111213141516171819202122232425class A &#123; static &#123; System.out.println(&quot;1&quot;); &#125; A() &#123; System.out.println(&quot;a&quot;); &#125;&#125;class B extends A &#123; static &#123; System.out.println(&quot;2&quot;); &#125; B() &#123; System.out.println(&quot;b&quot;); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; B b = new B(); A a = new A(); // 12aba &#125;&#125; 每一个类都可以有一个main方法 方法可以改变一个对象参数的状态,但不能整体改变(用x指向别的)因为是局部变量只是改变局部变量的值 真正的引用未被改变 12345678910//伪代码A a(a),b(b)swap(a,b);swap(x,y) &#123; A temp = x; x = y; y = x; &#125;// 如果是传应用sout(a,b) 输出 b,a但是并没有奏效输出还是a,b 重载 方法同名不同参数 继承覆盖方法 子类覆盖父类:方法签名覆盖 方法名与参数列表完全一致 区分重载: super不是一个对象的引用,不能将super赋给另一个对象变量,它只是一个指示编译器调用超类方法的特殊关键字 方法调用 方法名+参数列表称为方法签名 动态绑定 C x &#x3D; new B(); x.f(args) 获取父类public且对应名称的方法和声明类的对应名称方法 找一个参数类型完全匹配的方法(这个过程称为重载解析) 有可能类型转换 没找到就会报错 实际调用会调用x的真正类型的方法,虚拟机预先为每个类有一个方法表 作用: 无需对现存代码修改,就可以对程序扩展 静态绑定: private,static,final或者构造器等编译器可以准确的知道应该调用哪个方法,所以称为静态绑定 修饰类 final类 为不允许扩展的的类,所有方法自动成为final,不包括域,确保它们不会在子类改变语义 如果一个方法没有被覆盖并且很短就可被优化为内联 例如: e.getNname 将被替换为访问e.name域 详见:Java核心卷10 强制类型转换 允许子类引用赋值给父类反之必须类型转换才能通过运行时检查 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Employee &#123; public String name; // ... Employee () &#123; &#125; Employee (String name) &#123; this.name = name; &#125; public boolean equals(Employee other) &#123; //访问 return name.equals(other.name); &#125; public void work() &#123; System.out.println(&quot;员工共有之work&quot;+name); &#125;&#125;class Manager extends Employee&#123; Manager (String name) &#123; super(name); &#125; public void manage() &#123; System.out.println(&quot;经理独有之manage&quot;+name); &#125; //每个经理是一个员工&#125;public class Main &#123; public static void main(String[] args) &#123; double x = 3.405; int nx = (int) x; Manager a = new Manager(&quot;a&quot;); Employee[] staff = new Employee[3];// Employee b = new Employee(&quot;b&quot;);// Manager c = (Manager)b;// c.manage(); 这三行会报 ClassCastException //改进: Employee b = new Employee(&quot;b&quot;); Manager c; // 局部变量未使用不会报错 if (b instanceof Manager) &#123; c = (Manager) b; &#125; staff[0] = a;// 转化为E 实际为M// staff[0].manage();//报错 a = (Manager)staff[0];// 将父类强转为子类 唯一原因是:暂时忽视对象的?后,使用对象的全部功能 a.manage(); // 正常调用 &#125;&#125; 总结:父类转换为子类之前要使用instanceof,并且只能在继承层次内进行类型转换 抽象类 含有抽象方法(不需要实现的方法)的必须是抽象类 不含有抽象方法也可以声明为抽象类,只是不能被实例化 可以包含具体数据与方法 Object **hashCode()**每个对象都有,其值为对象的存储地址 1234567891011121314public class Main &#123; public static void main(String[] args) &#123; String s = &quot;ok&quot;; StringBuilder sb = new StringBuilder(s); System.out.println(s.hashCode() + &quot; &quot;+sb.hashCode()); String t = new String(&quot;ok&quot;); StringBuilder tb = new StringBuilder(t); System.out.println(t.hashCode() + &quot; &quot;+tb.hashCode()); &#125;&#125; 3548 4601419583548 1163157884 String 的hashcode()是内容导向的 如果equals为true那么hashcode也要一致 为什么重写equals方法就必须重写hashCode方法？ 在散列表中，1、如果两个对象相等，那么它们的hashCode()值一定要相同； 这里的相等是指，通过equals()比较两个对象 时返回true 2、如果两个对象hashCode()相等，它们并不一定相等。(不相等时就是哈希冲突) 注意：这是在散列表中的情况。在非散列表中一定如此！ 考虑只重写equals而不重写 hashcode 时，虽然两个属性值完全相同的对象通过equals方法判断为true，但是当把这两个对象加入到 HashSet 时。会发现HashSet中有重复元素，这就是因为HashSet 使用 hashcode 判断对象是否已存在时造成了歧义，结果会导 致HashSet 的不正常运行。所以重写 equals 方法必须重写 hashcode 方法。 map的put 拿到hashcode()进行散列定槽 如果槽为空直接加入 如果槽不为空(判断key是否相等(key不能重复),)如果key相同直接覆盖 value ,否则解决哈希冲突 还是从set角度说重写hashCode enum关键字接口、lambda表达式与内部类 接口,lambda 内部类机制,内部类中的方法可以访问外部的域 代理 1234567891011121314151617181920public class Main &#123; A a; public static void main(String[] args) &#123; Main main = new Main(); &#125; public interface Iinterface &#123; // 内部接口默认为static的 void print(); &#125; protected class A implements Iinterface &#123; @Override public void print() &#123; System.out.println(&quot;内部接口可被private,protectd static(默认static) public, 修饰&quot;); System.out.println(&quot;外部类(接口)权限修饰符只能是public 或者不写&quot;); System.out.println(&quot;外部类可被final修饰(接口不行),不能被继承,不能static修饰&quot;); &#125; &#125;&#125; 接口 1234public interface Comparable&lt;T&gt; &#123; int compareTo(T other);&#125;// 接口方法自动的是public,实现类必须写好 类泛型 写在类名之后&lt;&gt; 可以用在类内部域,方法参数,方法返回值,方法参数,局部变量等所有能声明的: 从结果来说可以理解为限定了一个限定符 等指定之后在进行替换不指定则为null,当第一次被确定时就被确定了比如set,在使用时都进行泛型指定在实现Comparable接口的类中必须提供下列方法int compareTo(Employee other )可用instanceof 检查对象是否实现了某个特定的接口,instancof限定上限 默认方法 可以为接口提供默认实现 123456789101112131415161718192021222324interface Comparable&lt;T&gt; &#123; default int compareTo(Tother) &#123;return 0&#125;;// 可以不用实现但既然 //用处并不大,每一个实际实现都要覆盖这个方法(逻辑覆盖并不是一定要实现),不过有些情况默认方法可能很有用 // 重要用法: 接口演化&#125;interface DeInterface &#123; int size(); default boolean isEmpty() &#123; return size() == 0; &#125;&#125;public class Main implements DeInterface&#123; //不用实现默认方法 public static void main(String[] args) &#123; &#125; @Override public int size() &#123; return 0; &#125;&#125; 异常、断言和日志 简述: 程序运行过程发生错误就会”抛出异常”,抛出异常比终止程序灵活,因为可以提供一个”捕获”异常的处理器(handler)对异常情况进行处理,如果没有提供处理器程序就会终止,并在控制台打印信息 异常有两种类型,未检查异常和已检查异常 对于已检查异常,编译器会检查是否提供处理器,(提示要添加异常处理的都是已检查异常) 常见的如空指针都属于未检查异常,编译器不会查看,因为应该精心编写代码来避免这些错误 1234try&#123;&#125; catch &#123; handler action&#125; 异常层次 Error（错误） Error 类及其子类：程序中无法处理的错误，表示运行应用程序中出现了严重的错误。 此类错误一般表示代码运行时 JVM 出现问题。通常有 Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如 OutOfMemoryError：内存不足错误；StackOverflowError：栈溢出错误。此类错误发生时，JVM 将终止线程。 这些错误是不受检异常，非代码性错误。因此，当此类错误发生时，应用程序不应该去处理此类错误。按照Java惯例，我们是不应该实现任何新的Error子类的！ Exception（异常） 程序本身可以捕获并且可以处理的异常。Exception 这种异常又分为两类：运行时异常和编译时异常。 运行时异常 未检查异常 都是RuntimeException类及其子类异常，如NullPointerException(空指针异常)、IndexOutOfBoundsException(下标越界异常)等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。 运行时异常的特点是Java编译器不会检查它，也就是说，当程序中可能出现这类异常，即使没有用try-catch语句捕获它，也没有用throws子句声明抛出它，也会编译通过。 非运行时异常 （编译异常必须从语法角度进行处理） 已检查异常 是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 应用 应该寻找更加适当的子类或创建自己的异常类 Stream API“集合讲的是数据，Stream讲的是计算！” 注意： ①Stream 自己不会存储元素。 ②Stream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。 ③Stream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行 Stream 的操作三个步骤 1、创建Stream 一个数据源（如：集合、数组），获取一个流 2、中间操作 一个中间操作链，对数据源的数据进行处理 3、终止操作(终端操作) 一旦执行终止操作，就执行中间操作链，才产生结果【也就是所谓的延迟执行】。之后，不会再被使用 创建steam API1234// 通过数组int[] arr = new int[]&#123;1,2,3,4,5,6&#125;; //调用Arrays类的static &lt;T&gt; Stream&lt;T&gt; stream(T[] array): 返回一个流 IntStream stream = Arrays.stream(arr); 实例1123456int[] q = new int[]&#123;9,8,7,3,4,1,10,1,2,2,2&#125;;Arrays.stream(q). // 创建steam流 boxed().//中间操作 collect(Collectors.toList());// 终止操作 // collect(Collector c)——将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法 IO流&amp;装饰器模式装饰器模式是对功能的增强，而不是附加新的功能。代理模式才是附加新的功能。 星巴克咖啡订单项目 咖啡种类&#x2F;单品咖啡：Espresso(意大利浓咖啡)、ShortBlack、LongBlack(美式咖啡)、Decaf(无因咖啡) 调料：Milk、Soy(豆浆)、Chocolate 要求在扩展新的咖啡种类时，具有良好的扩展性、改动方便、维护方便 使用 OO 的来计算不同种类咖啡的费用: 客户可以点单品咖啡，也可以单品咖啡+调料组合。 Drink【抽象类-主体Component】12345678910111213141516171819202122public abstract class Drink &#123; public String des; // 描述 private float price = 0.0f; public String getDes() &#123; return des; &#125; public void setDes(String des) &#123; this.des = des; &#125; public float getPrice() &#123; return price; &#125; public void setPrice(float price) &#123; this.price = price; &#125; //计算费用的抽象方法 //子类来实现 public abstract float cost(); &#125; Decorator1234567891011121314151617181920212223public class Decorator extends Drink &#123; private Drink obj; public Decorator(Drink obj) &#123; //组合 // TODO Auto-generated constructor stub this.obj = obj; &#125; @Override public float cost() &#123; // TODO Auto-generated method stub // getPrice 自己价格 return super.getPrice() + obj.cost(); &#125; @Override public String getDes() &#123; // TODO Auto-generated method stub // obj.getDes() 输出被装饰者的信息 return des + &quot; &quot; + getPrice() + &quot; &amp;&amp; &quot; + obj.getDes(); &#125; &#125; Coffee12345678public class Coffee extends Drink &#123; @Override public float cost() &#123; // TODO Auto-generated method stub return super.getPrice(); &#125;&#125; ShortBlack1234567public class ShortBlack extends Coffee&#123; public ShortBlack() &#123; setDes(&quot; shortblack &quot;); setPrice(4.0f); &#125;&#125; Soy12345678910public class Soy extends Decorator&#123; public Soy(Drink obj) &#123; super(obj); // TODO Auto-generated constructor stub setDes(&quot; 豆浆 &quot;); setPrice(1.5f); &#125;&#125; Main1234567891011121314151617package design_patterns.structual_type;public class Main &#123; public static void main(String[] args) &#123; Drink order = new ShortBlack(); System.out.println(&quot;费用1=&quot; + order.cost()); System.out.println(&quot;描述=&quot; + order.getDes()); // 2. order 加入一份牛奶 order = new Soy(order); System.out.println(&quot;order 加入一份豆浆 费用 =&quot; + order.cost()); System.out.println(&quot;order 加入一份豆浆 描述 = &quot; + order.getDes()); System.out.println(new Soy(new ShortBlack()).cost()); &#125;&#125; IO流 说明 InputStream 是抽象类, 类似我们前面讲的 Drink FileInputStream 是 InputStream 子类，类似我们前面的 DeCaf, LongBlack FilterInputStream 是 InputStream 子类：类似我们前面 的 Decorator 修饰者 DataInputStream 是 FilterInputStream 子类，具体的修饰者，类似前面的 Milk, Soy 等 FilterInputStream 类 有 protected volatile InputStream in; 即含被装饰者 分析得出在jdk 的io体系中，就是使用装饰者模式 第2章 Java 概述 JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在 JDK 中. 对于不同的平台，有不同的虚拟机。 Java 虚拟机机制屏蔽了底层运行平台的差别，实现了“一次编译，到处运行 JDK &#x3D; JRE + 开发工具集（例如 Javac,java 编译工具等) JRE &#x3D; JVM + Java SE 标准类库（java 核心类库） 如果只想运行开发好的 .class 文件，只需要JRE 第3章 变量3.1数据类型与API 3.3 char+字符编码 3.4Unicode 3.5UTF-8 3.6基本数据类型转换精度小的类型自动转化为精度大的 有多种类型混合运算，系统首先自动将所有数据转换成容量最大的那种数据类型再计算 当把容量大的赋值给容量小的就会报错，反之自动类型转换 byte short 和char 不会自动转换 ，在计算式首先转换为int类型 boolen不参与转换 1234567891011121314151617181920212223242526272829303132333435363738int n1 = 10; //ok//float d1 = n1 + 1.1;//错误 n1 + 1.1 =&gt; 结果类型是 double//double d1 = n1 + 1.1;//对 n1 + 1.1 =&gt; 结果类型是 doublefloat d1 = n1 + 1.1F;//对 n1 + 1.1 =&gt; 结果类型是 float//细节 2: 当我们把精度(容量)大 的数据类型赋值给精度(容量)小 的数据类型时，//就会报错，反之就会进行自动类型转换。////int n2 = 1.1;//错误 double -&gt; int//细节 3: (byte, short) 和 char 之间不会相互自动转换//当把具体数赋给 byte 时，(1)先判断该数是否在 byte 范围内，如果是就可以byte b1 = 10; //对 , -128-127// int n2 = 1; //n2 是 int// byte b2 = n2; //错误，原因： 如果是变量赋值，判断类型//// char c1 = b1; //错误， 原因 byte 不能自动转成 char////韩顺平循序渐进学 Java 零基础第 48页//细节 4: byte，short，char 他们三者可以计算，在计算时首先转换为 int 类型byte b2 = 1;byte b3 = 2;short s1 = 1;//short s2 = b2 + s1;//错, b2 + s1 =&gt; intint s2 = b2 + s1;//对, b2 + s1 =&gt; int//byte b4 = b2 + b3; //错误: b2 + b3 =&gt; int////boolean 不参与转换boolean pass = true;//int num100 = pass;// boolean 不参与类型的自动转换//自动提升原则： 表达式结果的类型自动提升为 操作数中最大的类型//看一道题byte b4 = 1;short s3 = 100;int num200 = 1;float num300 = 1.1F;double num500 = b4 + s3 + num200 + num300; //float -&gt; double&#125; 博客:类型转换自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意 所有的字符本质还是数字 编码问题 Unicode 表：（97 &#x3D; a，65 &#x3D; A） 编码 占了两个字节 转义字符 \\t 制表符 \\n 换行 … 由于java是强类型语言，所以要进行有些运算的时候，需要用到类型转换。 低 ———————————————-&gt;高 byte，short，char—&gt;，int—&gt;，float—&gt;，long—&gt;，double 运算中，不同类型的数据先转换为同一类型，然后进行运算。（小数优先级高于整数） 强制类型转换 高到低 12345int i = 128;byte b = (byte) i;//内存溢出System.out.println(i);//128System.out.println(b);//-1281234 自动类型转换 低到高 12345int i = 128;double d = i;System.out.println(i);//128System.out.println(d);//128.01234 总结： 1. 不能对布尔类型进行转换。（不能把人转成猪，可以把男人转女人） 2. 不能把对象类型转换为不相干的类型。 3. 在把高容量转换为低容量的时候，需要强制转换。 4. 转换的时候可能存在内存溢出，或者精度问题。 作用域（对类来说） 1.在Java编程中，主要的变量就是属性（成员变量）和局部变量 2.我们说的局部变量一般是指在成员方法中定义的变量 3.Java中作用域的分类 全局变量：也就是属性，作用域为整个类体 局部变量：也就是除了属性之外的其他变量，作用域为定义它的代码块中 4.属性可以不赋值，直接使用，因为有默认值，局部变量必须赋值后，才能使用因为没有默认值 5.注意事项和细节使用 属性和局部变量可以重名，访问时采用就近原则 同作用域不能重名 属性生命周期较长 伴随着对象的创建而创建，伴随着对象的销毁而销毁 局部变量伴随着代码快的执行而创建，伴随着代码的结束而销毁。即在一次方法调用过程中 作用域范围不同 ：属性可以被本类使用，或其他类使用（通过对象调用） 局部变量只能在本类中对应的方法中调用 修饰符不同 属性可以加修饰符，局部变量不可以加修饰符 第10章面向对象高级部分 10.5 final 关键字 可以修饰类，属性，方法和局部变量 第13 章 常用类13.1包装类 有了类的特点，就可以调用类的方法 13.1.1装箱拆箱13.1.2包装类之间的转化 案例演示, 以 Integer 和 String 转换为例，其它类似: 1234567891011121314151617181920212223package com.hspedu.wrapper;/*** @author 韩顺平* @version 1.0 */ public class WrapperVSString &#123; public static void main(String[] args) &#123; //包装类(Integer)-&gt;String Integer i = 100;//自动装箱 //方式 1 String str1 = i + &quot;&quot;; //方式 2 String str2 = i.toString(); //方式 3 String str3 = String.valueOf(i); //String -&gt; 包装类(Integer) String str4 = &quot;12345&quot;; Integer i2 = Integer.parseInt(str4);//使用到自动装箱 Integer i3 = new Integer(str4);//构造器 System.out.println(&quot;ok~~&quot;); &#125; &#125; 13.1.3Integer 类和 Character类的常用方法 public class WrapperMethod { public static void main(String[] args) { System.out.println(Character.isDigit(‘a’));&#x2F;&#x2F;判断是不是数字 System.out.println(Character.isLetter(‘a’));&#x2F;&#x2F;判断是不是字母 System.out.println(Character.isUpperCase(‘a’));&#x2F;&#x2F;判断是不是大写 System.out.println(Character.isLowerCase(‘a’));&#x2F;&#x2F;判断是不是小写 System.out.println(Character.isWhitespace(‘a’));&#x2F;&#x2F;判断是不是空格 System.out.println(Character.toUpperCase(‘a’));&#x2F;&#x2F;转成大写 System.out.println(Character.toLowerCase(‘A’));&#x2F;&#x2F;转成小写 13.2 String 类（双引号括起来）13.2.1 String 类的理解和创建对象 字符串常量用双引号扩起的字符序列 ： “你好”，”12.97”等 使用Unicode编码 一个字符（不区分字母还是汉子)占两个字节 String 类常用构造器（看手册） &#x2F;&#x2F; &#x2F;&#x2F; 常用的有 String s1 &#x3D; new String(); &#x2F;&#x2F; &#x2F;&#x2F;String s2 &#x3D; new String(String original); &#x2F;&#x2F;String s3 &#x3D; new String(char[] a); &#x2F;&#x2F;String s4 &#x3D; new String(char[] a,int startIndex,int count) &#x2F;&#x2F;String s5 &#x3D; new String(byte[] b) &#x2F;&#x2F;5. String 类实现了接口 Serializable【String 可以串行化:可以在网络传输】 &#x2F;&#x2F; 接口 Comparable [String 对象可以比较大小] &#x2F;&#x2F;6. String 是 final 类，不能被其他的类继承 &#x2F;&#x2F;7. String 有属性 private final char value[]; 用于存放字符串内容 13.6 StringBuilder 线程不安全的StringBuffer13.7 Math 类 Math类包含用于执行基本数学运算的方法，如初等函数、对数、平方根和三角函数。 均为静态方法 12345678910111213141516171819202122232425262728293031323334353637383940414243public class MathMethod &#123;public static void main(String[] args) &#123;//看看 Math 常用的方法(静态方法)//1.abs 绝对值int abs = Math.abs(-9);System.out.println(abs);//9//2.pow 求幂double pow = Math.pow(2, 4);//2 的 4 次方System.out.println(pow);//16//3.ceil 向上取整,返回&gt;=该参数的最小整数(转成 double);double ceil = Math.ceil(3.9);System.out.println(ceil);//4.0//4.floor 向下取整，返回&lt;=该参数的最大整数(转成 double)double floor = Math.floor(4.001);System.out.println(floor);//4.0//5.round 四舍五入 Math.floor(该参数+0.5)long round = Math.round(5.51);System.out.println(round);//6//6.sqrt 求开方double sqrt = Math.sqrt(9.0);System.out.println(sqrt);//3.0//7.random 求随机数// random 返回的是 0 &lt;= x &lt; 1 之间的一个随机小数// 思考：请写出获取 a-b 之间的一个随机整数,a,b 均为整数 ，比如 a = 2, b=7// 即返回一个数 x 2 &lt;= x &lt;= 7// 解读 Math.random() * (b-a) 返回的就是 0 &lt;= 数 &lt;= b-a// (1) (int)(a) &lt;= x &lt;= (int)(a + Math.random() * (b-a +1) )// (2) 使用具体的数给小伙伴介绍 a = 2 b = 7// (int)(a + Math.random() * (b-a +1) ) = (int)( 2 + Math.random()*6)// Math.random()*6 返回的是 0 &lt;= x &lt; 6 小数// 2 + Math.random()*6 返回的就是 2&lt;= x &lt; 8 小数// (int)(2 + Math.random()*6) = 2 &lt;= x &lt;= 7// (3) 公式就是 (int)(a + Math.random() * (b-a +1) )for(int i = 0; i &lt; 100; i++) &#123;System.out.println((int)(2 + Math.random() * (7 - 2 + 1)));&#125;//max , min 返回最大值和最小值int min = Math.min(1, 9);int max = Math.max(45, 90);System.out.println(&quot;min=&quot; + min);System.out.println(&quot;max=&quot; + max);&#125;&#125; 13.9 System类 13.10 BigInteger和BigDecimal类 应用场景： BigInteger适合保存较大的整形 BigDecimal适合保存精度更高的浮点型 两者的常见方法 1234567891011121314151617181920212223public class BigInteger_ &#123;public static void main(String[] args) &#123;//当我们编程中，需要处理很大的整数，long 不够用//可以使用 BigInteger 的类来搞定// long l = 23788888899999999999999999999l;// System.out.println(&quot;l=&quot; + l);BigInteger bigInteger = new BigInteger(&quot;23788888899999999999999999999&quot;);BigInteger bigInteger2 = newBigInteger(&quot;10099999999999999999999999999999999999999999999999999999999999999999999999999999999&quot;);System.out.println(bigInteger);//解读//1. 在对 BigInteger 进行加减乘除的时候，需要使用对应的方法，不能直接进行 + - * ///2. 可以创建一个 要操作的 BigInteger 然后进行相应操作BigInteger add = bigInteger.add(bigInteger2);System.out.println(add);//BigInteger subtract = bigInteger.subtract(bigInteger2);System.out.println(subtract);//减BigInteger multiply = bigInteger.multiply(bigInteger2);System.out.println(multiply);//乘BigInteger divide = bigInteger.divide(bigInteger2);System.out.println(divide);//除&#125;&#125; 123456789101112131415161718192021public class BigDecimal_ &#123;public static void main(String[] args) &#123;//当我们需要保存一个精度很高的数时，double 不够用//可以是 BigDecimal// double d = 1999.11111111111999999999999977788d;// System.out.println(d);BigDecimal bigDecimal = new BigDecimal(&quot;1999.11&quot;);BigDecimal bigDecimal2 = new BigDecimal(&quot;3&quot;);System.out.println(bigDecimal);//老韩解读//1. 如果对 BigDecimal 进行运算，比如加减乘除，需要使用对应的方法//2. 创建一个需要操作的 BigDecimal 然后调用相应的方法即可System.out.println(bigDecimal.add(bigDecimal2));System.out.println(bigDecimal.subtract(bigDecimal2));System.out.println(bigDecimal.multiply(bigDecimal2));//System.out.println(bigDecimal.divide(bigDecimal2));//可能抛出异常 ArithmeticException//在调用 divide 方法时，指定精度即可. BigDecimal.ROUND_CEILING//如果有无限循环小数，就会保留 分子 的精度System.out.println(bigDecimal.divide(bigDecimal2, BigDecimal.ROUND_CEILING));&#125;&#125; 14.1集合的理解与好处 数组长度固定不能更改 CRUD（增加 (Create)、读取(Retrieve) (重新得到数据)、更新 (Update)和删除 (Delete)增删改查）不方便 集合可以动态保存任意多个对象（底层源码运用扩容机制），提供了一系列方便的方法，add、remove、set、get等 14.2集合的框架体系 单列集合 add（“tom”） 接口定义方法类会自己具体实现各种方法 双列集合 put（“NO1”,“北京”） 14.3 Collection接口和常用方法第21章 网络编程 查询 API 的一般流程是：找包→找类或接口→查看类或接口→找方法或变量 21.1网络的相关概念 计算机网络 21.2InetAddress类21.2.1相关方法 1.获取本机InetAddress对象 InetAddress.getLocalHost 静态方法 ，return LAPTOP-PH64GORS&#x2F;192.168.137.1 2.根据指定主机名&#x2F;域名获取ip地址对象InetAddress.getByName（主机名） 3.获取InetAddress对象的主机名 getHostName 4.获取InetAddress对象的地址 getHostAddress 12345678910111213141516171819//1. 获取本机的InetAddress 对象InetAddress localHost = InetAddress.getLocalHost();System.out.println(localHost);//DESKTOP-S4MP84S/192.168.12.1//2. 根据指定主机名 获取 InetAddress对象InetAddress host1 = InetAddress.getByName(&quot;LAPTOP-PH64GORS&quot;);System.out.println(&quot;host1=&quot; + host1);//DESKTOP-S4MP84S/192.168.12.1//3. 根据域名返回 InetAddress对象, 比如 www.baidu.com 对应InetAddress host2 = InetAddress.getByName(&quot;www.baidu.com&quot;);System.out.println(&quot;host2=&quot; + host2);//www.baidu.com / 110.242.68.4//4. 通过 InetAddress 对象，获取对应的地址String hostAddress = host2.getHostAddress();//IP 110.242.68.4System.out.println(&quot;host2 对应的ip = &quot; + hostAddress);//110.242.68.4//5. 通过 InetAddress 对象，获取对应的主机名/或者的域名String hostName = host2.getHostName();System.out.println(&quot;host2对应的主机名/域名=&quot; + hostName); // www.baidu.com 21.3 Socket21.3.1基本介绍 1.套接字（Socket）开发网络应用程序被广泛采用，以至成为事实上的标准 2.通信两端都要是Socket，是两台机器间通信的端点 3.网络通信其实就是Socket的通信 4.Socket允许程序把网络连接当成一个流，数据在两个Socket间通过IO传输。 5.一般主动发起通信的应用程序属于客户端，等待的为服务端 21.4 TCP网络通信编程 21.4.1使用字节流 123456789101112131415161718public class SocketTCP01Client &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 连接服务端 (ip , 端口） //解读: 连接本机的 9999端口, 如果连接成功，返回Socket对象 Socket socket = new Socket(InetAddress.getLocalHost(), 9999); System.out.println(&quot;客户端 socket返回=&quot; + socket.getClass()); //2. 连接上后，生成Socket, 通过socket.getOutputStream() // 得到 和 socket对象关联的输出流对象 OutputStream outputStream = socket.getOutputStream(); //3. 通过输出流，写入数据到 数据通道 outputStream.write(&quot;hello, server&quot;.getBytes()); //4. 关闭流对象和socket, 必须关闭 outputStream.close(); socket.close(); System.out.println(&quot;客户端退出.....&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930public class SocketTCP01Server &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 在本机 的9999端口监听, 等待连接 // 细节: 要求在本机没有其它服务在监听9999 // 细节：这个 ServerSocket 可以通过 accept() 返回多个Socket[多个客户端连接服务器的并发] ServerSocket serverSocket = new ServerSocket(9999); System.out.println(&quot;服务端，在9999端口监听，等待连接..&quot;); //2. 当没有客户端连接9999端口时，程序会 阻塞, 等待连接 // 如果有客户端连接，则会返回Socket对象，程序继续 Socket socket = serverSocket.accept(); System.out.println(&quot;服务端 socket =&quot; + socket.getClass()); // //3. 通过socket.getInputStream() 读取客户端写入到数据通道的数据, 显示 InputStream inputStream = socket.getInputStream(); //4. IO读取 byte[] buf = new byte[1024]; int readLen = 0; while ((readLen = inputStream.read(buf)) != -1) &#123; System.out.println(new String(buf, 0, readLen));//根据读取到的实际长度，显示内容. &#125; //5.关闭流和socket inputStream.close(); socket.close(); serverSocket.close();//关闭 &#125;&#125; 21.4.2使用字符流 1234567891011121314151617181920212223242526272829public static void main(String[] args) throws IOException &#123; //思路 //1. 连接服务端 (ip , 端口） //解读: 连接本机的 9999端口, 如果连接成功，返回Socket对象 Socket socket = new Socket(InetAddress.getLocalHost(), 9999); System.out.println(&quot;客户端 socket返回=&quot; + socket.getClass()); //2. 连接上后，生成Socket, 通过socket.getOutputStream() // 得到 和 socket对象关联的输出流对象 OutputStream outputStream = socket.getOutputStream(); //3. 通过输出流，写入数据到 数据通道, 使用字符流 BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(outputStream)); bufferedWriter.write(&quot;hello, server 字符流&quot;); bufferedWriter.newLine();//插入一个换行符，表示写入的内容结束, 注意，要求对方使用readLine()!!!! bufferedWriter.flush();// 如果使用的字符流，需要手动刷新，否则数据不会写入数据通道 //4. 获取和socket关联的输入流. 读取数据(字符)，并显示 InputStream inputStream = socket.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); String s = bufferedReader.readLine(); System.out.println(s); //5. 关闭流对象和socket, 必须关闭 bufferedReader.close();//关闭外层流 bufferedWriter.close(); socket.close(); System.out.println(&quot;客户端退出.....&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839public class SocketTCP03Server &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 在本机 的9999端口监听, 等待连接 // 细节: 要求在本机没有其它服务在监听9999 // 细节：这个 ServerSocket 可以通过 accept() 返回多个Socket[多个客户端连接服务器的并发] ServerSocket serverSocket = new ServerSocket(9999); System.out.println(&quot;服务端，在9999端口监听，等待连接..&quot;); //2. 当没有客户端连接9999端口时，程序会 阻塞, 等待连接 // 如果有客户端连接，则会返回Socket对象，程序继续 Socket socket = serverSocket.accept(); System.out.println(&quot;服务端 socket =&quot; + socket.getClass()); // //3. 通过socket.getInputStream() 读取客户端写入到数据通道的数据, 显示 InputStream inputStream = socket.getInputStream(); //4. IO读取, 使用字符流, 老师使用 InputStreamReader 将 inputStream 转成字符流 BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); String s = bufferedReader.readLine(); System.out.println(s);//输出 //5. 获取socket相关联的输出流 OutputStream outputStream = socket.getOutputStream(); // 使用字符输出流的方式回复信息 BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(outputStream)); bufferedWriter.write(&quot;hello client 字符流&quot;); bufferedWriter.newLine();// 插入一个换行符，表示回复内容的结束 bufferedWriter.flush();//注意需要手动的flush //6.关闭流和socket bufferedWriter.close(); bufferedReader.close(); socket.close(); serverSocket.close();//关闭 &#125;&#125; 21.4.3文件发送21.4.4 netstat 指令 telnet ip 端口号 连接服务器 crtl+】 显示字符 用Java连接服务器 12345678try (var s = new Socket(&quot;time-a.nist.gov&quot;, 13); //打开一个关键字，负责启动该程序内部和外部的通信 若连接失败 它将抛出 // 一个UnknownHostException, // 如果存在其他问题将抛出一个IOException 因为UnknownException是IOException的一个子类 这仅是示例程序仅捕获超类的异常 var in = new Scanner(s.getInputStream(), StandardCharsets.UTF_8))//一旦套接字打开 Socket 类中的getInputStream 类就会返回一个InputSteam对象，该对象可以像任何其他流对象使用，该程序一旦获得了 //这个流 该程序会将直接把每一行打印到标准输出 该程序只适用非常简单的服务器 //在比较复杂的网络程序中 客户端发送请求而服务器可能在响应结束是并不立刻断开连接 //java库隐藏了建立网络连接和通过连接发送数据的复杂过程 12345678910111213141516171819202122232425262728import java.io.*;import java.net.*;/** * This program demonstrates the InetAddress class. Supply a host name as command-line * argument, or run without command-line arguments to see the address of the local host. * @version 1.02 2012-06-05 * @author Cay Horstmann */public class InetAddressTest&#123; public static void main(String[] args) throws IOException &#123; if (args.length &gt; 0) &#123; String host = args[0]; InetAddress[] addresses = InetAddress.getAllByName(host);//为给定的主机名创建一个InetAddress对象 // for (InetAddress a : addresses) System.out.println(a); &#125; else &#123; InetAddress localHostAddress = InetAddress.getLocalHost(); //为本机主机创建一个InetAddress对象 System.out.println(localHostAddress); &#125; &#125;&#125; InetAddress Socket 基础知识基础类型 byte是字节类型 char 是unicode 是万国码 16位 兼容assic 类型转换 自动向上类型转换 强制向下转换(对表达式强转) 比较器 &amp;gt 则返回 1 &amp;lt 则返回 -1 sort 默认从小到大,比较器不关注sort,只关注传递大小信息 lamda随便写写 基本类型不支持比较器 Date File类 创建文件对象,对对象进行操作 文件夹或文件皆可抽象为file,但文件夹不能被输入输出 IO流 字节与字符的区别 字节(Byte)是计量单位，表示数据量多少，是计算机信息技术用于计量存储容量的一种计量单位，通常情况下一字节等于八位。 字符(Character)计算机中使用的字母、数字、字和符号，比如’A’、’B’、’$’、’&amp;’等。 一般在英文状态下一个字母或字符占用一个字节，一个汉字用两个字节表示。 字节与字符： ASCII 码中，一个英文字母（不分大小写）为一个字节，一个中文汉字为两个字节。 UTF-8 编码中，一个英文字为一个字节，一个中文为三个字节。 Unicode 编码中，一个英文为一个字节，一个中文为两个字节。 符号：英文标点为一个字节，中文标点为两个字节。例如：英文句号 . 占1个字节的大小，中文句号 。占2个字节的大小。 UTF-16 编码中，一个英文字母字符或一个汉字字符存储都需要 2 个字节（Unicode 扩展区的一些汉字存储需要 4 个字节）。 UTF-32 编码中，世界上任何字符的存储都需要 4 个字节。 输出流默认覆盖写,文件不存在则创建文件 权限修饰符类 1、外部类前可以修饰：public、default、abstract、final 对于顶级类(外部类)来说，只有两种修饰符：public和默认(default)。因为外部类的上一单元是包，所以外部类只有两个作用域：同包，任何位置。因此，只需要两种控制权限：包控制权限和公开访问权限，也就对应两种控制修饰符：public和默认(default)。可以满足所有的情况了。 如果类使用了private修饰符，说明是个内部类。内部类的上一级是外部类，那么对应的有四种访问控制修饰符：本类(private)，同包(default)，父子类(protected)，任何位置(public)。当一个内部类使用了private修饰后，只能在该类的外部类内部使用。 上面这些都是平时使用司空见惯的，但是为什么是这种情况呢？ 可以想一下，一个java项目是不可能在一个class里面完成的。mvc模式中，是把类分为三层，一层层调用类。如果定义为私有的和受保护的就无法调用。换句话说，对于一个java文件，要么就是自己单独运行，要么就是被其他程序作为库调用，如果一个java文件的类被private修饰，那么是不是其他的程序或是类是无法使用它的，那么他作为一个单独的文件就没啥用了。如果它作为单个文件运行，类加载怎么找到它呢，因为它对外不可见。同时，也失去了类的存在意义。 2、内部类前可以修饰：public、protected、default、private、abstract、final、static 3、局部(指方法 代码块等)内部类前可以修饰：abstract、final 成员变量 1、 public ：对 所有用户 开放，所有用户都可直接调用 2、 private ：私有。 除了class自己之外，任何人都不可直接使用 ，私有财产神圣不可侵犯嘛，即便是子女，朋友，都不可使用。 default 类内部 与 同包 ,即对于外包的类和子类朋友类相当于为私有不能访问 3、 protected ：对于子女、朋友来说，就是public的，可自由使用，无任何限制；而对于其他的外部class，protected就变成private。（ 同一个包中的类，若不在同一个包中，必须为其子孙类才可使用 ） 接口抽象类 接口里的静态方法，即static修饰的有方法体的方法不会被继承或者实现，但是静态变量会被继承 接口中的static方法不能被继承，也不能被实现类调用，只能被自身调用 Guava入门 guava就是类库，是java api的增强与扩展，里面有大量的方法供我们使用，使用之前需要引入包 &lt;!--guava依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1&lt;/version&gt; &lt;/dependency&gt; guava有哪些方法呢？我们先从以下几方面开始学习： 字符串处理：分割，连接，填充 新增的集合类型 原生类型 1.原生类型 定义list，map public void test() { &#x2F;&#x2F;JDK List list &#x3D; new ArrayList(); list.add(“a”); list.add(“b”); list.add(“c”); list.add(“d”); &#x2F;&#x2F;guava List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); List lists1 &#x3D; Lists.newArrayList(); Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); } guava就是类库，是java api的增强与扩展，里面有大量的方法供我们使用，使用之前需要引入包 &lt;!--guava依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1&lt;/version&gt; &lt;/dependency&gt; guava有哪些方法呢？我们先从以下几方面开始学习： 字符串处理：分割，连接，填充 新增的集合类型 原生类型 1.原生类型 定义list，map public void test() { &#x2F;&#x2F;JDK List list &#x3D; new ArrayList(); list.add(“a”); list.add(“b”); list.add(“c”); list.add(“d”); &#x2F;&#x2F;guava List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); List lists1 &#x3D; Lists.newArrayList(); Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); } 2.新增集合（这里我只讲一下Mulitmap,平时用这个会使代码很方便，这里我就多讲一下） a Multimap的使用 Multimap就是将相同key的value值放在一个list里面，这样子取相同key下面的所有value值就非常简单了，不然还得for循环去匹配，把相同key值的value值找出来，在进行处理。map&lt;key,value&gt;键值key不能重复，所以当遇到这样子场景的时候map就非常不适合了，guava提供了Multimap适用于该场景。 当我们需要一个map中包含key为String类型，value为List类型的时候，以前我们是这样写的 &#x2F;&#x2F; jdk方式 Map&lt;String,List&gt; map &#x3D; new HashMap&lt;String,List&gt;(); List list &#x3D; new ArrayList(); list.add(1); list.add(2); map.put(“aa”, list); System.out.println(map.get(“aa”));&#x2F;&#x2F;[1, 2] &#x2F;&#x2F; guava方式 Multimap&lt;String,Integer&gt; map &#x3D; ArrayListMultimap.create(); map.put(“aa”, 1); map.put(“aa”, 2); System.out.println(map.get(“aa”)); &#x2F;&#x2F;[1, 2] Multimap.get(key)即使没有key值，会返回空的list。 Multimap.keySet()返回的用set表示的不重复key; Multimap.keys()返回的是用Multiset表示的key,key数量跟value值数量一致； Multimap.containKeys()是表示是否包含这个key; Multimap.size()返回所有值的个数，而非不同键的个数。要得到不同键的个数，要用Multimap.keySet().size() 想要更多了解Multimap可以参考 https://www.jianshu.com/p/e0537d878b6c 3.字符串的处理：分割，连接，填充 a. joiner 连接器 joiner on就是将list用，连接转成字符串 @Test public void joinerListTest() { List lists &#x3D; Lists.newArrayList(“a”,”b”,”g”,”8”,”9”); String result &#x3D; Joiner.on(“,”).join(lists); System.out.println(result); } 结果：a,b,g,8,9 joiner skipNulls()连接跳过null元素(第一个test为了跟第二个进行比对一下) @Test public void joinerListTest1() { List lists &#x3D; Lists.newArrayList(“a”,”b”,”g”,null,”8”,”9”); String result &#x3D; Joiner.on(“,”).join(lists); System.out.println(result); } 结果：a,b,g,null,8,9 @Test public void joinerListTest2() { List lists &#x3D; Lists.newArrayList(“a”,”b,”g”,null,”8”,”9”); String result &#x3D; Joiner.on(“,”).skipNulls().join(lists); System.out.println(result); } 结果：a,b,g,8,9 如果连接的时候list里面含有null值，会报空指针，因为join实现如下： public final String join(Iterable&lt;?&gt; parts) { return this.join(parts.iterator()); } public final String join(Iterator&lt;?&gt; parts) { return this.appendTo(new StringBuilder(), parts).toString(); } @CanIgnoreReturnValue public final StringBuilder appendTo(StringBuilder builder, Iterator&lt;?&gt; parts) { try { this.appendTo((Appendable)builder, (Iterator)parts); return builder; } catch (IOException var4) { throw new AssertionError(var4); } } @CanIgnoreReturnValue public A appendTo(A appendable, Iterator&lt;?&gt; parts) throws IOException { Preconditions.checkNotNull(appendable); if (parts.hasNext()) { appendable.append(this.toString(parts.next())); while(parts.hasNext()) { appendable.append(this.separator); appendable.append(this.toString(parts.next())); } } return appendable; } @CanIgnoreReturnValue public static T checkNotNull(T reference) { if (reference &#x3D;&#x3D; null) { throw new NullPointerException(); } else { return reference; } } joiner useForNull(final String value)用value替换null元素值 @Test public void useNullListTest() { List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); String result &#x3D; Joiner.on(“,”).useForNull(“哈哈”).join(lists); System.out.println(result); } 结果：a,b,g,哈哈,8,9 joiner withKeyValueSeparator(String value) map连接器，keyValueSeparator为key和value之间的分隔符 @Test public void withMapTest() { Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); maps.put(1, “哈哈”); maps.put(2, “压压”); String result &#x3D; Joiner.on(“,”).withKeyValueSeparator(“:”).join(maps); System.out.println(result); System.out.println(maps); } 结果： 1:哈哈,2:压压 {1&#x3D;哈哈, 2&#x3D;压压} b. splitter 拆分器 splitter on 拆分 @Test public void splitterListTest() { String test &#x3D; “34344,34,34,哈哈”; List lists &#x3D; Splitter.on(“,”).splitToList(test); System.out.println(lists); } 结果：[34344, 34, 34, 哈哈] splitter trimResults 拆分去除前后空格 @Test public void trimResultListTest() { String test &#x3D; “ 34344,34,34,哈哈 “; List lists &#x3D; Splitter.on(“,”).trimResults().splitToList(test); System.out.println(lists); } 结果：[34344, 34, 34, 哈哈] splitter omitEmptyStrings 去除拆分出来空的字符串 @Test public void omitEmptyStringsTest() { String test &#x3D; “ 3434,434,34,,哈哈 “; List lists &#x3D; Splitter.on(“,”).omitEmptyStrings().splitToList(test); System.out.println(lists); } 结果：[ 3434, 434, 34, 哈哈 ] splitter fixedLength(int lenght) 把字符串按固定长度分割 @Test public void fixedLengthTest() { String test &#x3D; “343443434哈哈”; List lists &#x3D; Splitter.fixedLength(3).splitToList(test); System.out.println(lists); } 结果：[343, 443, 434, 哈哈] b. charMatcher 匹配器 charMatcher is(Char char) 给单一字符匹配 @Test public void isTest() { String str &#x3D; “12312,agg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.retainFrom(str)); } 结果：gg charMatcher retainFrom(String s) 在字符序列中保留匹配字符，移除其他字符 @Test public void charMatcherTest() { String str &#x3D; “12312,agg “; &#x2F;&#x2F;两个匹配符,先匹配再操作 CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘1’); CharMatcher charMatcher2 &#x3D; CharMatcher.is(‘2’); &#x2F;&#x2F;两个CharMatcher或操作 CharMatcher charMatcher3 &#x3D; charMatcher1.or(charMatcher2); System.out.println(charMatcher3.retainFrom(str)); } 结果：1212 charMatcher matchersAllOf(Char char) 测试是否字符序列所有字符都匹配 @Test public void matchesAllOfTest() { String str &#x3D; “12312,agg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.matchesAllOf(str)); } 结果：false @Test public void matchesAllOfTest() { String str &#x3D; “ggggg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.matchesAllOf(str)); } 结果：true arr与list转换前言 123456int[] ints = &#123;2, 34, 55, 22, 11&#125;; long[] longs = &#123;1, 2, 3&#125;; double[] doubles = &#123;1, 2, 3&#125;; Arrays.stream(ints).boxed().collect(Collectors.toList()); Arrays.stream(longs).boxed().collect(Collectors.toList()); Arrays.stream(doubles).boxed().collect(Collectors.toList()); 直接for,add就完事了反之同理 java数组转list误区 一、不能把基本数据类型转化为列表 仔细观察可以发现asList接受的参数是一个泛型的变长参数，而基本数据类型是无法泛型化的，如下所示： 123456789101112public class App &#123; public static void main(String[] args) &#123; int [] intarray = &#123; 1 , 2 , 3 , 4 , 5 &#125;; //List&lt;Integer&gt; list = Arrays.asList(intarray); 编译通不过 List&lt; int []&gt; list = Arrays.asList(intarray); System.out.println(list); &#125;&#125;output：[[I @66d3c617 ]1234567891011 这是因为把int类型的数组当参数了，所以转换后的列表就只包含一个int[]元素。 解决方案： 要想把基本数据类型的数组转化为其包装类型的list，可以使用guava类库的工具方法，示例如下： 12int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;;List&lt;Integer&gt; list = Ints.asList(intArray); 第一种方式(未必最佳):使用ArrayList.asList(strArray) 使用Arrays工具类Arrays.asList(strArray)方式,转换完成后,只能对List数组进行查改,不能增删,增删就会抛出UnsupportedOperationException 异常 12345678910import java.util.Arrays;import java.util.List; public static void Demo1() &#123; String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; //注意这个List不是Collections包内的List,而是util包里面的List接口 List&lt;String&gt; ints = Arrays.asList(str); //这里会报错 ints.add(&quot;laopo&quot;); &#125;123456789 添加数据报错: 1234567891011Exception in thread &quot;main&quot; java.lang.UnsupportedOperationExceptionat java.util.AbstractList.add(AbstractList.java:148)at java.util.AbstractList.add(AbstractList.java:108)at JAVA基础.JDK8新特性.Java数组转List.Demo1(Java数组转List.java:20)at JAVA基础.JDK8新特性.Java数组转List.main(Java数组转List.java:13)报错原因:Arrays.asList(str)返回值是java.util.Arrays类中一个私有静态内部类 java.utiil.Arrays.Arraylist,并不是我们平时用的java.util.ArrayList();使用场景:Arrays.asList(strArray)方式仅能用在将数组转换为List后，不需要增删其中的值，仅作为数据源读取使用。12345678910 第二种方法(支持增删查改): 通过ArrayList的构造器,将Arrays.asList(strArray)的返回值由java.utilArrays.ArrayList转为java.util.ArrayList. 关键代码：ArrayList list &#x3D; new ArrayList(Arrays.asList(strArray)) ; 1234567 String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; //注意这个List不是Collections包内的List,而是util包里面的List接口 java.util.ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;(Arrays.asList(str)); strings.add(&quot;aop&quot;); strings.stream().forEach(System.out::println);123456 使用场景:需要在将数组转换为List后，对List进行增删改查操作，在List的数据量不大的情况下，可以使用。 第三种方式(通过集合工具类Collections.addAll()方法(最高效)) 通过Collections.addAll(arrayList, strArray)方式转换，根据数组的长度创建一个长度相同的List，然后通过Collections.addAll()方法，将数组中的元素转为二进制，然后添加到List中，这是最高效的方法。 123456public static void Demo3() &#123; //注意这个List不是Collections包内的List,而是util包里面的List接口 String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; java.util.ArrayList&lt;String&gt; stringList = new ArrayList&lt;&gt;(str.length); Collections.addAll(stringList,str); &#125; 第四种方式通过JDK8的Stream流将3总基本类型数组转为List 如果JDK版本在1.8以上,使用流stream来将下列3种数组快速转为List,分别是int[],long[],double[],不支持short[ ],byte[ ],char[]在JDK1.8中暂不支持. 1234567 int[] ints = &#123;2, 34, 55, 22, 11&#125;; long[] longs = &#123;1, 2, 3&#125;; double[] doubles = &#123;1, 2, 3&#125;; Arrays.stream(ints).boxed().collect(Collectors.toList()); Arrays.stream(longs).boxed().collect(Collectors.toList()); Arrays.stream(doubles).boxed().collect(Collectors.toList());123456 TIPs:为什么int[]不能直接转为List,而Integer[]可以转为List,而Integer[]就可以转为List了,因为List中的泛型必须是引用类型。 java数组转list误区 一、不能把基本数据类型转化为列表 仔细观察可以发现asList接受的参数是一个泛型的变长参数，而基本数据类型是无法泛型化的，如下所示： 123456789101112public class App &#123; public static void main(String[] args) &#123; int [] intarray = &#123; 1 , 2 , 3 , 4 , 5 &#125;; //List&lt;Integer&gt; list = Arrays.asList(intarray); 编译通不过 List&lt; int []&gt; list = Arrays.asList(intarray); System.out.println(list); &#125;&#125;output：[[I @66d3c617 ]1234567891011 这是因为把int类型的数组当参数了，所以转换后的列表就只包含一个int[]元素。 解决方案： 要想把基本数据类型的数组转化为其包装类型的list，可以使用guava类库的工具方法，示例如下： 123int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;;List&lt;Integer&gt; list = Ints.asList(intArray);12 二、asList方法返回的是数组的一个视图 视图意味着，对这个list的操作都会反映在原数组上，而且这个list是定长的，不支持add、remove等改变长度的方法。 12345678910111213141516public class App &#123; public static void main(String[] args) &#123; int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;; List&lt;Integer&gt; list = Ints.asList(intArray); list.set( 0 , 100 ); System.out.println(Arrays.toString(intArray)); list.add( 5 ); list.remove( 0 ); &#125;&#125;output：[ 100 , 2 , 3 , 4 ]UnsupportedOperationExceptionUnsupportedOperationException Java 容器一、概览容器主要包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对（两个对象）的映射表。 Collection 1. Set TreeSet：基于红黑树实现，支持有序性操作，例如根据一个范围查找元素的操作。但是查找效率不如 HashSet，HashSet 查找的时间复杂度为 O(1)，TreeSet 则为 O(logN)。 HashSet：基于哈希表实现，支持快速查找，但不支持有序性操作。并且失去了元素的插入顺序信息，也就是说使用 Iterator 遍历 HashSet 得到的结果是不确定的。 LinkedHashSet：具有 HashSet 的查找效率，并且内部使用双向链表维护元素的插入顺序。 2. List ArrayList：基于动态数组实现，支持随机访问。 Vector：和 ArrayList 类似，但它是线程安全的。 LinkedList：基于双向链表实现，只能顺序访问，但是可以快速地在链表中间插入和删除元素。不仅如此，LinkedList 还可以用作栈、队列和双向队列。 3. Queue LinkedList：可以用它来实现双向队列。 PriorityQueue：基于堆结构实现，可以用它来实现优先队列。 Map TreeMap：基于红黑树实现。 HashMap：基于哈希表实现。 HashTable：和 HashMap 类似，但它是线程安全的，这意味着同一时刻多个线程同时写入 HashTable 不会导致数据不一致。它是遗留类，不应该去使用它，而是使用 ConcurrentHashMap 来支持线程安全，ConcurrentHashMap 的效率会更高，因为 ConcurrentHashMap 引入了分段锁。 LinkedHashMap：使用双向链表来维护元素的顺序，顺序为插入顺序或者最近最少使用（LRU）顺序。 二、容器中的设计模式迭代器模式 Collection 继承了 Iterable 接口，其中的 iterator() 方法能够产生一个 Iterator 对象，通过这个对象就可以迭代遍历 Collection 中的元素。 从 JDK 1.5 之后可以使用 foreach 方法来遍历实现了 Iterable 接口的聚合对象。 123456List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;a&quot;);list.add(&quot;b&quot;);for (String item : list) &#123; System.out.println(item);&#125; 适配器模式java.util.Arrays#asList() 可以把数组类型转换为 List 类型。 12@SafeVarargspublic static &lt;T&gt; List&lt;T&gt; asList(T... a) 应该注意的是 asList() 的参数为泛型的变长参数，不能使用基本类型数组作为参数，只能使用相应的包装类型数组。 12Integer[] arr = &#123;1, 2, 3&#125;;List list = Arrays.asList(arr); 也可以使用以下方式调用 asList()： 1List list = Arrays.asList(1, 2, 3); 三、源码分析如果没有特别说明，以下源码分析基于 JDK 1.8。 在 IDEA 中 double shift 调出 Search EveryWhere，查找源码文件，找到之后就可以阅读源码。 ArrayList1. 概览因为 ArrayList 是基于数组实现的，所以支持快速随机访问。RandomAccess 接口标识着该类支持快速随机访问。 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable 数组的默认大小为 10。 1private static final int DEFAULT_CAPACITY = 10; 2. 扩容添加元素时使用 ensureCapacityInternal() 方法来保证容量足够，如果不够时，需要使用 grow() 方法进行扩容，新容量的大小为 oldCapacity + (oldCapacity &gt;&gt; 1)，即 oldCapacity+oldCapacity&#x2F;2。其中 oldCapacity &gt;&gt; 1 需要取整，所以新容量大约是旧容量的 1.5 倍左右。（oldCapacity 为偶数就是 1.5 倍，为奇数就是 1.5 倍-0.5） 扩容操作需要调用 Arrays.copyOf() 把原数组整个复制到新数组中，这个操作代价很高，因此最好在创建 ArrayList 对象时就指定大概的容量大小，减少扩容操作的次数。 12345678910111213141516171819202122232425262728293031public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity);&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);&#125; 3. 删除元素需要调用 System.arraycopy() 将 index+1 后面的元素都复制到 index 位置上，该操作的时间复杂度为 O(N)，可以看到 ArrayList 删除元素的代价是非常高的。 12345678910public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 4. 序列化ArrayList 基于数组实现，并且具有动态扩容特性，因此保存元素的数组不一定都会被使用，那么就没必要全部进行序列化。 保存元素的数组 elementData 使用 transient 修饰，该关键字声明数组默认不会被序列化。 1transient Object[] elementData; // non-private to simplify nested class access ArrayList 实现了 writeObject() 和 readObject() 来控制只序列化数组中有元素填充那部分内容。 123456789101112131415161718192021private void readObject(java.io.ObjectInputStream s) throws java.io.IOException, ClassNotFoundException &#123; elementData = EMPTY_ELEMENTDATA; // Read in size, and any hidden stuff s.defaultReadObject(); // Read in capacity s.readInt(); // ignored if (size &gt; 0) &#123; // be like clone(), allocate array based upon size not capacity ensureCapacityInternal(size); Object[] a = elementData; // Read in all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; a[i] = s.readObject(); &#125; &#125;&#125; 123456789101112131415161718private void writeObject(java.io.ObjectOutputStream s) throws java.io.IOException&#123; // Write out element count, and any hidden stuff int expectedModCount = modCount; s.defaultWriteObject(); // Write out size as capacity for behavioural compatibility with clone() s.writeInt(size); // Write out all elements in the proper order. for (int i=0; i&lt;size; i++) &#123; s.writeObject(elementData[i]); &#125; if (modCount != expectedModCount) &#123; throw new ConcurrentModificationException(); &#125;&#125; 序列化时需要使用 ObjectOutputStream 的 writeObject() 将对象转换为字节流并输出。而 writeObject() 方法在传入的对象存在 writeObject() 的时候会去反射调用该对象的 writeObject() 来实现序列化。反序列化使用的是 ObjectInputStream 的 readObject() 方法，原理类似。 123ArrayList list = new ArrayList();ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(file));oos.writeObject(list); 5. Fail-FastmodCount 用来记录 ArrayList 结构发生变化的次数。结构发生变化是指添加或者删除至少一个元素的所有操作，或者是调整内部数组的大小，仅仅只是设置元素的值不算结构发生变化。 在进行序列化或者迭代等操作时，需要比较操作前后 modCount 是否改变，如果改变了需要抛出 ConcurrentModificationException。代码参考上节序列化中的 writeObject() 方法。 Vector1. 同步它的实现与 ArrayList 类似，但是使用了 synchronized 进行同步。 12345678910111213public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true;&#125;public synchronized E get(int index) &#123; if (index &gt;= elementCount) throw new ArrayIndexOutOfBoundsException(index); return elementData(index);&#125; 2. 扩容Vector 的构造函数可以传入 capacityIncrement 参数，它的作用是在扩容时使容量 capacity 增长 capacityIncrement。如果这个参数的值小于等于 0，扩容时每次都令 capacity 为原来的两倍。 12345678public Vector(int initialCapacity, int capacityIncrement) &#123; super(); if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); this.elementData = new Object[initialCapacity]; this.capacityIncrement = capacityIncrement;&#125; 1234567891011private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + ((capacityIncrement &gt; 0) ? capacityIncrement : oldCapacity); if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); elementData = Arrays.copyOf(elementData, newCapacity);&#125; 调用没有 capacityIncrement 的构造函数时，capacityIncrement 值被设置为 0，也就是说默认情况下 Vector 每次扩容时容量都会翻倍。 1234567public Vector(int initialCapacity) &#123; this(initialCapacity, 0);&#125;public Vector() &#123; this(10);&#125; 3. 与 ArrayList 的比较 Vector 是同步的，因此开销就比 ArrayList 要大，访问速度更慢。最好使用 ArrayList 而不是 Vector，因为同步操作完全可以由程序员自己来控制； Vector 每次扩容请求其大小的 2 倍（也可以通过构造函数设置增长的容量），而 ArrayList 是 1.5 倍。 4. 替代方案可以使用 Collections.synchronizedList(); 得到一个线程安全的 ArrayList。 12List&lt;String&gt; list = new ArrayList&lt;&gt;();List&lt;String&gt; synList = Collections.synchronizedList(list); 也可以使用 concurrent 并发包下的 CopyOnWriteArrayList 类。 1List&lt;String&gt; list = new CopyOnWriteArrayList&lt;&gt;(); CopyOnWriteArrayList1. 读写分离写操作在一个复制的数组上进行，读操作还是在原始数组中进行，读写分离，互不影响。 写操作需要加锁，防止并发写入时导致写入数据丢失。 写操作结束之后需要把原始数组指向新的复制数组。 123456789101112131415161718public boolean add(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1); newElements[len] = e; setArray(newElements); return true; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 1234@SuppressWarnings(&quot;unchecked&quot;)private E get(Object[] a, int index) &#123; return (E) a[index];&#125; 2. 适用场景CopyOnWriteArrayList 在写操作的同时允许读操作，大大提高了读操作的性能，因此很适合读多写少的应用场景。 但是 CopyOnWriteArrayList 有其缺陷： 内存占用：在写操作时需要复制一个新的数组，使得内存占用为原来的两倍左右； 数据不一致：读操作不能读取实时性的数据，因为部分写操作的数据还未同步到读数组中。 所以 CopyOnWriteArrayList 不适合内存敏感以及对实时性要求很高的场景。 LinkedList1. 概览基于双向链表实现，使用 Node 存储链表节点信息。 12345private static class Node&lt;E&gt; &#123; E item; Node&lt;E&gt; next; Node&lt;E&gt; prev;&#125; 每个链表存储了 first 和 last 指针： 12transient Node&lt;E&gt; first;transient Node&lt;E&gt; last; 2. 与 ArrayList 的比较ArrayList 基于动态数组实现，LinkedList 基于双向链表实现。ArrayList 和 LinkedList 的区别可以归结为数组和链表的区别： 数组支持随机访问，但插入删除的代价很高，需要移动大量元素； 链表不支持随机访问，但插入删除只需要改变指针。 HashMap为了便于理解，以下源码分析以 JDK 1.7 为主。 1. 存储结构内部包含了一个 Entry 类型的数组 table。Entry 存储着键值对。它包含了四个字段，从 next 字段我们可以看出 Entry 是一个链表。即数组中的每个位置被当成一个桶，一个桶存放一个链表。HashMap 使用拉链法来解决冲突，同一个链表中存放哈希值和散列桶取模运算结果相同的 Entry。 1transient Entry[] table; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash; Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; public final int hashCode() &#123; return Objects.hashCode(getKey()) ^ Objects.hashCode(getValue()); &#125; public final String toString() &#123; return getKey() + &quot;=&quot; + getValue(); &#125;&#125; 2. 拉链法的工作原理1234HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(&quot;K1&quot;, &quot;V1&quot;);map.put(&quot;K2&quot;, &quot;V2&quot;);map.put(&quot;K3&quot;, &quot;V3&quot;); 新建一个 HashMap，默认大小为 16； 插入 &lt;K1,V1&gt; 键值对，先计算 K1 的 hashCode 为 115，使用除留余数法得到所在的桶下标 115%16&#x3D;3。 插入 &lt;K2,V2&gt; 键值对，先计算 K2 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6。 插入 &lt;K3,V3&gt; 键值对，先计算 K3 的 hashCode 为 118，使用除留余数法得到所在的桶下标 118%16&#x3D;6，插在 &lt;K2,V2&gt; 前面。 应该注意到链表的插入是以头插法方式进行的，例如上面的 &lt;K3,V3&gt; 不是插在 &lt;K2,V2&gt; 后面，而是插入在链表头部。 查找需要分成两步进行： 计算键值对所在的桶； 在链表上顺序查找，时间复杂度显然和链表的长度成正比。 3. put 操作1234567891011121314151617181920212223242526public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 键为 null 单独处理 if (key == null) return putForNullKey(value); int hash = hash(key); // 确定桶下标 int i = indexFor(hash, table.length); // 先找出是否已经存在键为 key 的键值对，如果存在的话就更新这个键值对的值为 value for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // 插入新键值对 addEntry(hash, key, value, i); return null;&#125; HashMap 允许插入键为 null 的键值对。但是因为无法调用 null 的 hashCode() 方法，也就无法确定该键值对的桶下标，只能通过强制指定一个桶下标来存放。HashMap 使用第 0 个桶存放键为 null 的键值对。 使用链表的头插法，也就是新的键值对插在链表的头部，而不是链表的尾部。 4. 确定桶下标很多操作都需要先确定一个键值对所在的桶下标。 12int hash = hash(key);int i = indexFor(hash, table.length); 4.1 计算 hash 值 1234567891011121314final int hash(Object k) &#123; int h = hashSeed; if (0 != h &amp;&amp; k instanceof String) &#123; return sun.misc.Hashing.stringHash32((String) k); &#125; h ^= k.hashCode(); // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 123public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value);&#125; 4.2 取模 令 x &#x3D; 1&lt;&lt;4，即 x 为 2 的 4 次方，它具有以下性质： 12x : 00010000x-1 : 00001111 令一个数 y 与 x-1 做与运算，可以去除 y 位级表示的第 4 位以上数： 123y : 10110010x-1 : 00001111y&amp;(x-1) : 00000010 这个性质和 y 对 x 取模效果是一样的： 123y : 10110010x : 00010000y%x : 00000010 我们知道，位运算的代价比求模运算小的多，因此在进行这种计算时用位运算的话能带来更高的性能。 确定桶下标的最后一步是将 key 的 hash 值对桶个数取模：hash%capacity，如果能保证 capacity 为 2 的 n 次方，那么就可以将这个操作转换为位运算。 123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 5. 扩容-基本原理设 HashMap 的 table 长度为 M，需要存储的键值对数量为 N，如果哈希函数满足均匀性的要求，那么每条链表的长度大约为 N&#x2F;M，因此查找的复杂度为 O(N&#x2F;M)。 为了让查找的成本降低，应该使 N&#x2F;M 尽可能小，因此需要保证 M 尽可能大，也就是说 table 要尽可能大。HashMap 采用动态扩容来根据当前的 N 值来调整 M 值，使得空间效率和时间效率都能得到保证。 和扩容相关的参数主要有：capacity、size、threshold 和 load_factor。 参数 含义 capacity table 的容量大小，默认为 16。需要注意的是 capacity 必须保证为 2 的 n 次方。 size 键值对数量。 threshold size 的临界值，当 size 大于等于 threshold 就必须进行扩容操作。 loadFactor 装载因子，table 能够使用的比例，threshold &#x3D; (int)(capacity* loadFactor)。 123456789101112131415static final int DEFAULT_INITIAL_CAPACITY = 16;static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;static final float DEFAULT_LOAD_FACTOR = 0.75f;transient Entry[] table;transient int size;int threshold;final float loadFactor;transient int modCount; 从下面的添加元素代码中可以看出，当需要扩容时，令 capacity 为原来的两倍。 123456void addEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); if (size++ &gt;= threshold) resize(2 * table.length);&#125; 扩容使用 resize() 实现，需要注意的是，扩容操作同样需要把 oldTable 的所有键值对重新插入 newTable 中，因此这一步是很费时的。 123456789101112131415161718192021222324252627282930void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor);&#125;void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 6. 扩容-重新计算桶下标在进行扩容时，需要把键值对重新计算桶下标，从而放到对应的桶上。在前面提到，HashMap 使用 hash%capacity 来确定桶下标。HashMap capacity 为 2 的 n 次方这一特点能够极大降低重新计算桶下标操作的复杂度。 假设原数组长度 capacity 为 16，扩容之后 new capacity 为 32： 12capacity : 00010000new capacity : 00100000 对于一个 Key，它的哈希值 hash 在第 5 位： 为 0，那么 hash%00010000 &#x3D; hash%00100000，桶位置和原来一致； 为 1，hash%00010000 &#x3D; hash%00100000 + 16，桶位置是原位置 + 16。 7. 计算数组容量HashMap 构造函数允许用户传入的容量不是 2 的 n 次方，因为它可以自动地将传入的容量转换为 2 的 n 次方。 先考虑如何求一个数的掩码，对于 10010000，它的掩码为 11111111，可以使用以下方法得到： 123mask |= mask &gt;&gt; 1 11011000mask |= mask &gt;&gt; 2 11111110mask |= mask &gt;&gt; 4 11111111 mask+1 是大于原始数字的最小的 2 的 n 次方。 12num 10010000mask+1 100000000 以下是 HashMap 中计算数组容量的代码： 123456789static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 8. 链表转红黑树从 JDK 1.8 开始，一个桶存储的链表长度大于等于 8 时会将链表转换为红黑树。 9. 与 Hashtable 的比较 Hashtable 使用 synchronized 来进行同步。 HashMap 可以插入键为 null 的 Entry。 HashMap 的迭代器是 fail-fast 迭代器。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 ConcurrentHashMap1. 存储结构 123456static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next;&#125; ConcurrentHashMap 和 HashMap 实现上类似，最主要的差别是 ConcurrentHashMap 采用了分段锁（Segment），每个分段锁维护着几个桶（HashEntry），多个线程可以同时访问不同分段锁上的桶，从而使其并发度更高（并发度就是 Segment 的个数）。 Segment 继承自 ReentrantLock。 1234567891011121314151617static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; static final int MAX_SCAN_RETRIES = Runtime.getRuntime().availableProcessors() &gt; 1 ? 64 : 1; transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 1final Segment&lt;K,V&gt;[] segments; 默认的并发级别为 16，也就是说默认创建 16 个 Segment。 1static final int DEFAULT_CONCURRENCY_LEVEL = 16; 2. size 操作每个 Segment 维护了一个 count 变量来统计该 Segment 中的键值对个数。 12345/** * The number of elements. Accessed only either within locks * or among other volatile reads that maintain visibility. */transient int count; 在执行 size 操作时，需要遍历所有 Segment 然后把 count 累计起来。 ConcurrentHashMap 在执行 size 操作时先尝试不加锁，如果连续两次不加锁操作得到的结果一致，那么可以认为这个结果是正确的。 尝试次数使用 RETRIES_BEFORE_LOCK 定义，该值为 2，retries 初始值为 -1，因此尝试次数为 3。 如果尝试的次数超过 3 次，就需要对每个 Segment 加锁。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * Number of unsynchronized retries in size and containsValue * methods before resorting to locking. This is used to avoid * unbounded retries if tables undergo continuous modification * which would make it impossible to obtain an accurate result. */static final int RETRIES_BEFORE_LOCK = 2;public int size() &#123; // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn&#x27;t retry try &#123; for (;;) &#123; // 超过尝试次数，则对每个 Segment 加锁 if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; // 连续两次得到的结果一致，则认为这个结果是正确的 if (sum == last) break; last = sum; &#125; &#125; finally &#123; if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 3. JDK 1.8 的改动JDK 1.7 使用分段锁机制来实现并发更新操作，核心类为 Segment，它继承自重入锁 ReentrantLock，并发度与 Segment 数量相等。 JDK 1.8 使用了 CAS 操作来支持更高的并发度，在 CAS 操作失败时使用内置锁 synchronized。 并且 JDK 1.8 的实现也在链表过长时会转换为红黑树。 LinkedHashMap存储结构继承自 HashMap，因此具有和 HashMap 一样的快速查找特性。 1public class LinkedHashMap&lt;K,V&gt; extends HashMap&lt;K,V&gt; implements Map&lt;K,V&gt; 内部维护了一个双向链表，用来维护插入顺序或者 LRU 顺序。 123456789/** * The head (eldest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; head;/** * The tail (youngest) of the doubly linked list. */transient LinkedHashMap.Entry&lt;K,V&gt; tail; accessOrder 决定了顺序，默认为 false，此时维护的是插入顺序。 1final boolean accessOrder; LinkedHashMap 最重要的是以下用于维护顺序的函数，它们会在 put、get 等方法中调用。 12void afterNodeAccess(Node&lt;K,V&gt; p) &#123; &#125;void afterNodeInsertion(boolean evict) &#123; &#125; afterNodeAccess()当一个节点被访问时，如果 accessOrder 为 true，则会将该节点移到链表尾部。也就是说指定为 LRU 顺序之后，在每次访问一个节点时，会将这个节点移到链表尾部，保证链表尾部是最近访问的节点，那么链表首部就是最近最久未使用的节点。 123456789101112131415161718192021222324void afterNodeAccess(Node&lt;K,V&gt; e) &#123; // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; if (accessOrder &amp;&amp; (last = tail) != e) &#123; LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else &#123; p.before = last; last.after = p; &#125; tail = p; ++modCount; &#125;&#125; afterNodeInsertion()在 put 等操作之后执行，当 removeEldestEntry() 方法返回 true 时会移除最晚的节点，也就是链表首部节点 first。 evict 只有在构建 Map 的时候才为 false，在这里为 true。 1234567void afterNodeInsertion(boolean evict) &#123; // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) &#123; K key = first.key; removeNode(hash(key), key, null, false, true); &#125;&#125; removeEldestEntry() 默认为 false，如果需要让它为 true，需要继承 LinkedHashMap 并且覆盖这个方法的实现，这在实现 LRU 的缓存中特别有用，通过移除最近最久未使用的节点，从而保证缓存空间足够，并且缓存的数据都是热点数据。 123protected boolean removeEldestEntry(Map.Entry&lt;K,V&gt; eldest) &#123; return false;&#125; LRU 缓存以下是使用 LinkedHashMap 实现的一个 LRU 缓存： 设定最大缓存空间 MAX_ENTRIES 为 3； 使用 LinkedHashMap 的构造函数将 accessOrder 设置为 true，开启 LRU 顺序； 覆盖 removeEldestEntry() 方法实现，在节点多于 MAX_ENTRIES 就会将最近最久未使用的数据移除。 1234567891011class LRUCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; &#123; private static final int MAX_ENTRIES = 3; protected boolean removeEldestEntry(Map.Entry eldest) &#123; return size() &gt; MAX_ENTRIES; &#125; LRUCache() &#123; super(MAX_ENTRIES, 0.75f, true); &#125;&#125; 123456789public static void main(String[] args) &#123; LRUCache&lt;Integer, String&gt; cache = new LRUCache&lt;&gt;(); cache.put(1, &quot;a&quot;); cache.put(2, &quot;b&quot;); cache.put(3, &quot;c&quot;); cache.get(1); cache.put(4, &quot;d&quot;); System.out.println(cache.keySet());&#125; 1[3, 1, 4] WeakHashMap存储结构WeakHashMap 的 Entry 继承自 WeakReference，被 WeakReference 关联的对象在下一次垃圾回收时会被回收。 WeakHashMap 主要用来实现缓存，通过使用 WeakHashMap 来引用缓存对象，由 JVM 对这部分缓存进行回收。 1private static class Entry&lt;K,V&gt; extends WeakReference&lt;Object&gt; implements Map.Entry&lt;K,V&gt; ConcurrentCacheTomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能。 ConcurrentCache 采取的是分代缓存： 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）； 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收。 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象。 1234567891011121314151617181920212223242526272829303132public final class ConcurrentCache&lt;K, V&gt; &#123; private final int size; private final Map&lt;K, V&gt; eden; private final Map&lt;K, V&gt; longterm; public ConcurrentCache(int size) &#123; this.size = size; this.eden = new ConcurrentHashMap&lt;&gt;(size); this.longterm = new WeakHashMap&lt;&gt;(size); &#125; public V get(K k) &#123; V v = this.eden.get(k); if (v == null) &#123; v = this.longterm.get(k); if (v != null) this.eden.put(k, v); &#125; return v; &#125; public void put(K k, V v) &#123; if (this.eden.size() &gt;= size) &#123; this.longterm.putAll(this.eden); this.eden.clear(); &#125; this.eden.put(k, v); &#125;&#125; 参考资料 Eckel B. Java 编程思想 [M]. 机械工业出版社, 2002. Java Collection Framework Iterator 模式 Java 8 系列之重新认识 HashMap What is difference between HashMap and Hashtable in Java? Java 集合之 HashMap The principle of ConcurrentHashMap analysis 探索 ConcurrentHashMap 高并发性的实现机制 HashMap 相关面试题及其解答 Java 集合细节（二）：asList 的缺陷 Java Collection Framework – The LinkedList Class 杂记HashMap 从源码中可以看出：HashMap提供四种构造方法：一是给定初始容量和加载因子的构造方法，二是给定初始容量，使用默认的加载因子，三是什么参数都不给，使用默认的初始容量和默认的加载因子，四是传进一个Map，使用默认的加载因子。从上面的构造方法可以看出，无论是使用默认的初始容量，还是使用默认的初始容量，当你调用HashMap的构造方法时，HashMap是没有进行初始化容量，也就是现在是一个空的HashMap（容量为0）,这是因为HashMap使用的懒加载机制，只有你第一次向HashMap中添加元素时，才进行第一次的容量设置，查看put(K,V)的源码：put(K key, V value)调用了putVal(int hash, K key, V value, boolean onlyIfAbsent,boolean evict)的方法，在putVal的方法中，当第一次向HashMap中添加对象时，会进行一个判空的处理，这时就调用resize()方法对HashMap进行容量设置，此时会有两种情况的根据容量初始化。 第一种情况：当我们没有设置初始化容量时，HashMap就使用默认的初始化容量，也就是16. 第二种情况：当我们设置了初始化容量，HashMap就会按照我们设置的容量进行设置吗？答案是不一定。当你设置的初始化容量是2的n次方时，就会按照你设置的容量设置；当你设置的初始化容量不是2的n次方时，就会按照大于你设置的那个值但是最接近你设置的那个值的2的n次方进行设置。","categories":[],"tags":[]},{"title":"云原生","slug":"22CloudNative","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-19T04:43:48.519Z","comments":true,"path":"2022/09/01/22CloudNative/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/22CloudNative/","excerpt":"","text":"入门蓝绿部署: ​ 蓝环境: 稳定部署v1 ​ 绿环境: 测试v2,蓝绿切换 CI&#x2F;CD 持续集成&#x2F;持续部署 DevOps 开发与运维的合体 容器化隔离原理: ​ namespace(命名空间隔离) ​ cgroups 资源限制() 安装 yum配置, 安装docker 123456789101112131415161718192021222324252627282930313233343536alpine:超级件经典linux版本 只有5mb没有alpine 就是centos 会大一些Ctrl + c 退出 1. 下载镜像 dockr pull xxx:版本2. docker images 查看已下载镜像 镜像管理操作 docker image --help3. docker run xxx:版本号 启动镜像4. dockr rm xxx:版本号 删运行的容器 默认前台启动 一般加上 -d 后台启动5. dockr rmi xxx:版本号 # 删镜像 -f 强制删6. 删除全部镜像 docker rmi -f $(docker images -aq)7. docker create [设置项] 镜像名 [启动] [启动参数..] docker create --name myredis -p 6379:6379 redis状态是已建好 需要 docker start xxxdocker pause xxx8. docker logs -f mynginxs9. docker attach xxx 绑定控制台10. docker exec -it xxx /bin/bash(控制台) exit 退出11. docker inspect 镜像/容器/volume 等的详情12. docker cp [OPTIONS] name:/ /export13. docker diff xxx 14. docker commit -a ggq -m &quot;first commit&quot; xxx myxxx:v4 docker images 就会有, 一般运行中的容器会一直更改, 就要用commit15. docker login 16. Dockerhub网站docker tag myxxx:tagname oneyeartwoyear/ggq_bigdata_env:v117. docker push oneyeartwoyear/ggq_bigdata_env:v118 docker export -o xxx.tar xxx: 方便优盘传输19. docker import xxx.tar xxx:tagname exec 镜像推送细节 docker buildls Dockerfile(这是一个文件) docker build -t xxx:xxx -f Dockerfile .&#x2F; 容器挂载 docker-compose 写一个yaml 指定所有需要的启动的容器 CI&#x2F;DI Jenkinskubernetes K8S CRI (Container Runtime Interface)","categories":[{"name":"云原生","slug":"云原生","permalink":"https://gouguoqiang.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"}],"tags":[{"name":"云原生","slug":"云原生","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"}]},{"title":"大数据","slug":"21bigdata","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-19T04:43:14.804Z","comments":true,"path":"2022/09/01/21bigdata/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/21bigdata/","excerpt":"","text":"大数据组件笔记一、 HadoopHDFS：分布式文件存储系统，大数据环境的基石MapReduce(MR)：基于磁盘计算，主要用于大量数据的批处理计算Spark(RDD)：基于内存计算SparkSQL：一般情况都是基于离线数据处理Spark Streaming：一般情况是基于微批(实时)处理 Flink 流式计算引擎Flink SQL：类似SparkSQL，可以写SQL，更快的使用批处理操作Flink Streaming：流式数据，(开发思路)生产库产生数据一部分发送至kafka、一部分落库，后续Filnk对接kafka中的Topic ，实时对kafka中数据进行去重、清洗、汇总、计算，维度可以存放至redis中。。。 二、 消息队列Kafka：可理解生产者和消费者之间的数据传递，大数据量发送传递及消费，主要功能点是削峰填谷MQ：主要运用于事务性消息队列，但是对于数据量来看，量级很小 三、 数据库(关系型、非关系型)Hive：hive只是有个管理工具，不存储任何数据，将hdfs中的文件映射成表的形式进行数据处理，主要面向于(离线)数据仓库使用，内部执行引擎(MapReduce&#x2F;Spark&#x2F;Tez)，也可以开发一下自定义的UTF函数。Impala：号称是当前大数据领域最快的查询sql工具，占用内存比较多，我们在工作中都是使用Hive+Impala做离线数仓。(即席查询)Presto：一个分布式SQL查询引擎，整体性能还是可以的。(即席查询)HBases： 典型的NoSQL、分布式存储的数据库，速度够快。Kudu：在更新更及时的基础上实现更快的数据分析，个人感觉是在大量数据中做到更快的查询速度。Kylin：分布式分析引擎，我们主要用于做OLAP多维数据立方体数据，就类似与Cognos中的动态CUBE。ClickHouse：(不基于Hadoop集群，可独立安装)列式数据库，主要用于实时数据仓库，这个也是基于内存的，特点就是快。单表查询块，多表关联是弊端。(即席查询)Doris：一个基于 MPP 架构的高性能、实时的分析型数据库，尽量使用星型模型，单表不要超过100列，(即席查询) 四、 ETL工具Sqoop：主要是用于关系型数据与分布式数据库的数据抽取任务，类似MYSQL数据抽取至HDFS&#x2F;Hive。sqoop底层运用的计算引擎也是MR，只不顾没有用到Reduce而已SeaTunnel（WaterDrop）：大数据集群数据同步工具，主要引擎有spark和flink，Kakfa-&gt;PostgreSQL、ClinkHouse-&gt;PostgreSQL、MongoDB-&gt;PostgreSQL，PostgreSQL-&gt;HDFS等等。我就使用了这么多，其他的功能还需小伙伴继续钻研DataX：阿里开发，可多线程抽取数据，但是会有一些数据丢失问题，可能是生产库数据有脏数据问题，有待考证。FlinkX： 袋鼠云开源，有待考证。 五、 数据可视化DataV：阿里开发Datart(Davinci)：宜信开发，可支持中国式复杂报表开发，也支持图表等二三十种图表样式支持，可自定义开发图表组件Superset：百度开源，现已贡献给Apache 开源基金会，图表绚丽 六、 任务调度工具Azkaban：一个脚本任务调度工具，一般用于ETL脚本执行调度，需要单独配置调度文件。在文件中需要配置依赖脚本等信息，全程在WEB端开发，在查看调度的时候有点费眼睛，DAG图不可以缩得太小或太大DolphinScheduler：俗称小海豚，国人开发的脚本调度工具，但是每次每一个脚本都得需要上传到服务器上，如果该工具能够支持git自动同步脚本文件的话，将是一个很好的脚本工具，开发及迁移都是很好使用的Kettle：传统的ETL工具+调度工具，有两大特性：job和转换，win和linux都可以兼容，很好使的一个ETL处理工具————————————————版权声明：本文为CSDN博主「W-DW」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。原文链接：https://blog.csdn.net/qq_36712507/article/details/106933379 Windows 环境搭建看过的博客: https://cloud.tencent.com/developer/article/1818595 https://blog.csdn.net/qq_44186838/article/details/119965991 https://zhuanlan.zhihu.com/p/508841769 hive在Hive中 如果本地数据库是8.0 驱动jar包换成8.的版本 hive –service metastore 也有可能报错 如果报错了 把hive库删了重新手动初始化下 报错信息： FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:An exception was thrown while adding&#x2F;validating class(es) : You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘[CHARACTER SET charset_name] [COLLATE collation_name] NULL, VIEW_ORIGINAL_T&#39; at line 14 java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;[CHARACTER SET charset_name] [COLLATE collation_name] NULL, VIEW_ORIGINAL_T’ at line 14 hive –service metastore SQL语法报错解决 123456手动初始化hive --service schematool -dbType mysql -initSchema直接启动hivehive Starting Hive Metastore Server 安装scalahttps://blog.csdn.net/qq_46864949/article/details/123203397 不知道在安装啥的时候把scala安装好了 怕影响其他大数据组件,目录有空格将环境变量去掉了 需要在IDEA 中安装Scala插件 ok后就可正常开发scala了 Flumehttps://blog.csdn.net/moshowgame/article/details/80379402 Hive与Hbase区别 一、Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据(十亿、百亿)的随机实时查询，如日志明细、交易清单、轨迹行为等。 二、Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。 \\1. Hive用于批处理，而HBase用于事务处理。 \\2. Hive是查询引擎,而Hbase是 非结构化数据的数据存储。 \\3. Hive是运行MapReduce作业的类似SQL的引擎，而HBase 是Hadoop上的NoSQL键&#x2F;值数据库 Spark启动spark : spark-shell.cmd sparkSQL集成 hive hbase mysqlhttps://blog.csdn.net/u011254180/article/details/79395227 spark1.6 -&gt;2.4升级问题https://blog.csdn.net/aoyugang0603/article/details/102442007 项目搭建JavaWebhttps://blog.csdn.net/u014636209/article/details/104261350/ https://blog.csdn.net/weixin_43977226/article/details/118614918 根据代码 修改组件的端口号设置 hadoop 123456789101112131415161718192021222324252627282930313233343536373839404142434445462）修改配置文件 A）vi /home/hadoop-2.6.4/etc/hadoop/hadoop-env.sh 修改为export JAVA_HOME=/usr/java/jdk1.7.0_79（jdk地址） B）vi /home/hadoop-2.6.4/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; //配置namenode主机和端口 &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://spark1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //配置hadoop的临时文件目录以及fsimage文件目录，不能放在temp下 &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; C）vi /home/hadoop-2.6.4/etc/hadoop/hdfs-site.xml，配置secondary namenode主机名和端口 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;spark1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;spark1:50090&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566672）修改配置文件 A）vi /home/hadoop-2.6.4/etc/hadoop/hadoop-env.sh 修改为export JAVA_HOME=/usr/java/jdk1.7.0_79（jdk地址） B）vi /home/hadoop-2.6.4/etc/hadoop/core-site.xml &lt;configuration&gt; &lt;property&gt; //配置namenode主机和端口 &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://spark1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; //配置hadoop的临时文件目录以及fsimage文件目录，不能放在temp下 &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/hadoop-2.6&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; C）vi /home/hadoop-2.6.4/etc/hadoop/hdfs-site.xml，配置secondary namenode主机名和端口 &lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;spark1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;spark1:50090&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; D）vi /home/hadoop-2.6.4/etc/hadoop/slaves，配置datanode的主机名 spark1 spark2 spark3 E）vi /home/hadoop-2.6.4/etc/hadoop/masters（这个文件原本是没有的） spark1（只需输入这个，为namenode主机） F）配置hadoop的环境变量 vi ~/.bash_profile export HADOOP_HOME=/home/hadoop-2.6.4 export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 3）格式化hdfs，只能在namenode主机（spark1）上执行 hdfs namenode -format 4）启动hdfs（在namenode上启动，已经配置免密码登录了，具体配置见免密码登录） start-dfs.sh （stop-dfs.sh为关闭） 5）在浏览器登录查看 spark1:50070 //50070为namenode的http监控的端口，spark1已经在hosts文件中与ip相映射 spark1:50090 //secondary namenode的监控端口 代码位置 D:\\IDEA\\projects\\Movie_Recommend-master\\Spark_Movie\\src\\main\\scala\\com\\zxl&gt; 数据清洗 12345678910111213141516171819202122**数据的清洗：** （上传数据至hdfs中，[hdfs操作](http://blog.csdn.net/u011254180/article/details/79399422)）到自己配置的响应的文件目录1）启动 hdfs： [root@spark1 ~]# start-dfs.sh2）启动 yarn： [root@spark1 ~]# start-yarn.sh3）启动 mysql： [root@spark2 ~]# service mysqld start**这一步配置好 hive后就不用了** 4）启动 hive： [root@spark1 ~]# hive --service metastore# 5）启动 spark集群： [root@spark1 spark-1.6.1]# ./sbin/start-all.sh 启动spark ..\\spark-2.4.2-bin-hadoop2.7\\bin\\spark-shell.cmd6）代码(com.zxl.datacleaner.ETL)打包上传（[spark-sql与hive集成](http://blog.csdn.net/u011254180/article/details/79395227)）- 代码位于 package com.zxl.datacleaner.ETL，打包为 ETL.jar# 打包操作 todoD:\\IDEA\\projects\\Movie_Recommend-master\\Spark_Movie\\src\\main\\scala\\com\\zxl&gt;- 运行代码 spark-submit --class com.zxl.datacleaner.ETL --total-executor-cores 2 --executor-memory 2g lib/ETL.jar- 成功于hive中建表 5.数据的加工， 根据ALS算法对数据建立模型(ALS论文) 1）启动 hdfs： [root@spark1 ~]# start-dfs.sh 2）启动 yarn： [root@spark1 ~]# start-yarn.sh 3）启动 mysql： [root@spark2 ~]# service mysqld start 4）启动 hive： [root@spark1 ~]# hive –service metastore 5）启动 spark集群： [root@spark1 spark-1.6.1]# .&#x2F;sbin&#x2F;start-all.sh 6）代码(com.zxl.datacleaner.RatingData)打包上传，测试建立模型 6.建立模型， 根据RMSE(均方根误差)选取较好的模型 1）启动上述的服务 2）代码(com.zxl.ml.ModelTraining)打包上传，建立模型 注：com.zxl.ml.ModelTraining2中代码训练单个模型，其中参数 rank&#x3D;50, iteration &#x3D; 10, lambda &#x3D; 0.01 代码位于 package com.zxl.ml.ModelTraining，打包为 Spark_Movie.jar 运行代码 spark-submit –class com.zxl.ml.ModelTraining lib&#x2F;Spark_Movie.jar 7.产生推荐结果 1）启动上述的服务 2）代码(com.zxl.ml.Recommender)打包上传，产生推荐结果 8.数据入库， 存储为所有用户推荐的电影结果，mysql中存入的格式为(userid, movieid,rating) 1）启动上述的服务 2）代码(com.zxl.ml.RecommendForAllUsers)打包上传，数据入库 运行代码 spark-submit –class com.zxl.ml.RecommendForAllUsers –jars lib&#x2F;mysql-connector-java-5.1.35-bin.jar lib&#x2F;Spark_Movie.jar 9.实时数据的发送 1）安装nginx，用来接收电影网站上用户的点击信息，写入本地文件 2）安装flume，实时监控本地文件，将数据发送至kafka消息队列中 10.实时数据的接收处理 ，如果打包到服务器运行错误，也可在本地IDEA上运行 1）安装zookeeper 2）安装kafka，用来接收发送数据 3）启动上述的服务 4）启动zookeeper： [root@spark1 soft]# zkServer.sh start 4）启动flume：[root@spark1 flume]# bin&#x2F;flume-ng agent -c .&#x2F;conf&#x2F; -f conf&#x2F;flume-conf.properties -Dflume.root.logger&#x3D;DEBUG,console -n a1 5）启动kafka： [root@spark1 kafka_2.11-0.10.1.0]# bin&#x2F;kafka-server-start.sh config&#x2F;server.properties 6）代码(com.zxl.datacleaner.PopularMovies2)运行，用于为没有登录或新用户推荐，默认推荐观看最多的5部电影 7）代码运行(需指定jar包 kafka-clients-0.10.1.0.jar) spark-submit –class com.zxl.streaming.SparkDrStreamALS –total-executor-cores 2 –executor-memory 1g –jars lib&#x2F;kafka-clients-0.10.1.0.jar lib&#x2F;Spark_Movie.jar 项目学习下载数据集 spark操作将数据集上传到hdfs “hdfs:&#x2F;&#x2F;spark1:9000&#x2F;movie&#x2F;data&#x2F;links.txt” 12val ratings = sc.textFile(&quot;hdfs://spark1:9000/movie/data/ratings.txt&quot;, minPartitions).filter &#123; !_.endsWith(&quot;,&quot;) &#125; .map(_.split(&quot;,&quot;)).map(x =&gt; Ratings(x(0).trim().toInt, x(1).trim().toInt, x(2).trim().toDouble, x(3).trim().toInt)).toDF() 这里面spark的作用 Spark(RDD)：基于内存计算SparkSQL：一般情况都是基于离线数据处理 ETL: 将表存到hive, 数据写入hdfs hive 与 hdfs: 7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库， 它是一个适合于非结构化数据存储的数据库。 8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张 数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运 行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开 发专门的 MapReduce 应用，十分适合数据仓库的统计分析。 hdfs的分布式: 是因为一个系统存储不下所有数据 Hive：hive只是有个管理工具，不存储任何数据，将hdfs中的文件映射成表的形式进行数据处理，主要面向于(离线)数据仓库使用，内部执行引擎 爬虫Spark概念 DataSet 是将DataFrame转换为对象的结果 操作外置Hive 实操默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能 够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里 的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。 RDD算子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869//mapval dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4))val dataRDD2: RDD[String] = dataRDD.map( num =&gt; &#123; &quot;&quot; + num &#125;)//mapPartitions //将处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处//理，哪怕是过滤数据。//Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处//理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能//不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作。val dataRDD1: RDD[Int] = dataRDD.mapPartitions( datas =&gt; &#123; datas.filter(_==2) &#125;)//flatmap 扁平映射val dataRDD = sparkContext.makeRDD(List( List(1,2),List(3,4)),1)val dataRDD1 = dataRDD.flatMap( list =&gt; list)//groupbyval dataRDD = sparkContext.makeRDD(List(1,2,3,4),1)val dataRDD1 = dataRDD.groupBy( _%2)// filter 符合规则的数据保留，不符合规则的数据丢弃val dataRDD = sparkContext.makeRDD(List( 1,2,3,4),1)val dataRDD1 = dataRDD.filter(_%2 == 0)//distinct val dataRDD = sparkContext.makeRDD(List( 1,2,3,4,1,2),1)val dataRDD1 = dataRDD.distinct()val dataRDD2 = dataRDD.distinct(2)//collect//在驱动程序中，以数组 Array 的形式返回数据集的所有元素val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 收集数据到 Driverrdd.collect().foreach(println)//take返回一个由 RDD 的前 n 个元素组成的数组val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 返回 RDD 中元素的个数val takeResult: Array[Int] = rdd.take(2)println(takeResult.mkString(&quot;,&quot;))val rdd: RDD[Int] = sc.makeRDD(List(1,3,2,4))// 返回 RDD 中元素的个数val result: Array[Int] = rdd.takeOrdered(2)// 将数据保存到不同格式的文件中// 保存成 Text 文件rdd.saveAsTextFile(&quot;output&quot;)// 序列化成对象保存到文件rdd.saveAsObjectFile(&quot;output1&quot;)// 保存成 Sequencefile 文件rdd.map((_,1)).saveAsSequenceFile(&quot;output2&quot;)//foreachval rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 收集后打印rdd.map(num=&gt;num).collect().foreach(println)println(&quot;****************&quot;)// 分布式打印rdd.foreach(println) RDD文件读取与保存 创建df1从spark数据源进行创建 2RDD转换 3HIVE Table 进行查询返回 打包代码 build之后打成jar包 运行jar包: 启动spark : 了解下master worker HadoopYarnYarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式 的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。 YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件 构成。 HDFS Scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667【scala】下划线用法总结1. 用于变量的初始化在Scala中，变量在声明时必须显示指定，可以使用下划线对变量进行初始化。而且该语法只适用于成员变量，不适用于局部变量。例：对于Int来说，它是0。对于Double来说，它是0.0对于引用类型，它是null2. 用于导包引入导包引入时使用_导入该包下所有内容，类比Java中的*。例如：3. 用于将方法转变为函数在Scala中方法不是值，而函数是。所以一个方法不能赋值给一个val变量，而函数可以。方法可以转换为函数赋值给变量，例：4. 用于模式匹配模式匹配中可以用下划线来作为Java中default的类比使用，也可以在匹配集合类型时，用于代表集合中元素，例：5. 用户访问tuple元素6. 下划线与星号向函数或方法传入可变参数时不能直接传入Range或集合或数组对象，需要使用:_*转换才可传入6.1 变长参数例如定义一个变长参数的方法sum，然后计算1-5的和，可以写为scala&gt; def sum(args: Int*) = &#123; | var result = 0 | for (arg &lt;- args) result += arg | result | &#125;sum: (args: Int*)Int​scala&gt; val s = sum(1,2,3,4,5)s: Int = 15但是如果写成这种方式就会报错scala&gt; val s = sum(1 to 5)&lt;console&gt;:12: error: type mismatch; found : scala.collection.immutable.Range.Inclusive required: Int val s = sum(1 to 5) ^这种情况必须在后面写上: _*将1 to 5转化为参数序列scala&gt; val s = sum(1 to 5: _*)s: Int = 15126.2 变量声明中的模式下面代码分别将arr中的第一个和第二个值赋给first和secondscala&gt; val arr = Array(1,2,3,4,5)arr: Array[Int] = Array(1, 2, 3, 4, 5)​scala&gt; val Array(first, second, _*) = arrfirst: Int = 1second: Int = 2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485861）说明（1）过滤遍历一个集合并从中获取满足指定条件的元素组成一个新的集合（2）转化/映射（map） 将集合中的每一个元素映射到某一个函数（3）扁平化（4）扁平化+映射 注：flatMap 相当于先进行 map 操作，在进行 flatten 操作集合中的每个元素的子元素映射到某个函数并返回新集合（5）分组(group)按照指定的规则对集合的元素进行分组（6）简化（归约） （7）折叠2）实操object TestList &#123; def main(args: Array[String]): Unit = &#123; val list: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9) val nestedList: List[List[Int]] = List(List(1, 2, 3), List(4,5, 6), List(7, 8, 9)) val wordList: List[String] = List(&quot;hello world&quot;, &quot;helloatguigu&quot;, &quot;hello scala&quot;) //（1）过滤 println(list.filter(x =&gt; x % 2 == 0)) //（2）转化/映射 println(list.map(x =&gt; x + 1)) //（3）扁平化 println(nestedList.flatten) //（4）扁平化+映射 注：flatMap 相当于先进行 map 操作，在进行 flatten操作 println(wordList.flatMap(x =&gt; x.split(&quot; &quot;))) //（5）分组 println(list.groupBy(x =&gt; x % 2)) &#125;&#125;3）Reduce 方法Reduce 简化（归约） ：通过指定的逻辑将集合中的数据进行聚合，从而减少数据，最终获取结果。案例实操object TestReduce &#123; def main(args: Array[String]): Unit = &#123; val list = List(1,2,3,4) // 将数据两两结合，实现运算规则 val i: Int = list.reduce( (x,y) =&gt; x-y ) println(&quot;i = &quot; + i) // 从源码的角度，reduce 底层调用的其实就是 reduceLeft //val i1 = list.reduceLeft((x,y) =&gt; x-y) // ((4-3)-2-1) = -2 val i2 = list.reduceRight((x,y) =&gt; x-y) println(i2) &#125;&#125;4）Fold 方法Fold 折叠：化简的一种特殊情况。（1）案例实操：fold 基本使用object TestFold &#123; def main(args: Array[String]): Unit = &#123; val list = List(1,2,3,4) // fold 方法使用了函数柯里化，存在两个参数列表 // 第一个参数列表为 ： 零值（初始值） // 第二个参数列表为： 简化规则 // fold 底层其实为 foldLeft val i = list.foldLeft(1)((x,y)=&gt;x-y) val i1 = list.foldRight(10)((x,y)=&gt;x-y) println(i) println(i1) &#125;&#125;object TestFold &#123; def main(args: Array[String]): Unit = &#123; // 两个 Map 的数据合并 val map1 = mutable.Map(&quot;a&quot;-&gt;1, &quot;b&quot;-&gt;2, &quot;c&quot;-&gt;3) val map2 = mutable.Map(&quot;a&quot;-&gt;4, &quot;b&quot;-&gt;5, &quot;d&quot;-&gt;6) val map3: mutable.Map[String, Int] = map2.foldLeft(map1)&#123; (map, kv) =&gt; &#123; val k = kv._1 val v = kv._2 map(k) = map.getOrElse(k, 0) + v map &#125; &#125; println(map3) &#125;&#125; case class，它其实就是一个普通的class。但是它又和普通的class略有区别，如下： 1、初始化的时候可以不用new，当然你也可以加上，普通类一定需要加new； object 伴生对象中的内容,都可以通过类名访问,来模拟java中的静态语法 伴生对象的语法规则：使用object声明[加上就一定能够使用类名来访问] Flume 数据采集 机器学习和推荐系统数学基础矩阵乘法: 行定列移 矩阵转置: 行列交换 倒数: 沿x正方向斜率 变化大的地方 好求最大值最小值 机器学习 推荐算法 1234567891011基于内容的推荐算法Content-based Recommendations (CB)根据推荐物品或内容的元数据,发现物品的相关性，再基于用户过去的喜好记录，为用户推荐相似的物品。通过抽取物品内在或者外在的特征值，实现相似度计算。-比如一个电影，有导演、演员、用户标签UGC、用户评论、时长、风格等等，都可以算是特征。， 将用户(user) 个人信息的特征(基于喜好记录或是预设兴趣标签)，和物品(item)的特征相匹配，就能得到用户对物品感兴趣的程度-在一些电影、音乐、图书的社交网站有很成功的应用，有些网站还请专业的人员对物品进行基因编码/打标签(PGC) 相似度 基于内容 基于UGC 协同过滤 12345678910基于内容的推荐算法Content-based Recommendations (CB)根据推荐物品或内容的元数据,发现物品的相关性，再基于用户过去的喜好记录，为用户推荐相似的物品。通过抽取物品内在或者外在的特征值，实现相似度计算。-比如一个电影，有导演、演员、用户标签UGC、用户评论、时长、风格等等，都可以算是特征。， 将用户(user) 个人信息的特征(基于喜好记录或是预设兴趣标签)，和物品(item)的特征相匹配，就能得到用户对物品感兴趣的程度-在一些电影、音乐、图书的社交网站有很成功的应用，有些网站还请专业的人员对物品进行基因编码/打标签(PGC) 基于近邻 1234LFM降维方法一矩阵因子分解●假设用户物品评分矩阵为R, 现在有m个用户，n个物品●我们想要发现k 个隐类,我们的任务就是找到两个矩阵P和Q,使这两个矩阵的乘积近似等于R,即将用户物品评分矩阵R分解成为两个低维矩阵相乘: 123矩阵因子分解如果得到的预测评分矩阵R与原评分矩阵R在已知评分位置上的值都近似，那么我们认为它们在预测位置上的值也是近似的 12345678LFM的进一步理解●我们可以认为，用户之所以给电影打出这样的分数,是有内在原因的，我们可以挖掘出影响用户打分的隐藏因素，进而根据未评分电影与这些隐藏因素的关联度,决定此未评分电影的预测评分● 应该有一些隐藏的因素，影响用户的打分，比如电影:演员、题材、年代..甚至不- -定是人直接可以理解的隐藏因子●找到隐藏因子，可以对user和item 进行关联(找到是由于什么使得user 喜欢/不喜欢此item,什么会决定user喜欢/不喜欢此item)，就可以推测用户是否会喜欢某一部未看过的电影 1234●对于用户看过的电影， 会有相应的打分，但一个用户不可能看过所有电影,对于用户没有看过的电影是没有评分的，因此用户评分矩阵大部分项都是空的，是一个稀疏矩阵●如果我们能够根据用户给已有电影的打分推测出用户会给没有看过的电影的打分,那么就可以根据预测结果给用户推荐他可能打高分的电影 预测的误差 就是损失函数 12345// rank：对应的是隐因子的个数，这个值设置越高越准，但是也会产生更多的计算量。一般将这个值设置为10-200// 隐因子：如一部电影，决定它评分的有导演、主演、特效和剧本4个隐因子// iterations：对应迭代次数，一般设置个10就够了；// lambda：该参数控制正则化过程，其值越高，正则化程度就越深。一般设置为0.01// val model = ALS.train(ratingRDD, 1, 10, 0.01) 项目 离线 矩阵存成表的问题 实时 Spark实操前言spark 处理数据: 1 项目 方法里面定义隐式参数, 后面调用的时候省去传参, 就会在当前 全局作用域里找到同类型的隐式变量传入 jblas java中跟线性代数相关的库 模型训练 ALS参数","categories":[{"name":"大数据","slug":"大数据","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"算法随记","slug":"2algorithm","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:28:07.032Z","comments":true,"path":"2022/09/01/2algorithm/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/2algorithm/","excerpt":"","text":"基础知识 力扣周赛java选手阿全,天堂 实用技巧Map.putIfAbsent();随机数常用的两种 Random random &#x3D; new Random（）; random.nextInt(l.size()) Math.random*l.size() Math.randow [0,1) HashMap getOrDefault() 方法getOrDefault() 法获取指定 key 对应的 value，如果找不到 key ，则返回设置的默认值。 hashMap.containsKey(); string.substring(); HashMap遍历先取出 所有的 Key1234567891011121314151617Set keyset = map.keySet();for(Object key: keyset)&#123; map.get(key);&#125;Iterator iterator= keyset.iterator();while(iterator.hasNext())&#123;Object key = iterator.next();map.get(key);&#125; 取出所有valueCollection values &#x3D; map.values(); EntrySet 获取k-vSet entrySet &#x3D; map.entrySet(); 123456789101112131415161718192021222324252627282930313233343536373839404142Set entrySet = map.entrySet();class Solution &#123; public int mostFrequentEven(int[] nums) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for (int num : nums) &#123; if (num % 2 == 0) &#123; map.put(num,map.getOrDefault(num,0)+1); &#125; &#125; Set&lt;Map.Entry&lt;Integer,Integer&gt;&gt; entrySet = map.entrySet(); int ans = -1; int max = 0; for (Map.Entry&lt;Integer,Integer&gt; entry : entrySet) &#123; if (entry.getValue() &gt; max) &#123; ans = entry.getKey(); max = entry.getValue(); &#125; &#125; return ans; &#125;&#125;/(1) 增强 forSystem.out.println(&quot;----使用 EntrySet 的 for 增强(第 3 种)----&quot;);for (Object entry : entrySet) &#123;//将 entry 转成 Map.EntryMap.Entry m = (Map.Entry) entry;System.out.println(m.getKey() + &quot;-&quot; + m.getValue());&#125;//(2) 迭代器System.out.println(&quot;----使用 EntrySet 的 迭代器(第 4 种)----&quot;);Iterator iterator3 = entrySet.iterator();while (iterator3.hasNext()) &#123;Object entry = iterator3.next();//System.out.println(next.getClass());//HashMap$Node -实现-&gt; Map.Entry (getKey,getValue)//向下转型 Map.EntryMap.Entry m = (Map.Entry) entry;System.out.println(m.getKey() + &quot;-&quot; + m.getValue());&#125; 一些值得学习的处理147. 对链表进行插入排序方法一 直观模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445//方法一 直观模拟class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //分割成单结点,去除链表的影响 所有都是单节点操作 //不分割可能会成环 if (head == null) return null; ListNode newHead = head; head = head.next; ListNode p = newHead; newHead.next = null; ListNode q = head; // 梳理下思路 //遍历输入结点,找输入结点在新链表的位置,小于等于新结点的头结点 直接头插 大于则往后遍历, // 直到 p.next.val &gt; q.val // 如果 p.next == null 直接尾插 while (q != null) &#123; //头插 ListNode t = q.next; q.next = null; if (q.val &lt; newHead.val) &#123; //头插法 q.next = newHead; newHead = q; q = t; p = newHead; continue; &#125; while (p == null || q.val &gt; p.val) &#123; if (p.next == null || p.next.val &gt; q.val ) &#123; break; &#125; p = p.next; &#125; // 插入 ListNode temp = p.next; p.next = q; q.next = temp; q = t; p = newHead; &#125; return newHead; &#125;&#125; 方法二12345678910111213141516171819202122232425class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //在一条环上处理 ListNode dummy = new ListNode(-1,head); ListNode newLast = dummy.next; ListNode cur = newLast.next; while (cur != null) &#123; if (cur.val &gt;= newLast.val) &#123; newLast = cur; &#125;else &#123; //从头找到p.next.val &gt; cur.val //因为last.val &gt; cur.val 所以.next不会为空的 ListNode p = dummy; while (p.next.val &lt;= cur.val) &#123; p = p.next; &#125; newLast.next = cur.next; cur.next = p.next; p.next = cur; &#125; cur = newLast.next; &#125; return dummy.next; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"设计模式","slug":"3design pattern","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:48:51.946Z","comments":true,"path":"2022/09/01/3design pattern/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/3design%20pattern/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mybatis随记","slug":"6Mybatis","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-24T12:15:33.475Z","comments":true,"path":"2022/09/01/6Mybatis/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/6Mybatis/","excerpt":"","text":"前类中查找一个方法快捷键: Ctrl + F12 常用注解1、@TableIdMyBatis-Plus在实现增删改查时，会默认将id作为主键列，并在插入数据时，默认基于雪花算法的策略生成id，这个雪花算法在这里就不明讲了。 当使用@TableId(value &#x3D; “id”)语句时，若实体类和表中表示主键的不是id，而是其他字段，例如代码中的uid，MyBatis-Plus会自动识别uid为主键列，否则就会报这样的错误： 当使用@TableId(value &#x3D; “id”,type &#x3D; IdType.AUTO)语句时，代表着使用数据库的自增策略，注意，该类型请确保数据库设置了id自增，否则无效！ 当然呢，@TableId的功能，也可以写在application.yml配置文件中，配置如下： 1234567891011mybatis-plus: global-config: banner: false db-config: # 配置MyBatis-Plus操作表的默认前缀 table-prefix: &quot;t_&quot; # 配置MyBatis-Plus的主键策略 id-type: auto # 配置MyBatis日志 configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 2、@TableFieldMyBatis-Plus在执行SQL语句时，要保证实体类中的属性名和表中的字段名一致，否则就会报错，语句@TableField(value &#x3D; “is_deleted”)代表着让数据库表中is_deleted与实体类中isDeleted字段名一样。 注意： 若实体类中的属性使用的是驼峰命名风格，而表中的字段使用的是下划线命名风格例如实体类属性userName，表中字段user_name，此时MyBatis-Plus会自动将下划线命名风格转化为驼峰命名风格 若实体类中的属性和表中的字段不满足上述条件，例如实体类属性name，表中字段username，此时需要在实体类属性上使用@TableField(“username”)设置属性所对应的字段名 3、@TableLogic在讲这个注解之前，我们先认识一下逻辑删除。 物理删除：真实删除，将对应数据从数据库中删除，之后查询不到此条被删除的数据 逻辑删除：假删除，将对应数据中代表是否被删除字段的状态修改为“被删除状态”，之后在数据库中仍旧能看到此条数据记录 使用场景：可以进行数据恢复 在我的数据库表中，is_delete为1时，代表着逻辑上的删除，is_delete为0时，表示没有删除 注解@TableLogic的使用，就代表着该类中的属性是逻辑删除的属性 注意： 在测试逻辑删除的时候，真正执行的是修改UPDATE t_user SET is_deleted&#x3D;1 WHERE id&#x3D;? AND is_deleted&#x3D;0 测试查询功能，被逻辑删除的数据默认不会被查询SELECT id,username AS name,age,email,is_deleted FROM t_user WHERE is_deleted&#x3D;0 在学习mybatis-plus分页插件的时候，我们需要配置拦截器，看代码： 123456789101112@Configurationpublic class MybatisPlusConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() &#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor (new PaginationInnerInterceptor(DbType.MYSQL)); return interceptor; &#125;&#125; 4、@Version在我们学习乐观锁的时候，肯定见过如下代码： 1234567891011121314151617181920212223@Data@TableName(&quot;t_product&quot;)public class Product &#123; private Long id; private String name; private Integer price; @Version private Integer version;&#125;@Configurationpublic class MybatisPlusConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() &#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); //分页插件 interceptor.addInnerInterceptor (new PaginationInnerInterceptor(DbType.MYSQL)); //乐观锁插件 interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor()); return interceptor; &#125;&#125; 而这个注解@Version就是实现乐观锁的重要注解，当要更新数据库中的数据时，例如价格，version 就会加 1，如果where语句中的version版本不对，则更新失败。 CRUD 接口#Service CRUD 接口说明: 通用 Service CRUD 封装IService (opens new window)接口，进一步封装 CRUD 采用 get 查询单行 remove 删除 list 查询集合 page 分页 前缀命名方式区分 Mapper 层避免混淆， 泛型 T 为任意实体对象 建议如果存在自定义通用 Service 方法的可能，请创建自己的 IBaseService 继承 Mybatis-Plus 提供的基类 对象 Wrapper 为 条件构造器 #Save123456// 插入一条记录（选择字段，策略插入）boolean save(T entity);// 插入（批量）boolean saveBatch(Collection&lt;T&gt; entityList);// 插入（批量）boolean saveBatch(Collection&lt;T&gt; entityList, int batchSize); #参数说明 类型 参数名 描述 T entity 实体对象 Collection entityList 实体对象集合 int batchSize 插入批次数量 #SaveOrUpdate12345678// TableId 注解存在更新记录，否插入一条记录boolean saveOrUpdate(T entity);// 根据updateWrapper尝试更新，否继续执行saveOrUpdate(T)方法boolean saveOrUpdate(T entity, Wrapper&lt;T&gt; updateWrapper);// 批量修改插入boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList);// 批量修改插入boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList, int batchSize); #参数说明 类型 参数名 描述 T entity 实体对象 Wrapper updateWrapper 实体对象封装操作类 UpdateWrapper Collection entityList 实体对象集合 int batchSize 插入批次数量 #Remove12345678// 根据 entity 条件，删除记录boolean remove(Wrapper&lt;T&gt; queryWrapper);// 根据 ID 删除boolean removeById(Serializable id);// 根据 columnMap 条件，删除记录boolean removeByMap(Map&lt;String, Object&gt; columnMap);// 删除（根据ID 批量删除）boolean removeByIds(Collection&lt;? extends Serializable&gt; idList); #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体包装类 QueryWrapper Serializable id 主键 ID Map&lt;String, Object&gt; columnMap 表字段 map 对象 Collection&lt;? extends Serializable&gt; idList 主键 ID 列表 #Update12345678910// 根据 UpdateWrapper 条件，更新记录 需要设置sqlsetboolean update(Wrapper&lt;T&gt; updateWrapper);// 根据 whereWrapper 条件，更新记录boolean update(T updateEntity, Wrapper&lt;T&gt; whereWrapper);// 根据 ID 选择修改boolean updateById(T entity);// 根据ID 批量更新boolean updateBatchById(Collection&lt;T&gt; entityList);// 根据ID 批量更新boolean updateBatchById(Collection&lt;T&gt; entityList, int batchSize); #参数说明 类型 参数名 描述 Wrapper updateWrapper 实体对象封装操作类 UpdateWrapper T entity 实体对象 Collection entityList 实体对象集合 int batchSize 更新批次数量 #Get12345678910// 根据 ID 查询T getById(Serializable id);// 根据 Wrapper，查询一条记录。结果集，如果是多个会抛出异常，随机取一条加上限制条件 wrapper.last(&quot;LIMIT 1&quot;)T getOne(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper，查询一条记录T getOne(Wrapper&lt;T&gt; queryWrapper, boolean throwEx);// 根据 Wrapper，查询一条记录Map&lt;String, Object&gt; getMap(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper，查询一条记录&lt;V&gt; V getObj(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper); #参数说明 类型 参数名 描述 Serializable id 主键 ID Wrapper queryWrapper 实体对象封装操作类 QueryWrapper boolean throwEx 有多个 result 是否抛出异常 T entity 实体对象 Function&lt;? super Object, V&gt; mapper 转换函数 #List123456789101112131415161718192021// 查询所有List&lt;T&gt; list();// 查询列表List&lt;T&gt; list(Wrapper&lt;T&gt; queryWrapper);// 查询（根据ID 批量查询）Collection&lt;T&gt; listByIds(Collection&lt;? extends Serializable&gt; idList);// 查询（根据 columnMap 条件）Collection&lt;T&gt; listByMap(Map&lt;String, Object&gt; columnMap);// 查询所有列表List&lt;Map&lt;String, Object&gt;&gt; listMaps();// 查询列表List&lt;Map&lt;String, Object&gt;&gt; listMaps(Wrapper&lt;T&gt; queryWrapper);// 查询全部记录List&lt;Object&gt; listObjs();// 查询全部记录&lt;V&gt; List&lt;V&gt; listObjs(Function&lt;? super Object, V&gt; mapper);// 根据 Wrapper 条件，查询全部记录List&lt;Object&gt; listObjs(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录&lt;V&gt; List&lt;V&gt; listObjs(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper); #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper Collection&lt;? extends Serializable&gt; idList 主键 ID 列表 Map&lt;String, Object&gt; columnMap 表字段 map 对象 Function&lt;? super Object, V&gt; mapper 转换函数 #Page12345678// 无条件分页查询IPage&lt;T&gt; page(IPage&lt;T&gt; page);// 条件分页查询IPage&lt;T&gt; page(IPage&lt;T&gt; page, Wrapper&lt;T&gt; queryWrapper);// 无条件分页查询IPage&lt;Map&lt;String, Object&gt;&gt; pageMaps(IPage&lt;T&gt; page);// 条件分页查询IPage&lt;Map&lt;String, Object&gt;&gt; pageMaps(IPage&lt;T&gt; page, Wrapper&lt;T&gt; queryWrapper); #参数说明 类型 参数名 描述 IPage page 翻页对象 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper #Count1234// 查询总记录数int count();// 根据 Wrapper 条件，查询总记录数int count(Wrapper&lt;T&gt; queryWrapper); #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper #Chain#query12345678// 链式查询 普通QueryChainWrapper&lt;T&gt; query();// 链式查询 lambda 式。注意：不支持 KotlinLambdaQueryChainWrapper&lt;T&gt; lambdaQuery();// 示例：query().eq(&quot;column&quot;, value).one();lambdaQuery().eq(Entity::getId, value).list(); #update12345678// 链式更改 普通UpdateChainWrapper&lt;T&gt; update();// 链式更改 lambda 式。注意：不支持 KotlinLambdaUpdateChainWrapper&lt;T&gt; lambdaUpdate();// 示例：update().eq(&quot;column&quot;, value).remove();lambdaUpdate().eq(Entity::getId, value).update(entity); #Mapper CRUD 接口说明: 通用 CRUD 封装BaseMapper (opens new window)接口，为 Mybatis-Plus 启动时自动解析实体表关系映射转换为 Mybatis 内部对象注入容器 泛型 T 为任意实体对象 参数 Serializable 为任意类型主键 Mybatis-Plus 不推荐使用复合主键约定每一张表都有自己的唯一 id 主键 对象 Wrapper 为 条件构造器 #Insert12// 插入一条记录int insert(T entity); #参数说明 类型 参数名 描述 T entity 实体对象 #Delete12345678// 根据 entity 条件，删除记录int delete(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; wrapper);// 删除（根据ID 批量删除）int deleteBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 ID 删除int deleteById(Serializable id);// 根据 columnMap 条件，删除记录int deleteByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap); #参数说明 类型 参数名 描述 Wrapper wrapper 实体对象封装操作类（可以为 null） Collection&lt;? extends Serializable&gt; idList 主键 ID 列表(不能为 null 以及 empty) Serializable id 主键 ID Map&lt;String, Object&gt; columnMap 表字段 map 对象 #Update1234// 根据 whereWrapper 条件，更新记录int update(@Param(Constants.ENTITY) T updateEntity, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; whereWrapper);// 根据 ID 修改int updateById(@Param(Constants.ENTITY) T entity); #参数说明 类型 参数名 描述 T entity 实体对象 (set 条件值,可为 null) Wrapper updateWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） #Select12345678910111213141516171819202122// 根据 ID 查询T selectById(Serializable id);// 根据 entity 条件，查询一条记录T selectOne(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据ID 批量查询）List&lt;T&gt; selectBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 entity 条件，查询全部记录List&lt;T&gt; selectList(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据 columnMap 条件）List&lt;T&gt; selectByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap);// 根据 Wrapper 条件，查询全部记录List&lt;Map&lt;String, Object&gt;&gt; selectMaps(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录。注意： 只返回第一个字段的值List&lt;Object&gt; selectObjs(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 entity 条件，查询全部记录（并翻页）IPage&lt;T&gt; selectPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录（并翻页）IPage&lt;Map&lt;String, Object&gt;&gt; selectMapsPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询总记录数Integer selectCount(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper); #参数说明 类型 参数名 描述 Serializable id 主键 ID Wrapper queryWrapper 实体对象封装操作类（可以为 null） Collection&lt;? extends Serializable&gt; idList 主键 ID 列表(不能为 null 以及 empty) Map&lt;String, Object&gt; columnMap 表字段 map 对象 IPage page 分页查询条件（可以为 RowBounds.DEFAULT） #mapper 层 选装件说明: 选装件位于 com.baomidou.mybatisplus.extension.injector.methods 包下 需要配合Sql 注入器使用,案例(opens new window)使用详细见源码注释(opens new window) #AlwaysUpdateSomeColumnById(opens new window)1int alwaysUpdateSomeColumnById(T entity); #insertBatchSomeColumn(opens new window)1int insertBatchSomeColumn(List&lt;T&gt; entityList); #logicDeleteByIdWithFill(opens new window)1int logicDeleteByIdWithFill(T entity); #ActiveRecord 模式说明: 实体类只需继承 Model 类即可进行强大的 CRUD 操作 需要项目中已注入对应实体的BaseMapper #操作步骤： 继承 Model(opens new window) 123class User extends Model&lt;User&gt;&#123; // fields...&#125; 调用CRUD方法(演示部分api，仅供参考) 123456User user = new User();user.insert();user.selectAll();user.updateById();user.deleteById();// ... #SimpleQuery 工具类说明: 对selectList查询后的结果用Stream流进行了一些封装，使其可以返回一些指定结果，简洁了api的调用 需要项目中已注入对应实体的BaseMapper 使用方式见: 测试用例(opens new window) 对于下方参数peeks，其类型为Consumer...，可一直往后叠加操作例如：List&lt;Long&gt; ids = SimpleQuery.list(Wrappers.lambdaQuery(), Entity::getId, System.out::println, user -&gt; userNames.add(user.getName())); #keyMap1234// 查询表内记录，封装返回为Map&lt;属性,实体&gt;Map&lt;A, E&gt; keyMap(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,实体&gt;，考虑了并行流的情况Map&lt;A, E&gt; keyMap(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, boolean isParallel, Consumer&lt;E&gt;... peeks); #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是map中key的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 实体中属性的getter,用于封装后map中作为key的条件 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #map1234// 查询表内记录，封装返回为Map&lt;属性,属性&gt;Map&lt;A, P&gt; map(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; keyFunc, SFunction&lt;E, P&gt; valueFunc, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,属性&gt;，考虑了并行流的情况Map&lt;A, P&gt; map(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; keyFunc, SFunction&lt;E, P&gt; valueFunc, boolean isParallel, Consumer&lt;E&gt;... peeks); 1234 #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是map中key的类型 P attribute 实体属性类型,也是map中value的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; keyFunc 封装后map中作为key的条件 SFunction&lt;E, P&gt; valueFunc 封装后map中作为value的条件 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #group12345678// 查询表内记录，封装返回为Map&lt;属性,List&lt;实体&gt;&gt;Map&lt;K, List&lt;T&gt;&gt; group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, A&gt; sFunction, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,List&lt;实体&gt;&gt;，考虑了并行流的情况Map&lt;K, List&lt;T&gt;&gt; group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, boolean isParallel, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,分组后对集合进行的下游收集器&gt;M group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, Collector&lt;? super T, A, D&gt; downstream, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,分组后对集合进行的下游收集器&gt;，考虑了并行流的情况M group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, Collector&lt;? super T, A, D&gt; downstream, boolean isParallel, Consumer&lt;T&gt;... peeks); 12345678 #参数说明 类型 参数名 描述 T entity 实体对象 K attribute 实体属性类型,也是map中key的类型 D - 下游收集器返回类型,也是map中value的类型 A - 下游操作中间类型 M - 最终结束返回的Map&lt;K, D&gt; LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 分组依据，封装后map中作为key的条件 Collector&lt;T, A, D&gt; downstream 下游收集器 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #list1234// 查询表内记录，封装返回为List&lt;属性&gt;List&lt;A&gt; list(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为List&lt;属性&gt;，考虑了并行流的情况List&lt;A&gt; list(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, boolean isParallel, Consumer&lt;E&gt;... peeks); 1234 #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是list中元素的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 封装后list中的元素 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 条件构造器说明: 以下出现的第一个入参boolean condition表示该条件是否加入最后生成的sql中，例如：query.like(StringUtils.isNotBlank(name), Entity::getName, name) .eq(age!&#x3D;null &amp;&amp; age &gt;&#x3D; 0, Entity::getAge, age) 以下代码块内的多个方法均为从上往下补全个别boolean类型的入参,默认为true 以下出现的泛型Param均为Wrapper的子类实例(均具有AbstractWrapper的所有方法) 以下方法在入参中出现的R为泛型,在普通wrapper中是String,在LambdaWrapper中是函数(例:Entity::getId,Entity为实体类,getId为字段id的getMethod) 以下方法入参中的R column均表示数据库字段,当R具体类型为String时则为数据库字段名(字段名是数据库关键字的自己用转义符包裹!)!而不是实体类数据字段名!!!,另当R具体类型为SFunction时项目runtime不支持eclipse自家的编译器!!! 以下举例均为使用普通wrapper,入参为Map和List的均以json形式表现! 使用中如果入参的Map或者List为空,则不会加入最后生成的sql中!!! 有任何疑问就点开源码看,看不懂函数的点击我学习新知识(opens new window) 警告: 不支持以及不赞成在 RPC 调用中把 Wrapper 进行传输 wrapper 很重 传输 wrapper 可以类比为你的 controller 用 map 接收值(开发一时爽,维护火葬场) 正确的 RPC 调用姿势是写一个 DTO 进行传输,被调用方再根据 DTO 执行相应的操作 我们拒绝接受任何关于 RPC 传输 Wrapper 报错相关的 issue 甚至 pr #AbstractWrapper说明: QueryWrapper(LambdaQueryWrapper) 和 UpdateWrapper(LambdaUpdateWrapper) 的父类用于生成 sql 的 where 条件, entity 属性也用于生成 sql 的 where 条件注意: entity 生成的 where 条件与 使用各个 api 生成的 where 条件没有任何关联行为 #allEq123allEq(Map&lt;R, V&gt; params)allEq(Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, Map&lt;R, V&gt; params, boolean null2IsNull) 123 全部 eq (或个别 isNull ) 个别参数说明: params : key为数据库字段名,value为字段值null2IsNull : 为true则在map的value为null时调用 isNull 方法,为false时则忽略value为null的 例1: allEq(&#123;id:1,name:&quot;老王&quot;,age:null&#125;)—&gt;id = 1 and name = &#39;老王&#39; and age is null 例2: allEq(&#123;id:1,name:&quot;老王&quot;,age:null&#125;, false)—&gt;id = 1 and name = &#39;老王&#39; 123allEq(BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params)allEq(BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull) 123 个别参数说明: filter : 过滤函数,是否允许字段传入比对条件中params 与 null2IsNull : 同上 例1: allEq((k,v) -&gt; k.indexOf(&quot;a&quot;) &gt;= 0, &#123;id:1,name:&quot;老王&quot;,age:null&#125;)—&gt;name = &#39;老王&#39; and age is null 例2: allEq((k,v) -&gt; k.indexOf(&quot;a&quot;) &gt;= 0, &#123;id:1,name:&quot;老王&quot;,age:null&#125;, false)—&gt;name = &#39;老王&#39; #eq12eq(R column, Object val)eq(boolean condition, R column, Object val) 12 等于 &#x3D; 例: eq(&quot;name&quot;, &quot;老王&quot;)—&gt;name = &#39;老王&#39; #ne12ne(R column, Object val)ne(boolean condition, R column, Object val) 12 不等于 &lt;&gt; 例: ne(&quot;name&quot;, &quot;老王&quot;)—&gt;name &lt;&gt; &#39;老王&#39; #gt12gt(R column, Object val)gt(boolean condition, R column, Object val) 12 大于 &gt; 例: gt(&quot;age&quot;, 18)—&gt;age &gt; 18 #ge12ge(R column, Object val)ge(boolean condition, R column, Object val) 12 大于等于 &gt;&#x3D; 例: ge(&quot;age&quot;, 18)—&gt;age &gt;= 18 #lt12lt(R column, Object val)lt(boolean condition, R column, Object val) 12 小于 &lt; 例: lt(&quot;age&quot;, 18)—&gt;age &lt; 18 #le12le(R column, Object val)le(boolean condition, R column, Object val) 12 小于等于 &lt;&#x3D; 例: le(&quot;age&quot;, 18)—&gt;age &lt;= 18 #between12between(R column, Object val1, Object val2)between(boolean condition, R column, Object val1, Object val2) 12 BETWEEN 值1 AND 值2 例: between(&quot;age&quot;, 18, 30)—&gt;age between 18 and 30 #notBetween12notBetween(R column, Object val1, Object val2)notBetween(boolean condition, R column, Object val1, Object val2) 12 NOT BETWEEN 值1 AND 值2 例: notBetween(&quot;age&quot;, 18, 30)—&gt;age not between 18 and 30 #like12like(R column, Object val)like(boolean condition, R column, Object val) 12 LIKE ‘%值%’ 例: like(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;%王%&#39; #notLike12notLike(R column, Object val)notLike(boolean condition, R column, Object val) 12 NOT LIKE ‘%值%’ 例: notLike(&quot;name&quot;, &quot;王&quot;)—&gt;name not like &#39;%王%&#39; #likeLeft12likeLeft(R column, Object val)likeLeft(boolean condition, R column, Object val) 12 LIKE ‘%值’ 例: likeLeft(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;%王&#39; #likeRight12likeRight(R column, Object val)likeRight(boolean condition, R column, Object val) 12 LIKE ‘值%’ 例: likeRight(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;王%&#39; #isNull12isNull(R column)isNull(boolean condition, R column) 12 字段 IS NULL 例: isNull(&quot;name&quot;)—&gt;name is null #isNotNull12isNotNull(R column)isNotNull(boolean condition, R column) 12 字段 IS NOT NULL 例: isNotNull(&quot;name&quot;)—&gt;name is not null #in12in(R column, Collection&lt;?&gt; value)in(boolean condition, R column, Collection&lt;?&gt; value) 12 字段 IN (value.get(0), value.get(1), …) 例: in(&quot;age&quot;,&#123;1,2,3&#125;)—&gt;age in (1,2,3) 12in(R column, Object... values)in(boolean condition, R column, Object... values) 12 字段 IN (v0, v1, …) 例: in(&quot;age&quot;, 1, 2, 3)—&gt;age in (1,2,3) #notIn12notIn(R column, Collection&lt;?&gt; value)notIn(boolean condition, R column, Collection&lt;?&gt; value) 12 字段 NOT IN (value.get(0), value.get(1), …) 例: notIn(&quot;age&quot;,&#123;1,2,3&#125;)—&gt;age not in (1,2,3) 12notIn(R column, Object... values)notIn(boolean condition, R column, Object... values) 12 字段 NOT IN (v0, v1, …) 例: notIn(&quot;age&quot;, 1, 2, 3)—&gt;age not in (1,2,3) #inSql12inSql(R column, String inValue)inSql(boolean condition, R column, String inValue) 12 字段 IN ( sql语句 ) 例: inSql(&quot;age&quot;, &quot;1,2,3,4,5,6&quot;)—&gt;age in (1,2,3,4,5,6) 例: inSql(&quot;id&quot;, &quot;select id from table where id &lt; 3&quot;)—&gt;id in (select id from table where id &lt; 3) #notInSql12notInSql(R column, String inValue)notInSql(boolean condition, R column, String inValue) 12 字段 NOT IN ( sql语句 ) 例: notInSql(&quot;age&quot;, &quot;1,2,3,4,5,6&quot;)—&gt;age not in (1,2,3,4,5,6) 例: notInSql(&quot;id&quot;, &quot;select id from table where id &lt; 3&quot;)—&gt;id not in (select id from table where id &lt; 3) #groupBy12groupBy(R... columns)groupBy(boolean condition, R... columns) 12 分组：GROUP BY 字段, … 例: groupBy(&quot;id&quot;, &quot;name&quot;)—&gt;group by id,name #orderByAsc12orderByAsc(R... columns)orderByAsc(boolean condition, R... columns) 12 排序：ORDER BY 字段, … ASC 例: orderByAsc(&quot;id&quot;, &quot;name&quot;)—&gt;order by id ASC,name ASC #orderByDesc12orderByDesc(R... columns)orderByDesc(boolean condition, R... columns) 12 排序：ORDER BY 字段, … DESC 例: orderByDesc(&quot;id&quot;, &quot;name&quot;)—&gt;order by id DESC,name DESC #orderBy1orderBy(boolean condition, boolean isAsc, R... columns) 1 排序：ORDER BY 字段, … 例: orderBy(true, true, &quot;id&quot;, &quot;name&quot;)—&gt;order by id ASC,name ASC #having12having(String sqlHaving, Object... params)having(boolean condition, String sqlHaving, Object... params) 12 HAVING ( sql语句 ) 例: having(&quot;sum(age) &gt; 10&quot;)—&gt;having sum(age) &gt; 10 例: having(&quot;sum(age) &gt; &#123;0&#125;&quot;, 11)—&gt;having sum(age) &gt; 11 #func12func(Consumer&lt;Children&gt; consumer)func(boolean condition, Consumer&lt;Children&gt; consumer) 12 func 方法(主要方便在出现if…else下调用不同方法能不断链) 例: func(i -&gt; if(true) &#123;i.eq(&quot;id&quot;, 1)&#125; else &#123;i.ne(&quot;id&quot;, 1)&#125;) #or12or()or(boolean condition) 12 拼接 OR 注意事项: 主动调用or表示紧接着下一个方法不是用and连接!(不调用or则默认为使用and连接) 例: eq(&quot;id&quot;,1).or().eq(&quot;name&quot;,&quot;老王&quot;)—&gt;id = 1 or name = &#39;老王&#39; 12or(Consumer&lt;Param&gt; consumer)or(boolean condition, Consumer&lt;Param&gt; consumer) 12 OR 嵌套 例: or(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;or (name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #and12and(Consumer&lt;Param&gt; consumer)and(boolean condition, Consumer&lt;Param&gt; consumer) 12 AND 嵌套 例: and(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;and (name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #nested12nested(Consumer&lt;Param&gt; consumer)nested(boolean condition, Consumer&lt;Param&gt; consumer) 12 正常嵌套 不带 AND 或者 OR 例: nested(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;(name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #apply12apply(String applySql, Object... params)apply(boolean condition, String applySql, Object... params) 12 拼接 sql 注意事项: 该方法可用于数据库函数 动态入参的params对应前面applySql内部的&#123;index&#125;部分.这样是不会有sql注入风险的,反之会有! 例: apply(&quot;id = 1&quot;)—&gt;id = 1 例: apply(&quot;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;)—&gt;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;) 例: apply(&quot;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#123;0&#125;&quot;, &quot;2008-08-08&quot;)—&gt;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;) #last12last(String lastSql)last(boolean condition, String lastSql) 12 无视优化规则直接拼接到 sql 的最后 注意事项: 只能调用一次,多次调用以最后一次为准 有sql注入的风险,请谨慎使用 例: last(&quot;limit 1&quot;) #exists12exists(String existsSql)exists(boolean condition, String existsSql) 12 拼接 EXISTS ( sql语句 ) 例: exists(&quot;select id from table where age = 1&quot;)—&gt;exists (select id from table where age = 1) #notExists12notExists(String notExistsSql)notExists(boolean condition, String notExistsSql) 12 拼接 NOT EXISTS ( sql语句 ) 例: notExists(&quot;select id from table where age = 1&quot;)—&gt;not exists (select id from table where age = 1) #QueryWrapper说明: 继承自 AbstractWrapper ,自身的内部属性 entity 也用于生成 where 条件及 LambdaQueryWrapper, 可以通过 new QueryWrapper().lambda() 方法获取 #select123select(String... sqlSelect)select(Predicate&lt;TableFieldInfo&gt; predicate)select(Class&lt;T&gt; entityClass, Predicate&lt;TableFieldInfo&gt; predicate) 123 设置查询字段 说明: 以上方法分为两类.第二类方法为:过滤查询字段(主键除外),入参不包含 class 的调用前需要wrapper内的entity属性有值! 这两类方法重复调用以最后一次为准 例: select(&quot;id&quot;, &quot;name&quot;, &quot;age&quot;) 例: select(i -&gt; i.getProperty().startsWith(&quot;test&quot;)) #UpdateWrapper说明: 继承自 AbstractWrapper ,自身的内部属性 entity 也用于生成 where 条件及 LambdaUpdateWrapper, 可以通过 new UpdateWrapper().lambda() 方法获取! #set12set(String column, Object val)set(boolean condition, String column, Object val) 12 SQL SET 字段 例: set(&quot;name&quot;, &quot;老李头&quot;) 例: set(&quot;name&quot;, &quot;&quot;)—&gt;数据库字段值变为空字符串 例: set(&quot;name&quot;, null)—&gt;数据库字段值变为null #setSql1setSql(String sql) 1 设置 SET 部分 SQL 例: setSql(&quot;name = &#39;老李头&#39;&quot;) #lambda 获取 LambdaWrapper在QueryWrapper中是获取LambdaQueryWrapper在UpdateWrapper中是获取LambdaUpdateWrapper #使用 Wrapper 自定义SQL注意事项: 需要mybatis-plus版本 &gt;&#x3D; 3.0.7 param 参数名要么叫ew,要么加上注解@Param(Constants.WRAPPER) 使用$&#123;ew.customSqlSegment&#125; 不支持 Wrapper 内的entity生成where语句 #kotlin持久化对象定义最佳实践Mybatis 缓存机制 将mybatis的接口调用通过与数据库建立SQLsession(内含Executor里有localcache)转化为SQL语句 一级缓存,针对一个事务里的查询,每个事务简历一个SQLsession 如果查询ID相同(xml中的表示SQL语句的ID相同)并且预编译的SQL相同,查询的参数相同,则会从localcache中找到这个缓存,因为是根据这三个生成的hash作为key,结果作为value存储, 二级缓存,caching Executor作为装饰器门面 Mapper的xml中会有namespace级的缓存,难以支持服务器集群","categories":[],"tags":[]},{"title":"redis","slug":"5redis","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T11:39:02.551Z","comments":true,"path":"2022/09/01/5redis/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/5redis/","excerpt":"","text":"零、日常基础使用 一些配置 与说明 (守护进程)daemonize yes 启动: 123456#可能需要进入redis目录redis-server /usr/local/redis/redis.confredis-cliauth (可以不写用户)密码keys* 关闭 1redis-cli shutdown 使用RedisDesktopManager连接远程服务器redis 修改 .conf ​ bind 0.0.0.0 允许所有主机 127.~ 只允许本机 ​ protectmode yes 只允许本机 no 允许所有主机 ​ requierpass 密码 服务器防火墙开放 6379端口就ok 一、多线程VS单线程Redis工作线程是单线程的,整个Redis来说，是多线程的；主要是指Redis的网络IO和键值对读写是由一个线程来完成的，Redis在处理客户端的请求时包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。这也是Redis对外提供键值存储服务的主要流程。 但Redis的其他功能，比如持久化、异步删除、集群数据同步等等，其实是由额外的线程执行的。Redis工作线程是单线程的，但是，整个Redis来说，是多线程的； 单线程快的原因1 避免上下文切换：因为是单线程模型，因此就避免了不必要的上下文切换和多线程竞争，这就省去了多线程切换带来的时间和性能上的消耗，而且单线程不会导致死锁问题的发生 2 即使使用单线程模型也并发的处理多客户端的请求，主要使用的是多路复用和非阻塞 IO； 3 对于 Redis 系统来说，主要的性能瓶颈是内存或者网络带宽而并非 CPU。 单线程的弊病正常情况下使用 del 指令可以很快的删除数据，而当被删除的 key 是一个非常大的对象时，例如时包含了成千上万个元素的 hash 集合时，那么 del 指令就会造成 Redis 主线程卡顿。 这就是redis3.x单线程时代最经典的故障，大key删除的头疼问题， 由于redis是单线程的，del bigKey …..等待很久这个线程才会释放，类似加了一个synchronized锁，你可以想象高并发下，程序堵成什么样子？ 在Redis 4.0就引入了多个线程来实现数据的异步惰性删除等功能，但是其处理读写请求的仍然只有一个线程，所以仍然算是狭义上的单线程 Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理8W到10W的QPS， 这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。 默认单线程在Redis6.0中，多线程机制默认是关闭的，如果需要使用多线程功能，需要在redis.conf中完成两个设置 1.设置io-thread-do-reads配置项为yes，表示启动多线程。 2。设置线程个数。关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。 二、 五种数据类型1. String1.1 常用指令12345678910111213141516171819202122set key valueget key同时设置/获取多个键值 MGET key [key ....]MSET key value [key value ....]数值增减INCR key增加指定的整数 INCRBY key increment递减数值 DECR key减少指定的整数DECRBY key decrement获取字符串长度STRLEN key分布式锁setnx key valueset key value [EX seconds] [PX milliseconds] [NX|XX] 1.2 应用场景比如抖音无限点赞某个视频或者商品，点一下加一次 2. HashMap&lt;String,Map&lt;Object,Object&gt;&gt; 2.1常用指令1234567891011121314一次设置一个字段值HSET key field value一次获取一个字段值HGET key field一次设置多个字段值HMSET key field value [field value ...]一次获取多个字段值HMGET key field [field ....]获取所有字段值hgetall key获取某个key内的全部数量hlen删除一个keyhdel 2.2应用场景​ Map&lt;String,Map&lt;Object,Object&gt;&gt; ​ hset key field value ​ JD早期购物车 中小场可用 ​ 新增商品 → hset shopcar:uid1024 334488 1 ​ 新增商品 → hset shopcar:uid1024 334477 1 ​ 增加商品数量 → hincrby shopcar:uid1024 334477 1 ​ 商品总数 → hlen shopcar:uid1024 ​ 全部选择 → hgetall shopcar:uid1024 3. List一个双端链表的结构，容量是2的32次方减1个元素，大概40多亿，主要功能有push&#x2F;pop等，一般用在栈、队列、消息队列等场景。 3.1 常用指令12345678向列表左边添加元素LPUSH key value [value ...]向列表右边添加元素RPUSH key value [value ....]查看列表LRANGE key start stop获取列表中元素的个数LLEN key 3.2 应用场景微信订阅号消息,关注了作者,如果作者发布了文章 就会向关注的list添加 ​ 1 订阅了的公众号和CSDN发布了文章分别是 11 和 22 ​ 2 ggq关注了他们两个，只要他们发布了新文章，就会安装进ggq的List lpush likearticle:ggqid 11 22 ​ 3 查看ggq自己的号订阅的全部文章，类似分页，下面0~10就是一次显示10条 ​ lrange likearticle:ggqid 0 9 ​ 商品评论 ​ 商品ID key 和 value评论信息 ​ 按时间顺序 4. Set4.1 常用指令1234567891011121314151617181920添加元素SADD key member [member ...]删除元素SREM key member [member ...]遍历集合中的所有元素SMEMBERS key判断元素是否在集合中SISMEMBER key member获取集合中的元素总数SCARD key从集合中随机弹出一个元素，元素不删除SRANDMEMBER key [数字]从集合中随机弹出一个元素，出一个删一个SPOP key [数字]集合运算SDIFF key [key ...] # 属于A但不属于B的元素构成的集合SINTER key [key ...] # 属于A同时也属于B的共同拥有的元素构成的集合SUNION key [key ...] # 属于A或者属于B的元素合并后的集合 4.2 应用场景抽奖 1 用户ID，立即参与按钮 sadd key 用户ID 2 显示已经有多少人参与了，上图23208人参加 SCARD key 3 抽奖(从set中任意选取N个中奖人) SRANDMEMBER key 2 随机抽奖2个人，元素不删除 SPOP key 3 随机抽奖3个人，元素会删除 ​ 集合运算 共同好友,共同关注 ​ 可能认识的人 5. sortedSet(Zset)向有序集合中加入一个元素和该元素的分数 5.1123456789101112131415161718192021ZADD key score member [score member ...]按照元素分数从小到大的顺序返回索引从start到stop之间的所有元素ZRANGE key start stop [WITHSCORES]获取元素的分数ZSCORE key member删除元素ZREM key member [member ...]获取指定分数范围的元素ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]增加某个元素的分数ZINCRBY key increment member获取集合中元素的数量ZCARD key获得指定分数范围内的元素个数ZCOUNT key min max按照排名范围删除元素ZREMRANGEBYRANK key start stop获取元素的排名ZRANK key member # 从小到大ZREVRANK key member # 从大到小 5.2 应用场景​ 商品排行 ​ 抖音热搜 incrby ​ 展示多少条 zrange min max 5.3 案例实战三、数据结构字典散列表采用拉链法解决哈希冲突, 扩容: rehash 渐进式 会有个过程,在过程中会在两个字典进行查询, 扩容完成后释放一个 跳跃表有序集合, 不需要进行旋转等维护平衡性, 方便实现, 支持无锁(todo) 四、使用场景计数器可以对 String 进行自增自减运算，从而实现计数器功能。 Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。 缓存将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 查找表例如 DNS 记录就很适合使用 Redis 进行存储。 查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。 消息队列List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息 不过最好使用 Kafka、RabbitMQ 等消息中间件。 会话缓存可以使用 Redis 来统一存储多台应用服务器的会话信息。 当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。 可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。 其它Set 可以实现交集、并集等操作，从而实现共同好友等功能。 ZSet 可以实现有序性操作，从而实现排行榜等功能。 五、键的过期时间内存中的数据可以通过TTL指令获取其状态 TTL返回的值有三种情况：正数，-1，-2 正数：代表该数据在内存中还能存活的时间 -1：永久有效的数据 -2 ：已经过期的数据 或被删除的数据 或 未定义的数据 删除策略就是针对已过期数据的处理策略 Redis 可以为每个键设置过期时间，当键过期时，会自动删除该键。 对于散列表这种容器，只能为整个键设置过期时间（整个散列表），而不能为键里面的单个元素设置过期时间。 六、数据淘汰策略可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Redis 具体有 6 种淘汰策略： 策略 描述 volatile-lru 从已设置过期时间的数据集中挑选最近最少使用的数据淘汰 volatile-ttl 从已设置过期时间的数据集中挑选将要过期的数据淘汰 volatile-random 从已设置过期时间的数据集中任意选择数据淘汰 allkeys-lru 从所有数据集中挑选最近最少使用的数据淘汰 allkeys-random 从所有数据集中任意选择数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，出于对性能和内存消耗的考虑，Redis 的淘汰算法实际实现上并非针对所有 key，而是抽样一小部分并且从中选出被淘汰的 key。 使用 Redis 缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用 allkeys-lru 淘汰策略，将最近最少使用的数据淘汰。 Redis 4.0 引入了 volatile-lfu 和 allkeys-lfu 淘汰策略，LFU 策略通过统计访问频率，将访问频率最少的键值对淘汰。 七、持久化Redis 是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 八、RDB 持久化将某个时间点的所有数据都存放到硬盘上。 可以将快照复制到其它服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 九、AOF 持久化将写命令添加到 AOF 文件（Append Only File）的末尾。 使用 AOF 持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统来决定何时同步 always 选项会严重减低服务器的性能； everysec 选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且 Redis 每秒执行一次同步对服务器性能几乎没有任何影响； no 选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF 文件会越来越大。Redis 提供了一种将 AOF 重写的特性，能够去除 AOF 文件中的冗余写命令。 十、事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其它客户端的命令请求。 事务中的多个命令被一次性发送给服务器，而不是一条一条发送，这种方式被称为流水线，它可以减少客户端与服务器之间的网络通信次数从而提升性能。 Redis 最简单的事务实现方式是使用 MULTI 和 EXEC 命令将事务操作包围起来。 十一、事件Redis 服务器是一个事件驱动程序。 文件事件服务器通过套接字与客户端或者其它服务器进行通信，文件事件就是对套接字操作的抽象。 Redis 基于 Reactor 模式开发了自己的网络事件处理器，使用 I&#x2F;O 多路复用程序来同时监听多个套接字，并将到达的事件传送给文件事件分派器，分派器会根据套接字产生的事件类型调用相应的事件处理器。 时间事件服务器有一些操作需要在给定的时间点执行，时间事件是对这类定时操作的抽象。 时间事件又分为： 定时事件：是让一段程序在指定的时间之内执行一次； 周期性事件：是让一段程序每隔指定时间就执行一次。 Redis 将所有时间事件都放在一个无序链表中，通过遍历整个链表查找出已到达的时间事件，并调用相应的事件处理器。 事件的调度与执行服务器需要不断监听文件事件的套接字才能得到待处理的文件事件，但是不能一直监听，否则时间事件无法在规定的时间内执行，因此监听时间应该根据距离现在最近的时间事件来决定。 事件调度与执行由 aeProcessEvents 函数负责，伪代码如下： 12345678910111213141516def aeProcessEvents(): # 获取到达时间离当前时间最接近的时间事件 time_event = aeSearchNearestTimer() # 计算最接近的时间事件距离到达还有多少毫秒 remaind_ms = time_event.when - unix_ts_now() # 如果事件已到达，那么 remaind_ms 的值可能为负数，将它设为 0 if remaind_ms &lt; 0: remaind_ms = 0 # 根据 remaind_ms 的值，创建 timeval timeval = create_timeval_with_ms(remaind_ms) # 阻塞并等待文件事件产生，最大阻塞时间由传入的 timeval 决定 aeApiPoll(timeval) # 处理所有已产生的文件事件 procesFileEvents() # 处理所有已到达的时间事件 processTimeEvents() 将 aeProcessEvents 函数置于一个循环里面，加上初始化和清理函数，就构成了 Redis 服务器的主函数，伪代码如下： 12345678def main(): # 初始化服务器 init_server() # 一直处理事件，直到服务器关闭为止 while server_is_not_shutdown(): aeProcessEvents() # 服务器关闭，执行清理操作 clean_server() 从事件处理的角度来看，服务器运行流程如下： 十二、复制通过使用 slaveof host port 命令来让一个服务器成为另一个服务器的从服务器。 一个从服务器只能有一个主服务器，并且不支持主主复制。 连接过程 主服务器创建快照文件，发送给从服务器，并在发送期间使用缓冲区记录执行的写命令。快照文件发送完毕之后，开始向从服务器发送存储在缓冲区中的写命令； 从服务器丢弃所有旧数据，载入主服务器发来的快照文件，之后从服务器开始接受主服务器发来的写命令； 主服务器每执行一次写命令，就向从服务器发送相同的写命令。 主从链随着负载不断上升，主服务器可能无法很快地更新所有从服务器，或者重新连接和重新同步从服务器将导致系统超载。为了解决这个问题，可以创建一个中间层来分担主服务器的复制工作。中间层的服务器是最上层服务器的从服务器，又是最下层服务器的主服务器。 十三、SentinelSentinel（哨兵）可以监听集群中的服务器，并在主服务器进入下线状态时，自动从从服务器中选举出新的主服务器。 十四、分片分片是将数据划分为多个部分的方法，可以将数据存储到多台机器里面，这种方法在解决某些问题时可以获得线性级别的性能提升。 假设有 4 个 Redis 实例 R0，R1，R2，R3，还有很多表示用户的键 user:1，user:2，… ，有不同的方式来选择一个指定的键存储在哪个实例中。 最简单的方式是范围分片，例如用户 id 从 0~1000 的存储到实例 R0 中，用户 id 从 1001~2000 的存储到实例 R1 中，等等。但是这样需要维护一张映射范围表，维护操作代价很高。 还有一种方式是哈希分片，使用 CRC32 哈希函数将键转换为一个数字，再对实例数量求模就能知道应该存储的实例。 根据执行分片的位置，可以分为三种分片方式： 客户端分片：客户端使用一致性哈希等算法决定键应当分布到哪个节点。 代理分片：将客户端请求发送到代理上，由代理转发请求到正确的节点上。 服务器分片：Redis Cluster。 十五、一个简单的论坛系统分析该论坛系统功能如下： 可以发布文章； 可以对文章进行点赞； 在首页可以按文章的发布时间或者文章的点赞数进行排序显示。 文章信息文章包括标题、作者、赞数等信息，在关系型数据库中很容易构建一张表来存储这些信息，在 Redis 中可以使用 HASH 来存储每种信息以及其对应的值的映射。 Redis 没有关系型数据库中的表这一概念来将同种类型的数据存放在一起，而是使用命名空间的方式来实现这一功能。键名的前面部分存储命名空间，后面部分的内容存储 ID，通常使用 : 来进行分隔。例如下面的 HASH 的键名为 article:92617，其中 article 为命名空间，ID 为 92617。 点赞功能当有用户为一篇文章点赞时，除了要对该文章的 votes 字段进行加 1 操作，还必须记录该用户已经对该文章进行了点赞，防止用户点赞次数超过 1。可以建立文章的已投票用户集合来进行记录。 为了节约内存，规定一篇文章发布满一周之后，就不能再对它进行投票，而文章的已投票集合也会被删除，可以为文章的已投票集合设置一个一周的过期时间就能实现这个规定。 对文章进行排序为了按发布时间和点赞数进行排序，可以建立一个文章发布时间的有序集合和一个文章点赞数的有序集合。（下图中的 score 就是这里所说的点赞数；下面所示的有序集合分值并不直接是时间和点赞数，而是根据时间和点赞数间接计算出来的） 参考资料 Carlson J L. Redis in Action[J]. Media.johnwiley.com.au, 2013. 黄健宏. Redis 设计与实现 [M]. 机械工业出版社, 2014. REDIS IN ACTION Skip Lists: Done Right 论述 Redis 和 Memcached 的差异 Redis 3.0 中文版- 分片 Redis 应用场景 [Using Redis as an LRU cache]( 第六章 redis新类型bitmap&#x2F;hyperloglgo&#x2F;GEO存的进+取得快+多统计 亿级系统中常见的四种统计统计的类型 聚合统计 统计多个集合元素的聚合结果，就是前面讲解过的交差并等集合统计 交并差集和聚合函数的应用 排序统计 抖音视频最新评论留言的场景，请你设计一个展现列表。 考察你的数据结构和设计思路 设计案例和回答思路 每个商品评价对应一个List集合，这个List包含了对这个商品的所有评论，而且会按照评论时间保存这些评论， 每来一个新评论就用LPUSH命令把它插入List的队头。但是，如果在演示第二页前，又产生了一个新评论， 第2页的评论不一样了。原因： List是通过元素在List中的位置来排序的，当有一个新元素插入时，原先的元素在List中的位置都后移了一位， 原来在第1位的元素现在排在了第2位，当LRANGE读取时，就会读到旧元素。 在⾯对需要展示最新列表、排行榜等场景时， 如果数据更新频繁或者需要分页显示，建议使⽤ZSet 二值统计 集合元素的取值就只有0和1两种。 在钉钉上班签到打卡的场景中，我们只用记录有签到(1)或没签到(0) bitmap 基数统计 指统计⼀个集合中不重复的元素个数 见hyperloglog 1. BitMap1.1 常用指令1.2应用场景​ 按字节(8位)扩容 ​ setbit key offset value 偏移位从0开始算 value 只能是01 ​ 由0和1状态表现的二进制位的bit数组 ​ 用于状态统计 ​ 2. HyperLogLog2.1 常用指令2.2应用场景3. GEO3.1 常用指令3.2应用场景第七章 布隆过滤器​ 由一个初值都为零的bit数组和多个哈希函数构成， ​ 用来快速判断某个数据是否存在 ​ 判断结果没有的一定没有,有的大概率有 只添加不删除 ​ 多重hash linux安装布隆过滤器的两种方式 采用docker安装RedisBloom，推荐 Redis 在 4.0 之后有了插件功能（Module），可以使用外部的扩展功能， 可以使用 RedisBloom 作为 Redis 布隆过滤器插件。 docker run -p 6379:6379 –name&#x3D;redis6379bloom -d redislabs&#x2F;rebloom docker exec -it redis6379bloom &#x2F;bin&#x2F;bash redis-cli 常用指令123456bf.reserve key error_rate的值 initial_size 的值 默认的error_rate是 0.01，默认的initial_size是 100。bf.add key 值bf.exists key 值bf.madd 一次添加多个元素bf.mexists 一次查询多个元素是否存在 Redis高级(具体操作)1. 数据过期删除1.2.2 定时删除创建一个定时器，当key设置有过期时间，且过期时间到达时，由定时器任务立即执行对键的删除操作 优点：节约内存，到时就删除，快速释放掉不必要的内存占用 缺点：CPU压力很大，无论CPU此时负载量多高，均占用CPU，会影响redis服务器响应时间和指令吞吐量 总结：用处理器性能换取存储空间（拿时间换空间） 1.2.3 惰性删除数据到达过期时间，不做处理。等下次访问该数据时，我们需要判断 如果未过期，返回数据 发现已过期，删除，返回不存在 优点：节约CPU性能，发现必须删除的时候才删除 缺点：内存压力很大，出现长期占用内存的数据 总结：用存储空间换取处理器性能（拿时间换空间） 1.2.4 定期删除定时删除和惰性删除这两种方案都是走的极端，那有没有折中方案？ 我们来讲redis的定期删除方案： Redis启动服务器初始化时，读取配置server.hz的值，默认为10 即每100ms一次 每秒钟执行server.hz次serverCron()——–&gt;databasesCron()———&gt;activeExpireCycle() **activeExpireCycle()*对每个expires[]逐一进行检测，每次执行耗时：250ms&#x2F;server.hz 对某个expires[*]检测时，随机挑选W个key检测 1234567如果key超时，删除key如果一轮中删除的key的数量&gt;W*25%，循环该过程如果一轮中删除的key的数量≤W*25%，检查下一个expires[*]，0-15循环W取值=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP属性值 参数current_db用于记录activeExpireCycle() 进入哪个expires[*] 执行 如果activeExpireCycle()执行时间到期，下次从current_db继续向下执行 总的来说：定期删除就是周期性轮询redis库中的时效性数据，采用随机抽取的策略，利用过期数据占比的方式控制删除频度 特点1：CPU性能占用设置有峰值，检测频度可自定义设置 特点2：内存压力不是很大，长期占用内存的冷数据会被持续清理 总结：周期性抽查存储空间（随机抽查，重点抽查） 1.3 数据淘汰策略（逐出算法）1.3.1 淘汰策略概述什么叫数据淘汰策略？什么样的应用场景需要用到数据淘汰策略？ 1.3.2 策略配置影响数据淘汰的相关配置如下： 1：最大可使用内存，即占用物理内存的比例，默认值为0，表示不限制。生产环境中根据需求设定，通常设置在50%以上 1maxmemory ?mb 2：每次选取待删除数据的个数，采用随机获取数据的方式作为待检测删除数据 1maxmemory-samples count 3：对数据进行删除的选择策略 1maxmemory-policy policy 那数据删除的策略policy到底有几种呢？一共是3类8种 第一类：检测易失数据（可能会过期的数据集server.db[i].expires ） 1234volatile-lru：挑选最近最少使用的数据淘汰volatile-lfu：挑选最近使用次数最少的数据淘汰volatile-ttl：挑选将要过期的数据淘汰volatile-random：任意选择数据淘汰 第二类：检测全库数据（所有数据集server.db[i].dict ） 123allkeys-lru：挑选最近最少使用的数据淘汰allkeLyRs-lfu：：挑选最近使用次数最少的数据淘汰allkeys-random：任意选择数据淘汰，相当于随机 第三类：放弃数据驱逐 1no-enviction（驱逐）：禁止驱逐数据(redis4.0中默认策略)，会引发OOM(Out Of Memory) 注意：这些策略是配置到哪个属性上？怎么配置？如下所示 1maxmemory-policy volatile-lru 数据淘汰策略配置依据 使用INFO命令输出监控信息，查询缓存 hit 和 miss 的次数，根据业务需求调优Redis配置 2.主从复制 高并发 应用要提供某一业务要能支持很多客户端同时访问的能力，我们称为并发，高并发意思就很明确了 高性能 性能带给我们最直观的感受就是：速度快，时间短 高可用 可用性：一年中应用服务正常运行的时间占全年时间的百分比 2.1.3 主从复制的作用 读写分离：master写、slave读，提高服务器的读写负载能力 负载均衡：基于主从结构，配合读写分离，由slave分担master负载，并根据需求的变化，改变slave的数 量，通过多个从节点分担数据读取负载，大大提高Redis服务器并发量与数据吞吐量 故障恢复：当master出现问题时，由slave提供服务，实现快速的故障恢复 数据冗余：实现数据热备份，是持久化之外的一种数据冗余方式 高可用基石：基于主从复制，构建哨兵模式与集群，实现Redis的高可用方案 2.2.1 主从复制的工作流程（三个阶段）2.2.1.1 阶段一：建立连接建立slave到master的连接，使master能够识别slave，并保存slave端口号 流程如下： 步骤1：设置master的地址和端口，保存master信息 步骤2：建立socket连接 步骤3：发送ping命令（定时器任务） 步骤4：身份验证 步骤5：发送slave端口信息 至此，主从连接成功！ 当前状态： slave：保存master的地址与端口 master：保存slave的端口 总体：之间创建了连接的socket master和slave互联 接下来就要通过某种方式将master和slave连接到一起 方式一：客户端发送命令 1slaveof masterip masterport 方式二：启动服务器参数 1redis-server --slaveof masterip masterport 方式三：服务器配置（主流方式） 1slaveof masterip masterport slave系统信息 12master_link_down_since_secondsmasterhost &amp; masterport master系统信息 1uslave_listening_port(多个) 主从断开连接 断开slave与master的连接，slave断开连接后，不会删除已有数据，只是不再接受master发送的数据 1slaveof no one 授权访问 master客户端发送命令设置密码 1requirepass password master配置文件设置密码 12config set requirepass passwordconfig get requirepass slave客户端发送命令设置密码 1auth password slave配置文件设置密码 1masterauth password slave启动服务器设置密码 1redis-server –a password 2.2.1.2 阶段二：数据同步 在slave初次连接master后，复制master中的所有数据到slave 将slave的数据库状态更新成master当前的数据库状态 同步过程如下： 步骤1：请求同步数据 步骤2：创建RDB同步数据 步骤3：恢复RDB同步数据 步骤4：请求部分同步数据 步骤5：恢复部分同步数据 至此，数据同步工作完成！ 当前状态： slave：具有master端全部数据，包含RDB过程接收的数据 master：保存slave当前数据同步的位置 总体：之间完成了数据克隆 数据同步阶段master说明 1：如果master数据量巨大，数据同步阶段应避开流量高峰期，避免造成master阻塞，影响业务正常执行 2：复制缓冲区大小设定不合理，会导致数据溢出。如进行全量复制周期太长，进行部分复制时发现数据已经存在丢失的情况，必须进行第二次全量复制，致使slave陷入死循环状态。 1repl-backlog-size ?mb master单机内存占用主机内存的比例不应过大，建议使用50%-70%的内存，留下30%-50%的内存用于执 行bgsave命令和创建复制缓冲区 数据同步阶段slave说明 为避免slave进行全量复制、部分复制时服务器响应阻塞或数据不同步，建议关闭此期间的对外服务 1slave-serve-stale-data yes|no 数据同步阶段，master发送给slave信息可以理解master是slave的一个客户端，主动向slave发送命令 多个slave同时对master请求数据同步，master发送的RDB文件增多，会对带宽造成巨大冲击，如果master带宽不足，因此数据同步需要根据业务需求，适量错峰 slave过多时，建议调整拓扑结构，由一主多从结构变为树状结构，中间的节点既是master，也是 slave。注意使用树状结构时，由于层级深度，导致深度越高的slave与最顶层master间数据同步延迟 较大，数据一致性变差，应谨慎选择 2.2.1.3 阶段三：命令传播 当master数据库状态被修改后，导致主从服务器数据库状态不一致，此时需要让主从数据同步到一致的状态，同步的动作称为命令传播 master将接收到的数据变更命令发送给slave，slave接收命令后执行命令 命令传播阶段的部分复制 命令传播阶段出现了断网现象： 网络闪断闪连：忽略 短时间网络中断：部分复制 长时间网络中断：全量复制 这里我们主要来看部分复制，部分复制的三个核心要素 服务器的运行 id（run id） 主服务器的复制积压缓冲区 主从服务器的复制偏移量 服务器运行ID（runid） 12345678910概念：服务器运行ID是每一台服务器每次运行的身份识别码，一台服务器多次运行可以生成多个运行id组成：运行id由40位字符组成，是一个随机的十六进制字符例如：fdc9ff13b9bbaab28db42b3d50f852bb5e3fcdce作用：运行id被用于在服务器间进行传输，识别身份如果想两次操作均对同一台服务器进行，必须每次操作携带对应的运行id，用于对方识别实现方式：运行id在每台服务器启动时自动生成的，master在首次连接slave时，会将自己的运行ID发送给slave，slave保存此ID，通过info Server命令，可以查看节点的runid 复制缓冲区 123456概念：复制缓冲区，又名复制积压缓冲区，是一个先进先出（FIFO）的队列，用于存储服务器执行过的命令，每次传播命令，master都会将传播的命令记录下来，并存储在复制缓冲区 复制缓冲区默认数据存储空间大小是1M 当入队元素的数量大于队列长度时，最先入队的元素会被弹出，而新元素会被放入队列作用：用于保存master收到的所有指令（仅影响数据变更的指令，例如set，select）数据来源：当master接收到主客户端的指令时，除了将指令执行，会将该指令存储到缓冲区中 复制缓冲区内部工作原理： 组成 偏移量 概念：一个数字，描述复制缓冲区中的指令字节位置 分类： master复制偏移量：记录发送给所有slave的指令字节对应的位置（多个） slave复制偏移量：记录slave接收master发送过来的指令字节对应的位置（一个） 作用：同步信息，比对master与slave的差异，当slave断线后，恢复数据使用 数据来源： master端：发送一次记录一次 slave端：接收一次记录一次 字节值 工作原理 通过offset区分不同的slave当前数据传播的差异 master记录已发送的信息对应的offset slave记录已接收的信息对应的offset 2.2.2 流程更新(全量复制&#x2F;部分复制)我们再次的总结一下主从复制的三个阶段的工作流程： 2.2.3 心跳机制什么是心跳机制？ 进入命令传播阶段候，master与slave间需要进行信息交换，使用心跳机制进行维护，实现双方连接保持在线 master心跳： 内部指令：PING 周期：由repl-ping-slave-period决定，默认10秒 作用：判断slave是否在线 查询：INFO replication 获取slave最后一次连接时间间隔，lag项维持在0或1视为正常 slave心跳任务 内部指令：REPLCONF ACK {offset} 周期：1秒 作用1：汇报slave自己的复制偏移量，获取最新的数据变更指令 作用2：判断master是否在线 心跳阶段注意事项： 当slave多数掉线，或延迟过高时，master为保障数据稳定性，将拒绝所有信息同步 12min-slaves-to-write 2min-slaves-max-lag 8 slave数量少于2个，或者所有slave的延迟都大于等于8秒时，强制关闭master写功能，停止数据同步 slave数量由slave发送REPLCONF ACK命令做确认 slave延迟由slave发送REPLCONF ACK命令做确认 至此：我们可以总结出完整的主从复制流程： 2.3 主从复制常见问题2.3.1 频繁的全量复制 伴随着系统的运行，master的数据量会越来越大，一旦master重启，runid将发生变化，会导致全部slave的全量复制操作 内部优化调整方案： 1：master内部创建master_replid变量，使用runid相同的策略生成，长度41位，并发送给所有slave 2：在master关闭时执行命令shutdown save，进行RDB持久化,将runid与offset保存到RDB文件中 123repl-id repl-offset通过redis-check-rdb命令可以查看该信息 3：master重启后加载RDB文件，恢复数据，重启后，将RDB文件中保存的repl-id与repl-offset加载到内存中 123master_repl_id=repl master_repl_offset =repl-offset通过info命令可以查看该信息 作用：本机保存上次runid，重启后恢复该值，使所有slave认为还是之前的master 第二种出现频繁全量复制的问题现象：网络环境不佳，出现网络中断，slave不提供服务 问题原因：复制缓冲区过小，断网后slave的offset越界，触发全量复制 最终结果：slave反复进行全量复制 解决方案：修改复制缓冲区大小 1repl-backlog-size ?mb 建议设置如下： 1.测算从master到slave的重连平均时长second 2.获取master平均每秒产生写命令数据总量write_size_per_second 3.最优复制缓冲区空间 &#x3D; 2 * second * write_size_per_second 2.3.2 频繁的网络中断 问题现象：master的CPU占用过高 或 slave频繁断开连接 问题原因 12345slave每1秒发送REPLCONFACK命令到master当slave接到了慢查询时（keys * ，hgetall等），会大量占用CPU性能master每1秒调用复制定时函数replicationCron()，比对slave发现长时间没有进行响应 最终结果：master各种资源（输出缓冲区、带宽、连接等）被严重占用 解决方案：通过设置合理的超时时间，确认是否释放slave 1repl-timeout seconds 该参数定义了超时时间的阈值（默认60秒），超过该值，释放slave 问题现象：slave与master连接断开 问题原因 12345master发送ping指令频度较低master设定超时时间较短ping指令在网络中存在丢包 解决方案：提高ping指令发送的频度 1repl-ping-slave-period seconds 超时时间repl-time的时间至少是ping指令频度的5到10倍，否则slave很容易判定超时 2.3.3 数据不一致问题现象：多个slave获取相同数据不同步 问题原因：网络信息不同步，数据发送有延迟 解决方案 123优化主从间的网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器时要注意此现象监控主从节点延迟（通过offset）判断，如果slave延迟过大，暂时屏蔽程序对该slave的数据访问 1slave-serve-stale-data yes|no 开启后仅响应info、slaveof等少数命令（慎用，除非对数据一致性要求很高） 3.哨兵模式哨兵 哨兵(sentinel) 是一个分布式系统，用于对主从结构中的每台服务器进行监控，当出现故障时通过投票机制选择新的master并将所有slave连接到新的master。 3.1.2 哨兵作用哨兵的作用： 监控：监控master和slave 不断的检查master和slave是否正常运行 master存活检测、master与slave运行情况检测 通知（提醒）：当被监控的服务器出现问题时，向其他（哨兵间，客户端）发送通知 自动故障转移：断开master与slave连接，选取一个slave作为master，将其他slave连接新的master，并告知客户端新的服务器地址 注意：哨兵也是一台redis服务器，只是不提供数据相关服务，通常哨兵的数量配置为单数 3.2 启用哨兵配置哨兵 配置一拖二的主从结构（利用之前的方式启动即可） 配置三个哨兵（配置相同，端口不同），参看sentinel.conf 1：设置哨兵监听的主服务器信息， sentinel_number表示参与投票的哨兵数量 1sentinel monitor master_name master_host master_port sentinel_number 2：设置判定服务器宕机时长，该设置控制是否进行主从切换 1sentinel down-after-milliseconds master_name million_seconds 3：设置故障切换的最大超时时 1sentinel failover-timeout master_name million_seconds 4：设置主从切换后，同时进行数据同步的slave数量，数值越大，要求网络资源越高，数值越小，同步时间越长 1sentinel parallel-syncs master_name sync_slave_number 启动哨兵 1redis-sentinel filename 3.3 哨兵工作原理哨兵在进行主从切换过程中经历三个阶段 监控 通知 故障转移 3.3.1 监控用于同步各个节点的状态信息 获取各个sentinel的状态（是否在线） 获取master的状态 1234master属性 prunid prole：master各个slave的详细信息 获取所有slave的状态（根据master中的slave信息） 12345slave属性 prunid prole：slave pmaster_host、master_port poffset 其内部的工作原理具体如下： 3.3.2 通知sentinel在通知阶段要不断的去获取master&#x2F;slave的信息，然后在各个sentinel之间进行共享，具体的流程如下： 3.3.3 故障转移当master宕机后sentinel是如何知晓并判断出master是真的宕机了呢？我们来看具体的操作流程 当sentinel认定master下线之后，此时需要决定更换master，那这件事由哪个sentinel来做呢？这时候sentinel之间要进行选举，如下图所示： 在选举的时候每一个人手里都有一票，而每一个人的又都想当这个处理事故的人，那怎么办？大家就开始抢，于是每个人都会发出一个指令，在内网里边告诉大家我要当选举人，比如说现在的sentinel1和sentinel4发出这个选举指令了，那么sentinel2既能接到sentinel1的也能接到sentinel4的，接到了他们的申请以后呢，sentinel2他就会把他的一票投给其中一方，投给谁呢？谁先过来我投给谁，假设sentinel1先过来，所以这个票就给到了sentinel1。那么给过去以后呢，现在sentinel1就拿到了一票，按照这样的一种形式，最终会有一个选举结果。对应的选举最终得票多的，那自然就成为了处理事故的人。需要注意在这个过程中有可能会存在失败的现象，就是一轮选举完没有选取，那就会接着进行第二轮第三轮直到完成选举。 接下来就是由选举胜出的sentinel去从slave中选一个新的master出来的工作，这个流程是什么样的呢？ 首先它有一个在服务器列表中挑选备选master的原则 不在线的OUT 响应慢的OUT 与原master断开时间久的OUT 优先原则 ​ 优先级​ offset​ runid 选出新的master之后，发送指令（ sentinel ）给其他的slave： 向新的master发送slaveof no one 向其他slave发送slaveof 新masterIP端口 总结：故障转移阶段 发现问题，主观下线与客观下线 竞选负责人 优选新master 新master上任，其他slave切换master，原master作为slave故障恢复后连接 4.集群cluster现状问题：业务发展过程中遇到的峰值瓶颈 redis提供的服务OPS可以达到10万&#x2F;秒，当前业务OPS已经达到10万&#x2F;秒 内存单机容量达到256G，当前业务需求内存容量1T 使用集群的方式可以快速解决上述问题 4.1 集群简介集群就是使用网络将若干台计算机联通起来，并提供统一的管理方式，使其对外呈现单机的服务效果 集群作用： 分散单台服务器的访问压力，实现负载均衡 分散单台服务器的存储压力，实现可扩展性 降低单台服务器宕机带来的业务灾难 4.2 Cluster集群结构设计数据存储设计： 通过算法设计，计算出key应该保存的位置 将所有的存储空间计划切割成16384份，每台主机保存一部分 注意：每份代表的是一个存储空间，不是一个key的保存空间 将key按照计算出的结果放到对应的存储空间 那redis的集群是如何增强可扩展性的呢？譬如我们要增加一个集群节点 当我们查找数据时，集群是如何操作的呢？ 各个数据库相互通信，保存各个库中槽的编号数据 一次命中，直接返回 一次未命中，告知具体位置 4.3 Cluster集群结构搭建首先要明确的几个要点： 配置服务器（3主3从） 建立通信（Meet） 分槽（Slot） 搭建主从（master-slave） Cluster配置 是否启用cluster，加入cluster节点 1cluster-enabled yes|no cluster配置文件名，该文件属于自动生成，仅用于快速查找文件并查询文件内容 1cluster-config-file filename 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点 1cluster-node-timeout milliseconds master连接的slave最小数量 1cluster-migration-barrier min_slave_number Cluster节点操作命令 查看集群节点信息 1cluster nodes 更改slave指向新的master 1cluster replicate master-id 发现一个新节点，新增master 1cluster meet ip:port 忽略一个没有solt的节点 1cluster forget server_id 手动故障转移 1cluster failover 集群操作命令： 创建集群 1redis-cli –-cluster create masterhost1:masterport1 masterhost2:masterport2 masterhost3:masterport3 [masterhostn:masterportn …] slavehost1:slaveport1 slavehost2:slaveport2 slavehost3:slaveport3 -–cluster-replicas n 注意：master与slave的数量要匹配，一个master对应n个slave，由最后的参数n决定 master与slave的匹配顺序为第一个master与前n个slave分为一组，形成主从结构 添加master到当前集群中，连接时可以指定任意现有节点地址与端口 1redis-cli --cluster add-node new-master-host:new-master-port now-host:now-port 添加slave 1redis-cli --cluster add-node new-slave-host:new-slave-port master-host:master-port --cluster-slave --cluster-master-id masterid 删除节点，如果删除的节点是master，必须保障其中没有槽slot 1redis-cli --cluster del-node del-slave-host:del-slave-port del-slave-id 重新分槽，分槽是从具有槽的master中划分一部分给其他master，过程中不创建新的槽 1redis-cli --cluster reshard new-master-host:new-master:port --cluster-from src- master-id1, src-master-id2, src-master-idn --cluster-to target-master-id -- cluster-slots slots 注意：将需要参与分槽的所有masterid不分先后顺序添加到参数中，使用，分隔 指定目标得到的槽的数量，所有的槽将平均从每个来源的master处获取 重新分配槽，从具有槽的master中分配指定数量的槽到另一个master中，常用于清空指定master中的槽 1redis-cli --cluster reshard src-master-host:src-master-port --cluster-from src- master-id --cluster-to target-master-id --cluster-slots slots --cluster-yes 5.企业级解决方案5.1 缓存预热场景：“宕机” 服务器启动后迅速宕机 问题排查： 1.请求数量较高，大量的请求过来之后都需要去从缓存中获取数据，但是缓存中又没有，此时从数据库中查找数据然后将数据再存入缓存，造成了短期内对redis的高强度操作从而导致问题 2.主从之间数据吞吐量较大，数据同步操作频度较高 解决方案： 前置准备工作： 1.日常例行统计数据访问记录，统计访问频度较高的热点数据 2.利用LRU数据删除策略，构建数据留存队列例如：storm与kafka配合 准备工作： 1.将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据 2.利用分布式多服务器同时进行数据读取，提速数据加载过程 3.热点数据主从同时预热 实施： 4.使用脚本程序固定触发数据预热过程 5.如果条件允许，使用了CDN（内容分发网络），效果会更好 总的来说：缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 5.2 缓存雪崩场景：数据库服务器崩溃，一连串的场景会随之儿来 1.系统平稳运行过程中，忽然数据库连接量激增 2.应用服务器无法及时处理请求 3.大量408，500错误页面出现 4.客户反复刷新页面获取数据 5.数据库崩溃 6.应用服务器崩溃 7.重启应用服务器无效 8.Redis服务器崩溃 9.Redis集群崩溃 10.重启数据库后再次被瞬间流量放倒 问题排查： 1.在一个较短的时间内，缓存中较多的key集中过期 2.此周期内请求访问过期的数据，redis未命中，redis向数据库获取数据 3.数据库同时接收到大量的请求无法及时处理 4.Redis大量请求被积压，开始出现超时现象 5.数据库流量激增，数据库崩溃 6.重启后仍然面对缓存中无数据可用 7.Redis服务器资源被严重占用，Redis服务器崩溃 8.Redis集群呈现崩塌，集群瓦解 9.应用服务器无法及时得到数据响应请求，来自客户端的请求数量越来越多，应用服务器崩溃 10.应用服务器，redis，数据库全部重启，效果不理想 总而言之就两点：短时间范围内，大量key集中过期 解决方案 思路： 1.更多的页面静态化处理 2.构建多级缓存架构 ​ Nginx缓存+redis缓存+ehcache缓存 3.检测Mysql严重耗时业务进行优化 ​ 对数据库的瓶颈排查：例如超时查询、耗时较高事务等 4.灾难预警机制 ​ 监控redis服务器性能指标 ​ CPU占用、CPU使用率 ​ 内存容量 ​ 查询平均响应时间 ​ 线程数 5.限流、降级 短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问 落地实践： 1.LRU与LFU切换 2.数据有效期策略调整 ​ 根据业务数据有效期进行分类错峰，A类90分钟，B类80分钟，C类70分钟 ​ 过期时间使用固定时间+随机值的形式，稀释集中到期的key的数量 3.超热数据使用永久key 4.定期维护（自动+人工） ​ 对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时 5.加锁：慎用！ 总的来说：缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的 出现（约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。 5.3 缓存击穿场景：还是数据库服务器崩溃，但是跟之前的场景有点不太一样 1.系统平稳运行过程中 2.数据库连接量瞬间激增 3.Redis服务器无大量key过期 4.Redis内存平稳，无波动 5.Redis服务器CPU正常 6.数据库崩溃 问题排查： 1.Redis中某个key过期，该key访问量巨大 2.多个数据请求从服务器直接压到Redis后，均未命中 3.Redis在短时间内发起了大量对数据库中同一数据的访问 总而言之就两点：单个key高热数据，key过期 解决方案： 1.预先设定 ​ 以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，加大此类信息key的过期时长 注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势 2.现场调整 ​ 监控访问量，对自然流量激增的数据延长过期时间或设置为永久性key 3.后台刷新数据 ​ 启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失 4.二级缓存 ​ 设置不同的失效时间，保障不会被同时淘汰就行 5.加锁 ​ 分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重！ 总的来说：缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中redis后，发起了大量对同一数据的数据库访问，导致对数 据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个key的过 期监控难度较高，配合雪崩处理策略即可。 5.4 缓存穿透场景：数据库服务器又崩溃了，跟之前的一样吗？ 1.系统平稳运行过程中 2.应用服务器流量随时间增量较大 3.Redis服务器命中率随时间逐步降低 4.Redis内存平稳，内存无压力 5.Redis服务器CPU占用激增 6.数据库服务器压力激增 7.数据库崩溃 问题排查： 1.Redis中大面积出现未命中 2.出现非正常URL访问 问题分析： 获取的数据在数据库中也不存在，数据库查询未得到对应数据 Redis获取到null数据未进行持久化，直接返回 下次此类数据到达重复上述过程 出现黑客攻击服务器 解决方案： 1.缓存null ​ 对查询结果为null的数据进行缓存（长期使用，定期清理），设定短时限，例如30-60秒，最高5分钟 2.白名单策略 ​ 提前预热各种分类数据id对应的bitmaps，id作为bitmaps的offset，相当于设置了数据白名单。当加载正常数据时放行，加载异常数据时直接拦截（效率偏低） ​ 使用布隆过滤器（有关布隆过滤器的命中问题对当前状况可以忽略） 2.实施监控 ​ 实时监控redis命中率（业务正常范围时，通常会有一个波动值）与null数据的占比 ​ 非活动时段波动：通常检测3-5倍，超过5倍纳入重点排查对象 ​ 活动时段波动：通常检测10-50倍，超过50倍纳入重点排查对象 ​ 根据倍数不同，启动不同的排查流程。然后使用黑名单进行防控（运营） 4.key加密 ​ 问题出现后，临时启动防灾业务key，对key进行业务层传输加密服务，设定校验程序，过来的key校验 ​ 例如每天随机分配60个加密串，挑选2到3个，混淆到页面数据id中，发现访问key不满足规则，驳回数据访问 总的来说：缓存击穿是指访问了不存在的数据，跳过了合法数据的redis数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据的出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。应对策略应该在临时预案防范方面多做文章。 无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除。 5.5 性能指标监控redis中的监控指标如下： 性能指标：Performance 响应请求的平均时间: 1&gt;latency 平均每秒处理请求总数 1&gt;instantaneous_ops_per_sec 缓存查询命中率（通过查询总次数与查询得到非nil数据总次数计算而来） 12&gt;hit_rate(calculated) 内存指标：Memory 当前内存使用量 1&gt;used_memory 内存碎片率（关系到是否进行碎片整理） 1&gt;mem_fragmentation_ratio 为避免内存溢出删除的key的总数量 1&gt;evicted_keys 基于阻塞操作（BLPOP等）影响的客户端数量 1&gt;blocked_clients 基本活动指标：Basic_activity 当前客户端连接总数 1&gt;connected_clients 当前连接slave总数 1&gt;connected_slaves 最后一次主从信息交换距现在的秒 1&gt;master_last_io_seconds_ago key的总数 1&gt;keyspace 持久性指标：Persistence 当前服务器最后一次RDB持久化的时间 1&gt;rdb_last_save_time 当前服务器最后一次RDB持久化后数据变化总量 1&gt;rdb_changes_since_last_save 错误指标：Error 被拒绝连接的客户端总数（基于达到最大连接值的因素） 1&gt;rejected_connections key未命中的总次数 1&gt;keyspace_misses 主从断开的秒数 1&gt;master_link_down_since_seconds 要对redis的相关指标进行监控，我们可以采用一些用具： CloudInsight Redis Prometheus Redis-stat Redis-faina RedisLive zabbix 也有一些命令工具： benchmark 测试当前服务器的并发性能 1&gt;redis-benchmark [-h ] [-p ] [-c ] [-n &lt;requests]&gt; [-k ] 范例1：50个连接，10000次请求对应的性能 1&gt;redis-benchmark 范例2：100个连接，5000次请求对应的性能 1&gt;redis-benchmark -c 100 -n 5000 redis-cli ​ monitor：启动服务器调试信息 1&gt;monitor slowlog：慢日志 获取慢查询日志 1&gt;slowlog [operator] ​ get ：获取慢查询日志信息 ​ len ：获取慢查询日志条目数 ​ reset ：重置慢查询日志 相关配置 12&gt;slowlog-log-slower-than 1000 #设置慢查询的时间下线，单位：微妙&gt;slowlog-max-len 100 #设置慢查询命令对应的日志显示长度，单位：命令数 Redis使用操作系统的多进程COW（Copy On Write）机制来实现快照持久化。 Redis在持久化时会调用glibc的函数fork产生一个子进程，快照持久化交给子进程处理，父进程继续提供服务。子进程生成时和父进程共用代码段和数据段。也就是说这时间父子进程共享内存数据，因此在分离的一瞬间，内存消耗几乎没有。接下来子内存进行数据持久化，他仅仅是读取，不会修改内存。而父进程对外提供服务，修改数据，但是操作系统的COW机制会进行数据段页面的分离，数据段由操作系统的页面组合而成，父进程修改数据时，COW机制就将数据所在页复制一份出来，父进程在这个复制出来的数据也修改，此时原数据页也就是子线程访问的数据页还是原样，也就是子进程所看到的数据在子进程产生的一瞬间就已经凝固了，可以安心复制，这也是为什么这种持久化方法称为快照原因。 随着父进程的修改，会有越来越多的页面被赋值，但是最多也就是全复制，达到原内存空间的二倍，但是这在大数据量情况下很难发生，因为总会有冷数据存在，而且可能占据多数，所以复制的一般只会是其中的一部分。另外提一下：一个页面的大小是4K。 1.2 AOF日志AOF日志是连续性的增量备份，记录的是修改内存数据的指令记录文本。这样就可以通过对一个空的Redis实例顺序执行记录的命令，也就是重放，来复原实例。Redis在收到修改指令后，会先进行校验，如果没问题，会首先把指令追加记录磁盘上的AOF日志中，然后再执行指令，这样即使突发宕机，重放时也能重放到这个指令。 AOF日志随着运行时间的增长会变的越来越庞大，Redis重启时需要加载AOF日志进行指令重放所需的时间也会更加漫长，所以需要定期对AOF重写，进行瘦身。 1.2.1 AOF重写AOF重写原理就是开辟一个子进程，然后将内存数据遍历并转换成指令，再记录到一个新的AOF文件中，完毕后再将期间发生的增量AOF日志追加到新的AOF日志中，替换旧的AOF文件，就完成了AOF重写的工作，完成了瘦身。 1.2.2 fsyncAOF日志是以文件方式存在的，程序对AOF日志进行操作时，实际上是先将内容写到内核为文件描述符分配的一块内存缓存上，然后内核异步将数据写入磁盘的。 但是如果机器突然宕机，内存缓存中的数据还没来的及写入磁盘，就会出现日志的丢失。Linux的gilbc提了了fsync(int fd)函数来强制把指定文件的内存缓存数据写入到磁盘中，实时使用fsync就能保证AOF日志不丢失。但是fsync涉及到磁盘写入，相较于内存操作会慢很多，如果每一个指令都fsync一次，Redis纯内存操作所带来的优势就不存在了。 因此目前主流的做法是Redis每隔1s执行一个fsync，1s是可配置的，可以根据需要配置。这样就在保持高效能的同时尽可能的减少日志丢失。Redis也提供了另外两种策略：一种是永不fsync，由操作系统决定什么时间将内存缓存同步到磁盘，这样无法掌控，很不安全。另一种是一次指令fsync一次，然后不会丢日志，单缺点上面也说过了，生产并不推荐。 Redis4.0新增了异步模型，可以打开fsync的异步处理开关，此时主线程不进行fsync，而是生成任务放到专门的fsync队列中去，由专门的fsync异步线程处理。 1.3 持久化选在从节点无论是快照还是AOF，都比较消耗资源。快照需要遍历整个内存，大块磁盘读写加重系统负载。AOF的fsync是一个耗时的IO操作，也会影响Redis性能，加重系统IO负担。因此Redis的持久化一般并不安排在主节点，而是在从节点进行，从节点没有客户端请求的压力，资源比较充足。但是如果出现网络分区，从节点连不上主节点，而主节点又宕机了，就会出现数据丢失产生数据一致性的问题。因此生产环境需要做好网络连通性检测，保证出现问题时能快速修复，除此之外可以再挂一个从节点，这样只要有一个从节点数据同步正常，数据就不会丢失。 1.4 Redis4.0的混合持久化Redis重启时，很少使用RDB来恢复数据，因为会丢失最后一次快照之后的数据。但是使用AOF日志重放，效率上又会慢很多。因此Redis4.0提供了混合持久化的策略，就是RDB和AOF同时使用。RDB正常持久化，而AOF不在记录全量指令，而是记录每次RDB快照之后的增量AOF，这样Redis重启时就可以先加载RDB的内容，然后再重放AOF日志，效率大大提升。 第八章 缓存预热+缓存雪崩+缓存击穿+缓存穿透缓存预热统计热点数据(访问频率高的)提前存入缓存中(数据多 多服务并行写) 具体 nginx + lua 将访问量上报到消息队列(?) ​ 要统计出来当前最新的实时的热数据是哪些，我们就得将商品详情页访问的请求对应的流量，日志，实时上报 到kafka中 缓存雪崩发生: ​ redis主机挂了，Redis 全盘崩溃, ​ 比如缓存中有大量数据同时过期 解决: ​ 主从+哨兵 ​ 集群 ​ ehcache本地缓存 + Hystrix或者阿里sentinel限流&amp;降级 ​ 开启Redis持久化机制aof&#x2F;rdb，尽快恢复缓存集群 缓存穿透发生: ​ 请求去查询一条记录，先redis后mysql发现都查询不到该条记录， ​ 但是请求每次都会打到数据库上面去，导致后台数据库压力暴增， ​ 这种现象我们称为缓存穿透，这个redis变成了一个摆设。。。。。。 ​ 简单说就是本来无一物，既不在Redis缓存中，也不在数据库中 危害: ​ 第一次来查询后，一般我们有回写redis机制 ​ 第二次来查的时候redis就有了，偶尔出现穿透现象一般情况无关紧要 解决: 方案1：空对象缓存或者缺省值 一般ok but 黑客会对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库去查询。 可能会导致你的数据库由于压力过大而宕掉 方案2：Google布隆过滤器Guava解决缓存穿透 只能单机使用 Guava 中布隆过滤器的实现算是比较权威的， 所以实际项目中我们不需要手动实现一个布隆过滤器 方案3：Redis布隆过滤器解决缓存穿透 缓存击穿发生: ​ 大量的请求同时查询一个 key 时， ​ 此时这个key正好失效了，就会导致大量的请求都打到数据库上面去 ​ 简单说就是热点key突然失效了，暴打mysql 危害: ​ 会造成某一时刻数据库请求量过大，压力剧增。 解决 方案2：对于访问频繁的热点key，干脆就不设置过期时间 方案3：互斥独占锁防止击穿 多个线程同时去查询数据库的这条数据，那么我们可以在第一个查询数据的请求上使用一个 互斥锁来锁住它。 其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。 案例第九章 Redis分布式锁第十一 章 经典五种数据类型底层实现dictEntry启动流程到内部的表: 源码结构体及解析 将dictEntry理解 &lt;String(sds),redisObject&gt; 不一定全对 key 是字符串，但是 Redis 没有直接使用 C 的字符数组，而是存储在redis自定义的 SDS(简单动态字符串,simple dynimic string)中。 value 既不是直接作为字符串存储，也不是直接存储在 SDS 中，而是存储在redisObject 中。 set hello word为例，因为Redis是KV键值对的数据库，每个键值对都会有一个dictEntry(源码位置：dict.h)， 里面指向了key和value的指针，next 指向下一个 dictEntry。 redisObject 后面的类型会加深对redisObject的理解 SringString的三种编码格式 int当字符串键值的内容可以用一个64位有符号整形来表示时，Redis会将键值转化为long型来进行存储，此时即对应 OBJ_ENCODING_INT 编码类型。内部的内存结构表示如下: Redis 启动时会预先建立 10000 个分别存储 09999 的 redisObject 变量作为共享对象，这就意味着如果 set字符串的键值在 010000 之间的话，则可以 直接指向共享对象 而不需要再建立新对象，此时键值不占空间！ set k1 123 set k2 123 保存long 型(长整型)的64位(8个字节)有符号整数 最多19位只能整数浮点数就是字符串值了 embstr对于长度小于 44的字符串，Redis 对键值采用OBJ_ENCODING_EMBSTR 方式，EMBSTR 顾名思义即：embedded string，表示嵌入式的String。从内存结构上来讲 即字符串 sds结构体与其对应的 redisObject 对象分配在同一块连续的内存空间，字符串sds嵌入在redisObject对象之中一样。 代表 embstr 格式的 SDS(Simple Dynamic String 简单动态字符串),保存长度小于44字节的字符串 Redis中字符串的实现,SDS有多种结构（sds.h）： sdshdr5、(2^5&#x3D;32byte) sdshdr8、(2 ^ 8&#x3D;256byte) sdshdr16、(2 ^ 16&#x3D;65536byte&#x3D;64KB) sdshdr32、 (2 ^ 32byte&#x3D;4GB) sdshdr64，2的64次方byte＝17179869184G用于存储不同的长度的字符串。 len 表示 SDS 的长度，使我们在获取字符串长度的时候可以在 O(1)情况下拿到，而不是像 C 那样需要遍历一遍字符串。 alloc 可以用来计算 free 就是字符串已经分配的未使用的空间，有了这个值就可以引入预分配空间的算法了，而不用去考虑内存分配的问题。 buf 表示字符串数组，真存数据的。 raw保存长度大于44字节的字符串 当字符串的键值为长度大于44的超长字符串时，Redis 则会将键值的内部编码方式改为OBJ_ENCODING_RAW格式，这与OBJ_ENCODING_EMBSTR编码方式的不同之处在于，此时动态字符串sds的内存与其依赖的redisObject的内存不再连续了 set hello 观察以String为例的redisObject实际上五种常用的数据类型的任何一种，都是通过 redisObject 来存储的。 debug指令可能出现异常 (error) ERR DEBUG command not allowed. If the enable-debug-command option is set to “local”, you can run it from a local connection, otherwise you need to set this option in the configuration file, and then restart the server 需要设置参数 直接加一行 raw &gt;&#x3D; 44位 流程图 Hash概述hash-max-ziplist-entries：使用压缩列表保存时哈希集合中的最大元素个数。 hash-max-ziplist-value：使用压缩列表保存哈希集合中单个元素的最大长度。 结论: 1.哈希对象保存的键值对数量小于 512 个； 2.所有的键值对的健和值的字符串长度都小于等于 64byte（一个英文字母一个字节） 时用ziplist，反之用hashtable ziplist升级到hashtable可以，反过来降级不可以 一旦从压缩列表转为了哈希表，Hash类型就会一直用哈希表进行保存而不会再转回压缩列表了。 在节省内存空间方面哈希表就没有压缩列表高效了。 后面会讲 回来再看 hash的两种编码格式ziplist(压缩列表)Ziplist 压缩列表是一种紧凑编码格式，总体思想是多花时间来换取节约空间，即以部分读写性能为代价，来换取极高的内存空间利用率， 因此只会用于 字段个数少，且字段值也较小 的场景。压缩列表内存利用率极高的原因与其连续内存的特性是分不开的 想想我们的学过的一种GC垃圾回收机制：标记–压缩算法 当一个 hash对象 只包含少量键值对且每个键值对的键和值要么就是小整数要么就是长度比较短的字符串，那么它用 ziplist 作为底层实现 不懂 todo ziplist是一个经过特殊编码的双向链表，它不存储指向上一个链表节点和指向下一个链表节点的指针，而是存储上一个节点长度和当前节点长度，通过牺牲部分读写性能，来换取高效的内存空间利用率，节约内存，是一种时间换空间的思想。只用在字段个数少，字段值小的场景里面 ZipList结构本质上是字节数组 zlend是一个单字节255(1111 1111)，用做ZipList的结尾标识符。见下：压缩列表结构：由zlbytes、zltail、zllen、entry、zlend这五部分组成 ziplistEntry结构 压缩列表zlentry节点结构：每个zlentry由前一个节点的长度、encoding和entry-data三部分组成 前节点：(前节点占用的内存字节数)表示前1个zlentry的长度，prev_len有两种取值情况：1字节或5字节。取值1字节时，表示上一个entry的长度小于254字节。虽然1字节的值能表示的数值范围是0到255，但是压缩列表中zlend的取值默认是255，因此，就默认用255表示整个压缩列表的结束，其他表示长度的地方就不能再用255这个值了。所以，当上一个entry长度小于254字节时，prev_len取值为1字节，否则，就取值为5字节。 enncoding：记录节点的content保存数据的类型和长度。 content：保存实际数据内容 123456789101112131415161718192021222324252627typedef struct zlentry &#123; // 压缩列表节点 // prevrawlen是前一个节点的长度 //prevrawlensize是指prevrawlen的大小，有1字节和5字节两种 unsigned int prevrawlensize, prevrawlen; // len为当前节点长度 lensize为编码len所需的字节大小 unsigned int lensize, len; // 当前节点的header大小 unsigned int headersize; // 节点的编码方式 unsigned char encoding; // 指向节点的指针 unsigned char *p; &#125; zlentry; 压缩列表的遍历通过指向表尾节点的位置指针p1, 减去节点的previous_entry_length(前一个结点的长度)，得到前一个节点起始地址的指针。如此循环，从表尾遍历到表头节点。从表尾向表头遍历操作就是使用这一原理实现的，只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点的previous_entry_length属性程序就可以一直向前一个节点回溯，最终到达压缩列表的表头节点。 存取情况 hashtable 在 Redis 中，hashtable 被称为字典（dictionary），它是一个数组+链表的结构 OBJ_ENCODING_HT源码分析OBJ_ENCODING_HT 这种编码方式内部才是真正的哈希表结构，或称为字典结构，其可以实现O(1)复杂度的读写操作，因此效率很高。 在 Redis内部，从 OBJ_ENCODING_HT类型到底层真正的散列表数据结构是一层层嵌套下去的，组织关系见面图： 源代码：dict.h Listlist的一种编码格式list用quicklist来存储，quicklist存储了一个双向链表，每个节点都是一个ziplist 在低版本的Redis中，list采用的底层数据结构是ziplist+linkedList； 高版本的Redis中底层数据结构是quicklist(它替换了ziplist+linkedList)，而quicklist也用到了ziplist quicklist 实际上是 zipList 和 linkedList 的混合体，它将 linkedList按段切分，每一段使用 zipList 来紧凑存储，多个 zipList 之间使用双向指针串接起来。 案例: (1) ziplist压缩配置：list-compress-depth 0 表示一个quicklist两端不被压缩的节点个数。这里的节点是指quicklist双向链表的节点，而不是指ziplist里面的数据项个数 参数list-compress-depth的取值含义如下：两端各有x个端点不压缩 0: 是个特殊值，表示都不压缩。这是Redis的默认值。 1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。 2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。 3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。 依此类推… (2) ziplist中entry配置：list-max-ziplist-size -2 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值， 每个值含义如下： -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb &#x3D;&gt; 1024 bytes） -4: 每个quicklist节点上的ziplist大小不能超过32 Kb。 -3: 每个quicklist节点上的ziplist大小不能超过16 Kb。 -2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值） -1: 每个quicklist节点上的ziplist大小不能超过4 Kb。 源码分析 SetSet的两种编码格式intset hashtable 案例 Redis用intset或hashtable存储set。如果元素都是整数类型，就用intset存储。 如果不是整数类型，就用hashtable（数组+链表的存来储结构）。key就是元素的值，value为null。 源码分析 ZsetZSet的两种编码格式ziplist skiplist 案例: 当有序集合中包含的元素数量超过服务器属性 server.zset_max_ziplist_entries 的值（默认值为 128 ）， 或者有序集合中新添加元素的 member 的长度大于服务器属性 server.zset_max_ziplist_value 的值（默认值为 64 ）时， redis会使用跳跃表作为有序集合的底层实现。 否则会使用ziplist作为有序集合的底层实现 源码分析 skipList跳表是可以实现二分查找的有序链表 skiplist是一种以空间换取时间的结构。 由于链表，无法进行二分查找，因此借鉴数据库索引的思想，提取出链表中关键节点（索引），先在关键节点上查找，再进入下层链表查找。 提取多层关键节点，就形成了跳跃表 总结来讲 跳表 &#x3D; 链表 + 多级索引 解决方法：升维，也叫空间换时间。 跳表查询的时间复杂度分析 首先每一级索引我们提升了2倍的跨度，那就是减少了2倍的步数，所以是n&#x2F;2、n&#x2F;4、n&#x2F;8以此类推； 第 k 级索引结点的个数就是 n&#x2F;(2^k)； 假设索引有 h 级， 最高的索引有2个结点；n&#x2F;(2^h) &#x3D; 2, 从这个公式我们可以求得 h &#x3D; log2(N)-1； 所以最后得出跳表的时间复杂度是O(logN) 跳表查询的空间复杂度分析 首先原始链表长度为n 如果索引是每2个结点有一个索引结点，每层索引的结点数：n&#x2F;2, n&#x2F;4, n&#x2F;8 … , 8, 4, 2 以此类推； 或者所以是每3个结点有一个索引结点，每层索引的结点数：n&#x2F;3, n&#x2F;9, n&#x2F;27 … , 9, 3, 1 以此类推； 所以空间复杂度是O(n)； 跳表是一个最典型的空间换时间解决方案，而且只有在数据量较大的情况下才能体现出来优势。而且应该是读多写少的情况下才能使用，所以它的适用范围应该还是比较有限的 维护成本相对要高 - 新增或者删除时需要把所有索引都更新一遍； 最后在新增和删除的过程中的更新，时间复杂度也是O(log n) 第十二章 Redis与MySQL数据双写一致性工程落地案例1. canal是什么Canal是基于MySQL变更日志增量订阅和消费的组件 canal [kə’næl]，中文翻译为 水道&#x2F;管道&#x2F;沟渠&#x2F;运河，主要用途是用于 MySQL 数据库增量日志数据的订阅、消费和解析，是阿里巴巴开发并开源的，采用Java语言开发； 历史背景是早期阿里巴巴因为杭州和美国双机房部署，存在跨机房数据同步的业务需求，实现方式主要是基于业务 trigger（触发器） 获取增量变更。从2010年开始，阿里巴巴逐步尝试采用解析数据库日志获取增量变更进行同步，由此衍生出了canal项目； 能干嘛 数据库镜像 数据库实时备份 索引构建和实时维护(拆分异构索引、倒排索引等) 业务 cache 刷新 带业务逻辑的增量数据处理 2. 相关面试2.1 MySQL的主从复制 MySQL的主从复制将经过如下步骤： 1、当 master 主服务器上的数据发生改变时，则将其改变写入二进制事件日志文件中； 2、salve 从服务器会在一定时间间隔内对 master 主服务器上的二进制日志进行探测，探测其是否发生过改变， 如果探测到 master 主服务器的二进制事件日志发生了改变，则开始一个 I&#x2F;O Thread 请求 master 二进制事件日志； 3、同时 master 主服务器为每个 I&#x2F;O Thread 启动一个dump Thread，用于向其发送二进制事件日志； 4、slave 从服务器将接收到的二进制事件日志保存至自己本地的中继日志文件中； 5、salve 从服务器将启动 SQL Thread 从中继日志中读取二进制日志，在本地重放，使得其数据和主服务器保持一致； 6、最后 I&#x2F;O Thread 和 SQL Thread 将进入睡眠状态，等待下一次被唤醒； 2.2 canal工作原理canal 工作原理 canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) canal 解析 binary log 对象(原始为 byte 流) mysql-canal-redis双写一致性Coding第十三章 缓存双写一致性之更新策略探讨缓存双写一致性，谈谈你的理解 如果redis中有数据 需要和数据库中的值相同 如果redis中无数据 数据库中的值要是最新值 缓存按照操作来分，有细分2种 ​ 只读缓存 ​ 读写缓存 ​ 同步直写策略：写缓存时也同步写数据库，缓存和数据库中的数据⼀致； ​ 对于读写缓存来说，要想保证缓存和数据库中的数据⼀致，就要采⽤同步直写策略 数据库和缓存一致性的几种更新策略 目的给缓存设置过期时间，是保证最终一致性的解决方案。 我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存，达到一致性，切记以mysql的数据库写入库为准。 上述方案和后续落地案例是调研后的主流+成熟的做法，但是考虑到各个公司业务系统的差距， 不是100%绝对正确，不保证绝对适配全部情况，请同学们自行酌情选择打法，合适自己的最好。 先更新数据库，再更新缓存问题 1 先更新mysql的某商品的库存，当前商品的库存是100，更新为99个。 2 先更新mysql修改为99成功，然后更新redis。 3 此时假设异常出现，更新redis失败了，这导致mysql里面的库存是99而redis里面的还是100 。 4 上述发生，会让数据库里面和缓存redis里面数据不一致，读到脏数据 先删除缓存，再更新数据库问题表示更新数据库可能失败 1 A线程先成功删除了redis里面的数据，然后去更新mysql，此时mysql正在更新中，还没有结束。（比如网络延时） B突然出现要来读取缓存数据。 异常问题2: 2 此时redis里面的数据是空的，B线程来读取，先去读redis里数据(已经被A线程delete掉了)，此处出来2个问题： 2.1 B从mysql获得了旧值 ​ B线程发现redis里没有(缓存缺失)马上去mysql里面读取，从数据库里面读取来的是旧值。 2.2 B会把获得的旧值写回redis 获得旧值数据后返回前台并回写进redis(刚被A线程删除的旧数据有极大可能又被写回了)。 3 A线程更新完mysql，发现redis里面的缓存是脏数据，A线程直接懵逼了，o(╥﹏╥)o 两个并发操作，一个是更新操作，另一个是查询操作，A更新操作删除缓存后，B查询操作没有命中缓存，B先把老数据读出来后放到缓存中，然后A更新操作更新了数据库。 于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 4 总结流程： （1）请求A进行写操作，删除缓存后，工作正在进行中……A还么有彻底更新完 （2）请求B开工，查询redis发现缓存不存在 （3）请求B继续，去数据库查询得到了myslq中的旧值 （4）请求B将旧值写入redis缓存 （5）请求A将新值写入mysql数据库 上述情况就会导致不一致的情形出现。 时间 线程A 线程B 出现的问题 t1 请求A进行写操作，删除缓存后，工作正在进行中…… t2 1 缓存中读取不到，立刻读mysql，由于A还没有对mysql更新完，读到的是旧值。 2 还把从mysql读取的旧值，写回了redis 1 A还更新完mysql，导致B读到了旧值 2 线程B遵守回写机制，把旧值写回redis，导致其它请求读取的还是旧值，A白干了。 t3 更新mysql数据库的值，over redis是被B写回的旧值， mysql是被A更新的新值。 出现了，数据不一致问题。 总结 先删除缓存，再更新数据库 如果数据库更新失败，导致B线程请求再次访问缓存时，发现redis里面没数据，缓存缺失，再去读取mysql时，从数据库中读取到旧值 解决方案多个线程同时去查询数据库的这条数据，那么我们可以在第一个查询数据的请求上使用一个 互斥锁来锁住它。 其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。 后面的线程进来发现已经有缓存了，就直接走缓存。 延时双删 双删方案面试题这个删除该休眠多久呢线程Asleep的时间，就需要大于线程B读取数据再写入缓存的时间。 这个时间怎么确定呢？ 在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，自行评估自己的项目的读数据业务逻辑的耗时， 以此为基础来进行估算。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上加百毫秒即可。 这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 当前演示的效果是mysql单机，如果mysql主从读写分离架构如何？（1）请求A进行写操作，删除缓存 （2）请求A将数据写入数据库了， （3）请求B查询缓存发现，缓存没有值 （4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值 （5）请求B将旧值写入缓存 （6）数据库完成主从同步，从库变为新值 上述情形，就是数据不一致的原因。还是使用双删延时策略。 只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms 这种同步淘汰策略，吞吐量降低怎么办？ 先更新数据库，再删除缓存问题时间 线程A 线程B 出现的问题 t1 删除数据库中的值 t2 缓存中立刻命中，此时B读取的是缓存旧值。 A还没有来得及删除缓存的值，导致B缓存命中读到旧值。 t3 更新缓存的数据，over 先更新数据库，再删除缓存 假如缓存删除失败或者来不及，导致请求再次访问redis时缓存命中，读取到的是缓存旧值 解决方案 1 可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用Kafka&#x2F;RabbitMQ等）。 2 当程序没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新。 3 如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了，否则还需要再次进行重试 4 如果重试超过的一定次数后还是没有成功，我们就需要向业务层发送报错信息了，通知运维人员。 总结方案2和方案3用那个？利弊如何在大多数业务场景下，我们会把Redis作为只读缓存使用。假如定位是只读缓存来说， 理论上我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存，但是没有完美方案，两害相衡趋其轻的原则 个人建议是，优先使用先更新数据库，再删除缓存的方案。理由如下： 1 先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，严重导致打满mysql。 2 如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。 多补充一句：如果使用先更新数据库，再删除缓存的方案 如果业务层要求必须读取一致性的数据，那么我们就需要在更新数据库时，先在Redis缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"SSM总结","slug":"4spring","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:55:57.692Z","comments":true,"path":"2022/09/01/4spring/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/4spring/","excerpt":"","text":"SSM概述2. Spring Framework五大功能模块 功能模块 功能介绍 Core Container 核心容器，在 Spring 环境下使用任何功能都必须基于 IOC 容器。 AOP&amp;Aspects 面向切面编程 Testing 提供了对 junit 或 TestNG 测试框架的整合。 Data Access&#x2F;Integration 提供了对数据访问&#x2F;集成的功能。 Spring MVC 提供了面向Web应用程序的集成功能。 [2]Filter生命周期 生命周期阶段 执行时机 执行次数 创建对象 Web应用启动时 一次 初始化 创建对象后 一次 拦截请求 接收到匹配的请求 多次 销毁 Web应用卸载前 一次 2. mybatis5、Mybatis特性 MyBatis支持定制化SQL、存储过程以及高级映射 MyBatis避免了几乎所有的JDBC代码和手动设置参数以及结果集解析操作 MyBatis可以使用简单的XML或注解实现配置和原始映射；将接口和Java的POJO（Plain Ordinary Java Object，普通的Java对象）映射成数据库中的记录 Mybatis是一个半自动的ORM（Object Relation Mapping）框架 6、和其它持久化层技术对比 JDBC SQL 夹杂在Java代码中耦合度高，导致硬编码内伤 维护不易且实际开发需求中 SQL 有变化，频繁修改的情况多见 代码冗长，开发效率低 Hibernate 和 JPA 操作简便，开发效率高 程序中的长难复杂 SQL 需要绕过框架 内部自动生产的 SQL，不容易做特殊优化 基于全映射的全自动框架，大量字段的 POJO 进行部分映射时比较困难。 反射操作太多，导致数据库性能下降 MyBatis 轻量级，性能出色 SQL 和 Java 编码分开，功能边界清晰。Java代码专注业务、SQL语句专注数据 开发效率稍逊于HIbernate，但是完全能够接收 3. spring mvc1、概述SpringMVC 是 Spring 为表述层开发提供的一整套完备的解决方案。 3、SpringMVC 代码对比①基于原生 Servlet API 开发代码片段1234567protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; String userName = request.getParameter(&quot;userName&quot;); System.out.println(&quot;userName=&quot;+userName); &#125; ②基于 SpringMVC 开发代码片段1234567@RequestMapping(&quot;/user/login&quot;)public String login(@RequestParam(&quot;userName&quot;) String userName)&#123; System.out.println(&quot;userName=&quot;+userName); return &quot;result&quot;;&#125; 第0章 Spring内置的能够使用的工具速查注册Bean到IOC容器1. BeanDefinition12345AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setBeanClass(User.class);context.registerBeanDefinition(&quot;user&quot;, beanDefinition);System.out.println(context.getBean(&quot;user&quot;)); 我们还可以通过BeanDefinition设置一个Bean的其他属性 123beanDefinition.setScope(&quot;prototype&quot;); // 设置作用域beanDefinition.setInitMethodName(&quot;init&quot;); // 设置初始化方法beanDefinition.setLazyInit(true); // 设置懒加载 和申明式事务、编程式事务类似，通过，@Bean，@Component等申明式方式所定义的Bean，最终都会被Spring解析为对应的BeanDefinition对象，并放入Spring容器中。 BeanDefinitionReaderAnnotatedBeanDefinitionReader可以直接把某个类转换为BeanDefinition，并且会解析该类上的注解，比如 12345678AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);AnnotatedBeanDefinitionReader annotatedBeanDefinitionReader = new AnnotatedBeanDefinitionReader(context);// 将User.class解析为BeanDefinitionannotatedBeanDefinitionReader.register(User.class);System.out.println(context.getBean(&quot;user&quot;)); 注意：它能解析的注解是：@Conditional，**@Scope**、@Lazy、@Primary、@DependsOn、@Role、@Description XmlBeanDefinitionReader可以解析标签 123456AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);XmlBeanDefinitionReader xmlBeanDefinitionReader = new XmlBeanDefinitionReader(context);int i = xmlBeanDefinitionReader.loadBeanDefinitions(&quot;spring.xml&quot;);System.out.println(context.getBean(&quot;user&quot;)); ClassPathBeanDefinitionScannerClassPathBeanDefinitionScanner是扫描器，但是它的作用和BeanDefinitionReader类似，它可以进行扫描，扫描某个包路径，对扫描到的类进行解析，比如，扫描到的类上如果存在@Component注解，那么就会把这个类解析为一个BeanDefinition，比如：​ 1234567AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();context.refresh();ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(context);scanner.scan(&quot;com.zhouyu&quot;);System.out.println(context.getBean(&quot;userService&quot;)); 事件发布先定义一个事件监听器 123456789@Beanpublic ApplicationListener applicationListener() &#123; return new ApplicationListener() &#123; @Override public void onApplicationEvent(ApplicationEvent event) &#123; System.out.println(&quot;接收到了一个事件&quot;); &#125; &#125;;&#125; 然后发布一个事件： 12345// 可以用ApplicationEventListener.publishEvent()context.publishEvent(&quot;kkk&quot;);// 这个底层用的就是上面的// 理解: 类似于消息队列 执行到publish之后 事件监听就会执行onApplicationEvent方法 类型转化在Spring源码中，有可能需要把String转成其他类型，所以在Spring源码中提供了一些技术来更方便的做对象的类型转化，关于类型转化的应用场景， 后续看源码的过程中会遇到很多。 123@Value(&quot;ggq&quot;)public User user;// 报错 类型不匹配 PropertyEditor这其实是JDK中提供的类型转化工具类 12345678910111213public class StringToUserPropertyEditor extends PropertyEditorSupport implements PropertyEditor &#123; @Override public void setAsText(String text) throws IllegalArgumentException &#123; User user = new User(); user.setName(text); this.setValue(user); &#125;&#125;StringToUserPropertyEditor propertyEditor = new StringToUserPropertyEditor();propertyEditor.setAsText(&quot;1&quot;);User value = (User) propertyEditor.getValue();System.out.println(value); 如何向Spring中注册PropertyEditor： 12345678910@Beanpublic CustomEditorConfigurer customEditorConfigurer() &#123; CustomEditorConfigurer customEditorConfigurer = new CustomEditorConfigurer(); Map&lt;Class&lt;?&gt;, Class&lt;? extends PropertyEditor&gt;&gt; propertyEditorMap = new HashMap&lt;&gt;(); // 表示StringToUserPropertyEditor可以将String转化成User类型，在Spring源码中，如果发现当前对象是String，而需要的类型是User，就会使用该PropertyEditor来做类型转化 propertyEditorMap.put(User.class, StringToUserPropertyEditor.class); customEditorConfigurer.setCustomEditors(propertyEditorMap); return customEditorConfigurer;&#125; 假设现在有如下Bean: 1234567891011@Componentpublic class UserService &#123; @Value(&quot;xxx&quot;) private User user; public void test() &#123; System.out.println(user); &#125;&#125; 那么test属性就能正常的完成属性赋值 ConversionServiceSpring中提供的类型转化服务，它比PropertyEditor更强大 1234567891011121314151617181920212223public class StringToUserConverter implements ConditionalGenericConverter &#123; @Override public boolean matches(TypeDescriptor sourceType, TypeDescriptor targetType) &#123; return sourceType.getType().equals(String.class) &amp;&amp; targetType.getType().equals(User.class); &#125; @Override public Set&lt;ConvertiblePair&gt; getConvertibleTypes() &#123; return Collections.singleton(new ConvertiblePair(String.class, User.class)); &#125; @Override public Object convert(Object source, TypeDescriptor sourceType, TypeDescriptor targetType) &#123; User user = new User(); user.setName((String)source); return user; &#125;&#125;DefaultConversionService conversionService = new DefaultConversionService();conversionService.addConverter(new StringToUserConverter());User value = conversionService.convert(&quot;1&quot;, User.class);System.out.println(value); 如何向Spring中注册ConversionService： 1234567@Beanpublic ConversionServiceFactoryBean conversionService() &#123; ConversionServiceFactoryBean conversionServiceFactoryBean = new ConversionServiceFactoryBean(); conversionServiceFactoryBean.setConverters(Collections.singleton(new StringToUserConverter())); return conversionServiceFactoryBean;&#125; TypeConverter整合了PropertyEditor和ConversionService的功能，是Spring内部用的 12345SimpleTypeConverter typeConverter = new SimpleTypeConverter();typeConverter.registerCustomEditor(User.class, new StringToUserPropertyEditor());//typeConverter.setConversionService(conversionService);User value = typeConverter.convertIfNecessary(&quot;1&quot;, User.class);System.out.println(value); OrderComparatorOrderComparator是Spring所提供的一种比较器，可以用来根据@Order注解或实现Ordered接口来执行值进行笔记，从而可以进行排序。 比如：​ 123456789101112public class A implements Ordered &#123; @Override public int getOrder() &#123; return 3; &#125; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125; 12345678910111213141516171819202122232425262728293031public class B implements Ordered &#123; @Override public int getOrder() &#123; return 2; &#125; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; A a = new A(); // order=3 B b = new B(); // order=2 OrderComparator comparator = new OrderComparator(); System.out.println(comparator.compare(a, b)); // 1 List list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); // 按order值升序排序 list.sort(comparator); System.out.println(list); // B，A &#125;&#125; 另外，Spring中还提供了一个OrderComparator的子类：AnnotationAwareOrderComparator，它支持用@Order来指定order值。比如：​ 12345678910111213141516171819202122232425262728293031323334353637@Order(3)public class A &#123; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;@Order(2)public class B &#123; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; A a = new A(); // order=3 B b = new B(); // order=2 AnnotationAwareOrderComparator comparator = new AnnotationAwareOrderComparator(); System.out.println(comparator.compare(a, b)); // 1 List list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); // 按order值升序排序 list.sort(comparator); System.out.println(list); // B，A &#125;&#125; MetadataReader、ClassMetadata、AnnotationMetadata在Spring中需要去解析类的信息，比如类名、类中的方法、类上的注解，这些都可以称之为类的元数据，所以Spring中对类的元数据做了抽象，并提供了一些工具类。 MetadataReader表示类的元数据读取器，默认实现类为SimpleMetadataReader。比如： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) throws IOException &#123; SimpleMetadataReaderFactory simpleMetadataReaderFactory = new SimpleMetadataReaderFactory(); // 构造一个MetadataReader MetadataReader metadataReader = simpleMetadataReaderFactory.getMetadataReader(&quot;com.zhouyu.service.UserService&quot;); // 得到一个ClassMetadata，并获取了类名 ClassMetadata classMetadata = metadataReader.getClassMetadata(); System.out.println(classMetadata.getClassName()); // 获取一个AnnotationMetadata，并获取类上的注解信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); for (String annotationType : annotationMetadata.getAnnotationTypes()) &#123; System.out.println(annotationType); &#125; &#125;&#125; 需要注意的是，SimpleMetadataReader去解析类时，使用的ASM技术。 并不是等Java类加载到JVM在解析,而是直接读取字节码文件 为什么要使用ASM技术，Spring启动的时候需要去扫描，如果指定的包路径比较宽泛，那么扫描的类是非常多的，那如果在Spring启动时就把这些类全部加载进JVM了，这样不太好，所以使用了ASM技术。 ExcludeFilter和IncludeFilter这两个Filter是Spring扫描过程中用来过滤的。ExcludeFilter表示排除过滤器，IncludeFilter表示包含过滤器。 比如以下配置，表示扫描com.zhouyu这个包下面的所有类，但是排除UserService类，也就是就算它上面有@Component注解也不会成为Bean。 123456@ComponentScan(value = &quot;com.zhouyu&quot;, excludeFilters = &#123;@ComponentScan.Filter( type = FilterType.ASSIGNABLE_TYPE, classes = UserService.class)&#125;.)public class AppConfig &#123;&#125; 再比如以下配置，就算UserService类上没有@Component注解，它也会被扫描成为一个Bean。 123456@ComponentScan(value = &quot;com.zhouyu&quot;, includeFilters = &#123;@ComponentScan.Filter( type = FilterType.ASSIGNABLE_TYPE, classes = UserService.class)&#125;)public class AppConfig &#123;&#125; ​ FilterType分为： ANNOTATION：表示是否包含某个注解 ASSIGNABLE_TYPE：表示是否是某个类 ASPECTJ：表示否是符合某个Aspectj表达式 REGEX：表示是否符合某个正则表达式 CUSTOM：自定义 在Spring的扫描逻辑中，默认会添加一个AnnotationTypeFilter给includeFilters，表示默认情况下Spring扫描过程中会认为类上有@Component注解的就是Bean。 第一章 Spring底层核心原理解析Spring串讲,从整体上了解Spring Bean的生命周期底层原理 依赖注入底层原理 初始化底层方法 (推断构造方法底层方法) AOP底层原理 事务底层原理 Spring的使用123456789@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;)public class SpingBootDemoApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext applicationContext = SpringApplication.run(SpingBootDemoApplication.class, args); //run方法返回一个IOC容器 &#125; Spring中是如何创建一个对象？Bean的创建过程Bean的大概创建生命周期 推断构造函数实例化一个对象 确定构造方法,确定入参的Bean对象 两种情况 1种构造方法和多种构造方法 1种则就选唯一的这一个 多种: 无指定默认找默认空参数构造函数 没有则报错,有指定(@Autowired的构造方法)则选择指定的 选择有参的会根据入参类型和名字去Spring找Bean对象 先根据入参类型找，如果只找到一个，那就直接用来作为入参 如果根据类型找到多个，则再根据入参名字来确定唯一一个 最终如果没有找到，则会报错，无法创建当前Bean对象 依赖注入 之后Aware回调(判断是否实现各种XxxxAware接口,实现接口的各种setXxxx放方法) 初始化前:判断是否存在@PostConstruct方法,存在则调用(初始化前) 初始化:是否实现InitaillizingBean接口,如果实现则要实现其的afterPropertySet()方法 初始化后:最后判断需不需要AOP,如果不需要那么Bean就创建完了,否则进行动态代理生成代理对象 AOP大致流程 如果当前Bean是单例Bean，那么会把该Bean对象存入一个Map&lt;String, Object&gt;，Map的key为beanName，value为Bean对象。这样下次getBean时就可以直接从Map中拿到对应的Bean对象了。（实际上，在Spring源码中，这个Map就是单例池） 如果当前Bean是原型Bean，那么后续没有其他动作，不会存入一个Map，下次getBean时会再次执行上述创建过程，得到一个新的Bean对象。 AOP大致流程AOP就是进行动态代理，在创建一个Bean的过程中，Spring在最后一步会去判断当前正在创建的这个Bean是不是需要进行AOP，如果需要则会进行动态代理。​ 如何判断当前Bean对象需不需要进行AOP: 找出所有的切面Bean 遍历切面中的每个方法，看是否写了@Before、@After等注解 如果写了，则判断所对应的Pointcut是否和当前Bean对象的类是否匹配 如果匹配则表示当前Bean对象有匹配的的Pointcut，表示需要进行AOP 利用cglib进行AOP的大致流程： 生成代理类UserServiceProxy，代理类继承UserService 代理类中重写了父类的方法，比如UserService中的test()方法 代理类中还会有一个target属性，该属性的值为被代理对象（也就是通过UserService类推断构造方法实例化出来的对象，进行了依赖注入、初始化等步骤的对象） 代理类中的test()方法被执行时的逻辑如下： 执行切面逻辑（@Before） 调用target.test() 当我们从Spring容器得到UserService的Bean对象时，拿到的就是UserServiceProxy所生成的对象，也就是代理对象。​ UserService代理对象.test()—&gt;执行切面逻辑—&gt;target.test()，注意target对象不是代理对象，而是被代理对象。 Spring事务当我们在某个方法上加了@Transactional注解后，就表示该方法在调用时会开启Spring事务，而这个方法所在的类所对应的Bean对象会是该类的代理对象。​ Spring事务的代理对象执行某个方法时的步骤： 判断当前执行的方法是否存在@Transactional注解 如果存在，则利用事务管理器（TransactionMananger）新建一个数据库连接 修改数据库连接的autocommit为false 执行target.test()，执行程序员所写的业务逻辑代码，也就是执行sql 执行完了之后如果没有出现异常，则提交，否则回滚 Spring事务是否会失效的判断标准：某个加了@Transactional注解的方法被调用时，要判断到底是不是直接被代理对象调用的，如果是则事务会生效，如果不是则失效。 第二章 Bean生命周期1. 生成BeanDefinitionSpring扫描的底层实现： 首先，通过ResourcePatternResolver获得指定包路径下的所有.class文件（Spring源码中将此文件包装成了Resource对象） 遍历每个Resource对象 利用MetadataReaderFactory解析Resource对象得到MetadataReader（在Spring源码中MetadataReaderFactory具体的实现类为CachingMetadataReaderFactory，MetadataReader的具体实现类为SimpleMetadataReader） 利用MetadataReader进行excludeFilters和includeFilters，以及条件注解@Conditional的筛选（条件注解并不能理解：某个类上是否存在@Conditional注解，如果存在则调用注解中所指定的类的match方法进行匹配，匹配成功则通过筛选，匹配失败则pass掉。） 筛选通过后，基于metadataReader生成ScannedGenericBeanDefinition 再基于metadataReader判断是不是对应的类是不是接口或抽象类 如果筛选通过，那么就表示扫描到了一个Bean，将ScannedGenericBeanDefinition加入结果集 MetadataReader表示类的元数据读取器，主要包含了一个AnnotationMetadata，功能有 获取类的名字、 获取父类的名字 获取所实现的所有接口名 获取所有内部类的名字 判断是不是抽象类 判断是不是接口 判断是不是一个注解 获取拥有某个注解的方法集合 获取类上添加的所有注解信息 获取类上添加的所有注解类型集合 值得注意的是，CachingMetadataReaderFactory解析某个.class文件得到MetadataReader对象是利用的ASM技术，并没有加载这个类到JVM。并且，最终得到的ScannedGenericBeanDefinition对象，beanClass属性存储的是当前类的名字，而不是class对象。（beanClass属性的类型是Object，它即可以存储类的名字，也可以存储class对象）​ 最后，上面是说的通过扫描得到BeanDefinition对象，我们还可以通过直接定义BeanDefinition，或解析spring.xml文件的，或者@Bean注解得到BeanDefinition对象。（后续课程会分析@Bean注解是怎么生成BeanDefinition的）。 2. 合并BeanDefinition通过扫描得到所有BeanDefinition之后，就可以根据BeanDefinition创建Bean对象了，但是在Spring中支持父子BeanDefinition，和Java父子类类似，但是完全不是一回事。 父子BeanDefinition实际用的比较少，使用是这样的，比如： 12&lt;bean id=&quot;parent&quot; class=&quot;com.zhouyu.service.Parent&quot; scope=&quot;prototype&quot;/&gt;&lt;bean id=&quot;child&quot; class=&quot;com.zhouyu.service.Child&quot;/&gt; 这么定义的情况下，child是单例Bean。 12&lt;bean id=&quot;parent&quot; class=&quot;com.zhouyu.service.Parent&quot; scope=&quot;prototype&quot;/&gt;&lt;bean id=&quot;child&quot; class=&quot;com.zhouyu.service.Child&quot; parent=&quot;parent&quot;/&gt; 但是这么定义的情况下，child就是原型Bean了。​ 因为child的父BeanDefinition是parent，所以会继承parent上所定义的scope属性。​ 而在根据child来生成Bean对象之前，需要进行BeanDefinition的合并，得到完整的child的BeanDefinition。​ 3. 加载类BeanDefinition合并之后，就可以去创建Bean对象了，而创建Bean就必须实例化对象，而实例化就必须先加载当前BeanDefinition所对应的class，在AbstractAutowireCapableBeanFactory类的createBean()方法中，一开始就会调用： 1Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); 这行代码就是去加载类，该方法是这么实现的： 12345678910111213if (mbd.hasBeanClass()) &#123; return mbd.getBeanClass();&#125;if (System.getSecurityManager() != null) &#123; return AccessController.doPrivileged((PrivilegedExceptionAction&lt;Class&lt;?&gt;&gt;) () -&gt; doResolveBeanClass(mbd, typesToMatch), getAccessControlContext()); &#125;else &#123; return doResolveBeanClass(mbd, typesToMatch);&#125;public boolean hasBeanClass() &#123; return (this.beanClass instanceof Class);&#125; 如果beanClass属性的类型是Class，那么就直接返回，如果不是，则会根据类名进行加载（doResolveBeanClass方法所做的事情） 会利用BeanFactory所设置的类加载器来加载类，如果没有设置，则默认使用**ClassUtils.getDefaultClassLoader()**所返回的类加载器来加载。 ClassUtils.getDefaultClassLoader() 优先返回当前线程中的ClassLoader 线程中类加载器为null的情况下，返回ClassUtils类的类加载器 如果ClassUtils类的类加载器为空，那么则表示是Bootstrap类加载器加载的ClassUtils类，那么则返回系统类加载器 4. 实例化前当前BeanDefinition对应的类成功加载后，就可以实例化对象了，但是…​ 在Spring中，实例化对象之前，Spring提供了一个扩展点，允许用户来控制是否在某个或某些Bean实例化之前做一些启动动作。这个扩展点叫**InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation()**。比如：​ 1234567891011@Componentpublic class ZhouyuBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;实例化前&quot;); &#125; return null; &#125;&#125; 如上代码会导致，在userService这个Bean实例化前，会进行打印。​ 值得注意的是，postProcessBeforeInstantiation()是有返回值的，如果这么实现： 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;实例化前&quot;); return new UserService(); &#125; return null; &#125;&#125; userService这个Bean，在实例化前会直接返回一个由我们所定义的UserService对象。如果是这样，表示不需要Spring来实例化了，并且后续的Spring依赖注入也不会进行了，会跳过一些步骤，直接执行初始化后这一步。 5. 实例化在这个步骤中就会根据BeanDefinition去创建一个对象了。 5.1 Supplier创建对象首先判断BeanDefinition中是否设置了Supplier，如果设置了则调用Supplier的get()得到对象。​ 得直接使用BeanDefinition对象来设置Supplier，比如： 12345678AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setInstanceSupplier(new Supplier&lt;Object&gt;() &#123; @Override public Object get() &#123; return new UserService(); &#125;&#125;);context.registerBeanDefinition(&quot;userService&quot;, beanDefinition); 5.2 工厂方法创建对象如果没有设置Supplier，则检查BeanDefinition中是否设置了factoryMethod，也就是工厂方法，有两种方式可以设置factoryMethod，比如：​ 方式一： 1&lt;bean id=&quot;userService&quot; class=&quot;com.zhouyu.service.UserService&quot; factory-method=&quot;createUserService&quot; /&gt; 对应的UserService类为： 12345678910111213public class UserService &#123; public static UserService createUserService() &#123; System.out.println(&quot;执行createUserService()&quot;); UserService userService = new UserService(); return userService; &#125; public void test() &#123; System.out.println(&quot;test&quot;); &#125;&#125; 方式二： 12&lt;bean id=&quot;commonService&quot; class=&quot;com.zhouyu.service.CommonService&quot;/&gt;&lt;bean id=&quot;userService1&quot; factory-bean=&quot;commonService&quot; factory-method=&quot;createUserService&quot; /&gt; 对应的CommonService的类为： 123456public class CommonService &#123; public UserService createUserService() &#123; return new UserService(); &#125;&#125; Spring发现当前BeanDefinition方法设置了工厂方法后，就会区分这两种方式，然后调用工厂方法得到对象。​ 值得注意的是，我们通过@Bean所定义的BeanDefinition，是存在factoryMethod和factoryBean的，也就是和上面的方式二非常类似，@Bean所注解的方法就是factoryMethod，AppConfig对象就是factoryBean。如果@Bean所所注解的方法是static的，那么对应的就是方式一。 5.3 推断构造方法第一节已经讲过一遍大概原理了，后面有一节课单独分析源码实现。推断完构造方法后，就会使用构造方法来进行实例化了。​ 额外的，在推断构造方法逻辑中除开会去选择构造方法以及查找入参对象意外，会还判断是否在对应的类中是否存在使用**@Lookup注解了方法。如果存在则把该方法封装为LookupOverride对象并添加到BeanDefinition中。**​ 在实例化时，如果判断出来当前BeanDefinition中没有LookupOverride，那就直接用构造方法反射得到一个实例对象。如果存在LookupOverride对象，也就是类中存在@Lookup注解了的方法，那就会生成一个代理对象。​ @Lookup注解就是方法注入，使用demo如下： 12345678910111213141516@Componentpublic class UserService &#123; private OrderService orderService; public void test() &#123; OrderService orderService = createOrderService(); System.out.println(orderService); &#125; @Lookup(&quot;orderService&quot;) public OrderService createOrderService() &#123; return null; &#125;&#125; 6. BeanDefinition的后置处理Bean对象实例化出来之后，接下来就应该给对象的属性赋值了。在真正给属性赋值之前，Spring又提供了一个扩展点**MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition()**，可以对此时的BeanDefinition进行加工，比如： 12345678910@Componentpublic class ZhouyuMergedBeanDefinitionPostProcessor implements MergedBeanDefinitionPostProcessor &#123; @Override public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; if (&quot;userService&quot;.equals(beanName)) &#123; beanDefinition.getPropertyValues().add(&quot;orderService&quot;, new OrderService()); &#125; &#125;&#125; 在Spring源码中，AutowiredAnnotationBeanPostProcessor就是一个MergedBeanDefinitionPostProcessor，它的postProcessMergedBeanDefinition()中会去查找注入点，并缓存在AutowiredAnnotationBeanPostProcessor对象的一个Map中（injectionMetadataCache）。 7. 实例化后在处理完BeanDefinition后，Spring又设计了一个扩展点：**InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation()**，比如：​ 1234567891011121314@Componentpublic class ZhouyuInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; UserService userService = (UserService) bean; userService.test(); &#125; return true; &#125;&#125; 上述代码就是对userService所实例化出来的对象进行处理。​ 这个扩展点，在Spring源码中基本没有怎么使用。 8. 自动注入这里的自动注入指的是Spring的自动注入，后续依赖注入课程中单独讲​ 9. 处理属性这个步骤中，就会处理@Autowired、@Resource、@Value等注解，也是通过**InstantiationAwareBeanPostProcessor.postProcessProperties()**扩展点来实现的，比如我们甚至可以实现一个自己的自动注入功能，比如： 123456789101112131415161718192021@Componentpublic class ZhouyuInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; for (Field field : bean.getClass().getFields()) &#123; if (field.isAnnotationPresent(ZhouyuInject.class)) &#123; field.setAccessible(true); try &#123; field.set(bean, &quot;123&quot;); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; return pvs; &#125;&#125; 关于@Autowired、@Resource、@Value的底层源码，会在后续的依赖注入课程中详解。 10. 执行Aware完成了属性赋值之后，Spring会执行一些回调，包括： BeanNameAware：回传beanName给bean对象。 BeanClassLoaderAware：回传classLoader给bean对象。 BeanFactoryAware：回传beanFactory给对象。 11. 初始化前初始化前，也是Spring提供的一个扩展点：**BeanPostProcessor.postProcessBeforeInitialization()**，比如 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;初始化前&quot;); &#125; return bean; &#125;&#125; 利用初始化前，可以对进行了依赖注入的Bean进行处理。​ 在Spring源码中： InitDestroyAnnotationBeanPostProcessor会在初始化前这个步骤中执行@PostConstruct的方法， ApplicationContextAwareProcessor会在初始化前这个步骤中进行其他Aware的回调： EnvironmentAware：回传环境变量 EmbeddedValueResolverAware：回传占位符解析器 ResourceLoaderAware：回传资源加载器 ApplicationEventPublisherAware：回传事件发布器 MessageSourceAware：回传国际化资源 ApplicationStartupAware：回传应用其他监听对象，可忽略 ApplicationContextAware：回传Spring容器ApplicationContext 12. 初始化 查看当前Bean对象是否实现了InitializingBean接口，如果实现了就调用其afterPropertiesSet()方法 执行BeanDefinition中指定的初始化方法 13. 初始化后这是Bean创建生命周期中的最后一个步骤，也是Spring提供的一个扩展点：**BeanPostProcessor.postProcessAfterInitialization()**，比如： 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;初始化后&quot;); &#125; return bean; &#125;&#125; 可以在这个步骤中，对Bean最终进行处理，Spring中的AOP就是基于初始化后实现的，初始化后返回的对象才是最终的Bean对象。 总结BeanPostProcessor InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation() 实例化 MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition() InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation() 自动注入 InstantiationAwareBeanPostProcessor.postProcessProperties() Aware对象 BeanPostProcessor.postProcessBeforeInitialization() 初始化 BeanPostProcessor.postProcessAfterInitialization() 第三章 IOC启动AC.runSpringApplication调用的run方法执行流程如下： \\1. 初始化监听器，以及添加到SpringApplication的自定义监听器。 \\2. 发布ApplicationStartedEvent事件，如果想监听ApplicationStartedEvent事件，你可以这样定义：public class ApplicationStartedListener implements ApplicationListener，然后通过SpringApplication.addListener(..)添加进去即可。 \\3. 装配参数和环境，确定是web环境还是非web环境。 \\4. 装配完环境后，就触发ApplicationEnvironmentPreparedEvent事件。 \\5. 如果SpringApplication的showBanner属性被设置为true，则打印启动的Banner。 \\6. 创建ApplicationContext，会根据是否是web环境，来决定创建什么类型的ApplicationContext。 \\7. 装配Context的环境变量，注册Initializers、beanNameGenerator等。 \\8. 发布ApplicationPreparedEvent事件。 \\9. 注册springApplicationArguments、springBootBanner，加载资源等 \\10. 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。 \\11. 调用ApplicationContext的refresh()方法,装配context beanfactory等非常重要的核心组件。 \\12. 查找当前ApplicationContext中是否注册有CommandLineRunner，如果有，则遍历执行它们。 \\13. 发布ApplicationReadyEvent事件，启动完毕，表示服务已经可以开始正常提供服务了。通常我们这里会监听这个事件来打印一些监控性质的日志，表示应用正常启动了。 前言分析通常，我们说的Spring启动，就是构造ApplicationContext对象以及调用refresh()方法的过程。​ 首先，Spring启动过程主要做了这么几件事情： 构造一个BeanFactory对象 解析配置类，得到BeanDefinition，并注册到BeanFactory中 解析@ComponentScan，此时就会完成扫描 解析@Import 解析@Bean … 因为ApplicationContext还支持国际化，所以还需要初始化MessageSource对象 因为ApplicationContext还支持事件机制，所以还需要初始化ApplicationEventMulticaster对象 把用户定义的ApplicationListener对象添加到ApplicationContext中，等Spring启动完了就要发布事件了 创建非懒加载的单例Bean对象，并存在BeanFactory的单例池中。 调用Lifecycle Bean的start()方法 发布ContextRefreshedEvent事件 由于Spring启动过程中要创建非懒加载的单例Bean对象，那么就需要用到BeanPostProcessor，所以Spring在启动过程中就需要做两件事： 生成默认的BeanPostProcessor对象，并添加到BeanFactory中 AutowiredAnnotationBeanPostProcessor：处理@Autowired、@Value CommonAnnotationBeanPostProcessor：处理@Resource、@PostConstruct、@PreDestroy ApplicationContextAwareProcessor：处理ApplicationContextAware等回调 找到外部用户所定义的BeanPostProcessor对象（类型为BeanPostProcessor的Bean对象），并添加到BeanFactory中 Spring自带的BeanPostProcessor类 BeanPostProcessor InstantiationAwareBeanPostProcessor MergeBeanDefinitionAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor spring的启动流程this() 首先要创建一个BeanFactory比较常见的是AnnotationConfigApplicationContext(隐藏会组合一个defaultListableBeanfactory对象来作为一个存储各种信息的工厂) 先创建reader读取各种bean定义信息,后续为了加载底层功能的后置处理器 注册一个注解配置处理器,给工厂准备些解析器等基础组件(传入的是一个自带的rootBeanDefinition)注册到default 注册核心组件() 给工厂准备些基础组件(各种解析器),给工厂注册核心组件 AnnotatitionConfigurationClassBeanfactory(BeanFactoryPostProcessor)处理配置类 AutowiredAnnotationBeanPostProcessor(SmartInstantiationAwareBeanPostProcessor) CommonAnnotationBeanPostProcessor(普通JSR250支持@PostContruct @PreDestroy @Resource) JPA支持后置处理器 EventListenerMethorPostProcessor(事件功能(事件方法)的后置处理器) DefaultListenerFactory(事件工厂) 创建一个scanner准备一些环境变量 register(MainConfig) reader来注册所有配置类, 完善主配置类的定义信息(解析@Lazy @ Primary @DependsOn @ Description) 把主配置注册进去 refresh() 准备上下文环境 获取this()准备好的工厂 预准备工厂 给工厂设置必要的工具比如:el表达式解析器,资源解析器,基本的后置处理器(ApplicationContextAwareProcessor判断当前组件是否实现了xxxaware接口) 还注册了一些单实例Bean: 系统属性,系统环境 后置处理BeanFactory(空方法) 执行工厂的后置增强 ConfigurationClassPostProcessor解析配置类后置处理器在此工作 通过一个代理PostProecessorRegistrationDelegate管理所有后置处理器的执行 在工厂左右定义信息中获取配置类的信息使用parser进行配置类解析 处理@Lazy,@ComponentScan,@PropertySource@Import等注解将所有的Bean定义信息全部准备好 (利用反射把BeanClass的所有元数据准备好放入BeanDefinitionMap中) 注册Bean的后置处理器 for创建所有后置处理器对象 工厂提前保存所有处理器,方便后面创建Bean使用 初始化国际化组件 初始化事件多播器组件 看容器是否有(用户自己定义的) 没有就注册个默认的 放入单例池 OnRefresh()留给子类继续增强处理逻辑 注册监听器 获取容器中定义的所有ApplicationListener并保存起来 完成工厂初始化 遍历所有BeanName创建Bean对象 详情参照Bean初始化 finishRefresh() 最后的一些清理,时间发送等 Bean初始化+生命周期 todo循环引用 Bean的销毁过程Bean销毁是发送在Spring容器关闭过程中的。​ 在Spring容器关闭时，比如： 123456AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);UserService userService = (UserService) context.getBean(&quot;userService&quot;);userService.test();// 容器关闭context.close(); 在Bean创建过程中，在最后（初始化之后），有一个步骤会去判断当前创建的Bean是不是DisposableBean： 当前Bean是否实现了DisposableBean接口 或者，当前Bean是否实现了AutoCloseable接口 BeanDefinition中是否指定了destroyMethod 调用DestructionAwareBeanPostProcessor.requiresDestruction(bean)进行判断 ApplicationListenerDetector中直接使得ApplicationListener是DisposableBean InitDestroyAnnotationBeanPostProcessor中使得拥有@PreDestroy注解了的方法就是DisposableBean 把符合上述任意一个条件的Bean适配成DisposableBeanAdapter对象，并存入disposableBeans中（一个LinkedHashMap） 在Spring容器关闭过程时： 首先发布ContextClosedEvent事件 调用lifecycleProcessor的onCloese()方法 销毁单例Bean 遍历disposableBeans 把每个disposableBean从单例池中移除 调用disposableBean的destroy() 如果这个disposableBean还被其他Bean依赖了，那么也得销毁其他Bean 如果这个disposableBean还包含了inner beans，将这些Bean从单例池中移除掉 (inner bean参考https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-inner-beans) 清空manualSingletonNames，是一个Set，存的是用户手动注册的单例Bean的beanName 清空allBeanNamesByType，是一个Map，key是bean类型，value是该类型所有的beanName数组 清空singletonBeanNamesByType，和allBeanNamesByType类似，只不过只存了单例Bean 这里涉及到一个设计模式：适配器模式 在销毁时，Spring会找出实现了DisposableBean接口的Bean。​ 但是我们在定义一个Bean时，如果这个Bean实现了DisposableBean接口，或者实现了AutoCloseable接口，或者在BeanDefinition中指定了destroyMethodName，那么这个Bean都属于“DisposableBean”，这些Bean在容器关闭时都要调用相应的销毁方法。 所以，这里就需要进行适配，将实现了DisposableBean接口、或者AutoCloseable接口等适配成实现了DisposableBean接口，所以就用到了DisposableBeanAdapter。 会把实现了AutoCloseable接口的类封装成DisposableBeanAdapter，而DisposableBeanAdapter实现了DisposableBean接口。 BeanFactoryPostProcessorBeanPostProcessor表示Bean的后置处理器，是用来对Bean进行加工的，类似的，BeanFactoryPostProcessor理解为BeanFactory的后置处理器，用来用对BeanFactory进行加工的。​ Spring支持用户定义BeanFactoryPostProcessor的实现类Bean，来对BeanFactory进行加工，比如： 123456789@Componentpublic class ZhouyuBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition beanDefinition = beanFactory.getBeanDefinition(&quot;userService&quot;); beanDefinition.setAutowireCandidate(false); &#125;&#125; 以上代码，就利用了BeanFactoryPostProcessor来拿到BeanFactory，然后获取BeanFactory内的某个BeanDefinition对象并进行修改，注意这一步是发生在Spring启动时，创建单例Bean之前的，所以此时对BeanDefinition就行修改是会生效的。​ 注意：在ApplicationContext内部有一个核心的DefaultListableBeanFactory，它实现了ConfigurableListableBeanFactory和BeanDefinitionRegistry接口，所以ApplicationContext和DefaultListableBeanFactory是可以注册BeanDefinition的，但是ConfigurableListableBeanFactory是不能注册BeanDefinition的，只能获取BeanDefinition，然后做修改。 所以Spring还提供了一个BeanFactoryPostProcessor的子接口：BeanDefinitionRegistryPostProcessor​ BeanDefinitionRegistryPostProcessor12345public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor &#123; void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException;&#125; 我们可以看到BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor接口，并新增了一个方法，注意方法的参数为BeanDefinitionRegistry，所以如果我们提供一个类来实现BeanDefinitionRegistryPostProcessor，那么在postProcessBeanDefinitionRegistry()方法中就可以注册BeanDefinition了。比如：​ 12345678910111213141516@Componentpublic class ZhouyuBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition(); beanDefinition.setBeanClass(User.class); registry.registerBeanDefinition(&quot;user&quot;, beanDefinition); &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition beanDefinition = beanFactory.getBeanDefinition(&quot;userService&quot;); beanDefinition.setAutowireCandidate(false); &#125;&#125; 如何理解refresh()？1234567891011/** * Load or refresh the persistent representation of the configuration, * which might an XML file, properties file, or relational database schema. * &lt;p&gt;As this is a startup method, it should destroy already created singletons * if it fails, to avoid dangling resources. In other words, after invocation * of that method, either all or no singletons at all should be instantiated. * @throws BeansException if the bean factory could not be initialized * @throws IllegalStateException if already initialized and multiple refresh * attempts are not supported */ void refresh() throws BeansException, IllegalStateException; 这是ConfigurableApplicationContext接口上refresh()方法的注释，意思是：加载或刷新持久化的配置，可能是XML文件、属性文件或关系数据库中存储的。由于这是一个启动方法，如果失败，它应该销毁已经创建的单例，以避免暂用资源。换句话说，在调用该方法之后，应该实例化所有的单例，或者根本不实例化单例 。 有个理念需要注意：ApplicationContext关闭之后不代表JVM也关闭了，ApplicationContext是属于JVM的，说白了ApplicationContext也是JVM中的一个对象。​ 在Spring的设计中，也提供可以刷新的ApplicationContext和不可以刷新的ApplicationContext。比如： 1AbstractRefreshableApplicationContext extends AbstractApplicationContext 就是可以刷新的 1GenericApplicationContext extends AbstractApplicationContext 就是不可以刷新的。 AnnotationConfigApplicationContext继承的是GenericApplicationContext，所以它是不能刷新的。AnnotationConfigWebApplicationContext继承的是AbstractRefreshableWebApplicationContext，所以它是可以刷的。 上面说的不能刷新是指不能重复刷新，只能调用一次refresh方法，第二次时会报错。 refresh()底层原理流程底层原理流程图：https://www.processon.com/view/link/5f60a7d71e08531edf26a919 下面以AnnotationConfigApplicationContext为例子，来介绍refresh的底层原理。 在调用AnnotationConfigApplicationContext的构造方法之前，会调用父类GenericApplicationContext的无参构造方法，会构造一个BeanFactory，为DefaultListableBeanFactory。 构造AnnotatedBeanDefinitionReader（ 主要作用添加一些基础的PostProcessor，同时可以通过reader进行BeanDefinition的注册 ），同时对BeanFactory进行设置和添加 PostProcessor （后置处理器） 设置dependencyComparator：AnnotationAwareOrderComparator，它是一个Comparator，是用来进行排序的，会获取某个对象上的Order注解或者通过实现Ordered接口所定义的值进行排序，在日常开发中可以利用这个类来进行排序。 设置autowireCandidateResolver：ContextAnnotationAutowireCandidateResolver，用来解析某个Bean能不能进行自动注入，比如某个Bean的autowireCandidate属性是否等于true 向BeanFactory中添加ConfigurationClassPostProcessor对应的BeanDefinition，它是一个BeanDefinitionRegistryPostProcessor，并且实现了PriorityOrdered接口 向BeanFactory中添加AutowiredAnnotationBeanPostProcessor对应的BeanDefinition，它是一个InstantiationAwareBeanPostProcessorAdapter，MergedBeanDefinitionPostProcessor 向BeanFactory中添加CommonAnnotationBeanPostProcessor对应的BeanDefinition，它是一个InstantiationAwareBeanPostProcessor，InitDestroyAnnotationBeanPostProcessor 向BeanFactory中添加EventListenerMethodProcessor对应的BeanDefinition，它是一个BeanFactoryPostProcessor，SmartInitializingSingleton 向BeanFactory中添加DefaultEventListenerFactory对应的BeanDefinition，它是一个EventListenerFactory 构造ClassPathBeanDefinitionScanner（ 主要作用可以用来扫描得到并注册BeanDefinition ），同时进行设置： 设置this.includeFilters &#x3D; AnnotationTypeFilter(Component.class) 设置environment 设置resourceLoader 利用reader注册AppConfig为BeanDefinition，类型为AnnotatedGenericBeanDefinition 接下来就是调用refresh方法 prepareRefresh()： 记录启动时间 可以允许子容器设置一些内容到Environment中 验证Environment中是否包括了必须要有的属性 obtainFreshBeanFactory()：进行BeanFactory的refresh，在这里会去调用子类的refreshBeanFactory方法，具体子类是怎么刷新的得看子类，然后再调用子类的getBeanFactory方法，重新得到一个BeanFactory prepareBeanFactory(beanFactory)： 设置beanFactory的类加载器 设置表达式解析器：StandardBeanExpressionResolver，用来解析Spring中的表达式 添加PropertyEditorRegistrar：ResourceEditorRegistrar，PropertyEditor类型转化器注册器，用来注册一些默认的PropertyEditor 添加一个Bean的后置处理器：ApplicationContextAwareProcessor，是一个BeanPostProcessor，用来执行EnvironmentAware、ApplicationEventPublisherAware等回调方法 添加 ignoredDependencyInterface ：可以向这个属性中添加一些接口，如果某个类实现了这个接口，并且这个类中的某些set方法在接口中也存在，那么这个set方法在自动注入的时候是不会执行的，比如EnvironmentAware这个接口，如果某个类实现了这个接口，那么就必须实现它的setEnvironment方法，而这是一个set方法，和Spring中的autowire是冲突的，那么Spring在自动注入时是不会调用setEnvironment方法的，而是等到回调Aware接口时再来调用（注意，这个功能仅限于xml的autowire，@Autowired注解是忽略这个属性的） EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware 另外其实在构造BeanFactory的时候就已经提前添加了另外三个： BeanNameAware BeanClassLoaderAware BeanFactoryAware 添加 resolvableDependencies ：在byType进行依赖注入时，会先从这个属性中根据类型找bean BeanFactory.class：当前BeanFactory对象 ResourceLoader.class：当前ApplicationContext对象 ApplicationEventPublisher.class：当前ApplicationContext对象 ApplicationContext.class：当前ApplicationContext对象 添加一个Bean的后置处理器：ApplicationListenerDetector，是一个BeanPostProcessor，用来判断某个Bean是不是ApplicationListener，如果是则把这个Bean添加到ApplicationContext中去，注意一个ApplicationListener只能是单例的 添加一个Bean的后置处理器：LoadTimeWeaverAwareProcessor，是一个BeanPostProcessor，用来判断某个Bean是不是实现了LoadTimeWeaverAware接口，如果实现了则把ApplicationContext中的loadTimeWeaver回调setLoadTimeWeaver方法设置给该Bean。 添加一些单例bean到单例池： “environment”：Environment对象 “systemProperties”：System.getProperties()返回的Map对象 “systemEnvironment”：System.getenv()返回的Map对象 postProcessBeanFactory(beanFactory) ： 提供给AbstractApplicationContext的子类进行扩展，具体的子类，可以继续向BeanFactory中再添加一些东西 invokeBeanFactoryPostProcessors(beanFactory)： 执行BeanFactoryPostProcessor 此时在BeanFactory中会存在一个BeanFactoryPostProcessor：ConfigurationClassPostProcessor，它也是一个BeanDefinitionRegistryPostProcessor 第一阶段 从BeanFactory中找到类型为BeanDefinitionRegistryPostProcessor的beanName，也就是ConfigurationClassPostProcessor， 然后调用BeanFactory的getBean方法得到实例对象 执行**ConfigurationClassPostProcessor的postProcessBeanDefinitionRegistry()**方法: 解析AppConfig类 扫描得到BeanDefinition并注册 解析@Import，@Bean等注解得到BeanDefinition并注册 详细的看另外的笔记，专门分析了ConfigurationClassPostProcessor是如何工作的 在这里，我们只需要知道在这一步会去得到BeanDefinition，而这些BeanDefinition中可能存在BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor，所以执行完ConfigurationClassPostProcessor的postProcessBeanDefinitionRegistry()方法后，还需要继续执行其他BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行其他BeanDefinitionRegistryPostProcessor的**postProcessBeanDefinitionRegistry()**方法 执行所有BeanDefinitionRegistryPostProcessor的**postProcessBeanFactory()**方法 第二阶段 从BeanFactory中找到类型为BeanFactoryPostProcessor的beanName，而这些BeanFactoryPostProcessor包括了上面的BeanDefinitionRegistryPostProcessor 执行还没有执行过的BeanFactoryPostProcessor的**postProcessBeanFactory()**方法 到此，所有的BeanFactoryPostProcessor的逻辑都执行完了，主要做的事情就是得到BeanDefinition并注册到BeanFactory中 registerBeanPostProcessors(beanFactory)：因为上面的步骤完成了扫描，这个过程中程序员可能自己定义了一些BeanPostProcessor，在这一步就会把BeanFactory中所有的BeanPostProcessor找出来并实例化得到一个对象，并添加到BeanFactory中去（属性beanPostProcessors），最后再重新添加一个ApplicationListenerDetector对象（之前其实就添加了过，这里是为了把ApplicationListenerDetector移动到最后） initMessageSource()：如果BeanFactory中存在一个叫做”messageSource“的BeanDefinition，那么就会把这个Bean对象创建出来并赋值给ApplicationContext的messageSource属性，让ApplicationContext拥有国际化的功能 initApplicationEventMulticaster()：如果BeanFactory中存在一个叫做”applicationEventMulticaster“的BeanDefinition，那么就会把这个Bean对象创建出来并赋值给ApplicationContext的applicationEventMulticaster属性，让ApplicationContext拥有事件发布的功能 onRefresh()：提供给AbstractApplicationContext的子类进行扩展，没用 registerListeners()：从BeanFactory中获取ApplicationListener类型的beanName，然后添加到ApplicationContext中的事件广播器applicationEventMulticaster中去，到这一步因为FactoryBean还没有调用getObject()方法生成Bean对象，所以这里要在根据类型找一下ApplicationListener，记录一下对应的beanName finishBeanFactoryInitialization(beanFactory)：完成BeanFactory的初始化，主要就是实例化非懒加载的单例Bean，单独的笔记去讲。 finishRefresh()：BeanFactory的初始化完后，就到了Spring启动的最后一步了 设置ApplicationContext的lifecycleProcessor，默认情况下设置的是DefaultLifecycleProcessor 调用lifecycleProcessor的onRefresh()方法，如果是DefaultLifecycleProcessor，那么会获取所有类型为Lifecycle的Bean对象，然后调用它的start()方法，这就是ApplicationContext的生命周期扩展机制 发布ContextRefreshedEvent事件 执行BeanFactoryPostProcessor 执行通过ApplicationContext添加进来的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中实现了PriorityOrdered接口的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中实现了Ordered接口的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中其他的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行上面所有的BeanDefinitionRegistryPostProcessor的postProcessBeanFactory()方法 执行通过ApplicationContext添加进来的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中实现了PriorityOrdered接口的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中实现了Ordered接口的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中其他的BeanFactoryPostProcessor的postProcessBeanFactory()方法 Lifecycle的使用Lifecycle表示的是ApplicationContext的生命周期，可以定义一个SmartLifecycle来监听ApplicationContext的启动和关闭： 1234567891011121314151617181920212223@Componentpublic class ZhouyuLifecycle implements SmartLifecycle &#123; private boolean isRunning = false; @Override public void start() &#123; System.out.println(&quot;启动&quot;); isRunning = true; &#125; @Override public void stop() &#123; // 要触发stop()，要调用context.close()，或者注册关闭钩子（context.registerShutdownHook();） System.out.println(&quot;停止&quot;); isRunning = false; &#125; @Override public boolean isRunning() &#123; return isRunning; &#125;&#125; 解析配置类解析配置类流程图：https://www.processon.com/view/link/5f9512d5e401fd06fda0b2dd解析配置类思维脑图：https://www.processon.com/view/link/614c83cae0b34d7b342f6d14 在启动Spring时，需要传入一个AppConfig.class给ApplicationContext，ApplicationContext会根据AppConfig类封装为一个BeanDefinition，这种BeanDefinition我们把它称为配置类BeanDefinition。 ConfigurationClassPostProcessor中会把配置类BeanDefinition取出来 构造一个ConfigurationClassParser用来解析配置类BeanDefinition，并且会生成一个配置类对象ConfigurationClass 如果配置类上存在@Component注解，那么解析配置类中的内部类（这里有递归，如果内部类也是配置类的话） 如果配置类上存在@PropertySource注解，那么则解析该注解，并得到PropertySource对象，并添加到environment中去 如果配置类上存在@ComponentScan注解，那么则解析该注解，进行扫描，扫描得到一系列的BeanDefinition对象，然后判断这些BeanDefinition是不是也是配置类BeanDefinition（只要存在@Component注解就是配置类，所以基本上扫描出来的都是配置类），如果是则继续解析该配置类，（也有递归），并且会生成对应的ConfigurationClass 如果配置类上存在@Import注解，那么则判断Import的类的类型： 如果是ImportSelector，那么调用执行selectImports方法得到类名，然后在把这个类当做配置类进行解析（也是递归） 如果是ImportBeanDefinitionRegistrar，那么则生成一个ImportBeanDefinitionRegistrar实例对象，并添加到配置类对象中（ConfigurationClass）的importBeanDefinitionRegistrars属性中。 如果配置类上存在@ImportResource注解，那么则把导入进来的资源路径存在配置类对象中的importedResources属性中。 如果配置类中存在@Bean的方法，那么则把这些方法封装为BeanMethod对象，并添加到配置类对象中的beanMethods属性中。 如果配置类实现了某些接口，则看这些接口内是否定义了@Bean的默认方法 如果配置类有父类，则把父类当做配置类进行解析 AppConfig这个配置类会对应一个ConfigurationClass，同时在解析的过程中也会生成另外的一些ConfigurationClass，接下来就利用reader来进一步解析ConfigurationClass 如果ConfigurationClass是通过@Import注解导入进来的，则把这个类生成一个BeanDefinition，同时解析这个类上@Scope,@Lazy等注解信息，并注册BeanDefinition 如果ConfigurationClass中存在一些BeanMethod，也就是定义了一些@Bean，那么则解析这些@Bean，并生成对应的BeanDefinition，并注册 如果ConfigurationClass中导入了一些资源文件，比如xx.xml，那么则解析这些xx.xml文件，得到并注册BeanDefinition 如果ConfigurationClass中导入了一些ImportBeanDefinitionRegistrar，那么则执行对应的registerBeanDefinitions进行BeanDefinition的注册 总结一下 解析AppConfig类，生成对应的ConfigurationClass 再扫描，扫描到的类都会生成对应的BeanDefinition，并且同时这些类也是ConfigurationClass 再解析ConfigurationClass的其他信息，比如@ImportResource注解的处理，@Import注解的处理，@Bean注解的处理 第四章 依赖注入Spring中到底有几种依赖注入的方式？首先分两种： 手动注入 自动注入 手动注入在XML中定义Bean时，就是手动注入，因为是程序员手动给某个属性指定了值。 123&lt;bean name=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot;&gt; &lt;property name=&quot;orderService&quot; ref=&quot;orderService&quot;/&gt;&lt;/bean&gt; 上面这种底层是通过set方法进行注入。 123&lt;bean name=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;orderService&quot;/&gt;&lt;/bean&gt; 上面这种底层是通过构造方法进行注入。 所以手动注入的底层也就是分为两种： set方法注入 构造方法注入 自动注入自动注入又分为两种： XML的autowire自动注入 @Autowired注解的自动注入 XML的autowire自动注入在XML中，我们可以在定义一个Bean时去指定这个Bean的自动注入模式： byType byName constructor default no 比如： 1&lt;bean id=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot; autowire=&quot;byType&quot;/&gt; 这么写，表示Spring会自动的给userService中所有的属性自动赋值（不需要这个属性上有@Autowired注解，但需要这个属性有对应的set方法）。 在创建Bean的过程中，在填充属性时，Spring会去解析当前类，把当前类的所有方法都解析出来，Spring会去解析每个方法得到对应的PropertyDescriptor对象，PropertyDescriptor中有几个属性： name：这个name并不是方法的名字，而是拿方法名字进过处理后的名字 如果方法名字以“get”开头，比如“getXXX”,那么name&#x3D;XXX 如果方法名字以“is”开头，比如“isXXX”,那么name&#x3D;XXX 如果方法名字以“set”开头，比如“setXXX”,那么name&#x3D;XXX readMethodRef：表示get方法的Method对象的引用 readMethodName：表示get方法的名字 writeMethodRef：表示set方法的Method对象的引用 writeMethodName：表示set方法的名字 propertyTypeRef：如果有get方法那么对应的就是返回值的类型，如果是set方法那么对应的就是set方法中唯一参数的类型 get方法的定义是： 方法参数个数为0个，并且 （方法名字以”get”开头 或者 方法名字以”is”开头并且方法的返回类型为boolean） set方法的定义是：方法参数个数为1个，并且 （方法名字以”set”开头并且方法返回类型为void） 所以，Spring在通过byName的自动填充属性时流程是： 找到所有set方法所对应的XXX部分的名字 根据XXX部分的名字去获取bean Spring在通过byType的自动填充属性时流程是： 获取到set方法中的唯一参数的参数类型，并且根据该类型去容器中获取bean 如果找到多个，会报错。 以上，分析了autowire的byType和byName情况，那么接下来分析constructor，constructor表示通过构造方法注入，其实这种情况就比较简单了，没有byType和byName那么复杂。​ 如果是constructor，那么就可以不写set方法了，当某个bean是通过构造方法来注入时，spring利用构造方法的参数信息从Spring容器中去找bean，找到bean之后作为参数传给构造方法，从而实例化得到一个bean对象，并完成属性赋值（属性赋值的代码得程序员来写）。 我们这里先不考虑一个类有多个构造方法的情况，后面单独讲推断构造方法。我们这里只考虑只有一个有参构造方法。 其实构造方法注入相当于byType+byName，普通的byType是根据set方法中的参数类型去找bean，找到多个会报错，而constructor就是通过构造方法中的参数类型去找bean，如果找到多个会根据参数名确定。 另外两个： no，表示关闭autowire default，表示默认值，我们一直演示的某个bean的autowire，而也可以直接在标签中设置autowire，如果设置了，那么标签中设置的autowire如果为default，那么则会用标签中设置的autowire。 可以发现XML中的自动注入是挺强大的，那么问题来了，为什么我们平时都是用的@Autowired注解呢？而没有用上文说的这种自动注入方式呢？ @Autowired注解相当于XML中的autowire属性的注解方式的替代。这是在官网上有提到的。 1Essentially, the @Autowired annotation provides the same capabilities as described in Autowiring Collaborators but with more fine-grained control and wider applicability 翻译一下：从本质上讲，@Autowired注解提供了与autowire相同的功能，但是拥有更细粒度的控制和更广泛的适用性。 注意：更细粒度的控制。 XML中的autowire控制的是整个bean的所有属性，而@Autowired注解是直接写在某个属性、某个set方法、某个构造方法上的。 再举个例子，如果一个类有多个构造方法，那么如果用XML的autowire&#x3D;constructor，你无法控制到底用哪个构造方法，而你可以用@Autowired注解来直接指定你想用哪个构造方法。 同时，用@Autowired注解，还可以控制，哪些属性想被自动注入，哪些属性不想，这也是细粒度的控制。 但是@Autowired无法区分byType和byName，@Autowired是先byType，如果找到多个则byName。 那么XML的自动注入底层其实也就是: set方法注入 构造方法注入 @Autowired注解的自动注入上文说了@Autowired注解，是byType和byName的结合。 @Autowired注解可以写在： 属性上：先根据属性类型去找Bean，如果找到多个再根据属性名确定一个 构造方法上：先根据方法参数类型去找Bean，如果找到多个再根据参数名确定一个 set方法上：先根据方法参数类型去找Bean，如果找到多个再根据参数名确定一个 而这种底层到了： 属性注入 set方法注入 构造方法注入 寻找注入点在创建一个Bean的过程中，Spring会利用AutowiredAnnotationBeanPostProcessor的**postProcessMergedBeanDefinition()**找出注入点并缓存，找注入点的流程为： 遍历当前类的所有的属性字段Field 查看字段上是否存在@Autowired、@Value、@Inject中的其中任意一个，存在则认为该字段是一个注入点 如果字段是static的，则不进行注入 怎么判断 获取@Autowired中的required属性的值 将字段信息构造成一个AutowiredFieldElement对象，作为一个注入点对象添加到currElements集合中。 遍历当前类的所有方法Method 判断当前Method是否是桥接方法，如果是找到原方法 查看方法上是否存在@Autowired、@Value、@Inject中的其中任意一个，存在则认为该方法是一个注入点 如果方法是static的，则不进行注入 获取@Autowired中的required属性的值 将方法信息构造成一个AutowiredMethodElement对象，作为一个注入点对象添加到currElements集合中。 遍历完当前类的字段和方法后，将遍历父类的，直到没有父类。 最后将currElements集合封装成一个InjectionMetadata对象，作为当前Bean对于的注入点集合对象，并缓存。 static的字段或方法为什么不支持123456@Component@Scope(&quot;prototype&quot;)public class OrderService &#123;&#125; 123456789101112@Component@Scope(&quot;prototype&quot;)public class UserService &#123; @Autowired private static OrderService orderService; public void test() &#123; System.out.println(&quot;test123&quot;); &#125;&#125; 看上面代码，UserService和OrderService都是原型Bean，假设Spring支持static字段进行自动注入，那么现在调用两次 UserService userService1 &#x3D; context.getBean(“userService”) UserService userService2 &#x3D; context.getBean(“userService”) 问此时，userService1的orderService值是什么？还是它自己注入的值吗？​ 答案是不是，一旦userService2 创建好了之后，static orderService字段的值就发生了修改了，从而出现bug。 桥接方法12345678910111213141516171819public interface UserInterface&lt;T&gt; &#123; void setOrderService(T t);&#125;@Componentpublic class UserService implements UserInterface&lt;OrderService&gt; &#123; private OrderService orderService; @Override @Autowired public void setOrderService(OrderService orderService) &#123; this.orderService = orderService; &#125; public void test() &#123; System.out.println(&quot;test123&quot;); &#125;&#125; UserService对应的字节码为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// class version 52.0 (52)// access flags 0x21// signature Ljava/lang/Object;Lcom/zhouyu/service/UserInterface&lt;Lcom/zhouyu/service/OrderService;&gt;;// declaration: com/zhouyu/service/UserService implements com.zhouyu.service.UserInterface&lt;com.zhouyu.service.OrderService&gt;public class com/zhouyu/service/UserService implements com/zhouyu/service/UserInterface &#123; // compiled from: UserService.java @Lorg/springframework/stereotype/Component;() // access flags 0x2 private Lcom/zhouyu/service/OrderService; orderService // access flags 0x1 public &lt;init&gt;()V L0 LINENUMBER 12 L0 ALOAD 0 INVOKESPECIAL java/lang/Object.&lt;init&gt; ()V RETURN L1 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L1 0 MAXSTACK = 1 MAXLOCALS = 1 // access flags 0x1 public setOrderService(Lcom/zhouyu/service/OrderService;)V @Lorg/springframework/beans/factory/annotation/Autowired;() L0 LINENUMBER 19 L0 ALOAD 0 ALOAD 1 PUTFIELD com/zhouyu/service/UserService.orderService : Lcom/zhouyu/service/OrderService; L1 LINENUMBER 20 L1 RETURN L2 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L2 0 LOCALVARIABLE orderService Lcom/zhouyu/service/OrderService; L0 L2 1 MAXSTACK = 2 MAXLOCALS = 2 // access flags 0x1 public test()V L0 LINENUMBER 23 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; LDC &quot;test123&quot; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L1 LINENUMBER 24 L1 RETURN L2 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L2 0 MAXSTACK = 2 MAXLOCALS = 1 // access flags 0x1041 public synthetic bridge setOrderService(Ljava/lang/Object;)V @Lorg/springframework/beans/factory/annotation/Autowired;() L0 LINENUMBER 11 L0 ALOAD 0 ALOAD 1 CHECKCAST com/zhouyu/service/OrderService INVOKEVIRTUAL com/zhouyu/service/UserService.setOrderService (Lcom/zhouyu/service/OrderService;)V RETURN L1 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L1 0 MAXSTACK = 2 MAXLOCALS = 2&#125; 可以看到在UserSerivce的字节码中有两个setOrderService方法： public setOrderService(Lcom&#x2F;zhouyu&#x2F;service&#x2F;OrderService;)V public synthetic bridge setOrderService(Ljava&#x2F;lang&#x2F;Object;)V 并且都是存在@Autowired注解的。​ 所以在Spring中需要处理这种情况，当遍历到桥接方法时，得找到原方法。 注入点进行注入Spring在AutowiredAnnotationBeanPostProcessor的**postProcessProperties()**方法中，会遍历所找到的注入点依次进行注入。​ 字段注入 遍历所有的AutowiredFieldElement对象。 将对应的字段封装为DependencyDescriptor对象。 调用BeanFactory的resolveDependency()方法，传入DependencyDescriptor对象，进行依赖查找，找到当前字段所匹配的Bean对象。 将DependencyDescriptor对象和所找到的结果对象beanName封装成一个ShortcutDependencyDescriptor对象作为缓存，比如如果当前Bean是原型Bean，那么下次再来创建该Bean时，就可以直接拿缓存的结果对象beanName去BeanFactory中去那bean对象了，不用再次进行查找了 利用反射将结果对象赋值给字段。 Set方法注入 遍历所有的AutowiredMethodElement对象 遍历将对应的方法的参数，将每个参数封装成MethodParameter对象 将MethodParameter对象封装为DependencyDescriptor对象 调用BeanFactory的resolveDependency()方法，传入DependencyDescriptor对象，进行依赖查找，找到当前方法参数所匹配的Bean对象。 将DependencyDescriptor对象和所找到的结果对象beanName封装成一个ShortcutDependencyDescriptor对象作为缓存，比如如果当前Bean是原型Bean，那么下次再来创建该Bean时，就可以直接拿缓存的结果对象beanName去BeanFactory中去那bean对象了，不用再次进行查找了 利用反射将找到的所有结果对象传给当前方法，并执行。 上节课我们讲了Spring中的自动注入(byName,byType)和@Autowired注解的工作原理以及源码分析，那么今天这节课，我们来分析还没讲完的，剩下的核心的方法： 123@NullableObject resolveDependency(DependencyDescriptor descriptor, @Nullable String requestingBeanName, @Nullable Set&lt;String&gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException; 该方法表示，传入一个依赖描述（DependencyDescriptor），该方法会根据该依赖描述从BeanFactory中找出对应的唯一的一个Bean对象。 下面来分析一下DefaultListableBeanFactory中resolveDependency()方法的具体实现，具体流程图：https://www.processon.com/view/link/5f8d3c895653bb06ef076688 findAutowireCandidates()实现根据类型找beanName的底层流程：https://www.processon.com/view/link/6135bb430e3e7412ecd5d1f2对应执行流程图为：https://www.processon.com/view/link/5f8fdfa8e401fd06fd984f20​ 找出BeanFactory中类型为type的所有的Bean的名字，注意是名字，而不是Bean对象，因为我们可以根据BeanDefinition就能判断和当前type是不是匹配，不用生成Bean对象 把resolvableDependencies中key为type的对象找出来并添加到result中 遍历根据type找出的beanName，判断当前beanName对应的Bean是不是能够被自动注入 先判断beanName对应的BeanDefinition中的autowireCandidate属性，如果为false，表示不能用来进行自动注入，如果为true则继续进行判断 判断当前type是不是泛型，如果是泛型是会把容器中所有的beanName找出来的，如果是这种情况，那么在这一步中就要获取到泛型的真正类型，然后进行匹配，如果当前beanName和当前泛型对应的真实类型匹配，那么则继续判断 如果当前DependencyDescriptor上存在@Qualifier注解，那么则要判断当前beanName上是否定义了Qualifier，并且是否和当前DependencyDescriptor上的Qualifier相等，相等则匹配 经过上述验证之后，当前beanName才能成为一个可注入的，添加到result中 关于依赖注入中泛型注入的实现首先在Java反射中，有一个Type接口，表示类型，具体分类为： raw types：也就是普通Class parameterized types：对应ParameterizedType接口，泛型类型 array types：对应GenericArrayType，泛型数组 type variables：对应TypeVariable接口，表示类型变量，也就是所定义的泛型，比如T、K primitive types：基本类型，int、boolean 大家可以好好看看下面代码所打印的结果：​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class TypeTest&lt;T&gt; &#123; private int i; private Integer it; private int[] iarray; private List list; private List&lt;String&gt; slist; private List&lt;T&gt; tlist; private T t; private T[] tarray; public static void main(String[] args) throws NoSuchFieldException &#123; test(TypeTest.class.getDeclaredField(&quot;i&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;it&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;iarray&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;list&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;slist&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;tlist&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;t&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;tarray&quot;)); &#125; public static void test(Field field) &#123; if (field.getType().isPrimitive()) &#123; System.out.println(field.getName() + &quot;是基本数据类型&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是基本数据类型&quot;); &#125; if (field.getGenericType() instanceof ParameterizedType) &#123; System.out.println(field.getName() + &quot;是泛型类型&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型类型&quot;); &#125; if (field.getType().isArray()) &#123; System.out.println(field.getName() + &quot;是普通数组&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是普通数组&quot;); &#125; if (field.getGenericType() instanceof GenericArrayType) &#123; System.out.println(field.getName() + &quot;是泛型数组&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型数组&quot;); &#125; if (field.getGenericType() instanceof TypeVariable) &#123; System.out.println(field.getName() + &quot;是泛型变量&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型变量&quot;); &#125; &#125;&#125; Spring中，但注入点是一个泛型时，也是会进行处理的，比如：​ 1234567891011121314151617@Componentpublic class UserService extends BaseService&lt;OrderService, StockService&gt; &#123; public void test() &#123; System.out.println(o); &#125;&#125;public class BaseService&lt;O, S&gt; &#123; @Autowired protected O o; @Autowired protected S s;&#125; Spring扫描时发现UserService是一个Bean 那就取出注入点，也就是BaseService中的两个属性o、s 接下来需要按注入点类型进行注入，但是o和s都是泛型，所以Spring需要确定o和s的具体类型。 因为当前正在创建的是UserService的Bean，所以可以通过userService.getClass().getGenericSuperclass().getTypeName()获取到具体的泛型信息，比如com.zhouyu.service.BaseService&lt;com.zhouyu.service.OrderService, com.zhouyu.service.StockService&gt; 然后再拿到UserService的父类BaseService的泛型变量： for (TypeVariable&lt;? extends Class&lt;?&gt;&gt; typeParameter : userService.getClass().getSuperclass().getTypeParameters()) &#123; System._out_.println(typeParameter.getName()); &#125; 通过上面两段代码，就能知道，o对应的具体就是OrderService，s对应的具体类型就是StockService 然后再调用oField.getGenericType()就知道当前field使用的是哪个泛型，就能知道具体类型了 @Qualifier的使用定义两个注解： 12345678910@Target(&#123;ElementType.TYPE, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifier(&quot;random&quot;)public @interface Random &#123;&#125;@Target(&#123;ElementType.TYPE, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifier(&quot;roundRobin&quot;)public @interface RoundRobin &#123;&#125; 定义一个接口和两个实现类，表示负载均衡： 123public interface LoadBalance &#123; String select();&#125; 123456789101112131415161718@Component@Randompublic class RandomStrategy implements LoadBalance &#123; @Override public String select() &#123; return null; &#125;&#125;@Component@RoundRobinpublic class RoundRobinStrategy implements LoadBalance &#123; @Override public String select() &#123; return null; &#125;&#125; 使用： 123456789101112@Componentpublic class UserService &#123; @Autowired @RoundRobin private LoadBalance loadBalance; public void test() &#123; System.out.println(loadBalance); &#125;&#125; @Resource@Resource注解底层工作流程图：https://www.processon.com/view/link/5f91275f07912906db381f6e 第五章 循环依赖什么是循环依赖？很简单，就是A对象依赖了B对象，B对象依赖了A对象。 比如： 123456789// A依赖了Bclass A&#123; public B b;&#125;// B依赖了Aclass B&#123; public A a;&#125; 那么循环依赖是个问题吗？ 如果不考虑Spring，循环依赖并不是问题，因为对象之间相互依赖是很正常的事情。 比如 12345A a = new A();B b = new B();a.b = b;b.a = a; 这样，A,B就依赖上了。 但是，在Spring中循环依赖就是一个问题了，为什么？因为，在Spring中，一个对象并不是简单new出来了，而是会经过一系列的Bean的生命周期，就是因为Bean的生命周期所以才会出现循环依赖问题。当然，在Spring中，出现循环依赖的场景很多，有的场景Spring自动帮我们解决了，而有的场景则需要程序员来解决，下文详细来说。 要明白Spring中的循环依赖，得先明白Spring中Bean的生命周期。 Bean的生命周期这里不会对Bean的生命周期进行详细的描述，只描述一下大概的过程。 Bean的生命周期指的就是：在Spring中，Bean是如何生成的？ 被Spring管理的对象叫做Bean。Bean的生成步骤如下： Spring扫描class得到BeanDefinition 根据得到的BeanDefinition去生成bean 首先根据class推断构造方法 根据推断出来的构造方法，反射，得到一个对象（暂时叫做原始对象） 填充原始对象中的属性（依赖注入） 如果原始对象中的某个方法被AOP了，那么则需要根据原始对象生成一个代理对象 把最终生成的代理对象放入单例池（源码中叫做singletonObjects）中，下次getBean时就直接从单例池拿即可 可以看到，对于Spring中的Bean的生成过程，步骤还是很多的，并且不仅仅只有上面的7步，还有很多很多，比如Aware回调、初始化等等，这里不详细讨论。 可以发现，在Spring中，构造一个Bean，包括了new这个步骤（第4步构造方法反射）。 得到一个原始对象后，Spring需要给对象中的属性进行依赖注入，那么这个注入过程是怎样的？ 比如上文说的A类，A类中存在一个B类的b属性，所以，当A类生成了一个原始对象之后，就会去给b属性去赋值，此时就会根据b属性的类型和属性名去BeanFactory中去获取B类所对应的单例bean。如果此时BeanFactory中存在B对应的Bean，那么直接拿来赋值给b属性；如果此时BeanFactory中不存在B对应的Bean，则需要生成一个B对应的Bean，然后赋值给b属性。 问题就出现在第二种情况，如果此时B类在BeanFactory中还没有生成对应的Bean，那么就需要去生成，就会经过B的Bean的生命周期。 那么在创建B类的Bean的过程中，如果B类中存在一个A类的a属性，那么在创建B的Bean的过程中就需要A类对应的Bean，但是，触发B类Bean的创建的条件是A类Bean在创建过程中的依赖注入，所以这里就出现了循环依赖： ABean创建–&gt;依赖了B属性–&gt;触发BBean创建—&gt;B依赖了A属性—&gt;需要ABean（但ABean还在创建过程中） 从而导致ABean创建不出来，BBean也创建不出来。 这是循环依赖的场景，但是上文说了，在Spring中，通过某些机制帮开发者解决了部分循环依赖的问题，这个机制就是三级缓存。 三级缓存三级缓存是通用的叫法。一级缓存为：singletonObjects二级缓存为：earlySingletonObjects三级缓存为：singletonFactories​ 先稍微解释一下这三个缓存的作用，后面详细分析： singletonObjects中缓存的是已经经历了完整生命周期的bean对象。 earlySingletonObjects比singletonObjects多了一个early，表示缓存的是早期的bean对象。早期是什么意思？表示Bean的生命周期还没走完就把这个Bean放入了earlySingletonObjects。 singletonFactories中缓存的是ObjectFactory，表示对象工厂，表示用来创建早期bean对象的工厂。 解决循环依赖思路分析先来分析为什么缓存能解决循环依赖。 上文分析得到，之所以产生循环依赖的问题，主要是： A创建时—&gt;需要B—-&gt;B去创建—&gt;需要A，从而产生了循环 那么如何打破这个循环，加个中间人（缓存） A的Bean在创建过程中，在进行依赖注入之前，先把A的原始Bean放入缓存（提早暴露，只要放到缓存了，其他Bean需要时就可以从缓存中拿了），放入缓存后，再进行依赖注入，此时A的Bean依赖了B的Bean，如果B的Bean不存在，则需要创建B的Bean，而创建B的Bean的过程和A一样，也是先创建一个B的原始对象，然后把B的原始对象提早暴露出来放入缓存中，然后在对B的原始对象进行依赖注入A，此时能从缓存中拿到A的原始对象（虽然是A的原始对象，还不是最终的Bean），B的原始对象依赖注入完了之后，B的生命周期结束，那么A的生命周期也能结束。 因为整个过程中，都只有一个A原始对象，所以对于B而言，就算在属性注入时，注入的是A原始对象，也没有关系，因为A原始对象在后续的生命周期中在堆中没有发生变化。 从上面这个分析过程中可以得出，只需要一个缓存就能解决循环依赖了，那么为什么Spring中还需要singletonFactories呢？ 这是难点，基于上面的场景想一个问题：如果A的原始对象注入给B的属性之后，A的原始对象进行了AOP产生了一个代理对象，此时就会出现，对于A而言，它的Bean对象其实应该是AOP之后的代理对象，而B的a属性对应的并不是AOP之后的代理对象，这就产生了冲突。 B依赖的A和最终的A不是同一个对象。 AOP就是通过一个BeanPostProcessor来实现的，这个BeanPostProcessor就是AnnotationAwareAspectJAutoProxyCreator，它的父类是AbstractAutoProxyCreator，而在Spring中AOP利用的要么是JDK动态代理，要么CGLib的动态代理，所以如果给一个类中的某个方法设置了切面，那么这个类最终就需要生成一个代理对象。 一般过程就是：A类—&gt;生成一个普通对象–&gt;属性注入–&gt;基于切面生成一个代理对象–&gt;把代理对象放入singletonObjects单例池中。 而AOP可以说是Spring中除开IOC的另外一大功能，而循环依赖又是属于IOC范畴的，所以这两大功能想要并存，Spring需要特殊处理。 如何处理的，就是利用了第三级缓存singletonFactories。 首先，singletonFactories中存的是某个beanName对应的ObjectFactory，在bean的生命周期中，生成完原始对象之后，就会构造一个ObjectFactory存入singletonFactories中。这个ObjectFactory是一个函数式接口，所以支持Lambda表达式：**() -&gt; getEarlyBeanReference(beanName, mbd, bean)** 上面的Lambda表达式就是一个ObjectFactory，执行该Lambda表达式就会去执行getEarlyBeanReference方法，而该方法如下： 123456789101112protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; 该方法会去执行SmartInstantiationAwareBeanPostProcessor中的getEarlyBeanReference方法，而这个接口下的实现类中只有两个类实现了这个方法，一个是AbstractAutoProxyCreator，一个是InstantiationAwareBeanPostProcessorAdapter，它的实现如下： 123456789101112// InstantiationAwareBeanPostProcessorAdapter@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) throws BeansException &#123; return bean;&#125;// AbstractAutoProxyCreator@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 在整个Spring中，默认就只有AbstractAutoProxyCreator真正意义上实现了getEarlyBeanReference方法，而该类就是用来进行AOP的。上文提到的AnnotationAwareAspectJAutoProxyCreator的父类就是AbstractAutoProxyCreator。 那么getEarlyBeanReference方法到底在干什么？首先得到一个cachekey，cachekey就是beanName。然后把beanName和bean（这是原始对象）存入earlyProxyReferences中调用wrapIfNecessary进行AOP，得到一个代理对象。 那么，什么时候会调用getEarlyBeanReference方法呢？回到循环依赖的场景中 左边文字：这个ObjectFactory就是上文说的labmda表达式，中间有getEarlyBeanReference方法，注意存入singletonFactories时并不会执行lambda表达式，也就是不会执行getEarlyBeanReference方法 右边文字：从singletonFactories根据beanName得到一个ObjectFactory，然后执行ObjectFactory，也就是执行getEarlyBeanReference方法，此时会得到一个A原始对象经过AOP之后的代理对象，然后把该代理对象放入earlySingletonObjects中，注意此时并没有把代理对象放入singletonObjects中，那什么时候放入到singletonObjects中呢？ 我们这个时候得来理解一下earlySingletonObjects的作用，此时，我们只得到了A原始对象的代理对象，这个对象还不完整，因为A原始对象还没有进行属性填充，所以此时不能直接把A的代理对象放入singletonObjects中，所以只能把代理对象放入earlySingletonObjects，假设现在有其他对象依赖了A，那么则可以从earlySingletonObjects中得到A原始对象的代理对象了，并且是A的同一个代理对象。 当B创建完了之后，A继续进行生命周期，而A在完成属性注入后，会按照它本身的逻辑去进行AOP，而此时我们知道A原始对象已经经历过了AOP，所以对于A本身而言，不会再去进行AOP了，那么怎么判断一个对象是否经历过了AOP呢？会利用上文提到的earlyProxyReferences，在AbstractAutoProxyCreator的postProcessAfterInitialization方法中，会去判断当前beanName是否在earlyProxyReferences，如果在则表示已经提前进行过AOP了，无需再次进行AOP。 对于A而言，进行了AOP的判断后，以及BeanPostProcessor的执行之后，就需要把A对应的对象放入singletonObjects中了，但是我们知道，应该是要把A的代理对象放入singletonObjects中，所以此时需要从earlySingletonObjects中得到代理对象，然后入singletonObjects中。 整个循环依赖解决完毕。 总结至此，总结一下三级缓存： singletonObjects：缓存经过了完整生命周期的bean earlySingletonObjects：缓存未经过完整生命周期的bean，如果某个bean出现了循环依赖，就会提前把这个暂时未经过完整生命周期的bean放入earlySingletonObjects中，这个bean如果要经过AOP，那么就会把代理对象放入earlySingletonObjects中，否则就是把原始对象放入earlySingletonObjects，但是不管怎么样，就是是代理对象，代理对象所代理的原始对象也是没有经过完整生命周期的，所以放入earlySingletonObjects我们就可以统一认为是未经过完整生命周期的bean。 singletonFactories：缓存的是一个ObjectFactory，也就是一个Lambda表达式。在每个Bean的生成过程中，经过实例化得到一个原始对象后，都会提前基于原始对象暴露一个Lambda表达式，并保存到三级缓存中，这个Lambda表达式可能用到，也可能用不到，如果当前Bean没有出现循环依赖，那么这个Lambda表达式没用，当前bean按照自己的生命周期正常执行，执行完后直接把当前bean放入singletonObjects中，如果当前bean在依赖注入时发现出现了循环依赖（当前正在创建的bean被其他bean依赖了），则从三级缓存中拿到Lambda表达式，并执行Lambda表达式得到一个对象，并把得到的对象放入二级缓存（(如果当前Bean需要AOP，那么执行lambda表达式，得到就是对应的代理对象，如果无需AOP，则直接得到一个原始对象)）。 其实还要一个缓存，就是earlyProxyReferences，它用来记录某个原始对象是否进行过AOP了。 反向分析一下singletonFactories为什么需要singletonFactories？假设没有singletonFactories，只有earlySingletonObjects，earlySingletonObjects是二级缓存，它内部存储的是为经过完整生命周期的bean对象，Spring原有的流程是出现了循环依赖的情况下： 先从singletonFactories中拿到lambda表达式，这里肯定是能拿到的，因为每个bean实例化之后，依赖注入之前，就会生成一个lambda表示放入singletonFactories中 执行lambda表达式，得到结果，将结果放入earlySingletonObjects中 那如果没有singletonFactories，该如何把原始对象或AOP之后的代理对象放入earlySingletonObjects中呢？何时放入呢？​ 首先，将原始对象或AOP之后的代理对象放入earlySingletonObjects中的有两种： 实例化之后，依赖注入之前：如果是这样，那么对于每个bean而言，都是在依赖注入之前会去进行AOP，这是不符合bean生命周期步骤的设计的。 真正发现某个bean出现了循环依赖时：按现在Spring源码的流程来说，就是getSingleton(String beanName, boolean allowEarlyReference)中，是在这个方法中判断出来了当前获取的这个bean在创建中，就表示获取的这个bean出现了循环依赖，那在这个方法中该如何拿到原始对象呢？更加重要的是，该如何拿到AOP之后的代理对象呢？难道在这个方法中去循环调用BeanPostProcessor的初始化后的方法吗？不是做不到，不太合适，代码太丑。最关键的是在这个方法中该如何拿到原始对象呢？还是得需要一个Map，预习把这个Bean实例化后的对象存在这个Map中，那这样的话还不如直接用第一种方案，但是第一种又直接打破了Bean生命周期的设计。 所以，我们可以发现，现在Spring所用的singletonFactories，为了调和不同的情况，在singletonFactories中存的是lambda表达式，这样的话，只有在出现了循环依赖的情况，才会执行lambda表达式，才会进行AOP，也就说只有在出现了循环依赖的情况下才会打破Bean生命周期的设计，如果一个Bean没有出现循环依赖，那么它还是遵守了Bean的生命周期的设计的。 推断构造方法流程图：https://www.processon.com/view/link/5f97bc717d9c0806f291d7eb​ AutowiredAnnotationBeanPostProcessor中推断构造方法不同情况思维脑图：https://www.processon.com/view/link/6146def57d9c08198c58bb26​ Spring中的一个bean，需要实例化得到一个对象，而实例化就需要用到构造方法。 一般情况下，一个类只有一个构造方法： 要么是无参的构造方法 要么是有参的构造方法 如果只有一个无参的构造方法，那么实例化就只能使用这个构造方法了。如果只有一个有参的构造方法，那么实例化时能使用这个构造方法吗？要分情况讨论： 使用AnnotationConfigApplicationContext，会使用这个构造方法进行实例化，那么Spring会根据构造方法的参数信息去寻找bean，然后传给构造方法 使用ClassPathXmlApplicationContext，表示使用XML的方式来使用bean，要么在XML中指定构造方法的参数值(手动指定)，要么配置autowire&#x3D;constructor让Spring自动去寻找bean做为构造方法参数值。 上面是只有一个构造方法的情况，那么如果有多个构造方法呢？ 又分为两种情况，多个构造方法中存不存在无参的构造方法。 分析：一个类存在多个构造方法，那么Spring进行实例化之前，该如何去确定到底用哪个构造方法呢？ 如果开发者指定了想要使用的构造方法，那么就用这个构造方法 如果开发者没有指定想要使用的构造方法，则看开发者有没有让Spring自动去选择构造方法 如果开发者也没有让Spring自动去选择构造方法，则Spring利用无参构造方法，如果没有无参构造方法，则报错 针对第一点，开发者可以通过什么方式来指定使用哪个构造方法呢？ xml中的标签，这个标签表示构造方法参数，所以可以根据这个确定想要使用的构造方法的参数个数，从而确定想要使用的构造方法 通过@Autowired注解，@Autowired注解可以写在构造方法上，所以哪个构造方法上写了@Autowired注解，表示开发者想使用哪个构造方法，当然，它和第一个方式的不同点是，通过xml的方式，我们直接指定了构造方法的参数值，而通过@Autowired注解的方式，需要Spring通过byType+byName的方式去找到符合条件的bean作为构造方法的参数值 再来看第二点，如果开发者没有指定想要使用的构造方法，则看开发者有没有让Spring自动去选择构造方法，对于这一点，只能用在ClassPathXmlApplicationContext，因为通过AnnotationConfigApplicationContext没有办法去指定某个bean可以自动去选择构造方法，而通过ClassPathXmlApplicationContext可以在xml中指定某个bean的autowire为constructor，虽然这个属性表示通过构造方法自动注入，所以需要自动的去选择一个构造方法进行自动注入，因为是构造方法，所以顺便是进行实例化。 当然，还有一种情况，就是多个构造方法上写了@Autowired注解，那么此时Spring会报错。但是，因为@Autowired还有一个属性required，默认为ture，所以一个类中，只有能一个构造方法标注了@Autowired或@Autowired（required&#x3D;true），有多个会报错。但是可以有多个@Autowired（required&#x3D;false）,这种情况下，需要Spring从这些构造方法中去自动选择一个构造方法。 源码思路 AbstractAutowireCapableBeanFactory类中的createBeanInstance()方法会去创建一个Bean实例 根据BeanDefinition加载类得到Class对象 如果BeanDefinition绑定了一个Supplier，那就调用Supplier的get方法得到一个对象并直接返回 如果BeanDefinition中存在factoryMethodName，那么就调用该工厂方法得到一个bean对象并返回 如果BeanDefinition已经自动构造过了，那就调用autowireConstructor()自动构造一个对象 调用SmartInstantiationAwareBeanPostProcessor的determineCandidateConstructors()方法得到哪些构造方法是可以用的 如果存在可用得构造方法，或者当前BeanDefinition的autowired是AUTOWIRE_CONSTRUCTOR，或者BeanDefinition中指定了构造方法参数值，或者创建Bean的时候指定了构造方法参数值，那么就调用**autowireConstructor()**方法自动构造一个对象 最后，如果不是上述情况，就根据无参的构造方法实例化一个对象 autowireConstructor() 先检查是否指定了具体的构造方法和构造方法参数值，或者在BeanDefinition中缓存了具体的构造方法或构造方法参数值，如果存在那么则直接使用该构造方法进行实例化 如果没有确定的构造方法或构造方法参数值，那么 如果没有确定的构造方法，那么则找出类中所有的构造方法 如果只有一个无参的构造方法，那么直接使用无参的构造方法进行实例化 如果有多个可用的构造方法或者当前Bean需要自动通过构造方法注入 根据所指定的构造方法参数值，确定所需要的最少的构造方法参数值的个数 对所有的构造方法进行排序，参数个数多的在前面 遍历每个构造方法 如果不是调用getBean方法时所指定的构造方法参数值，那么则根据构造方法参数类型找值 如果时调用getBean方法时所指定的构造方法参数值，就直接利用这些值 如果根据当前构造方法找到了对应的构造方法参数值，那么这个构造方法就是可用的，但是不一定这个构造方法就是最佳的，所以这里会涉及到是否有多个构造方法匹配了同样的值，这个时候就会用值和构造方法类型进行匹配程度的打分，找到一个最匹配的 为什么分越少优先级越高？主要是计算找到的bean和构造方法参数类型匹配程度有多高。 假设bean的类型为A，A的父类是B，B的父类是C，同时A实现了接口D如果构造方法的参数类型为A，那么完全匹配，得分为0如果构造方法的参数类型为B，那么得分为2如果构造方法的参数类型为C，那么得分为4如果构造方法的参数类型为D，那么得分为1 可以直接使用如下代码进行测试： 12345678910111213Object[] objects = new Object[]&#123;new A()&#125;;// 0System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;A.class&#125;, objects));// 2System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;B.class&#125;, objects));// 4System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;C.class&#125;, objects));// 1System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;D.class&#125;, objects)); 所以，我们可以发现，越匹配分数越低。 @Bean的情况首先，Spring会把@Bean修饰的方法解析成BeanDefinition： 如果方法是static的，那么解析出来的BeanDefinition中： factoryBeanName为AppConfig所对应的beanName，比如”appConfig” factoryMethodName为对应的方法名，比如”aService” factoryClass为AppConfig.class 如果方法不是static的，那么解析出来的BeanDefinition中： factoryBeanName为null factoryMethodName为对应的方法名，比如”aService” factoryClass也为AppConfig.class 在由@Bean生成的BeanDefinition中，有一个重要的属性isFactoryMethodUnique，表示factoryMethod是不是唯一的，在普通情况下@Bean生成的BeanDefinition的isFactoryMethodUnique为true，但是如果出现了方法重载，那么就是特殊的情况，比如： 123456789@Beanpublic static AService aService()&#123; return new AService();&#125;@Beanpublic AService aService(BService bService)&#123; return new AService();&#125; 虽然有两个@Bean，但是肯定只会生成一个aService的Bean，那么Spring在处理@Bean时，也只会生成一个aService的BeanDefinition，比如Spring先解析到第一个@Bean，会生成一个BeanDefinition，此时isFactoryMethodUnique为true，但是解析到第二个@Bean时，会判断出来beanDefinitionMap中已经存在一个aService的BeanDefinition了，那么会把之前的这个BeanDefinition的isFactoryMethodUnique修改为false，并且不会生成新的BeanDefinition了。​ 并且后续在根据BeanDefinition创建Bean时，会根据isFactoryMethodUnique来操作，如果为true，那就表示当前BeanDefinition只对应了一个方法，那也就是只能用这个方法来创建Bean了，但是如果isFactoryMethodUnique为false，那就表示当前BeanDefition对应了多个方法，需要和推断构造方法的逻辑一样，去选择用哪个方法来创建Bean。 第六章 Spring之整合Mybatis mybatis-spring jar包 JDK动态代理返回xxxMapper代理类 注册组件的整合 整合核心思路由很多框架都需要和Spring进行整合，而整合的核心思想就是把其他框架所产生的对象放到Spring容器中，让其成为Bean。 \\ 比如Mybatis，Mybatis框架可以单独使用，而单独使用Mybatis框架就需要用到Mybatis所提供的一些类构造出对应的对象，然后使用该对象，就能使用到Mybatis框架给我们提供的功能，和Mybatis整合Spring就是为了将这些对象放入Spring容器中成为Bean，只要成为了Bean，在我们的Spring项目中就能很方便的使用这些对象了，也就能很方便的使用Mybatis框架所提供的功能了。 实现简易mybatismybatis使用&#x2F;需求分析 根据类型注入Mapper增强类型(将增强类型注册到容器) 多个Mapper写活 给构造器指定值 通过后置处理器增强 写死不好,想要拿到扫描路径 也可以写 这个 接口会给你一些工具 比如这个注解原信息 spring自带Scanner 但是不关心接口 ,可mybatis只关心接口 需要重写扫描器的逻辑 两个判断 带component和另一个sbd才能被扫描器扫入 mybatis解决: 不符合要求 Beandefinition里是接口mapper 需要 是factoryBean里的代理类 解决 Mybatis-Spring 1.3.2版本底层源码执行流程 通过@MapperScan导入了MapperScannerRegistrar类 MapperScannerRegistrar类实现了ImportBeanDefinitionRegistrar接口，所以Spring在启动时会调用MapperScannerRegistrar类中的registerBeanDefinitions方法 在registerBeanDefinitions方法中定义了一个ClassPathMapperScanner对象，用来扫描mapper 设置ClassPathMapperScanner对象可以扫描到接口，因为在Spring中是不会扫描接口的 同时因为ClassPathMapperScanner中重写了isCandidateComponent方法，导致isCandidateComponent只会认为接口是备选者Component 通过利用Spring的扫描后，会把接口扫描出来并且得到对应的BeanDefinition 接下来把扫描得到的BeanDefinition进行修改，把BeanClass修改为MapperFactoryBean，把AutowireMode修改为byType 扫描完成后，Spring就会基于BeanDefinition去创建Bean了，相当于每个Mapper对应一个FactoryBean 在MapperFactoryBean中的getObject方法中，调用了getSqlSession()去得到一个sqlSession对象，然后根据对应的Mapper接口生成一个Mapper接口代理对象，这个代理对象就成为Spring容器中的Bean sqlSession对象是Mybatis中的，一个sqlSession对象需要SqlSessionFactory来产生 MapperFactoryBean的AutowireMode为byType，所以Spring会自动调用set方法，有两个set方法，一个setSqlSessionFactory，一个setSqlSessionTemplate，而这两个方法执行的前提是根据方法参数类型能找到对应的bean，所以Spring容器中要存在SqlSessionFactory类型的bean或者SqlSessionTemplate类型的bean。 如果你定义的是一个SqlSessionFactory类型的bean，那么最终也会被包装为一个SqlSessionTemplate对象，并且赋值给sqlSession属性 而在SqlSessionTemplate类中就存在一个getMapper方法，这个方法中就产生一个Mapper接口代理对象 到时候，当执行该代理对象的某个方法时，就会进入到Mybatis框架的底层执行流程，详细的请看下图 Spring整合Mybatis之后SQL执行流程：https://www.processon.com/view/link/6152cc385653bb6791db436c Mybatis-Spring 2.0.6版本(最新版)底层源码执行流程 通过@MapperScan导入了MapperScannerRegistrar类 MapperScannerRegistrar类实现了ImportBeanDefinitionRegistrar接口，所以Spring在启动时会调用MapperScannerRegistrar类中的registerBeanDefinitions方法 在registerBeanDefinitions方法中注册一个MapperScannerConfigurer类型的BeanDefinition 而MapperScannerConfigurer实现了BeanDefinitionRegistryPostProcessor接口，所以Spring在启动过程中时会调用它的postProcessBeanDefinitionRegistry()方法 在postProcessBeanDefinitionRegistry方法中会生成一个ClassPathMapperScanner对象，然后进行扫描 后续的逻辑和1.3.2版本一样。 带来的好处是，可以不使用@MapperScan注解，而可以直接定义一个Bean，比如： 123456@Beanpublic MapperScannerConfigurer mapperScannerConfigurer() &#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setBasePackage(&quot;com.luban&quot;); return mapperScannerConfigurer;&#125; Spring整合Mybatis后一级缓存失效问题先看下图：Spring整合Mybatis之后SQL执行流程：https://www.processon.com/view/link/6152cc385653bb6791db436c​ Mybatis中的一级缓存是基于SqlSession来实现的，所以在执行同一个sql时，如果使用的是同一个SqlSession对象，那么就能利用到一级缓存，提高sql的执行效率。​ 但是在Spring整合Mybatis后，如果没有执行某个方法时，该方法上没有加@Transactional注解，也就是没有开启Spring事务，那么后面在执行具体sql时，没执行一个sql时都会新生成一个SqlSession对象来执行该sql，这就是我们说的一级缓存失效（也就是没有使用同一个SqlSession对象），而如果开启了Spring事务，那么该Spring事务中的多个sql，在执行时会使用同一个SqlSession对象，从而一级缓存生效，具体的底层执行流程在上图。​ 个人理解：实际上Spring整合Mybatis后一级缓存失效并不是问题，是正常的实现，因为，一个方法如果没有开启Spring事务，那么在执行sql时候，那就是每个sql单独一个事务来执行，也就是单独一个SqlSession对象来执行该sql，如果开启了Spring事务，那就是多个sql属于同一个事务，那自然就应该用一个SqlSession来执行这多个sql。所以，在没有开启Spring事务的时候，SqlSession的一级缓存并不是失效了，而是存在的生命周期太短了（执行完一个sql后就被销毁了，下一个sql执行时又是一个新的SqlSession了）。​ 第七章 AOP动态代理代理模式的解释：为其他对象提供一种代理以控制对这个对象的访问，增强一个类中的某个方法，对程序进行扩展。 比如，现在存在一个UserService类： 1234567public class UserService &#123; public void test() &#123; System.out.println(&quot;test...&quot;); &#125;&#125; 此时，我们new一个UserService对象，然后执行test()方法，结果是显而易见的。​ 如果我们现在想在不修改UserService类的源码前提下，给test()增加额外逻辑，那么就可以使用动态代理机制来创建UserService对象了，比如： 12345678910111213141516171819202122UserService target = new UserService();// 通过cglib技术Enhancer enhancer = new Enhancer();enhancer.setSuperclass(UserService.class);// 定义额外逻辑，也就是代理逻辑enhancer.setCallbacks(new Callback[]&#123;new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = methodProxy.invoke(target, objects); System.out.println(&quot;after...&quot;); return result; &#125;&#125;&#125;);// 动态代理所创建出来的UserService对象UserService userService = (UserService) enhancer.create();// 执行这个userService的test方法时，就会额外会执行一些其他逻辑userService.test(); 得到的都是UserService对象，但是执行test()方法时的效果却不一样了，这就是代理所带来的效果。 上面是通过cglib来实现的代理对象的创建，是基于父子类的，被代理类（UserService）是父类，代理类是子类，代理对象就是代理类的实例对象，代理类是由cglib创建的，对于程序员来说不用关心。​ 除开cglib技术，jdk本身也提供了一种创建代理对象的动态代理机制，但是它只能代理接口，也就是UserService得先有一个接口才能利用jdk动态代理机制来生成一个代理对象，比如： 1234567891011public interface UserInterface &#123; public void test();&#125;public class UserService implements UserInterface &#123; public void test() &#123; System.out.println(&quot;test...&quot;); &#125;&#125; 利用JDK动态代理来生成一个代理对象： 123456789101112131415UserService target = new UserService();// UserInterface接口的代理对象Object proxy = Proxy.newProxyInstance(UserService.class.getClassLoader(), new Class[]&#123;UserInterface.class&#125;, new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = method.invoke(target, args); System.out.println(&quot;after...&quot;); return result; &#125;&#125;);UserInterface userService = (UserInterface) proxy;userService.test(); 如果你把new Class[]{UserInterface.class}，替换成new Class[]{UserService.class}，允许代码会直接报错： 1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: com.zhouyu.service.UserService is not an interface 表示一定要是个接口。​ 由于这个限制，所以产生的代理对象的类型是UserInterface，而不是UserService，这是需要注意的。 ProxyFactory上面我们介绍了两种动态代理技术，那么在Spring中进行了封装，封装出来的类叫做ProxyFactory，表示是创建代理对象的一个工厂，使用起来会比上面的更加方便，比如： 1234567891011121314151617UserService target = new UserService();ProxyFactory proxyFactory = new ProxyFactory();proxyFactory.setTarget(target);proxyFactory.addAdvice(new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; //增强所有的方法 System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125;&#125;);UserInterface userService = (UserInterface) proxyFactory.getProxy();userService.test(); 通过ProxyFactory，我们可以不再关系到底是用cglib还是jdk动态代理了，ProxyFactory会帮我们去判断，如果UserService实现了接口，那么ProxyFactory底层就会用jdk动态代理，如果没有实现接口，就会用cglib技术，上面的代码，就是由于UserService实现了UserInterface接口，所以最后产生的代理对象是UserInterface类型。 &#x2F;&#x2F; 123456789101112131415 //增强逻辑Object result = null; // 判断哪些方法需要增强 if (&quot;selectAll&quot;.equals(method.getName())) &#123; //权限校验 System.out.println(&quot;权限校验&quot;); //执行被代理类的目标方法 result = methodProxy.invokeSuper(proxy, args); //日志记录 System.out.println(&quot;日志记录&quot;); &#125; else &#123; //不是addUser和deleteUser执行目标方法 result = methodProxy.invokeSuper(proxy, args); &#125; return result; Advice的分类 Before Advice：方法之前执行 After returning advice：方法return后执行 After throwing advice：方法抛异常后执行 After (finally) advice：方法执行完finally之后执行，这是最后的，比return更后 Around advice：这是功能最强大的Advice，可以自定义执行顺序 看课上给的代码例子将一目了然​ Advisor的理解跟Advice类似的还有一个Advisor的概念，一个Advisor是有一个Pointcut和一个Advice组成的，通过Pointcut可以指定要需要被代理的逻辑，比如一个UserService类中有两个方法，按上面的例子，这两个方法都会被代理，被增强，那么我们现在可以通过Advisor，来控制到具体代理哪一个方法，比如： 12345678910111213141516171819202122232425262728293031323334353637 UserService target = new UserService(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(new PointcutAdvisor() &#123; @Override public Pointcut getPointcut() &#123; return new StaticMethodMatcherPointcut() &#123; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().equals(&quot;testAbc&quot;); &#125; &#125;; &#125; @Override public Advice getAdvice() &#123; return new MethodInterceptor() &#123; //注意是覆写invoke方法的方法拦截器 @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;; &#125;// 这一步 @Override public boolean isPerInstance() &#123; return false; &#125; &#125;); UserInterface userService = (UserInterface) proxyFactory.getProxy(); userService.test(); 上面代码表示，产生的代理对象，只有在执行testAbc这个方法时才会被增强，会执行额外的逻辑，而在执行其他方法时是不会增强的。​ 创建代理对象的方式 proxyFactoryBean 类似proxyfacoty(只能单个Bean增强)返回一个代理对象放到IOC容器中 指定一个Advice 根据Beanname (BeanNameAutoProxyCreater) 指定一个Advice 找所有Advisor根据Advisor中的PointCut和Advice信息，确定要代理的Bean以及代理逻辑 上面介绍了Spring中所提供了ProxyFactory、Advisor、Advice、PointCut等技术来实现代理对象的创建，但是我们在使用Spring时，我们并不会直接这么去使用ProxyFactory，比如说，我们希望ProxyFactory所产生的代理对象能直接就是Bean，能直接从Spring容器中得到UserSerivce的代理对象，而这些，Spring都是支持的，只不过，作为开发者的我们肯定得告诉Spring，那些类需要被代理，代理逻辑是什么。 ProxyFactoryBean1234567891011121314151617@Beanpublic ProxyFactoryBean userServiceProxy()&#123; UserService userService = new UserService(); ProxyFactoryBean proxyFactoryBean = new ProxyFactoryBean(); proxyFactoryBean.setTarget(userService); proxyFactoryBean.addAdvice(new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;); return proxyFactoryBean;&#125; 通过这种方法来定义一个UserService的Bean，并且是经过了AOP的。但是这种方式只能针对某一个Bean。它是一个FactoryBean，所以利用的就是FactoryBean技术，间接的将UserService的代理对象作为了Bean。​ ProxyFactoryBean还有额外的功能，比如可以把某个Advise或Advisor定义成为Bean，然后在ProxyFactoryBean中进行设置 12345678910111213141516171819202122@Beanpublic MethodInterceptor zhouyuAroundAdvise()&#123; return new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;;&#125;@Beanpublic ProxyFactoryBean userService()&#123; UserService userService = new UserService(); ProxyFactoryBean proxyFactoryBean = new ProxyFactoryBean(); proxyFactoryBean.setTarget(userService); proxyFactoryBean.setInterceptorNames(&quot;zhouyuAroundAdvise&quot;); return proxyFactoryBean;&#125; BeanNameAutoProxyCreatorProxyFactoryBean得自己指定被代理的对象，那么我们可以通过BeanNameAutoProxyCreator来通过指定某个bean的名字，来对该bean进行代理 123456789@Beanpublic BeanNameAutoProxyCreator beanNameAutoProxyCreator() &#123; BeanNameAutoProxyCreator beanNameAutoProxyCreator = new BeanNameAutoProxyCreator(); beanNameAutoProxyCreator.setBeanNames(&quot;userSe*&quot;); beanNameAutoProxyCreator.setInterceptorNames(&quot;zhouyuAroundAdvise&quot;); beanNameAutoProxyCreator.setProxyTargetClass(true); return beanNameAutoProxyCreator;&#125; 通过BeanNameAutoProxyCreator可以对批量的Bean进行AOP，并且指定了代理逻辑，指定了一个InterceptorName，也就是一个Advise，前提条件是这个Advise也得是一个Bean，这样Spring才能找到的，但是BeanNameAutoProxyCreator的缺点很明显，它只能根据beanName来指定想要代理的Bean。​ DefaultAdvisorAutoProxyCreator12345678910111213141516171819@Beanpublic DefaultPointcutAdvisor defaultPointcutAdvisor()&#123; NameMatchMethodPointcut pointcut = new NameMatchMethodPointcut(); pointcut.addMethodName(&quot;test&quot;); DefaultPointcutAdvisor defaultPointcutAdvisor = new DefaultPointcutAdvisor(); defaultPointcutAdvisor.setPointcut(pointcut); defaultPointcutAdvisor.setAdvice(new ZhouyuAfterReturningAdvise()); return defaultPointcutAdvisor;&#125;@Beanpublic DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123; DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); return defaultAdvisorAutoProxyCreator;&#125; 通过DefaultAdvisorAutoProxyCreator会直接去找所有Advisor类型的Bean，根据Advisor中的PointCut和Advice信息，确定要代理的Bean以及代理逻辑。 但是，我们发现，通过这种方式，我们得依靠某一个类来实现定义我们的Advisor，或者Advise，或者Pointcut，那么这个步骤能不能更加简化一点呢？​ 对的，通过注解！ 比如我们能不能只定义一个类，然后通过在类中的方法上通过某些注解，来定义PointCut以及Advice，可以的，比如： 12345678910@Aspect@Componentpublic class ZhouyuAspect &#123; @Before(&quot;execution(public void com.zhouyu.service.UserService.test())&quot;) public void zhouyuBefore(JoinPoint joinPoint) &#123; System.out.println(&quot;zhouyuBefore&quot;); &#125;&#125; 通过上面这个类，我们就直接定义好了所要代理的方法(通过一个表达式)，以及代理逻辑（被@Before修饰的方法），简单明了，这样对于Spring来说，它要做的就是来解析这些注解了，解析之后得到对应的Pointcut对象、Advice对象，生成Advisor对象，扔进ProxyFactory中，进而产生对应的代理对象，具体怎么解析这些注解就是**@EnableAspectJAutoProxy注解**所要做的事情了，后面详细分析。 对Spring AOP的理解OOP表示面向对象编程，是一种编程思想，AOP表示面向切面编程，也是一种编程思想，而我们上面所描述的就是Spring为了让程序员更加方便的做到面向切面编程所提供的技术支持，换句话说，就是Spring提供了一套机制，可以让我们更加容易的来进行AOP，所以这套机制我们也可以称之为Spring AOP。​ 但是值得注意的是，上面所提供的注解的方式来定义Pointcut和Advice，Spring并不是首创，首创是AspectJ，而且也不仅仅只有Spring提供了一套机制来支持AOP，还有比如 JBoss 4.0、aspectwerkz等技术都提供了对于AOP的支持。而刚刚说的注解的方式，Spring是依赖了AspectJ的，或者说，Spring是直接把AspectJ中所定义的那些注解直接拿过来用，自己没有再重复定义了，不过也仅仅只是把注解的定义赋值过来了，每个注解具体底层是怎么解析的，还是Spring自己做的，所以我们在用Spring时，如果你想用@Before、@Around等注解，是需要单独引入aspecj相关jar包的，比如： 12compile group: &#x27;org.aspectj&#x27;, name: &#x27;aspectjrt&#x27;, version: &#x27;1.9.5&#x27;compile group: &#x27;org.aspectj&#x27;, name: &#x27;aspectjweaver&#x27;, version: &#x27;1.9.5&#x27; 值得注意的是：AspectJ是在编译时对字节码进行了修改，是直接在UserService类对应的字节码中进行增强的，也就是可以理解为是在编译时就会去解析@Before这些注解，然后得到代理逻辑，加入到被代理的类中的字节码中去的，所以如果想用AspectJ技术来生成代理对象 ，是需要用单独的AspectJ编译器的。我们在项目中很少这么用，我们仅仅只是用了@Before这些注解，而我们在启动Spring的过程中，Spring会去解析这些注解，然后利用动态代理机制生成代理对象的。​ IDEA中使用Aspectj：https://blog.csdn.net/gavin_john/article/details/80156963 AOP中的概念 Aspect：表示切面，比如被@Aspect注解的类就是切面，可以在切面中去定义Pointcut、Advice等等 Join point：表示连接点，表示一个程序在执行过程中的一个点，比如一个方法的执行，比如一个异常的处理，在Spring AOP中，一个连接点通常表示一个方法的执行。 Advice：表示通知，表示在一个特定连接点上所采取的动作。Advice分为不同的类型，后面详细讨论，在很多AOP框架中，包括Spring，会用Interceptor拦截器来实现Advice，并且在连接点周围维护一个Interceptor链 Pointcut：表示切点，用来匹配一个或多个连接点，Advice与切点表达式是关联在一起的，Advice将会执行在和切点表达式所匹配的连接点上 Introduction：可以使用@DeclareParents来给所匹配的类添加一个接口，并指定一个默认实现 Target object：目标对象，被代理对象 AOP proxy：表示代理工厂，用来创建代理对象的，在Spring Framework中，要么是JDK动态代理，要么是CGLIB代理 Weaving：表示织入，表示创建代理对象的动作，这个动作可以发生在编译时期（比如Aspejctj），或者运行时，比如Spring AOP Advice在Spring AOP中对应API上面说到的Aspject中的注解，其中有五个是用来定义Advice的，表示代理逻辑，以及执行时机： @Before @AfterReturning @AfterThrowing @After @Around 我们前面也提到过，Spring自己也提供了类似的执行实际的实现类： 接口MethodBeforeAdvice，继承了接口BeforeAdvice 接口AfterReturningAdvice 接口ThrowsAdvice 接口AfterAdvice 接口MethodInterceptor Spring会把五个注解解析为对应的Advice类： @Before：AspectJMethodBeforeAdvice，实际上就是一个MethodBeforeAdvice @AfterReturning：AspectJAfterReturningAdvice，实际上就是一个AfterReturningAdvice @AfterThrowing：AspectJAfterThrowingAdvice，实际上就是一个MethodInterceptor @After：AspectJAfterAdvice，实际上就是一个MethodInterceptor @Around：AspectJAroundAdvice，实际上就是一个MethodInterceptor TargetSource的使用在我们日常的AOP中，被代理对象就是Bean对象，是由BeanFactory给我们创建出来的，但是Spring AOP中提供了TargetSource机制，可以让我们用来自定义逻辑来创建被代理对象。​ 比如之前所提到的**@Lazy注解，当加在属性上时，会产生一个代理对象赋值给这个属性**，产生代理对象的代码为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455protected Object buildLazyResolutionProxy(final DependencyDescriptor descriptor, final @Nullable String beanName) &#123; BeanFactory beanFactory = getBeanFactory(); Assert.state(beanFactory instanceof DefaultListableBeanFactory, &quot;BeanFactory needs to be a DefaultListableBeanFactory&quot;); final DefaultListableBeanFactory dlbf = (DefaultListableBeanFactory) beanFactory; TargetSource ts = new TargetSource() &#123; @Override public Class&lt;?&gt; getTargetClass() &#123; return descriptor.getDependencyType(); &#125; @Override public boolean isStatic() &#123; return false; &#125; @Override public Object getTarget() &#123; Set&lt;String&gt; autowiredBeanNames = (beanName != null ? new LinkedHashSet&lt;&gt;(1) : null); Object target = dlbf.doResolveDependency(descriptor, beanName, autowiredBeanNames, null); if (target == null) &#123; Class&lt;?&gt; type = getTargetClass(); if (Map.class == type) &#123; return Collections.emptyMap(); &#125; else if (List.class == type) &#123; return Collections.emptyList(); &#125; else if (Set.class == type || Collection.class == type) &#123; return Collections.emptySet(); &#125; throw new NoSuchBeanDefinitionException(descriptor.getResolvableType(), &quot;Optional dependency not present for lazy injection point&quot;); &#125; if (autowiredBeanNames != null) &#123; for (String autowiredBeanName : autowiredBeanNames) &#123; if (dlbf.containsBean(autowiredBeanName)) &#123; dlbf.registerDependentBean(autowiredBeanName, beanName); &#125; &#125; &#125; return target; &#125; @Override public void releaseTarget(Object target) &#123; &#125; &#125;; ProxyFactory pf = new ProxyFactory(); pf.setTargetSource(ts); Class&lt;?&gt; dependencyType = descriptor.getDependencyType(); if (dependencyType.isInterface()) &#123; pf.addInterface(dependencyType); &#125; return pf.getProxy(dlbf.getBeanClassLoader()); &#125; 这段代码就利用了ProxyFactory来生成代理对象，以及使用了TargetSource，以达到代理对象在执行某个方法时，调用TargetSource的getTarget()方法实时得到一个被代理对象。 ProxyFactory选择cglib或jdk动态代理原理ProxyFactory在生成代理对象之前需要决定到底是使用JDK动态代理还是CGLIB技术： 1234567891011121314151617181920// config就是ProxyFactory对象// optimize为true,或proxyTargetClass为true,或用户没有给ProxyFactory对象添加interfaceif (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; // targetClass是接口，直接使用Jdk动态代理 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; // 使用Cglib return new ObjenesisCglibAopProxy(config);&#125;else &#123; // 使用Jdk动态代理 return new JdkDynamicAopProxy(config);&#125; 代理对象创建过程JdkDynamicAopProxy 在构造JdkDynamicAopProxy对象时，会先拿到被代理对象自己所实现的接口，并且额外的增加SpringProxy、Advised、DecoratingProxy三个接口，组合成一个Class[]，并赋值给proxiedInterfaces属性 并且检查这些接口中是否定义了equals()、hashcode()方法 执行Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this)，得到代理对象，JdkDynamicAopProxy作为InvocationHandler，代理对象在执行某个方法时，会进入到JdkDynamicAopProxy的**invoke()**方法中 ObjenesisCglibAopProxy 创建Enhancer对象 设置Enhancer的superClass为通过ProxyFactory.setTarget()所设置的对象的类 设置Enhancer的interfaces为通过ProxyFactory.addInterface()所添加的接口，以及SpringProxy、Advised、DecoratingProxy接口 设置Enhancer的Callbacks为DynamicAdvisedInterceptor 最后创建一个代理对象，代理对象在执行某个方法时，会进入到DynamicAdvisedInterceptor的intercept()方法中 代理对象执行过程 在使用ProxyFactory创建代理对象之前，需要往ProxyFactory先添加Advisor 代理对象在执行某个方法时，会把ProxyFactory中的Advisor拿出来和当前正在执行的方法进行匹配筛选 把和方法所匹配的Advisor适配成MethodInterceptor 把和当前方法匹配的MethodInterceptor链，以及被代理对象、代理对象、代理类、当前Method对象、方法参数封装为MethodInvocation对象 调用MethodInvocation的proceed()方法，开始执行各个MethodInterceptor以及被代理对象的对应方法 按顺序调用每个MethodInterceptor的invoke()方法，并且会把MethodInvocation对象传入invoke()方法 直到执行完最后一个MethodInterceptor了，就会调用invokeJoinpoint()方法，从而执行被代理对象的当前方法 各注解对应的MethodInterceptor @Before 对应的是AspectJMethodBeforeAdvice，在进行动态代理时会把AspectJMethodBeforeAdvice转成 MethodBeforeAdviceInterceptor 先执行advice对应的方法 再执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 @After 对应的是AspectJAfterAdvice，直接实现了 MethodInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 再执行advice对应的方法 @Around 对应的是AspectJAroundAdvice，直接实现了 MethodInterceptor 直接执行advice对应的方法，由@Around自己决定要不要继续往后面调用 @AfterThrowing 对应的是AspectJAfterThrowingAdvice，直接实现了 MethodInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 如果上面抛了Throwable，那么则会执行advice对应的方法 @AfterReturning 对应的是AspectJAfterReturningAdvice，在进行动态代理时会把AspectJAfterReturningAdvice转成 AfterReturningAdviceInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 执行上面的方法后得到最终的方法的返回值 再执行Advice对应的方法 AbstractAdvisorAutoProxyCreatorDefaultAdvisorAutoProxyCreator的父类是AbstractAdvisorAutoProxyCreator。 AbstractAdvisorAutoProxyCreator非常强大以及重要，只要Spring容器中存在这个类型的Bean，就相当于开启了AOP，AbstractAdvisorAutoProxyCreator实际上就是一个BeanPostProcessor，所以在创建某个Bean时，就会进入到它对应的生命周期方法中，比如：在某个Bean初始化之后，会调用wrapIfNecessary()方法进行AOP，底层逻辑是，AbstractAdvisorAutoProxyCreator会找到所有的Advisor，然后判断当前这个Bean是否存在某个Advisor与之匹配（根据Pointcut），如果匹配就表示当前这个Bean有对应的切面逻辑，需要进行AOP，需要产生一个代理对象。 @EnableAspectJAutoProxy这个注解主要就是往Spring容器中添加了一个AnnotationAwareAspectJAutoProxyCreator类型的Bean。AspectJAwareAdvisorAutoProxyCreator继承了AbstractAdvisorAutoProxyCreator，重写了findCandidateAdvisors()方法，AbstractAdvisorAutoProxyCreator只能找到所有Advisor类型的Bean对象，但是AspectJAwareAdvisorAutoProxyCreator除开可以找到所有Advisor类型的Bean对象，还能把@Aspect注解所标注的Bean中的@Before等注解及方法进行解析，并生成对应的Advisor对象。​ 所以，我们可以理解@EnableAspectJAutoProxy，其实就是像Spring容器中添加了一个AbstractAdvisorAutoProxyCreator类型的Bean，从而开启了AOP，并且还会解析@Before等注解生成Advisor。 第八章 事务增强已有类的组合 @EnableTransactionManagement工作原理开启Spring事务本质上就是增加了一个Advisor，但我们使用@EnableTransactionManagement注解来开启Spring事务是，该注解代理的功能就是向Spring容器中添加了两个Bean： AutoProxyRegistrar ProxyTransactionManagementConfiguration AutoProxyRegistrar主要的作用是向Spring容器中注册了一个InfrastructureAdvisorAutoProxyCreator的Bean。而InfrastructureAdvisorAutoProxyCreator继承了AbstractAdvisorAutoProxyCreator，所以这个类的主要作用就是开启自动代理的作用，也就是一个BeanPostProcessor，会在初始化后步骤中去寻找Advisor类型的Bean，并判断当前某个Bean是否有匹配的Advisor，是否需要利用动态代理产生一个代理对象。​ ProxyTransactionManagementConfiguration是一个配置类，它又定义了另外三个bean： BeanFactoryTransactionAttributeSourceAdvisor：一个Advisor AnnotationTransactionAttributeSource：相当于BeanFactoryTransactionAttributeSourceAdvisor中的Pointcut TransactionInterceptor：相当于BeanFactoryTransactionAttributeSourceAdvisor中的Advice AnnotationTransactionAttributeSource就是用来判断某个类上是否存在@Transactional注解，或者判断某个方法上是否存在@Transactional注解的。​ TransactionInterceptor就是代理逻辑，当某个类中存在@Transactional注解时，到时就产生一个代理对象作为Bean，代理对象在执行某个方法时，最终就会进入到TransactionInterceptor的invoke()方法。 Spring事务基本执行原理一个Bean在执行Bean的创建生命周期时，会经过InfrastructureAdvisorAutoProxyCreator的初始化后的方法，会判断当前当前Bean对象是否和BeanFactoryTransactionAttributeSourceAdvisor匹配，匹配逻辑为判断该Bean的类上是否存在@Transactional注解，或者类中的某个方法上是否存在@Transactional注解，如果存在则表示该Bean需要进行动态代理产生一个代理对象作为Bean对象。​ 该代理对象在执行某个方法时，会再次判断当前执行的方法是否和BeanFactoryTransactionAttributeSourceAdvisor匹配，如果匹配则执行该Advisor中的TransactionInterceptor的invoke()方法，执行基本流程为： 利用所配置的PlatformTransactionManager事务管理器新建一个数据库连接 修改数据库连接的autocommit为false 执行MethodInvocation.proceed()方法，简单理解就是执行业务方法，其中就会执行sql 如果没有抛异常，则提交 如果抛了异常，则回滚 Spring事务详细执行流程Spring事务执行流程图：https://www.processon.com/view/link/5fab6edf1e0853569633cc06 Spring事务传播机制在开发过程中，经常会出现一个方法调用另外一个方法，那么这里就涉及到了多种场景，比如a()调用b()： a()和b()方法中的所有sql需要在同一个事务中吗？ a()和b()方法需要单独的事务吗？ a()需要在事务中执行，b()还需要在事务中执行吗？ 等等情况… 所以，这就要求Spring事务能支持上面各种场景，这就是Spring事务传播机制的由来。那Spring事务传播机制是如何实现的呢?​ 先来看上述几种场景中的一种情况，a()在一个事务中执行，调用b()方法时需要新开一个事务执行：​ 首先，代理对象执行a()方法前，先利用事务管理器新建一个数据库连接a 将数据库连接a的autocommit改为false 把数据库连接a设置到ThreadLocal中 执行a()方法中的sql 执行a()方法过程中，调用了b()方法（注意用代理对象调用b()方法） 代理对象执行b()方法前，判断出来了当前线程中已经存在一个数据库连接a了，表示当前线程其实已经拥有一个Spring事务了，则进行挂起 挂起就是把ThreadLocal中的数据库连接a从ThreadLocal中移除，并放入一个挂起资源对象中 挂起完成后，再次利用事务管理器新建一个数据库连接b 将数据库连接b的autocommit改为false 把数据库连接b设置到ThreadLocal中 执行b()方法中的sql b()方法正常执行完，则从ThreadLocal中拿到数据库连接b进行提交 提交之后会恢复所挂起的数据库连接a，这里的恢复，其实只是把在挂起资源对象中所保存的数据库连接a再次设置到ThreadLocal中 a()方法正常执行完，则从ThreadLocal中拿到数据库连接a进行提交 这个过程中最为核心的是：在执行某个方法时，判断当前是否已经存在一个事务，就是判断当前线程的ThreadLocal中是否存在一个数据库连接对象，如果存在则表示已经存在一个事务了。 Spring事务传播机制分类其中，以非事务方式运行，表示以非Spring事务运行，表示在执行这个方法时，Spring事务管理器不会去建立数据库连接，执行sql时，由Mybatis或JdbcTemplate自己来建立数据库连接来执行sql。 案例分析情况112345678910111213141516@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional public void a() &#123; // a方法中的sql &#125;&#125; 默认情况下传播机制为REQUIRED，表示当前如果没有事务则新建一个事务，如果有事务则在当前事务中执行。​ 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 执行conn的commit()方法进行提交 情况2假如是这种情况 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); int result = 100/0; &#125; @Transactional public void a() &#123; // a方法中的sql &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 抛出异常 执行conn的rollback()方法进行回滚，所以两个方法中的sql都会回滚掉 情况3假如是这种情况： 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional public void a() &#123; // a方法中的sql int result = 100/0; &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 抛出异常 执行conn的rollback()方法进行回滚，所以两个方法中的sql都会回滚掉 情况4如果是这种情况： 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW) public void a() &#123; // a方法中的sql int result = 100/0; &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 又新建一个数据库连接conn2 执行a方法中的sql 抛出异常 执行conn2的rollback()方法进行回滚 继续抛异常，对于test()方法而言，它会接收到一个异常，然后抛出 执行conn的rollback()方法进行回滚，最终还是两个方法中的sql都回滚了 Spring事务强制回滚正常情况下，a()调用b()方法时，如果b()方法抛了异常，但是在a()方法捕获了，那么a()的事务还是会正常提交的，但是有的时候，我们捕获异常可能仅仅只是不把异常信息返回给客户端，而是为了返回一些更友好的错误信息，而这个时候，我们还是希望事务能回滚的，那这个时候就得告诉Spring把当前事务回滚掉，做法就是： 12345678910111213141516@Transactionalpublic void test()&#123; // 执行sql try &#123; b(); &#125; catch (Exception e) &#123; // 构造友好的错误信息返回 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); &#125; &#125;public void b() throws Exception &#123; throw new Exception();&#125; TransactionSynchronizationSpring事务有可能会提交，回滚、挂起、恢复，所以Spring事务提供了一种机制，可以让程序员来监听当前Spring事务所处于的状态。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990@Componentpublic class UserService &#123; @Autowired private JdbcTemplate jdbcTemplate; @Autowired private UserService userService; @Transactional public void test()&#123; TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() &#123; @Override public void suspend() &#123; System.out.println(&quot;test被挂起了&quot;); &#125; @Override public void resume() &#123; System.out.println(&quot;test被恢复了&quot;); &#125; @Override public void beforeCommit(boolean readOnly) &#123; System.out.println(&quot;test准备要提交了&quot;); &#125; @Override public void beforeCompletion() &#123; System.out.println(&quot;test准备要提交或回滚了&quot;); &#125; @Override public void afterCommit() &#123; System.out.println(&quot;test提交成功了&quot;); &#125; @Override public void afterCompletion(int status) &#123; System.out.println(&quot;test提交或回滚成功了&quot;); &#125; &#125;); jdbcTemplate.execute(&quot;insert into t1 values(1,1,1,1,&#x27;1&#x27;)&quot;); System.out.println(&quot;test&quot;); userService.a(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW) public void a()&#123; TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() &#123; @Override public void suspend() &#123; System.out.println(&quot;a被挂起了&quot;); &#125; @Override public void resume() &#123; System.out.println(&quot;a被恢复了&quot;); &#125; @Override public void beforeCommit(boolean readOnly) &#123; System.out.println(&quot;a准备要提交了&quot;); &#125; @Override public void beforeCompletion() &#123; System.out.println(&quot;a准备要提交或回滚了&quot;); &#125; @Override public void afterCommit() &#123; System.out.println(&quot;a提交成功了&quot;); &#125; @Override public void afterCompletion(int status) &#123; System.out.println(&quot;a提交或回滚成功了&quot;); &#125; &#125;); jdbcTemplate.execute(&quot;insert into t1 values(2,2,2,2,&#x27;2&#x27;)&quot;); System.out.println(&quot;a&quot;); &#125;&#125; 第九章 Spring MVC请求流程源码Spring集成Sring MVC (无SpringBoot) 1234567public class MyApplicationInitializer implements WebApplicationInitializer &#123; @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; //注册DispatcherServlet到容器里 &#125;&#125;// todo 启动Tomcat Servlet规范定义接口,tomcat通过SPI机制加载 实现是 @HandlerTypes(感兴趣的类) 12345678910111213141516171819202122232425262728293031323334353637383940@HandlesTypes(WebApplicationInitializer.class)public class SpringServletContainerInitializer implements ServletContainerInitializer &#123; //.... @Override public void onStartup(@Nullable Set&lt;Class&lt;?&gt;&gt; webAppInitializerClasses, ServletContext servletContext) throws ServletException &#123; List&lt;WebApplicationInitializer&gt; initializers = Collections.emptyList();// 这里面会有我们自己写的myWebApplicationInitializer if (webAppInitializerClasses != null) &#123; initializers = new ArrayList&lt;&gt;(webAppInitializerClasses.size()); for (Class&lt;?&gt; waiClass : webAppInitializerClasses) &#123; if (!waiClass.isInterface() &amp;&amp; !Modifier.isAbstract(waiClass.getModifiers()) &amp;&amp; WebApplicationInitializer.class.isAssignableFrom(waiClass)) &#123; try &#123; initializers.add((WebApplicationInitializer) ReflectionUtils.accessibleConstructor(waiClass).newInstance()); &#125; catch (Throwable ex) &#123; throw new ServletException(&quot;Failed to instantiate WebApplicationInitializer class&quot;, ex); &#125; &#125; &#125; &#125; if (initializers.isEmpty()) &#123; servletContext.log(&quot;No Spring WebApplicationInitializer types detected on classpath&quot;); return; &#125; servletContext.log(initializers.size() + &quot; Spring WebApplicationInitializers detected on classpath&quot;); AnnotationAwareOrderComparator.sort(initializers); for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext); &#125; &#125;&#125; SpringMVC —请求源码流程 有道云链接：http://note.youdao.com/noteshare?id=ec3ca523300ad31d7f6f673b9e92bbeb&amp;sub=1D1BF1D55D0148879F53878CB24F8214 ​ SpringMVC —请求源码流程 ​ 前言 ​ 从Servlet到SpringMVC ​ 传统Servlet： ​ SpringMVC ​ SpringMVC的具体执行流程： ​ HandlerMapping 前言 Spring官网的MVC模块介绍： Spring Web MVC is the original web framework built on the Servlet API and has been included in the Spring Framework from the very beginning. The formal name, “Spring Web MVC,” comes from the name of its source module (spring-webmvc), but it is more commonly known as “Spring MVC”. Spring Web MVC是基于Servlet API构建的原始Web框架，从一开始就已包含在Spring框架中。正式名称“ Spring Web MVC”来自其源模块的名称（spring-webmvc），但它通常被称为“ Spring MVC”。 从Servlet到SpringMVC 最典型的MVC就是JSP + servlet + javabean的模式。 传统Servlet： ​ 弊端： 1.xml下配置servlet的映射非常麻烦 开发效率低 2.必须要继承父类、重写方法 侵入性强 2.如果想在一个Servlet中处理同一业务模块的的功能分发给不同方法进行处理非常麻烦 3.参数解析麻烦:单个参数（转换类型）—&gt;pojo对象 Json文本—&gt;pojo对象 4.**数据响应麻烦:**pojo对象—&gt;json … Content-type 5.跳转页面麻烦, 对path的控制、 如果使用其他模板也很麻烦 、设置编码麻烦…等等… 所以SpringMVC 就是在Servlet的基础上进行了封装，帮我把这些麻烦事都给我们做了。 Web框架的升级是一个不断偷懒的过程 从最开始的Servlet到现在的SpringMVC、SpringBoot等等 SpringMVC 基于xml的实现方式： 1.给Servlet容器配置一个DispatcherServlet（web.xml ) 2.添加SpringMVC的配置信息 继承类&#x2F;实现接口 方式： ​ implements HttpRequestHandler 不同的HandlerMapping ​ simpleController 注解方式： 配置控制器@Controller和处理方法的映射—@RequstMapping 即可 其实SpringMVC请求原理很简单：说白了就是用一个DispatcherServlet 封装了一个Servlet的调度中心， 由调度中心帮我们调用我们的处理方法： 在这个过程中调度中心委托给各个组件执行具体工作 ，比如帮我们映射方法请求、帮我解析参数、调用处理方法、响应数据和页面 等 这就相当于你在家自己做饭和去饭店吃饭的区别了， 在家你买菜、洗菜、蒸饭、炒菜、洗碗都得自己来. 饭店都给你做好了， 你只要分服务员说你吃什么、就能得到响应. 殊不知呢， 你只是说了吃什么（请求）， 后厨（DispatcherServlet）就有配菜员你给找到菜单-对应的食材（映射） 、切菜员切菜（解析参数）、 厨师给你炒菜（调用处理方法）、装盘（处理返回值)、 抄完给你端出来（响应） SpringMVC的具体执行流程： Spring MVC 是围绕前端控制器模式设计的，其中：中央 Servlet DispatcherServlet 为请求处理流程提供统一调度，实际工作则交给可配置组件执行。这个模型是灵活的且开放的，我们可以通过自己去定制这些组件从而进行定制自己的工作流。 ​ DispatcherServlet： 前端调度器 ， 负责将请求拦截下来分发到各控制器方法中 HandlerMapping: 负责根据请求的URL和配置@RequestMapping映射去匹配， 匹配到会返回Handler（具体控制器的方法） HandlerAdaper: 负责调用Handler-具体的方法- 返回视图的名字 Handler将它封装到ModelAndView(封装视图名，request域的数据） ViewReslover: 根据ModelAndView里面的视图名地址去找到具体的jsp封装在View对象中 View：进行视图渲染（将jsp转换成html内容 –这是Servlet容器的事情了） 最终response到的客户端 用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用处理器映射器HandlerMapping。 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter,执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler(Controller，也叫页面控制器)。 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 DispatcherServlet响应用户。 整个调用过程其实都在doDispatch中体现了： 用户发送请求至前端控制器DispatcherServlet 由于它是个Servlet会先进入service方法——&gt;doGet&#x2F;doPost——&gt;processRequestdoService——&gt;doDispatch ↓ 这个doDispatch非常重要–体现了整个请求流程 ​ protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { try { try { &#x2F;&#x2F; 文件上传相关 processedRequest &#x3D; checkMultipart(request); multipartRequestParsed &#x3D; (processedRequest !&#x3D; request); &#x2F;&#x2F; DispatcherServlet收到请求调用处理器映射器HandlerMapping。 &#x2F;&#x2F; 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 mappedHandler &#x3D; getHandler(processedRequest); if (mappedHandler &#x3D;&#x3D; null) { noHandlerFound(processedRequest, response); return; } 4.DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter, HandlerAdapter ha &#x3D; getHandlerAdapter(mappedHandler.getHandler()); &#x2F;&#x2F; Process last-modified header, if supported by the handler. HTTP缓存相关 String method &#x3D; request.getMethod(); boolean isGet &#x3D; HttpMethod.GET.matches(method); if (isGet || HttpMethod.HEAD.matches(method)) { long lastModified &#x3D; ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) { return; } } &#x2F;&#x2F; 前置拦截器 if (!mappedHandler.applyPreHandle(processedRequest, response)) { &#x2F;&#x2F; 返回false就不进行后续处理了 return; } &#x2F;&#x2F; 执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 &#x2F;&#x2F; 执行处理器Handler(Controller，也叫页面控制器)。 &#x2F;&#x2F; Handler执行完成返回ModelAndView &#x2F;&#x2F; HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet mv &#x3D; ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } &#x2F;&#x2F; 如果没有视图，给你设置默认视图 json忽略 applyDefaultViewName(processedRequest, mv); &#x2F;&#x2F;后置拦截器 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException &#x3D; ex; } catch (Throwable err) { &#x2F;&#x2F; As of 4.3, we’re processing Errors thrown from handler methods as well, &#x2F;&#x2F; making them available for @ExceptionHandler methods and other scenarios. dispatchException &#x3D; new NestedServletException(“Handler dispatch failed”, err); } &#x2F;&#x2F; DispatcherServlet将ModelAndView传给ViewReslover视图解析器 &#x2F;&#x2F; ViewReslover解析后返回具体View &#x2F;&#x2F; DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 &#x2F;&#x2F; DispatcherServlet响应用户。 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(“Handler processing failed”, err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { &#x2F;&#x2F; Instead of postHandle and afterCompletion if (mappedHandler !&#x3D; null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { &#x2F;&#x2F; Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } 详细过程我们课程中分析…. HandlerMapping 在整个过程中，涉及到非常多的组件，每个组件解析各个环节，其中HandlerMapping最为重要它是用来映射请求的，我们就着重介绍下HandlerMapping的解析过程和请求映射过程： 附上流程图： https://www.processon.com/view/link/615ea79e1efad4070b2d6707 ​ 第十章 Spring MVC父子容器1、Spring整合SpringMVC 特性： 说到Spring整合SpringMVC唯一的体现就是父子容器： 通常我们会设置父容器（Spring）管理Service、Dao层的Bean, 子容器(SpringMVC)管理Controller的Bean . 子容器可以访问父容器的Bean, 父容器无法访问子容器的Bean。 实现： 也就是零配置（零xml）的放式来说明SpringMVC的原理！！ 此方式作为我们本文重点介绍，也是很多人缺失的一种方式， 其实早在Spring3+就已经提供， 只不过我们直到SpringBoot才使用该方式进行自动配置， 这也是很多人从xml调到SpringBoot不适应的原因， 因为你缺失了这个版本。 所以我们以这种方式作为源码切入点既可以理解到XML的方式又能兼顾到SpringBoot的方式 。 3、实现基于SPI规范的SpringMVC TulingStarterInitializer 此类继承AbstractAnnotationConfigDispatcherServletInitializer 这是个啥？ 待会我们讲原理来介绍 getRootConfigClasses 提供父容器的配置类 getServletConfigClasses 提供子容器的配置类 getServletMappings 设置DispatcherServlet的映射 ​ public class TulingStarterInitializer extends AbstractAnnotationConfigDispatcherServletInitializer { &#x2F;** * 方法实现说明:IOC 父容器的启动类 * @author:xsls * @date:2019&#x2F;7&#x2F;31 22:12 &#x2F; @Override protected Class&lt;?&gt;[] getRootConfigClasses() { return new Class[]{RootConfig.class}; } &#x2F;* * 方法实现说明 IOC子容器配置 web容器配置 * @author:xsls * @date:2019&#x2F;7&#x2F;31 22:12 &#x2F; @Override protected Class&lt;?&gt;[] getServletConfigClasses() { return new Class[]{WebAppConfig.class}; } &#x2F;* * 方法实现说明 * @author:xsls * @return: 我们前端控制器DispatcherServlet的拦截路径 * @exception: * @date:2019&#x2F;7&#x2F;31 22:16 *&#x2F; @Override protected String[] getServletMappings() { return new String[]{“&#x2F;“}; RootConfig 父容器的配置类 &#x3D;以前的spring.xml 扫描的包排除掉@Controller ​ @Configuration @ComponentScan(basePackages &#x3D; “com.tuling”,excludeFilters &#x3D; { @ComponentScan.Filter(type &#x3D; FilterType.ANNOTATION,value&#x3D;{RestController.class,Controller.class}), @ComponentScan.Filter(type &#x3D; ASSIGNABLE_TYPE,value &#x3D;WebAppConfig.class ), }) public class RootConfig { WebAppConfig 子容器的配置类 &#x3D;以前的spring-mvc.xml 扫描的包：包含掉@Controller ​ @Configuration @ComponentScan(basePackages &#x3D; {“com.tuling”},includeFilters &#x3D; { @ComponentScan.Filter(type &#x3D; FilterType.ANNOTATION,value &#x3D; {RestController.class, Controller.class}) },useDefaultFilters &#x3D;false) @EnableWebMvc &#x2F;&#x2F; ≈mvc:annotation-driven/ public class WebAppConfig implements WebMvcConfigurer{ &#x2F;** * 配置拦截器 * @return &#x2F; @Bean public TulingInterceptor tulingInterceptor() { return new TulingInterceptor(); } &#x2F;* * 文件上传下载的组件 * @return &#x2F; @Bean public MultipartResolver multipartResolver() { CommonsMultipartResolver multipartResolver &#x3D; new CommonsMultipartResolver(); multipartResolver.setDefaultEncoding(“UTF-8”); multipartResolver.setMaxUploadSize(1024102410); return multipartResolver; } &#x2F;* * 注册处理国际化资源的组件 * @return &#x2F; &#x2F; @Bean public AcceptHeaderLocaleResolver localeResolver() { AcceptHeaderLocaleResolver acceptHeaderLocaleResolver &#x3D; new AcceptHeaderLocaleResolver(); return acceptHeaderLocaleResolver; }&#x2F; @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(tulingInterceptor()).addPathPatterns(“&#x2F;“); } &#x2F;** * 方法实现说明:配置试图解析器 * @author:xsls * @exception: * @date:2019&#x2F;8&#x2F;6 16:23 *&#x2F; @Bean public InternalResourceViewResolver internalResourceViewResolver() { InternalResourceViewResolver viewResolver &#x3D; new InternalResourceViewResolver(); viewResolver.setSuffix(“.jsp”); viewResolver.setPrefix(“&#x2F;WEB-INF&#x2F;jsp&#x2F;“); return viewResolver; } @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { converters.add(new MappingJackson2HttpMessageConverter()); } 自己去添加个Controller进行测试 OK， 现在可以访问你的SpringMVC了 4、SPI的方式****SpringMVC启动原理 接着我们来看看SPI方式的原理是什么： SpringMVC 大致可以分为 启动 和请求 2大部分， 所以我们本文先研究启动部分 流程图： ​ 源码流程 外置Tomcat启动的时候通过SPI 找到我们应用中的&#x2F;META-INF&#x2F;service&#x2F;javax.servlet.ServletContainerInitializer ​ 调用SpringServletContainerInitializer.onStartUp() ​ 调用onStartUp()前会先找到@HandlesTypes(WebApplicationInitializer.class) 所有实现了WebApplicationInitializer的类，传入到OnStartup的webAppInitializerClasses参数中，并传入Servlet上下文对象。 重点关注这组类：他们组成了父子容器 ​ 找到所有WebApplicationInitializer的实现类后， 不是接口、不是抽象则通过反射进行实例化（所以，你会发现内部实现类都是抽象的，你想让其起作用我们必须添加一个自定义实现类，在下文提供我的自定义实现类） 调用所有上一步实例化后的对象的onStartup方法 ​ ​ \\1. 首先来到AbstractDispatcherServletInitializer#onStartup再执行super.onStartup(servletContext); ​ @Override public void onStartup(ServletContext servletContext) throws ServletException { &#x2F;&#x2F;实例化我们的spring root上下文 super.onStartup(servletContext); &#x2F;&#x2F;注册我们的DispatcherServlet 创建我们spring web 上下文对象 registerDispatcherServlet(servletContext); 创建父容器——ContextLoaderListener 2.父类AbstractContextLoaderInitializer#onStartup执行registerContextLoaderListener(servletContext); createRootApplicationContext()该方法中会创建父容器 该方法是抽象方法，实现类是AbstractAnnotationConfigDispatcherServletInitializer 调用getRootConfigClasses();方法获取父容器配置类（此抽象方法在我们自定义的子类中实现提供我们自定义的映射路径 ） 创建父容器，注册配置类 ​ 会创建ContextLoaderListener并通过ServletContext注册 ​ 看完大家是不是感觉跟我们XML的配置ContextLoaderListener对上了： ​ 创建子容器——DispatcherServlet 3.回到AbstractDispatcherServletInitializer#onStartup再执行registerDispatcherServlet(servletContext); ​ registerDispatcherServlet方法说明： 调用createServletApplicationContext创建子容器 该方法是抽象方法，实现类是AbstractAnnotationConfigDispatcherServletInitializer 创建子容器（下图很明显不多介绍） 调用抽象方法：getServletConfigClasses();获得配置类（此抽象方法在我们自定义的子类中实现提供我们自定义的配置类 ） 配置类除了可以通过ApplicationContext()构造函数的方式传入 ， 也可以通过这种方式动态添加，不知道了吧~ ​ 调用createDispatcherServlet(servletAppContext);创建DispatcherServlet 设置启动时加载：registration.setLoadOnStartup(1); 调用抽象方法设置映射路径：getServletMappings()（此抽象方法在我们自定义的子类中实现提供我们自定义的映射路径 ） 看完大家是不是感觉跟我们XML的配置DispatcherServlet对上了 ​ 4. 初始化ContextLoaderListener ​ ContextLoaderListener加载过程比较简单： 外置tomcat会帮我们调用ContextLoaderListener#contextInitialized 进行初始化 xml的方式下会判断容器为空时创建父容器 在里面会调用父容器的refresh方法加载 将父容器存入到Servlet域中供子容器使用 ​ 5. 初始化DispatcherServlet ​ 可以看到流程比ContextLoaderListener流程更多 外置tomcat会帮我们调用DispatcherServlet#init() 进行初始化—&gt;重点关注：initWebApplicationContext方法 getWebApplicationContext(getServletContext())获得父容器（从之前的Servlet域中拿到） cwac.setParent(rootContext);给子容器设置父容器 调用configureAndRefreshWebApplicationContext(cwac); ​ 注册一个监听器（该监听会初始化springmvc所需信息） ContextRefreshedEvent可以看到该监听器监听的是容器refreshed事件， 会在finishRefresh中发布 刷新容器 ​ 当执行refresh 即加载ioc容器 完了会调用finishRefresh(): publishEvent(new ContextRefreshedEvent(this));发布ContextRefreshedEvent事件 触发上面的ContextRefreshListener监听器： —-&gt;FrameworkServlet.this.onApplicationEvent(event); ——–&gt;onRefresh(event.getApplicationContext()); ————–&gt;initStrategies(context); ​ protected void initStrategies(ApplicationContext context) { &#x2F;&#x2F;初始化我们web上下文对象的 用于文件上传下载的解析器对象 initMultipartResolver(context); &#x2F;&#x2F;初始化我们web上下文对象用于处理国际化资源的 initLocaleResolver(context); &#x2F;&#x2F;主题解析器对象初始化 initThemeResolver(context); &#x2F;&#x2F;初始化我们的HandlerMapping initHandlerMappings(context); &#x2F;&#x2F;实例化我们的HandlerAdapters initHandlerAdapters(context); &#x2F;&#x2F;实例化我们处理器异常解析器对象 initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); &#x2F;&#x2F;给DispatcherSerlvet的ViewResolvers处理器 initViewResolvers(context); initFlashMapManager(context); 这里面的每一个方法不用太细看， 就是给SpringMVC准备初始化的数据， 为后续SpringMVC处理请求做准备 基本都是从容器中拿到已经配置的Bean（RequestMappingHandlerMapping、RequestMappingHandlerAdapter、HandlerExceptionResolver ）放到dispatcherServlet中做准备: ​ ​ ​ … 但是这些Bean又是从哪来的呢？？ 来来来， 回到我们的WebAppConfig 我们使用的一个@EnableWebMvc 导入了DelegatingWebMvcConfiguration@Import(DelegatingWebMvcConfiguration.class) DelegatingWebMvcConfiguration的父类就配置了这些Bean 而且我告诉你SpringBoot也是用的这种方式， ​ 总结 Tomcat在启动时会通过SPI注册 ContextLoaderListener和DispatcherServlet对象 同时创建父子容器 分别创建在ContextLoaderListener初始化时创建父容器设置配置类 在DispatcherServlet初始化时创建子容器 即2个ApplicationContext实例设置配置类 Tomcat在启动时执行ContextLoaderListener和DispatcherServlet对象的初始化方法， 执行容器refresh进行加载 在子容器加载时 创建SpringMVC所需的Bean和预准备的数据：(通过配置类+@EnableWebMvc配置（DelegatingWebMvcConfiguration）——可实现WebMvcConfigurer进行定制扩展） RequestMappingHandlerMapping，它会处理@RequestMapping 注解 RequestMappingHandlerAdapter，则是处理请求的适配器，确定调用哪个类的哪个方法，并且构造方法参数，返回值。 HandlerExceptionResolver 错误视图解析器 addDefaultHttpMessageConverters 添加默认的消息转换器（解析json、解析xml） 等…. 子容器需要注入父容器的Bean时（比如Controller中需要@Autowired Service的Bean）; 会先从子容器中找，没找到会去父容器中找： 详情见AbstractBeanFactory#doGetBean方法 ​ &#x2F;** * 一般情况下,只有Spring 和SpringMvc整合的时才会有父子容器的概念, * 作用： * 比如我们的Controller中注入Service的时候，发现我们依赖的是一个引用对象，那么他就会调用getBean去把service找出来 * 但是当前所在的容器是web子容器，那么就会在这里的 先去父容器找 *&#x2F; BeanFactory parentBeanFactory &#x3D; getParentBeanFactory(); &#x2F;&#x2F;若存在父工厂,且当前的bean工厂不存在当前的bean定义,那么bean定义是存在于父beanFacotry中 if (parentBeanFactory !&#x3D; null &amp;&amp; !containsBeanDefinition(beanName)) { &#x2F;&#x2F;获取bean的原始名称 String nameToLookup &#x3D; originalBeanName(name); &#x2F;&#x2F;若为 AbstractBeanFactory 类型，委托父类处理 if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); } else if (args !&#x3D; null) { &#x2F;&#x2F; 委托给构造函数 getBean() 处理 return (T) parentBeanFactory.getBean(nameToLookup, args); } else { &#x2F;&#x2F; 没有 args，委托给标准的 getBean() 处理 return parentBeanFactory.getBean(nameToLookup, requiredType); } 用几道面试题做个总结: Spring和SpringMVC为什么需要父子容器？不要不行吗？ 就实现层面来说不用子父容器也可以完成所需功能（参考：SpringBoot就没用子父容器） 所以父子容器的主要作用应该是早期Spring为了划分框架边界。有点单一职责的味道。service、dao层我们一般使用spring框架来管理、controller层交给springmvc管理 规范整体架构 使 父容器service无法访问子容器controller、子容器controller可以访问父容器 service 方便子容器的切换。如果现在我们想把web层从spring mvc替换成struts，那么只需要将spring-mvc.xml替换成Struts的配置文件struts.xml即可，而spring-core.xml不需要改变。 为了节省重复bean创建 是否可以把所有Bean都通过Spring容器来管理？（Spring的applicationContext.xml中配置全局扫描) 不可以，这样会导致我们请求接口的时候产生404。 如果所有的Bean都交给父容器，SpringMVC在初始化HandlerMethods的时候（initHandlerMethods）无法根据Controller的handler方法注册HandlerMethod，并没有去查找父容器的bean； 也就无法根据请求URI 获取到 HandlerMethod来进行匹配. ​ 是否可以把我们所需的Bean都放入Spring-mvc子容器里面来管理（springmvc的spring-servlet.xml中配置全局扫描）? 可以 ， 因为父容器的体现无非是为了获取子容器不包含的bean, 如果全部包含在子容器完全用不到父容器了， 所以是可以全部放在springmvc子容器来管理的。 虽然可以这么做不过一般应该是不推荐这么去做的，一般人也不会这么干的。如果你的项目里有用到事物、或者aop记得也需要把这部分配置需要放到Spring-mvc子容器的配置文件来，不然一部分内容在子容器和一部分内容在父容器,可能就会导致你的事物或者AOP不生效。 所以如果aop或事物如果不生效也有可能是通过父容器(spring)去增强子容器(Springmvc)，也就无法增强 这也是很多同学会遇到的问题。 第十一章 Mybatis本章着重介绍MyBatis执行Sql的流程，关于在执行过程中缓存、动态SQl生成等细节不在本章中体现 还是以之前的查询作为列子： 之前提到拿到sqlSession之后就能进行各种CRUD操作了，所以我们就从sqlSession.getMapper这个方法开始分析，看下整个Sql的执行流程是怎么样的。 openSession的过程: Executor分成两大类，一类是CacheExecutor，另一类是普通Executor。 普通Executor又分为三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。 ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。 BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。 CacheExecutor其实是封装了普通的Executor，和普通的区别是在查询前先会查询缓存中是否存在结果，如果存在就使用缓存中的结果，如果不存在还是使用普通的Executor进行查询，再将查询出来的结果存入缓存。 ​ 到此为止，我们已经获得了SqlSession，拿到SqlSession就可以执行各种CRUD方法了。 简单总结 拿到SqlSessionFactory对象后，会调用SqlSessionFactory的openSesison方法，这个方法会创建一个Sql执行器（Executor），这个Sql执行器会代理你配置的拦截器方法。 获得上面的Sql执行器后，会创建一个SqlSession（默认使用DefaultSqlSession）,这个SqlSession中也包含了Configration对象，所以通过SqlSession也能拿到全局配置； 获得SqlSession对象后就能执行各种CRUD方法了。 SQL的具体执行流程见后续博客。 一些重要类总结： SqlSessionFactory SqlSessionFactoryBuilder SqlSession（默认使用DefaultSqlSession） Executor接口 Plugin、InterceptorChain的pluginAll方法 获取Mapper的流程 进入sqlSession.getMapper方法，会发现调的是Configration对象的getMapper方法： public T getMapper(Class type, SqlSession sqlSession) { &#x2F;&#x2F;mapperRegistry实质上是一个Map，里面注册了启动过程中解析的各种Mapper.xml &#x2F;&#x2F;mapperRegistry的key是接口的Class类型 &#x2F;&#x2F;mapperRegistry的Value是MapperProxyFactory,用于生成对应的MapperProxy（动态代理类） return mapperRegistry.getMapper(type, sqlSession); } 进入getMapper方法： public T getMapper(Class type, SqlSession sqlSession) { final MapperProxyFactory mapperProxyFactory &#x3D; (MapperProxyFactory) knownMappers.get(type); &#x2F;&#x2F;如果配置文件中没有配置相关Mapper,直接抛异常 if (mapperProxyFactory &#x3D;&#x3D; null) { throw new BindingException(“Type “ + type + “ is not known to the MapperRegistry.”); } try { &#x2F;&#x2F;关键方法 return mapperProxyFactory.newInstance(sqlSession); } catch (Exception e) { throw new BindingException(“Error getting mapper instance. Cause: “ + e, e); } } 进入MapperProxyFactory的newInstance方法： public class MapperProxyFactory { private final Class mapperInterface; private final Map methodCache &#x3D; new ConcurrentHashMap(); public MapperProxyFactory(Class mapperInterface) { this.mapperInterface &#x3D; mapperInterface; } public Class getMapperInterface() { return mapperInterface; } public Map getMethodCache() { return methodCache; } &#x2F;&#x2F;生成Mapper接口的动态代理类MapperProxy，MapperProxy实现了InvocationHandler 接口 @SuppressWarnings(“unchecked”) protected T newInstance(MapperProxy mapperProxy) { return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy); } public T newInstance(SqlSession sqlSession) { final MapperProxy mapperProxy &#x3D; new MapperProxy(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); } } 获取Mapper的流程总结如下： ​ Mapper方法的执行流程 下面是动态代理类MapperProxy，调用Mapper接口的所有方法都会先调用到这个代理类的invoke方法（注意由于Mybatis中的Mapper接口没有实现类，所以MapperProxy这个代理对象中没有委托类，也就是说MapperProxy干了代理类和委托类的事情）。好了下面重点看下invoke方法。 &#x2F;&#x2F;MapperProxy代理类 public class MapperProxy implements InvocationHandler, Serializable { private static final long serialVersionUID &#x3D; -6424540398559729838L; private final SqlSession sqlSession; private final Class mapperInterface; private final Map methodCache; public MapperProxy(SqlSession sqlSession, Class mapperInterface, Map methodCache) { this.sqlSession &#x3D; sqlSession; this.mapperInterface &#x3D; mapperInterface; this.methodCache &#x3D; methodCache; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { if (Object.class.equals(method.getDeclaringClass())) { return method.invoke(this, args); } else if (isDefaultMethod(method)) { return invokeDefaultMethod(proxy, method, args); } } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } &#x2F;&#x2F;获取MapperMethod，并调用MapperMethod final MapperMethod mapperMethod &#x3D; cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); } MapperProxy的invoke方法非常简单，主要干的工作就是创建MapperMethod对象或者是从缓存中获取MapperMethod对象。获取到这个对象后执行execute方法。 所以这边需要进入MapperMethod的execute方法：这个方法判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作。（这边以sqlSession.selectOne这种方式进行分析~） public Object execute(SqlSession sqlSession, Object[] args) { Object result; &#x2F;&#x2F;判断是CRUD那种方法 switch (command.getType()) { case INSERT: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.insert(command.getName(), param)); break; } case UPDATE: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.update(command.getName(), param)); break; } case DELETE: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.delete(command.getName(), param)); break; } case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) { executeWithResultHandler(sqlSession, args); result &#x3D; null; } else if (method.returnsMany()) { result &#x3D; executeForMany(sqlSession, args); } else if (method.returnsMap()) { result &#x3D; executeForMap(sqlSession, args); } else if (method.returnsCursor()) { result &#x3D; executeForCursor(sqlSession, args); } else { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; sqlSession.selectOne(command.getName(), param); } break; case FLUSH: result &#x3D; sqlSession.flushStatements(); break; default: throw new BindingException(“Unknown execution method for: “ + command.getName()); } if (result &#x3D;&#x3D; null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) { throw new BindingException(“Mapper method ‘“ + command.getName() + “ attempted to return null from a method with a primitive return type (“ + method.getReturnType() + “).”); } return result; } 详细流程图 https://www.processon.com/view/link/5efc23966376891e81f2a37e sqlSession.selectOne方法会会调到DefaultSqlSession的selectList方法。这个方法获取了获取了MappedStatement对象，并最终调用了Executor的query方法。 public List selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms &#x3D; configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception e) { throw ExceptionFactory.wrapException(“Error querying database. Cause: “ + e, e); } finally { ErrorContext.instance().reset(); } } 然后，通过一层一层的调用（这边省略了缓存操作的环节，会在后面的文章中介绍），最终会来到doQuery方法， 这儿咱们就随便找个Excutor看看doQuery方法的实现吧，我这儿选择了SimpleExecutor: Copy public List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException { Statement stmt &#x3D; null; try { Configuration configuration &#x3D; ms.getConfiguration(); &#x2F;&#x2F;内部封装了ParameterHandler和ResultSetHandler StatementHandler handler &#x3D; configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt &#x3D; prepareStatement(handler, ms.getStatementLog()); &#x2F;&#x2F;StatementHandler封装了Statement, 让 StatementHandler 去处理 return handler.query(stmt, resultHandler); } finally { closeStatement(stmt); } } 接下来，咱们看看StatementHandler 的一个实现类 PreparedStatementHandler（这也是我们最常用的，封装的是PreparedStatement）, 看看它使怎么去处理的： Copy public List query(Statement statement, ResultHandler resultHandler) throws SQLException { &#x2F;&#x2F;到此，原形毕露， PreparedStatement, 这个大家都已经滚瓜烂熟了吧 PreparedStatement ps &#x3D; (PreparedStatement) statement; ps.execute(); &#x2F;&#x2F;结果交给了ResultSetHandler 去处理,处理完之后返回给客户端 return resultSetHandler. handleResultSets(ps); } 到此，整个调用流程结束。 ​ 简单总结 这边结合获取SqlSession的流程，做下简单的总结： SqlSessionFactoryBuilder解析配置文件，包括属性配置、别名配置、拦截器配置、环境（数据源和事务管理器）、Mapper配置等；解析完这些配置后会生成一个Configration对象，这个对象中包含了MyBatis需要的所有配置，然后会用这个Configration对象创建一个SqlSessionFactory对象，这个对象中包含了Configration对象； 拿到SqlSessionFactory对象后，会调用SqlSessionFactory的openSesison方法，这个方法会创建一个Sql执行器（Executor组件中包含了Transaction对象），这个Sql执行器会代理你配置的拦截器方法。 获得上面的Sql执行器后，会创建一个SqlSession（默认使用DefaultSqlSession）,这个SqlSession中也包含了Configration对象和上面创建的Executor对象，所以通过SqlSession也能拿到全局配置； 获得SqlSession对象后就能执行各种CRUD方法了。 以上是获得SqlSession的流程，下面总结下本博客中介绍的Sql的执行流程： 调用SqlSession的getMapper方法，获得Mapper接口的动态代理对象MapperProxy，调用Mapper接口的所有方法都会调用到MapperProxy的invoke方法（动态代理机制）； MapperProxy的invoke方法中唯一做的就是创建一个MapperMethod对象，然后调用这个对象的execute方法，sqlSession会作为execute方法的入参； 往下，层层调下来会进入Executor组件（如果配置插件会对Executor进行动态代理）的query方法，这个方法中会创建一个StatementHandler对象，这个对象中同时会封装ParameterHandler和ResultSetHandler对象。调用StatementHandler预编译参数以及设置参数值，使用ParameterHandler来给sql设置参数。 Executor组件有两个直接实现类，分别是BaseExecutor和CachingExecutor。CachingExecutor静态代理了BaseExecutor。Executor组件封装了Transction组件，Transction组件中又分装了Datasource组件。 调用StatementHandler的增删改查方法获得结果，ResultSetHandler对结果进行封装转换，请求结束。 Executor、StatementHandler 、ParameterHandler、ResultSetHandler，Mybatis的插件会对上面的四个组件进行动态代理。 Mybatis-插件原理 链接：http://note.youdao.com/noteshare?id=80acf548788cef82ffb924f043241365&amp;sub=FAE1C62BE5C4422EBA80EF27A171C067 重要类 MapperRegistry：本质上是一个Map，其中的key是Mapper接口的全限定名，value的MapperProxyFactory； MapperProxyFactory：这个类是MapperRegistry中存的value值，在通过sqlSession获取Mapper时，其实先获取到的是这个工厂，然后通过这个工厂创建Mapper的动态代理类； MapperProxy：实现了InvocationHandler接口，Mapper的动态代理接口方法的调用都会到达这个类的invoke方法； MapperMethod：判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作； SqlSession：作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能； Executor：MyBatis执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护； StatementHandler:封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler:负责对用户传递的参数转换成JDBC Statement 所需要的参数， ResultSetHandler:负责将JDBC返回的ResultSet结果集对象转换成List类型的集合； TypeHandler:负责java数据类型和jdbc数据类型之间的映射和转换 MappedStatement:MappedStatement维护了一条节点的封装， SqlSource:负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql:表示动态生成的SQL语句以及相应的参数信息 Configuration:MyBatis所有的配置信息都维持在Configuration对象之中。 调试主要关注点 MapperProxy.invoke方法：MyBatis的所有Mapper对象都是通过动态代理生成的，任何方法的调用都会调到invoke方法，这个方法的主要功能就是创建MapperMethod对象，并放进缓存。所以调试时我们可以在这个位置打个断点，看下是否成功拿到了MapperMethod对象，并执行了execute方法。 MapperMethod.execute方法：这个方法会判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作。Debug时也建议在此打个断点看下。 DefaultSqlSession.selectList方法：这个方法获取了获取了MappedStatement对象，并最终调用了Executor的query方法； 问题： 1.请介绍下MyBatissql语句的解析过程原理 2.请介绍下MyBatis缓存的原理 3.请介绍下MyBatis插件的原理 第十二章Spring容器(Beanfactory整体架构)BeanFactoryBeanFactory表示Bean工厂，所以很明显，BeanFactory会负责创建Bean，并且提供获取Bean的API。​ 而ApplicationContext是BeanFactory的一种，在Spring源码中，是这么定义的：​ 12345public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver &#123; ...&#125; 所以，我们可以直接来使用DefaultListableBeanFactory，而不用使用ApplicationContext的某个实现类，比如： 12345678DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory();AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setBeanClass(User.class);beanFactory.registerBeanDefinition(&quot;user&quot;, beanDefinition);System.out.println(beanFactory.getBean(&quot;user&quot;)); DefaultListableBeanFactory是非常强大的，支持很多功能，可以通过查看DefaultListableBeanFactory的类继承实现结构来看 这部分现在看不懂没关系，源码熟悉一点后回来再来看都可以。 它实现了很多接口，表示，它拥有很多功能： AliasRegistry：支持别名功能，一个名字可以对应多个别名 BeanDefinitionRegistry：可以注册、保存、移除、获取某个BeanDefinition BeanFactory：Bean工厂，可以根据某个bean的名字、或类型、或别名获取某个Bean对象 SingletonBeanRegistry：可以直接注册、获取某个单例Bean SimpleAliasRegistry：它是一个类，实现了AliasRegistry接口中所定义的功能，支持别名功能 ListableBeanFactory：在BeanFactory的基础上，增加了其他功能，可以获取所有BeanDefinition的beanNames，可以根据某个类型获取对应的beanNames，可以根据某个类型获取{类型：对应的Bean}的映射关系 HierarchicalBeanFactory：在BeanFactory的基础上，添加了获取父BeanFactory的功能 DefaultSingletonBeanRegistry：它是一个类，实现了SingletonBeanRegistry接口，拥有了直接注册、获取某个单例Bean的功能 ConfigurableBeanFactory：在HierarchicalBeanFactory和SingletonBeanRegistry的基础上，添加了设置父BeanFactory、类加载器（表示可以指定某个类加载器进行类的加载）、设置Spring EL表达式解析器（表示该BeanFactory可以解析EL表达式）、设置类型转化服务（表示该BeanFactory可以进行类型转化）、可以添加BeanPostProcessor（表示该BeanFactory支持Bean的后置处理器），可以合并BeanDefinition，可以销毁某个Bean等等功能 FactoryBeanRegistrySupport：支持了FactoryBean的功能 AutowireCapableBeanFactory：是直接继承了BeanFactory，在BeanFactory的基础上，支持在创建Bean的过程中能对Bean进行自动装配 AbstractBeanFactory：实现了ConfigurableBeanFactory接口，继承了FactoryBeanRegistrySupport，这个BeanFactory的功能已经很全面了，但是不能自动装配和获取beanNames ConfigurableListableBeanFactory：继承了ListableBeanFactory、AutowireCapableBeanFactory、ConfigurableBeanFactory AbstractAutowireCapableBeanFactory：继承了AbstractBeanFactory，实现了AutowireCapableBeanFactory，拥有了自动装配的功能 DefaultListableBeanFactory：继承了AbstractAutowireCapableBeanFactory，实现了ConfigurableListableBeanFactory接口和BeanDefinitionRegistry接口，所以DefaultListableBeanFactory的功能很强大 ApplicationContext上面有分析到，ApplicationContext是个接口，实际上也是一个BeanFactory，不过比BeanFactory更加强大，比如：​ HierarchicalBeanFactory：拥有获取父BeanFactory的功能 ListableBeanFactory：拥有获取beanNames的功能 ResourcePatternResolver：资源加载器，可以一次性获取多个资源（文件资源等等） EnvironmentCapable：可以获取运行时环境（没有设置运行时环境功能） ApplicationEventPublisher：拥有广播事件的功能（没有添加事件监听器的功能） MessageSource：拥有国际化功能 具体的功能演示，后面会有。 我们先来看ApplicationContext两个比较重要的实现类： AnnotationConfigApplicationContext ClassPathXmlApplicationContext AnnotationConfigApplicationContext 这部分现在看不懂没关系，源码熟悉一点后回来再来看都可以。 ConfigurableApplicationContext：继承了ApplicationContext接口，增加了，添加事件监听器、添加BeanFactoryPostProcessor、设置Environment，获取ConfigurableListableBeanFactory等功能 AbstractApplicationContext：实现了ConfigurableApplicationContext接口 GenericApplicationContext：继承了AbstractApplicationContext，实现了BeanDefinitionRegistry接口，拥有了所有ApplicationContext的功能，并且可以注册BeanDefinition，注意这个类中有一个属性(DefaultListableBeanFactory beanFactory) AnnotationConfigRegistry：可以单独注册某个为类为BeanDefinition（可以处理该类上的**@Configuration注解，已经可以处理@Bean注解**），同时可以扫描 AnnotationConfigApplicationContext：继承了GenericApplicationContext，实现了AnnotationConfigRegistry接口，拥有了以上所有的功能 ClassPathXmlApplicationContext它也是继承了AbstractApplicationContext，但是相对于AnnotationConfigApplicationContext而言，功能没有AnnotationConfigApplicationContext强大，比如不能注册BeanDefinition 国际化 创建两个国际化文件 一个设置为test &#x3D; a 一个设置为test &#x3D; b 先定义一个MessageSource: 123456@Beanpublic MessageSource messageSource() &#123; ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); messageSource.setBasename(&quot;messages&quot;); return messageSource;&#125; 有了这个Bean，你可以在你任意想要进行国际化的地方使用该MessageSource。同时，因为ApplicationContext也拥有国家化的功能，所以可以直接这么用： 1context.getMessage(&quot;test&quot;, null, new Locale(&quot;en&quot;)) 输出 b 实际使用 就是一个类实现ApplicationContextAware 来回调进行使用 资源加载ApplicationContext还拥有资源加载的功能，比如，可以直接利用ApplicationContext获取某个文件的内容： 1234AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);// 获取一个文件Resource resource = context.getResource(&quot;file://D:\\\\IdeaProjects\\\\spring-framework\\\\luban\\\\src\\\\main\\\\java\\\\com\\\\luban\\\\entity\\\\User.java&quot;);System.out.println(resource.contentLength()); // 获得字符长度 你可以想想，如果你不使用ApplicationContext，而是自己来实现这个功能，就比较费时间了。 还比如你可以： 12345678910111213AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);Resource resource = context.getResource(&quot;file://D:\\\\IdeaProjects\\\\spring-framework-5.3.10\\\\tuling\\\\src\\\\main\\\\java\\\\com\\\\zhouyu\\\\service\\\\UserService.java&quot;);System.out.println(resource.contentLength());System.out.println(resource.getFilename());Resource resource1 = context.getResource(&quot;https://www.baidu.com&quot;);System.out.println(resource1.contentLength());System.out.println(resource1.getURL());Resource resource2 = context.getResource(&quot;classpath:spring.xml&quot;);System.out.println(resource2.contentLength());System.out.println(resource2.getURL()); 还可以一次性获取多个： 12345Resource[] resources = context.getResources(&quot;classpath:com/zhouyu/*.class&quot;);for (Resource resource : resources) &#123; System.out.println(resource.contentLength()); System.out.println(resource.getFilename());&#125; 获取运行时环境1234567891011121314151617181920212223AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);Map&lt;String, Object&gt; systemEnvironment = context.getEnvironment().getSystemEnvironment();// 系统环境变量System.out.println(systemEnvironment);System.out.println(&quot;=======&quot;);Map&lt;String, Object&gt; systemProperties = context.getEnvironment().getSystemProperties();//java环境变量System.out.println(systemProperties);System.out.println(&quot;=======&quot;);MutablePropertySources propertySources = context.getEnvironment().getPropertySources();// 包含前两者和加的配置文件System.out.println(propertySources);System.out.println(&quot;=======&quot;);// 具体的获取值System.out.println(context.getEnvironment().getProperty(&quot;NO_PROXY&quot;));System.out.println(context.getEnvironment().getProperty(&quot;sun.jnu.encoding&quot;));System.out.println(context.getEnvironment().getProperty(&quot;zhouyu&quot;)); 注意，可以利用 123456加载配置类上面@PropertySource(&quot;classpath:spring.properties&quot;) //创建对应的配置文件MutablePropertySources propertySources = context.getEnvironment().getPropertySources();System.out.println(propertySources); 来使得某个properties文件中的参数添加到运行时环境中 第十三章 实践Spring的使用 与 真正实现的逻辑代码(增强代码&#x2F;业务代码) Bean生命周期(重要紧急)理解各个生命周期的作用与记忆, 依赖注入实现通过Spring提供的组件(解析器等)实现具体Bean后置处理器的逻辑,解析注解等 整合Mybatis其他框架注册Bean到容器中并有些定制处理的实现(BeanDefinition,BeanFacory的增强),解析配置类等等 逻辑: AOP与事务(重要紧急)可涉及到aop,事务,对Bean进行增强 aop: AbstractAdvisorAutoProxyCreator实际上就是一个BeanPostProcessor在初始化后进行增强 逻辑: 使用自带的ProxyFactory生成代理类替换需要增强的类 第十四章 使用Spring注解@AnnotationConfigApplicationContext组件添加@Configuration+@BeanXML文件方式Person1234567891011121314151617181920212223242526272829303132333435363738394041public class Person &#123; private String name; private Integer age; private String nickName; public String getNickName() &#123; return nickName; &#125; public void setNickName(String nickName) &#123; this.nickName = nickName; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Person(String name, Integer age) &#123; super(); this.name = name; this.age = age; &#125; public Person() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return &quot;Person [name=&quot; + name + &quot;, age=&quot; + age + &quot;, nickName=&quot; + nickName + &quot;]&quot;; &#125;&#125; beans.xml-配置文件1234567891011121314151617&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.3.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.3.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.atguigu&quot; use-default-filters=&quot;false&quot;&gt;&lt;/context:component-scan&gt; &lt;bean id=&quot;person&quot; class=&quot;com.atguigu.bean.Person&quot;&gt; &lt;property name=&quot;age&quot; value=&quot;18&quot;&gt;&lt;/property&gt; &lt;property name=&quot;name&quot; value=&quot;zhangsan&quot;&gt;&lt;/property&gt; &lt;/bean&gt;&lt;/beans&gt; MainTest123456789public class MainTest &#123; @SuppressWarnings(&quot;resource&quot;) public static void main(String[] args) &#123; ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;beans.xml&quot;); Person bean = (Person) applicationContext.getBean(&quot;person&quot;); System.out.println(bean); &#125;&#125; 输出 1Person [name=zhangsan, age=18, nickName=null] 注解方式1234567891011//配置类==配置文件@Configuration //告诉Spring这是一个配置类public class MainConfig &#123; //给容器中注册一个Bean;类型为返回值的类型，id默认是用方法名作为id(就是bean的名字)，在这里就是person01 @Bean public Person person01()&#123; return new Person(&quot;lisi&quot;, 20); &#125;&#125; 或者以下面的这种方式 1234567891011//配置类==配置文件@Configuration //告诉Spring这是一个配置类public class MainConfig &#123; //这里bean的name就是person @Bean(&quot;person&quot;) public Person person01()&#123; return new Person(&quot;lisi&quot;, 20); &#125;&#125; @ComponentScans1234567891011121314151617181920212223242526272829//配置类==配置文件@Configuration //告诉Spring这是一个配置类@ComponentScans( value = &#123; @ComponentScan(value=&quot;com.atguigu&quot;,includeFilters = &#123;/* @Filter(type=FilterType.ANNOTATION,classes=&#123;Controller.class&#125;), @Filter(type=FilterType.ASSIGNABLE_TYPE,classes=&#123;BookService.class&#125;),*/ @Filter(type=FilterType.CUSTOM,classes=&#123;MyTypeFilter.class&#125;) &#125;,useDefaultFilters = false) &#125; )//@ComponentScan value:指定要扫描的包//excludeFilters = Filter[] ：指定扫描的时候按照什么规则排除那些组件//includeFilters = Filter[] ：指定扫描的时候只需要包含哪些组件//FilterType.ANNOTATION：按照注解//FilterType.ASSIGNABLE_TYPE：按照给定的类型；//FilterType.ASPECTJ：使用ASPECTJ表达式//FilterType.REGEX：使用正则指定//FilterType.CUSTOM：使用自定义规则public class MainConfig &#123; //给容器中注册一个Bean;类型为返回值的类型，id默认是用方法名作为id @Bean(&quot;person&quot;) public Person person01()&#123; return new Person(&quot;lisi&quot;, 20); &#125;&#125; 自定义TypeFilter指定包扫描规则1234567891011121314151617181920212223242526public class MyTypeFilter implements TypeFilter &#123; /** * metadataReader：读取到的当前正在扫描的类的信息 * metadataReaderFactory:可以获取到其他任何类信息的 */ @Override public boolean match(MetadataReader metadataReader, MetadataReaderFactory metadataReaderFactory) throws IOException &#123; // TODO Auto-generated method stub //获取当前类注解的信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); //获取当前正在扫描的类的类信息 ClassMetadata classMetadata = metadataReader.getClassMetadata(); //获取当前类资源（类的路径） Resource resource = metadataReader.getResource(); String className = classMetadata.getClassName(); System.out.println(&quot;---&gt;&quot;+className); if(className.contains(&quot;er&quot;))&#123; return true; &#125; return false; &#125;&#125; @Scope1234567891011121314151617181920212223242526272829@Configurationpublic class MainConfig2 &#123; /** * @see ConfigurableBeanFactory#SCOPE_PROTOTYPE 任何环境都可以使用 * @see ConfigurableBeanFactory#SCOPE_SINGLETON 任何环境都可以使用 * @see org.springframework.web.context.WebApplicationContext#SCOPE_REQUEST request 只能在web容器里用 * @see org.springframework.web.context.WebApplicationContext#SCOPE_SESSION sesssion 只能在web容器里用 * * @Scope:调整作用域 * prototype：多实例的：ioc容器启动并不会去调用方法创建对象放在容器中。 * 每次获取的时候才会调用方法创建对象； * singleton：单实例的（默认值）：ioc容器启动会调用方法创建对象放到ioc容器中。 * 以后每次获取就是直接从容器（map.get()）中拿， * request：同一次请求创建一个实例 * session：同一个session创建一个实例 * * 默认是单实例的 * */ @Scope(&quot;prototype&quot;) @Lazy @Bean(&quot;person&quot;) public Person person()&#123; System.out.println(&quot;给容器中添加Person....&quot;); return new Person(&quot;张三&quot;, 25); &#125;&#125; @Lazy12345678910111213141516@Configurationpublic class MainConfig2 &#123; /** * * 懒加载： * 单实例bean：默认在容器启动的时候创建对象； * 懒加载：容器启动不创建对象。第一次使用(获取)Bean创建对象，并初始化； * */ @Lazy @Bean(&quot;person&quot;) public Person person()&#123; System.out.println(&quot;给容器中添加Person....&quot;); return new Person(&quot;张三&quot;, 25); &#125; @ConditionalMainConfig2123456789101112131415161718192021222324//类中组件统一设置。满足当前条件，这个类中配置的所有bean注册才能生效；@Conditional(&#123;WindowsCondition.class&#125;)@Configurationpublic class MainConfig2 &#123; /** * @Conditional(&#123;Condition&#125;) ： 按照一定的条件进行判断，满足条件给容器中注册bean * * 如果系统是windows，给容器中注册(&quot;bill&quot;) * 如果是linux系统，给容器中注册(&quot;linus&quot;) */ @Bean(&quot;bill&quot;) public Person person01()&#123; return new Person(&quot;Bill Gates&quot;,62); &#125; @Conditional(LinuxCondition.class) @Bean(&quot;linus&quot;) public Person person02()&#123; return new Person(&quot;linus&quot;, 48); &#125;&#125; LinuxCondition1234567891011121314151617181920212223242526272829//判断是否linux系统public class LinuxCondition implements Condition &#123; /** * ConditionContext：判断条件能使用的上下文（环境） * AnnotatedTypeMetadata：注释信息 */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; // TODO是否linux系统 //1、能获取到ioc使用的beanfactory ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); //2、获取类加载器 ClassLoader classLoader = context.getClassLoader(); //3、获取当前环境信息 Environment environment = context.getEnvironment(); //4、获取到bean定义的注册类 BeanDefinitionRegistry registry = context.getRegistry(); String property = environment.getProperty(&quot;os.name&quot;); //可以判断容器中的bean注册情况，也可以给容器中注册bean boolean definition = registry.containsBeanDefinition(&quot;person&quot;); if(property.contains(&quot;linux&quot;))&#123; return true; &#125; return false; &#125; WindowsCondition1234567891011121314//判断是否windows系统public class WindowsCondition implements Condition &#123; @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; Environment environment = context.getEnvironment(); String property = environment.getProperty(&quot;os.name&quot;); if(property.contains(&quot;Windows&quot;))&#123; return true; &#125; return false; &#125;&#125; @ImportMainConfig212345678910111213141516171819@Configuration@Import(&#123;Color.class,Red.class,MyImportSelector.class,MyImportBeanDefinitionRegistrar.class&#125;)//@Import导入组件，id默认是组件的全类名public class MainConfig2 &#123; /** * 给容器中注册组件； * 1）、包扫描+组件标注注解（@Controller/@Service/@Repository/@Component）[只能注册自己写的类] * 2）、@Bean[导入的第三方包里面的组件] * 3）、@Import[快速给容器中导入一个组件] * 1）、@Import(要导入到容器中的组件)；容器中就会自动注册这个组件，id默认是全类名 * 2）、ImportSelector:返回需要导入的组件的全类名数组； * 3）、ImportBeanDefinitionRegistrar:手动注册bean到容器中 */ @Bean public ColorFactoryBean colorFactoryBean()&#123; return new ColorFactoryBean(); &#125;&#125; MyImportSelector12345678910111213//自定义逻辑返回需要导入的组件public class MyImportSelector implements ImportSelector &#123; //返回值，就是到导入到容器中的组件全类名 //AnnotationMetadata:@Import引入MyImportSelector的类的所有注解信息 @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; //importingClassMetadata.get //方法不要返回null值，不然会报错 return new String[]&#123;&quot;com.atguigu.bean.Blue&quot;,&quot;com.atguigu.bean.Yellow&quot;&#125;; &#125;&#125; MyImportBeanDefinitionRegistrar12345678910111213141516171819202122public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * AnnotationMetadata：当前类的注解信息 * BeanDefinitionRegistry:BeanDefinition注册类； * 把所有需要添加到容器中的bean；调用 * BeanDefinitionRegistry.registerBeanDefinition手工注册进来 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; boolean definition = registry.containsBeanDefinition(&quot;com.atguigu.bean.Red&quot;); boolean definition2 = registry.containsBeanDefinition(&quot;com.atguigu.bean.Blue&quot;); if(definition &amp;&amp; definition2)&#123; //指定Bean定义信息；（Bean的类型，Bean。。。） RootBeanDefinition beanDefinition = new RootBeanDefinition(RainBow.class); //注册一个Bean，指定bean名 registry.registerBeanDefinition(&quot;rainBow&quot;, beanDefinition); &#125; &#125;&#125; 123456789101112131415161718public class Color &#123; private Car car; public Car getCar() &#123; return car; &#125; public void setCar(Car car) &#123; this.car = car; &#125; @Override public String toString() &#123; return &quot;Color [car=&quot; + car + &quot;]&quot;; &#125; &#125; 123456789101112131415public class Blue &#123; public Blue()&#123; System.out.println(&quot;blue...constructor&quot;); &#125; public void init()&#123; System.out.println(&quot;blue...init...&quot;); &#125; public void detory()&#123; System.out.println(&quot;blue...detory...&quot;); &#125; &#125; FactoryBeanMainConfig212345678910111213141516171819202122232425@Configurationpublic class MainConfig2 &#123; /** * 给容器中注册组件； * 1）、包扫描+组件标注注解（@Controller/@Service/@Repository/@Component）[自己写的类] * 2）、@Bean[导入的第三方包里面的组件] * 3）、@Import[快速给容器中导入一个组件] * 1）、@Import(要导入到容器中的组件)；容器中就会自动注册这个组件，id默认是全类名 * 2）、ImportSelector:返回需要导入的组件的全类名数组； * 3）、ImportBeanDefinitionRegistrar:手动注册bean到容器中 * 4）、使用Spring提供的 FactoryBean（工厂Bean）; * 1）、默认获取到的是工厂bean调用getObject创建的对象 * 2）、要获取工厂Bean本身，我们需要给id前面加一个&amp; * &amp;colorFactoryBean * * 虽然这里装配的是ColorFactoryBean，但实际上beand的类型是Color */ @Bean public ColorFactoryBean colorFactoryBean()&#123; return new ColorFactoryBean(); &#125;&#125; ColorFactoryBean123456789101112131415161718192021222324252627//创建一个Spring定义的FactoryBeanpublic class ColorFactoryBean implements FactoryBean&lt;Color&gt; &#123; //返回一个Color对象，这个对象会添加到容器中 @Override public Color getObject() throws Exception &#123; // TODO Auto-generated method stub System.out.println(&quot;ColorFactoryBean...getObject...&quot;); return new Color(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; // TODO Auto-generated method stub return Color.class; &#125; //是单例？ //true：这个bean是单实例，在容器中保存一份 //false：多实例，每次获取都会创建一个新的bean； @Override public boolean isSingleton() &#123; // TODO Auto-generated method stub return false; &#125;&#125; IOCTest12345678910111213141516171819202122232425public class IOCTest &#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfig2.class); @Test public void testImport()&#123; printBeans(applicationContext); Blue bean = applicationContext.getBean(Blue.class); System.out.println(bean); //工厂Bean获取的是调用getObject创建的对象 Object bean2 = applicationContext.getBean(&quot;colorFactoryBean&quot;); System.out.println(&quot;bean的类型：&quot;+bean2.getClass()); //pos_1 输出：bean的类型：class com.atguigu.bean.Color Object bean4 = applicationContext.getBean(&quot;&amp;colorFactoryBean&quot;); System.out.println(bean4.getClass()); //pos_2 输出：class com.atguigu.bean.ColorFactoryBean &#125; private void printBeans(AnnotationConfigApplicationContext applicationContext)&#123; String[] definitionNames = applicationContext.getBeanDefinitionNames(); for (String name : definitionNames) &#123; System.out.println(name); &#125; &#125;&#125; 输出: 123456//前面无关的输出省略colorFactoryBeanColorFactoryBean...getObject...bean的类型：class com.atguigu.bean.Colorclass com.atguigu.bean.ColorFactoryBean 生命周期@Bean指定初始化和销毁方法IOCTest_LifeCycle 后面的几个用的都是这个测试类 1234567891011121314public class IOCTest_LifeCycle &#123; @Test public void test01()&#123; //1、创建ioc容器 AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfLifeCycle.class); System.out.println(&quot;容器创建完成...&quot;); //applicationContext.getBean(&quot;car&quot;); //关闭容器 applicationContext.close(); &#125;&#125; MainConfigOfLifeCycle123456789101112131415161718192021222324252627282930313233343536373839404142package com.atguigu.config;import org.springframework.context.ApplicationListener;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.ComponentScan;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.Scope;import com.atguigu.bean.Car;/** * bean的生命周期： * bean创建---初始化----销毁的过程 * 容器管理bean的生命周期； * 我们可以自定义初始化和销毁方法；容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造（对象创建） * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象\\ * 初始化： * 对象创建完成，并赋值好，调用初始化方法。。。 * BeanPostProcessor.postProcessAfterInitialization * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean；容器不会调用销毁方法； * * 1）、指定初始化和销毁方法； * 通过@Bean指定init-method和destroy-method； * @author lfy * */@ComponentScan(&quot;com.atguigu.bean&quot;)@Configurationpublic class MainConfigOfLifeCycle &#123; //@Scope(&quot;prototype&quot;) @Bean(initMethod=&quot;init&quot;,destroyMethod=&quot;detory&quot;) public Car car()&#123; return new Car(); &#125;&#125; 12345678910111213141516@Componentpublic class Car &#123; public Car()&#123; System.out.println(&quot;car constructor...&quot;); &#125; public void init()&#123; System.out.println(&quot;car ... init...&quot;); &#125; public void detory()&#123; System.out.println(&quot;car ... detory...&quot;); &#125;&#125; 输出1234car constructor...car ... init...容器创建完成car ... detory... InitializingBean和DisposableBeanMainConfigOfLifeCycle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * bean的生命周期： * bean创建---初始化----销毁的过程 * 容器管理bean的生命周期； * 我们可以自定义初始化和销毁方法；容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造（对象创建） * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象\\ * * BeanPostProcessor.postProcessBeforeInitialization * 初始化： * 对象创建完成，并赋值好，调用初始化方法。。。 * BeanPostProcessor.postProcessAfterInitialization * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean；容器不会调用销毁方法； * * * 遍历得到容器中所有的BeanPostProcessor；挨个执行beforeInitialization， * 一但返回null，跳出for循环，不会执行后面的BeanPostProcessor.postProcessorsBeforeInitialization * * BeanPostProcessor原理 * populateBean(beanName, mbd, instanceWrapper);给bean进行属性赋值 * initializeBean * &#123; * applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); * invokeInitMethods(beanName, wrappedBean, mbd);执行自定义初始化 * applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); *&#125; * * * * 1）、指定初始化和销毁方法； * 通过@Bean指定init-method和destroy-method； * 2）、通过让Bean实现InitializingBean（定义初始化逻辑）， * DisposableBean（定义销毁逻辑）; * * @author lfy * */@ComponentScan(&quot;com.atguigu.bean&quot;)@Configurationpublic class MainConfigOfLifeCycle &#123; //@Scope(&quot;prototype&quot;) @Bean(initMethod=&quot;init&quot;,destroyMethod=&quot;detory&quot;) public Car car()&#123; return new Car(); &#125;&#125; Cat1234567891011121314151617181920@Componentpublic class Cat implements InitializingBean,DisposableBean &#123; public Cat()&#123; System.out.println(&quot;cat constructor...&quot;); &#125; @Override public void destroy() throws Exception &#123; // TODO Auto-generated method stub System.out.println(&quot;cat...destroy...&quot;); &#125; @Override public void afterPropertiesSet() throws Exception &#123; // TODO Auto-generated method stub System.out.println(&quot;cat...afterPropertiesSet...&quot;); &#125;&#125; 输出1234567cat constructor...cat...afterPropertiesSet...car constructor...car ... init...容器创建完成car ... detory...cat...destroy... @PostConstruct和@PreDestroyMainConfigOfLifeCycle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * bean的生命周期： * bean创建---初始化----销毁的过程 * 容器管理bean的生命周期； * 我们可以自定义初始化和销毁方法；容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造（对象创建） * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象\\ * * BeanPostProcessor.postProcessBeforeInitialization * 初始化： * 对象创建完成，并赋值好，调用初始化方法。。。 * BeanPostProcessor.postProcessAfterInitialization * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean；容器不会调用销毁方法； * * * 遍历得到容器中所有的BeanPostProcessor；挨个执行beforeInitialization， * 一但返回null，跳出for循环，不会执行后面的BeanPostProcessor.postProcessorsBeforeInitialization * * BeanPostProcessor原理 * populateBean(beanName, mbd, instanceWrapper);给bean进行属性赋值 * initializeBean * &#123; * applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); * invokeInitMethods(beanName, wrappedBean, mbd);执行自定义初始化 * applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); *&#125; * * * * 1）、指定初始化和销毁方法； * 通过@Bean指定init-method和destroy-method； * 2）、通过让Bean实现InitializingBean（定义初始化逻辑）， * DisposableBean（定义销毁逻辑）; * 3）、可以使用JSR250； * @PostConstruct：在bean创建完成并且属性赋值完成；来执行初始化方法 * @PreDestroy：在容器销毁bean之前通知我们进行清理工作 * * @author lfy * */@ComponentScan(&quot;com.atguigu.bean&quot;)@Configurationpublic class MainConfigOfLifeCycle &#123; //@Scope(&quot;prototype&quot;) @Bean(initMethod=&quot;init&quot;,destroyMethod=&quot;detory&quot;) public Car car()&#123; return new Car(); &#125;&#125; Dog12345678910111213141516171819202122232425262728@Componentpublic class Dog implements ApplicationContextAware &#123; //@Autowired private ApplicationContext applicationContext; public Dog()&#123; System.out.println(&quot;dog constructor...&quot;); &#125; //对象创建并赋值之后调用 @PostConstruct public void init()&#123; System.out.println(&quot;Dog....@PostConstruct...&quot;); &#125; //容器移除对象之前 @PreDestroy public void detory()&#123; System.out.println(&quot;Dog....@PreDestroy...&quot;); &#125; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; // TODO Auto-generated method stub this.applicationContext = applicationContext; &#125;&#125; 输出12345678910cat constructor...cat...afterPropertiesSet...dog constructor...Dog....@PostConstruct...car constructor...car ... init...容器创建完成car ... detory...Dog....@PreDestroy...cat...destroy... BeanPostProcessor1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * bean的生命周期： * bean创建---初始化----销毁的过程 * 容器管理bean的生命周期； * 我们可以自定义初始化和销毁方法；容器在bean进行到当前生命周期的时候来调用我们自定义的初始化和销毁方法 * * 构造（对象创建） * 单实例：在容器启动的时候创建对象 * 多实例：在每次获取的时候创建对象\\ * * BeanPostProcessor.postProcessBeforeInitialization * 初始化： * 对象创建完成，并赋值好，调用初始化方法。。。 * BeanPostProcessor.postProcessAfterInitialization * 销毁： * 单实例：容器关闭的时候 * 多实例：容器不会管理这个bean；容器不会调用销毁方法； * * * 遍历得到容器中所有的BeanPostProcessor；挨个执行beforeInitialization， * 一但返回null，跳出for循环，不会执行后面的BeanPostProcessor.postProcessorsBeforeInitialization * * BeanPostProcessor原理 * populateBean(beanName, mbd, instanceWrapper);给bean进行属性赋值 * initializeBean * &#123; * applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); * invokeInitMethods(beanName, wrappedBean, mbd);执行自定义初始化 * applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); *&#125; * * * * 1）、指定初始化和销毁方法； * 通过@Bean指定init-method和destroy-method； * 2）、通过让Bean实现InitializingBean（定义初始化逻辑）， * DisposableBean（定义销毁逻辑）; * 3）、可以使用JSR250； * @PostConstruct：在bean创建完成并且属性赋值完成；来执行初始化方法 * @PreDestroy：在容器销毁bean之前通知我们进行清理工作 * 4）、BeanPostProcessor【interface】：bean的后置处理器； * 在bean初始化前后进行一些处理工作； * postProcessBeforeInitialization:在初始化之前工作 * postProcessAfterInitialization:在初始化之后工作 * * Spring底层对 BeanPostProcessor 的使用； * bean赋值，注入其他组件，@Autowired，生命周期注解功能，@Async,xxx BeanPostProcessor; * * @author lfy * */@ComponentScan(&quot;com.atguigu.bean&quot;)@Configurationpublic class MainConfigOfLifeCycle &#123; //@Scope(&quot;prototype&quot;) @Bean(initMethod=&quot;init&quot;,destroyMethod=&quot;detory&quot;) public Car car()&#123; return new Car(); &#125;&#125; MyBeanPostProcessor1234567891011121314151617181920212223/** * 后置处理器：初始化前后进行处理工作 * 将后置处理器加入到容器中 * @author lfy */@Componentpublic class MyBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println(&quot;postProcessBeforeInitialization...&quot;+beanName+&quot;=&gt;&quot;+bean); return bean; &#125; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; // TODO Auto-generated method stub System.out.println(&quot;postProcessAfterInitialization...&quot;+beanName+&quot;=&gt;&quot;+bean); return bean; &#125;&#125; 输出 自己写的组件输出内容 12345678910111213141516car constructor...postProcessBeforeInitialization...car=&gt;com.atguigu.bean.Car@5ef60048car ... init...postProcessAfterInitialization...car=&gt;com.atguigu.bean.Car@5ef60048cat constructor...postProcessBeforeInitialization...cat=&gt;com.atguigu.bean.Cat@780cb77cat...afterPropertiesSet...postProcessAfterInitialization...cat=&gt;com.atguigu.bean.Cat@780cb77dog constructor...postProcessBeforeInitialization...dog=&gt;com.atguigu.bean.Dog@4034c28cDog....@PostConstruct...postProcessAfterInitialization...dog=&gt;com.atguigu.bean.Dog@4034c28c容器创建完成...Dog....@PreDestroy...cat...destroy...car ... detory... BeanPostProcessor在Spring源码里大量被使用到，仅凭这里雷丰阳老师讲的一点点原理，是无法体会的，建议自己去看看Spring源码。所以这里的原理部分我也就直接省略了，在本视频中讲的太浅了。 属性赋值@Value和@PropertySourcePerson1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Person &#123; //使用@Value赋值； //1、基本数值 //2、可以写SpEL； #&#123;&#125; //3、可以写$&#123;&#125;；取出配置文件【properties】中的值（在运行环境变量里面的值） @Value(&quot;张三&quot;) private String name; @Value(&quot;#&#123;20-2&#125;&quot;) private Integer age; @Value(&quot;$&#123;person.nickName&#125;&quot;) private String nickName; public String getNickName() &#123; return nickName; &#125; public void setNickName(String nickName) &#123; this.nickName = nickName; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public Integer getAge() &#123; return age; &#125; public void setAge(Integer age) &#123; this.age = age; &#125; public Person(String name, Integer age) &#123; super(); this.name = name; this.age = age; &#125; public Person() &#123; super(); // TODO Auto-generated constructor stub &#125; @Override public String toString() &#123; return &quot;Person [name=&quot; + name + &quot;, age=&quot; + age + &quot;, nickName=&quot; + nickName + &quot;]&quot;; &#125;&#125; person.properties1person.nickName=\\u5C0F\\u674E\\u56DB MainConfigOfPropertyValues1234567891011//使用@PropertySource读取外部配置文件中的k/v保存到运行的环境变量中;加载完外部的配置文件以后使用$&#123;&#125;取出配置文件的值@PropertySource(value=&#123;&quot;classpath:/person.properties&quot;&#125;)@Configurationpublic class MainConfigOfPropertyValues &#123; @Bean public Person person()&#123; return new Person(); &#125;&#125; IOCTest_PropertyValue12345678910111213141516171819202122232425public class IOCTest_PropertyValue &#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConfigOfPropertyValues.class); @Test public void test01()&#123; printBeans(applicationContext); System.out.println(&quot;=============&quot;); Person person = (Person) applicationContext.getBean(&quot;person&quot;); System.out.println(person); ConfigurableEnvironment environment = applicationContext.getEnvironment(); String property = environment.getProperty(&quot;person.nickName&quot;); System.out.println(property); applicationContext.close(); &#125; private void printBeans(AnnotationConfigApplicationContext applicationContext)&#123; String[] definitionNames = applicationContext.getBeanDefinitionNames(); for (String name : definitionNames) &#123; System.out.println(name); &#125; &#125;&#125; 输出12345mainConfigOfPropertyValuesperson=============Person [name=张三, age=18, nickName=小李四]小李四 自动装配@Autowired-@Qualifier-@Primary-@Resource-@Inject1234567@Controllerpublic class BookController &#123; @Autowired private BookService bookService;&#125; 1234567891011121314151617181920@Servicepublic class BookService &#123; //@Qualifier(&quot;bookDao&quot;) //@Autowired(required=false) //@Resource(name=&quot;bookDao2&quot;) @Inject private BookDao bookDao; public void print()&#123; System.out.println(bookDao); &#125; @Override public String toString() &#123; return &quot;BookService [bookDao=&quot; + bookDao + &quot;]&quot;; &#125; &#125; 12345678910111213141516171819202122//名字默认是类名首字母小写@Repositorypublic class BookDao &#123; private String lable = &quot;1&quot;; public String getLable() &#123; return lable; &#125; public void setLable(String lable) &#123; this.lable = lable; &#125; @Override public String toString() &#123; return &quot;BookDao [lable=&quot; + lable + &quot;]&quot;; &#125; &#125; MainConifgOfAutowired1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 自动装配; * Spring利用依赖注入（DI），完成对IOC容器中中各个组件的依赖关系赋值； * * 1）、@Autowired：自动注入： * 1）、默认优先按照类型去容器中找对应的组件:applicationContext.getBean(BookDao.class);找到就赋值 * 2）、如果找到多个相同类型的组件，再将属性的名称作为组件的id去容器中查找 * applicationContext.getBean(&quot;bookDao&quot;) * 3）、@Qualifier(&quot;bookDao&quot;)：使用@Qualifier指定需要装配的组件的id，而不是使用属性名 * 4）、自动装配默认一定要将属性赋值好，没有就会报错； * 可以使用@Autowired(required=false); * 5）、@Primary：让Spring进行自动装配的时候，默认使用首选的bean； * 也可以继续使用@Qualifier指定需要装配的bean的名字 * BookService&#123; * @Autowired * BookDao bookDao; * &#125; * * 2）、Spring还支持使用@Resource(JSR250)和@Inject(JSR330)[java规范的注解] * @Resource: * 可以和@Autowired一样实现自动装配功能；默认是按照组件名称进行装配的； * 没有能支持@Primary功能没有支持@Autowired（reqiured=false）; * @Inject: * 需要导入javax.inject的包，和Autowired的功能一样。没有required=false的功能； * @Autowired:Spring定义的； @Resource、@Inject都是java规范 * * AutowiredAnnotationBeanPostProcessor:解析完成自动装配功能； * * 3）、 @Autowired:构造器，参数，方法，属性；都是从容器中获取参数组件的值 * 1）、[标注在方法位置]：@Bean+方法参数；参数从容器中获取;默认不写@Autowired效果是一样的；都能自动装配 * 2）、[标在构造器上]：如果组件只有一个有参构造器，这个有参构造器的@Autowired可以省略，参数位置的组件还是可以自动从容器中获取 * 3）、放在参数位置： public Boss(@Autowired Car car)&#123; this.car = car; System.out.println(&quot;Boss...有参构造器&quot;); &#125; * * 4）、自定义组件想要使用Spring容器底层的一些组件（ApplicationContext，BeanFactory，xxx）； * 自定义组件实现xxxAware；在创建对象的时候，会调用接口规定的方法注入相关组件；Aware； * 把Spring底层一些组件注入到自定义的Bean中； * xxxAware：功能使用xxxProcessor； * ApplicationContextAware==》ApplicationContextAwareProcessor； * * * @author lfy * */@Configuration@ComponentScan(&#123;&quot;com.atguigu.service&quot;,&quot;com.atguigu.dao&quot;, &quot;com.atguigu.controller&quot;,&quot;com.atguigu.bean&quot;&#125;)public class MainConifgOfAutowired &#123; @Primary @Bean(&quot;bookDao2&quot;) public BookDao bookDao()&#123; BookDao bookDao = new BookDao(); bookDao.setLable(&quot;2&quot;); return bookDao; &#125; /** * @Bean标注的方法创建对象的时候，方法参数的值默认从容器中获取 * @param car * @return */ @Bean public Color color(Car car)&#123; Color color = new Color(); color.setCar(car); return color; &#125; &#125; IOCTest_Autowired123456789101112131415161718192021222324public class IOCTest_Autowired &#123; @Test public void test01()&#123; AnnotationConfigApplicationContext applicationContext = new AnnotationConfigApplicationContext(MainConifgOfAutowired.class); BookService bookService = applicationContext.getBean(BookService.class); System.out.println(bookService); //BookDao bean = applicationContext.getBean(BookDao.class); //System.out.println(bean); Boss boss = applicationContext.getBean(Boss.class); System.out.println(boss); Car car = applicationContext.getBean(Car.class); System.out.println(car); Color color = applicationContext.getBean(Color.class); System.out.println(color); System.out.println(applicationContext); applicationContext.close(); &#125;&#125; @ProfleMainConfigOfProfile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * Profile： * Spring为我们提供的可以根据当前环境，动态的激活和切换一系列组件的功能； * * 开发环境、测试环境、生产环境； * 数据源：(/A)(/B)(/C)； * * * @Profile：指定组件在哪个环境的情况下才能被注册到容器中，不指定，任何环境下都能注册这个组件 * * 1）、加了环境标识的bean，只有这个环境被激活的时候才能注册到容器中。默认是default环境 * 2）、写在配置类上，只有是指定的环境的时候，整个配置类里面的所有配置才能开始生效 * 3）、没有标注环境标识的bean在，任何环境下都是加载的； */@PropertySource(&quot;classpath:/dbconfig.properties&quot;)@Configurationpublic class MainConfigOfProfile implements EmbeddedValueResolverAware&#123; @Value(&quot;$&#123;db.user&#125;&quot;) private String user; private StringValueResolver valueResolver; private String driverClass; @Bean public Yellow yellow()&#123; return new Yellow(); &#125; @Profile(&quot;test&quot;) @Bean(&quot;testDataSource&quot;) public DataSource dataSourceTest(@Value(&quot;$&#123;db.password&#125;&quot;)String pwd) throws Exception&#123; ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setUser(user); dataSource.setPassword(pwd); dataSource.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/test&quot;); dataSource.setDriverClass(driverClass); return dataSource; &#125; @Profile(&quot;dev&quot;) @Bean(&quot;devDataSource&quot;) public DataSource dataSourceDev(@Value(&quot;$&#123;db.password&#125;&quot;)String pwd) throws Exception&#123; ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setUser(user); dataSource.setPassword(pwd); dataSource.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/ssm_crud&quot;); dataSource.setDriverClass(driverClass); return dataSource; &#125; @Profile(&quot;prod&quot;) @Bean(&quot;prodDataSource&quot;) public DataSource dataSourceProd(@Value(&quot;$&#123;db.password&#125;&quot;)String pwd) throws Exception&#123; ComboPooledDataSource dataSource = new ComboPooledDataSource(); dataSource.setUser(user); dataSource.setPassword(pwd); dataSource.setJdbcUrl(&quot;jdbc:mysql://localhost:3306/scw_0515&quot;); dataSource.setDriverClass(driverClass); return dataSource; &#125; @Override public void setEmbeddedValueResolver(StringValueResolver resolver) &#123; // TODO Auto-generated method stub this.valueResolver = resolver; driverClass = valueResolver.resolveStringValue(&quot;$&#123;db.driverClass&#125;&quot;); &#125;&#125; dbconfig.properties123db.user=rootdb.password=123456db.driverClass=com.mysql.jdbc.Driver 面试总结Spring 设计目标,理念,核心是什么 一站式轻量级应用开发平台 IoC(inversion of control)容器对对象进行管理,依赖反转将依赖关系交给容器达到解耦的目的 IoC与AOP, IOC容器管理对象以及之间的依赖关系, AOP实现动态非侵入功能增强, 把遍布于应用各层的功能分离出来形成可重用的功能组件 IoC的作用与实现原理 管理对象的创建与依赖关系 托管了类的整个生命周期 工厂模式与反射机制: BeanFactory与ApplicationContext的区别https://blog.csdn.net/m0_45406092/article/details/114655627 BeanFactory是低级容器, 包含Bean定义信息, 读取Bean配置, 完成Bean的加载, 初始化, 控制Bean的生命周期, 维护Bean之间的依赖关系 ApplicationContext除了有所有功能外还有额外功能, 比如国际化,同时加载多个配置文, 除编程式new之外还可声明式(对web应用进行支持)ContextLoaderListen(https://blog.51cto.com/lsieun/1828581), 强大的事件机制(当AC发布了一个事件, 所有扩展了ApplicationListener的Bean都会接受事件,并进行相应处理 ) 加载方式不同: BeanFactory 只有在使用到某个bean时才会加载实例化, 这样就不会发现已存在Spring配置问题, 比如如果有一个属性未注入, 直至第一次调用才发现异常 AC则是一次性创建好所有bean: 占用空间, 启动较慢 什么是依赖注入实现松耦合, 没有依赖注入时, 在组件内部创建对象都是具体的子类, 依赖注入声明接口后可以注入各种子类, 基础是多态机制与反射机制 Spring AOP 与AspectJ aop方式关键是代理模式 AspectJ 是静态代理, 即编译阶段将切面织入到字节码, 运行的时候是增强后的AOP对象 Spring AOP是动态代理, 不改变字节码, 而是在内存生成一个组合了原对象的的AOP对象, 在对应切点怎强, 回调原对象方法","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://gouguoqiang.github.io/tags/Spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gouguoqiang.github.io/tags/Mybatis/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gouguoqiang.github.io/tags/SpringBoot/"}]},{"title":"KOB项目","slug":"KOB","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-27T07:53:36.575Z","comments":true,"path":"2022/09/01/KOB/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/KOB/","excerpt":"","text":"配置git与项目环境划分项目: 按页面分前后端分离@Controller 与@RestController 传统 跨域不好用, 分布式Session麻烦 JWT不要各个服务器存储 根据用户信息 用秘钥进行加密判断 JWT-TOKEN是否一样 TOKEN防窃取: 两个token(access(5min,get), refresh(14days,post))","categories":[{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gouguoqiang.github.io/tags/SpringBoot/"}]},{"title":"微服务","slug":"9MicroService","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:23:34.268Z","comments":true,"path":"2022/09/01/9MicroService/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/9MicroService/","excerpt":"","text":"微服务架构1. 服务组件化 1. 服务间交互采用restful 风格 1. 去中心化 :每个服务有自己的私有数据库持久化系统 1. 自动化部署:把应用拆分为一个一个独立的单个服务,方便自动化部署,测试,运维 1.传统集中式架构 ​ 开发速度快 ​ 并发能力差 ​ 代码耦合度高,维护困难 ​ 容错率低 2.垂直拆分 ​ 一个数据库,多个系统模块 ​ 拆分实现了流量分担,解决并发 ​ 可针对不同模块进行优化 ​ 方便水平扩展(集群化),负载均衡,容错率高 ​ 系统间相互独立,会有很多重复开发,影响开发效率 3.分布式服务 ​ 垂直应用越来越多,应用交互不可避免 ​ 系统间相互调用,调高代码复用和效率 ​ 系统间耦合度高,调用关系错综复杂,难以维护 ​ 服务提供方一旦产生变更,所有消费方都需要变更 4.面向服务架构(SOA) ​ 中间增加一层ESB,用来连接各个服务结点,集成不同系统,不同协议的服务 ​ 做消息的转化解释和路由工作 ​ ESB产品实现复杂,服务粒度较大, ESB包含的功能如:负载均衡,流量控制 ​ 加密处理,服务监控,异常处理等等 ​ 集成整合所有服务,数据转换使运维,测试部署困难 ​ 所有服务通过一个通路通信,降低通信速度 ​ 通常松耦合 基于socket工作在会话层,自定义数据格式,速度快,效率高 5.微服务架构 ​ 基于TCP工作在应用层,规定了数据传输格式 ​ 服务粒度小,使用轻量级通信,通常是HTTP API ​ 各服务可使用不同的编程语言实现,以及不同的数据存储技术 ​ 服务间相互独立 ​ 并保持最低限度的集中式管理 ​ 网关给用户提供统一的方式接入微服务,在网关层处理所有的非业务功能,例如身份验证,监控,负载均衡,缓存,请求分片与管理,静态响应处理,服务端通过服务注册中心进行服务注册和管理 dubbo1-今日内容 分布式系统中的相关概念 dubbo 概述 dubbo快速入门 dubbo的高级特性 2.2-互联网项目架构-目标●QPS: Query Per Second每秒查询数。 ●TPS: Transactions Per Second每秒事务数。 ●一个事务是指一 个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束 计时，以此来计算使用的时间和完成的事务个数。 ●一个页面的一次访问，只会形成一 个TPS; 但-次页面请求，可能产生多次对服务器的请求，就会有多个QPS QPS&gt;=并发连接数&gt;= TPS 大型互联网项目架构目标: ​ ●**高性能:提供快速的访问体验。​ ●高可用:**网站服务- 可以正常访问 2.3-集群和分布式集群和分布式，●集群:很多“人”一起，干一样的事。●一个业务模块，部署在多台服务器上。●分布式:很多”人”一起，干不样的事。这些不一样的事， 合起来是一件大事。 2.4-架构演进分布式架构：是指在垂直架构的基础上,将公共业务模块抽取出来,作为独立的服务供其他调用者消费，以实现服务的共享和重用。底层通过RPC（远程过程调用实现） RPC: Remote Procedure Call远程过程调用。有非常多的协议和技术来都实现了RPC的过程。比如: HTTP REST风格，Java RMI规范、WebService SOAP协议Hession等等。 分布式架构存在的问题:​ ●服务提供方- -旦产生变更,所有消费方都需要变更。 微服务架构： ●微服务架构是在SOA上做的升华,微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。 特点:●服务实现组件化:开发者可以自由选择开发技术。也不需要协调其他团队●服务之间交互一 般使用REST API●去中心化:每个微服务有自己私有的数据库持久化业务数据●自动化部署:把应用拆分成为一 个个独立的单个服务,方便自动化部署、测试、运维 3-dubbo 概述Dubbo概念 ●Dubbo是阿里巴巴公司开源的一个高性能、轻量级的Java RPC框架。●致力于提供高性能和透明化的RPC远程服务调用方案,以及SOA服务治理方案。●官网: htp:&#x2F;&#x2F;ubbo.apache.orgo 节点角色说明: .●Provider: 暴露服务的服务提供方●Container: 服务运行容器●Consumer: 调用远程服务的服务消费方●Registry: 服务注册与发现的注册中心●Monitor:统计服务的调用次数和调用时间的监控中心 5-dubbo高级特性5.1dubbo-admin安装dubbo- admin ●dubbo-admin管理平台，是图形化的服务管理页面 ●从注册中心中获取到所有的提供者 &#x2F;消费者进行配置管理 ●路由规则、动态配置、服务降级、访问控制、权重调整、负载均衡等管理功能 ●dubbo- admin是一个前后端分离的项目。前端使用vue，后端使用springboot ●安装dubbo-admin其实就是部署该项目 5.3序列化 dubbo 内部已经将序列化和反序列化的过程内部封装了 我们只需要在定义pojo类时实现seriali zable接口即可 一般会定义一 个公共的pojo模块,让生产者和消费者都依赖该模块。 User对象未实现serializable接口 错误信息： 解决办法： 1User implements Serializable 5.4地址缓存注册中心挂了，服务是否可以正常访问？ 可以，因为dubbo服务消费者在第一-次调用时，会将服务提供方地址缓存到本地，以后在调用则不会访问注册中心。 当服务提供者地址发生变化时，注册中心会通知服务消费者。 5.5 超时 服务消费者在调用服务提供者的时候发生了阻塞、等待的情形,这个时候,服务消费者会直等待下去。 在某个峰值时刻，大量的请求都在同时请求服务消费者,会造成线程的大量堆积，势必会造成雪崩。 dubbo利用超时机制来解决这个问题，设置-个超时时间, 在这个时间段内，无法完成服务访问,则自动断开连接。 使用timeout属性配置超时时间，默认值1000，单位毫秒 12//timeout 超时时间 单位毫秒 retries 重试次数@Service(timeout = 3000,retries=0) 5.6重试 设置了超时时间，在这个时间段内，无法完成服务访问,则自动断开连接。 如果出现网络抖动,则这一-次请求就会失败。 Dubbo提供重试机制来避免类似问题的发生。 通过retries属性来设置重试次数。默认为2次 12//timeout 超时时间 单位毫秒 retries 重试次数@Service(timeout = 3000,retries=0) 5.7多版本 **灰度发布:**当出现新功能时,会让一部分用户先使用新功能，用户反馈没问题时，再将所有用户迁移到新功能。 dubbo中使用version属性来设置和调用同一个接口的不同版本 生产者配置 12@Service(version=&quot;v2.0&quot;)public class UserServiceImp12 implements UserService &#123;...&#125; 消费者配置 12@Reference(version = &quot;v2.0&quot;)//远程注入private UserService userService; 5.8负载均衡负载均衡策略(4种) :**Random:**按权重随机，默认值。按权重设置随机概率。 RoundRobin: 按权重轮询。 LeastActive: 最少活跃调用数,相同活跃数的随机。 **ConsistentHash:**一 致性Hash,相同参数的请求总是发到同一提供者。 服务提供者配置 12@Service(weight = 100)public class UserServiceImp12 implements UserService &#123;...&#125; application.xml 配置parameter key 消费者配置 12345//@Reference(loadbalance = &quot;roundrobin&quot;)//@Reference(loadbalance = &quot;leastactive&quot;)//@Reference(loadbalance = &quot;consistenthash&quot;)@Reference(loadbalance = &quot;random&quot;)//默认 按权重随机private UserService userService; 5.9集群容错 集群容错模式: **Failover Cluster:**失败重试。默认值。当出现失败，重试其它服务器，默认重试2次，使用retries配置。一般用于读操作 **Failfast Cluster :**快速失败,发起-次调用，失败立即报错。通常用于写操作。 **Failsafe Cluster:**失败安全，出现异常时，直接忽略。返回一个空结果。 **Failback Cluster:**失败自动恢复,后台记录失败请求,定时重发。 **Forking Cluster :**并行调用多个服务器，只要一个成功即返回。 Broadcast Cluster: 广播调用所有提供者,逐个调用，任意一台报错则报错。 消费者配置 12@Reference(cluster = &quot;failover&quot;)//远程注入private UserService userService; 5.10服务降级服务降级：当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作 服务降级方式:mock&#x3D; force:return null：表示消费方对该服务的方法调用都直接返回null值,不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock&#x3D;fail:return null：表示消费方对该服务的方法调用在失败后，再返回null值,不抛异常。用来容忍不重要服务不稳定时对调用方的影响 消费方配置 123//远程注入@Reference(mock =“ force :return null&quot;)//不再调用userService的服务private UserService userService; 总结概述SOA,RPC框架 用Zookeeper作为注册中心 超时,重试,负载均衡,灰度发布(多版本)@Service(timeout &#x3D; ,retries &#x3D; ,version&#x3D;””) 同spring @service一致采用dubbo包 用于服务发布 @Reference(loadbalance &#x3D; “”,version &#x3D; “”)) 类似Autowire 用于服务发现 &#x2F;&#x2F;@Reference(loadbalance &#x3D; “roundrobin”)&#x2F;&#x2F;@Reference(loadbalance &#x3D; “leastactive”)&#x2F;&#x2F;@Reference(loadbalance &#x3D; “consistenthash”)@Reference(loadbalance &#x3D; “random”)&#x2F;&#x2F;默认 按权重随机 @Service(timeout &#x3D; 3000,retries&#x3D;0) @Reference(version &#x3D; “v2.0”)&#x2F;&#x2F;远程注入 private UserService userService; 集群容错**Failover Cluster:**失败重试。默认值。当出现失败，重试其它服务器，默认重试2次，使用retries配置。一般用于读操作 **Failfast Cluster :**快速失败,发起-次调用，失败立即报错。通常用于写操作。 **Failsafe Cluster:**失败安全，出现异常时，直接忽略。返回一个空结果。 **Failback Cluster:**失败自动恢复,后台记录失败请求,定时重发。 **Forking Cluster :**并行调用多个服务器，只要一个成功即返回。 Broadcast Cluster: 广播调用所有提供者,逐个调用，任意一台报错则报错。 消费者配置 12@Reference(cluster = &quot;failover&quot;)//远程注入private UserService userService; 服务降级服务降级：当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作 服务降级方式:mock&#x3D; force:return null：表示消费方对该服务的方法调用都直接返回null值,不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock&#x3D;fail:return null：表示消费方对该服务的方法调用在失败后，再返回null值,不抛异常。用来容忍不重要服务不稳定时对调用方的影响 消费方配置 123//远程注入@Reference(mock =“ force :return null&quot;)//不再调用userService的服务private UserService userService; 分布式 很多人一起干不一样的事 合起来是一件大事 0-5 共六步 图片 todo 功能 一个RPC框架 远程过程调用,能够将单体上的应用的各个模块变成分布式,关键是call过程,通过zookeeper注册中心(服务治理) zookeepper 是一个树结构,结点可以存储信息 &#x2F; 根节点 分布式锁的实现 临时有序结点,选取顺序最小的 每个线程申请都会生成一结点, 集群治理 其他高级特性 心跳 负载均衡 容灾 服务注册&#x2F;发现 配置管理 服务提供者将服务信息注册到服务中心,消费者对生产者进行调用,会在本地存有地址和端口缓存,如果服务提供者的地址端口信息等进行更新,zookeepper会通知消费者更新缓存 1. (在下次调用时更新还是一更改就更新?思考:一更改就更新,会有缓存的嗅探()好处是只要缓存命中就能保证服务的可靠性,如果调用时更新,缓存的意义就不大了,) 还有 monitor (sentinel(或duboo-admin)对rpc进行监控) 特性 序列化 地址缓存 超时与重试 消费者的请求阻塞,等待的时候,dubbo设置了超时机制,无法完成服务访问则自动断开连接(基于TCP的连接?) 如果出现网络抖动,请求失败,通过retries设置重传次数 默认2 多版本 用version属性来调用同一个接口的不同版本 负载均衡选用哪个服务 random :按权重随机,每个默认都为100 roundRobin权重轮询 LeastActive最少活跃调用,相同活跃随机 ConsistentHash 一致性hash,相同参数的请求总是发到同一提供者 6)集群容错 Zookeeper1)初识 Zookeeper1.1)Zookeeper概念•Zookeeper 是 Apache Hadoop 项目下的一个子项目，是一个树形目录服务。 •Zookeeper 提供的主要功能包括： •配置管理 •分布式锁 •集群管理 2.2.2、启动ZooKeeper123cd /opt/zooKeeper/apache-zooKeeper-3.5.6-bin/bin/#启动 ./zkServer.sh start 看到上图表示ZooKeeper成功启动 3、查看ZooKeeper状态 1./zkServer.sh status zookeeper启动成功。standalone代表zk没有搭建集群，现在是单节点 zookeeper没有启动 3)ZooKeeper 命令操作3.1)Zookeeper命令操作数据模型•ZooKeeper 是一个树形目录服务,其数据模型和Unix的文件系统目录树很类似，拥有一个层次化结构。 •这里面的每一个节点都被称为： ZNode，每个节点上都会保存自己的数据和节点信息。 • 节点可以拥有子节点，同时也允许少量（1MB）数据存储在该节点之下。 •节点可以分为四大类： •PERSISTENT 持久化节点 •EPHEMERAL 临时节点 ：-e •PERSISTENT_SEQUENTIAL 持久化顺序节点 ：-s •EPHEMERAL_SEQUENTIAL 临时顺序节点 ：-es 3.2)Zookeeper命令操作服务端命令•启动 ZooKeeper 服务: .&#x2F;zkServer.sh start •查看 ZooKeeper 服务状态: .&#x2F;zkServer.sh status •停止 ZooKeeper 服务: .&#x2F;zkServer.sh stop •重启 ZooKeeper 服务: .&#x2F;zkServer.sh restart 3.3)Zookeeper客户端常用命令•连接ZooKeeper服务端 1./zkCli.sh –server ip:port •断开连接 1quit •查看命令帮助 1help •显示指定目录下节点 1ls 目录 •创建节点 1create /节点path value •获取节点值 1get /节点path •设置节点值 1set /节点path value •删除单个节点 1delete /节点path •删除带有子节点的节点 1deleteall /节点path 3.4)客户端命令-创建临时有序节点•创建临时节点 1create -e /节点path value •创建顺序节点 1create -s /节点path value •查询节点详细信息 1ls –s /节点path •czxid：节点被创建的事务ID •ctime: 创建时间 •mzxid: 最后一次被更新的事务ID •mtime: 修改时间 •pzxid：子节点列表最后一次被更新的事务ID •cversion：子节点的版本号 •dataversion：数据版本号 •aclversion：权限版本号 •ephemeralOwner：用于临时节点，代表临时节点的事务ID，如果为持久节点则为0 •dataLength：节点存储的数据的长度 •numChildren：当前节点的子节点个数 4)ZooKeeper JavaAPI 操作4.1)urator介绍•Curator 是 Apache ZooKeeper 的Java客户端库。 •常见的ZooKeeper Java API ： •原生Java API •ZkClient •Curator •Curator 项目的目标是简化 ZooKeeper 客户端的使用。 •Curator 最初是 Netfix 研发的,后来捐献了 Apache 基金会,目前是 Apache 的顶级项目。 •官网：http://curator.apache.org/ 4.2)JavaAPI操作建立连接1，搭建项目 创建项目curator-zk 引入pom和日志文件 资料文件夹下pom.xml和log4j.properties 2、创建测试类，使用curator连接zookeeper 12345678910111213141516@Beforepublic void testConnect() &#123; //重试策略 RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 10); //2.第二种方式 //CuratorFrameworkFactory.builder(); client = CuratorFrameworkFactory.builder() .connectString(&quot;192.168.200.130:2181&quot;) .sessionTimeoutMs(60 * 1000) .connectionTimeoutMs(15 * 1000) .retryPolicy(retryPolicy) .namespace(&quot;itheima&quot;) .build(); //开启连接 client.start();&#125; 4.3)Zookeeper JavaAPI操作-创建节点1234567891011121314/*** 创建节点：create 持久 临时 顺序 数据* 1. 基本创建 ：create().forPath(&quot;&quot;)* 2. 创建节点 带有数据:create().forPath(&quot;&quot;,data)* 3. 设置节点的类型：create().withMode().forPath(&quot;&quot;,data)* 4. 创建多级节点 /app1/p1 ：create().creatingParentsIfNeeded().forPath(&quot;&quot;,data)*/@Testpublic void testCreate() throws Exception &#123; //2. 创建节点 带有数据 //如果创建节点，没有指定数据，则默认将当前客户端的ip作为数据存储 String path = client.create().forPath(&quot;/app2&quot;, &quot;hehe&quot;.getBytes()); System.out.println(path);&#125; 123456789101112131415161718192021@Testpublic void testCreate2() throws Exception &#123; //1. 基本创建 //如果创建节点，没有指定数据，则默认将当前客户端的ip作为数据存储 String path = client.create().forPath(&quot;/app1&quot;); System.out.println(path);&#125;@Testpublic void testCreate3() throws Exception &#123; //3. 设置节点的类型 //默认类型：持久化 String path = client.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;/app3&quot;); System.out.println(path);&#125;@Testpublic void testCreate4() throws Exception &#123; //4. 创建多级节点 /app1/p1 //creatingParentsIfNeeded():如果父节点不存在，则创建父节点 String path = client.create().creatingParentsIfNeeded().forPath(&quot;/app4/p1&quot;); System.out.println(path);&#125; 4.4)ZookeeperJavaAPI操作-查询节点123456789101112/*** 查询节点：* 1. 查询数据：get: getData().forPath()* 2. 查询子节点： ls: getChildren().forPath()* 3. 查询节点状态信息：ls -s:getData().storingStatIn(状态对象).forPath()*/@Testpublic void testGet1() throws Exception &#123; //1. 查询数据：get byte[] data = client.getData().forPath(&quot;/app1&quot;); System.out.println(new String(data));&#125; 1234567891011121314@Testpublic void testGet2() throws Exception &#123; // 2. 查询子节点： ls List&lt;String&gt; path = client.getChildren().forPath(&quot;/&quot;); System.out.println(path);&#125;@Testpublic void testGet3() throws Exception &#123; Stat status = new Stat(); System.out.println(status); //3. 查询节点状态信息：ls -s client.getData().storingStatIn(status).forPath(&quot;/app1&quot;); System.out.println(status);&#125; 4.5)Zookeeper JavaAPI操作-修改节点123456789101112/*** 修改数据* 1. 基本修改数据：setData().forPath()* 2. 根据版本修改: setData().withVersion().forPath()* * version 是通过查询出来的。目的就是为了让其他客户端或者线程不干扰我。** @throws Exception*/@Testpublic void testSet() throws Exception &#123; client.setData().forPath(&quot;/app1&quot;, &quot;itcast&quot;.getBytes());&#125; 123456789@Testpublic void testSetForVersion() throws Exception &#123; Stat status = new Stat(); //3. 查询节点状态信息：ls -s client.getData().storingStatIn(status).forPath(&quot;/app1&quot;); int version = status.getVersion();//查询出来的 3 System.out.println(version); client.setData().withVersion(version).forPath(&quot;/app1&quot;, &quot;hehe&quot;.getBytes());&#125; 4.6)Zookeeper JavaAPI操作-删除节点123456789101112131415161718/*** 删除节点： delete deleteall* 1. 删除单个节点:delete().forPath(&quot;/app1&quot;);* 2. 删除带有子节点的节点:delete().deletingChildrenIfNeeded().forPath(&quot;/app1&quot;);* 3. 必须成功的删除:为了防止网络抖动。本质就是重试。 client.delete().guaranteed().forPath(&quot;/app2&quot;);* 4. 回调：inBackground* @throws Exception*/@Testpublic void testDelete() throws Exception &#123; // 1. 删除单个节点 client.delete().forPath(&quot;/app1&quot;);&#125;@Testpublic void testDelete2() throws Exception &#123; //2. 删除带有子节点的节点 client.delete().deletingChildrenIfNeeded().forPath(&quot;/app4&quot;);&#125; 12345678910111213141516@Testpublic void testDelete3() throws Exception &#123; //3. 必须成功的删除 client.delete().guaranteed().forPath(&quot;/app2&quot;);&#125;@Testpublic void testDelete4() throws Exception &#123; //4. 回调 client.delete().guaranteed().inBackground(new BackgroundCallback()&#123; @Override public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println(&quot;我被删除了~&quot;); System.out.println(event); &#125; &#125;).forPath(&quot;/app1&quot;);&#125; 4.7)Zookeeper JavaAPI操作-Watch监听概述•ZooKeeper 允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。 •ZooKeeper 中引入了Watcher机制来实现了发布&#x2F;订阅功能能，能够让多个订阅者同时监听某一个对象，当一个对象自身状态变化时，会通知所有订阅者。 •ZooKeeper 原生支持通过注册Watcher来进行事件监听，但是其使用并不是特别方便 ​ 需要开发人员自己反复注册Watcher，比较繁琐。 •Curator引入了 Cache 来实现对 ZooKeeper 服务端事件的监听。 •ZooKeeper提供了三种Watcher： •NodeCache : 只是监听某一个特定的节点 •PathChildrenCache : 监控一个ZNode的子节点. •TreeCache : 可以监控整个树上的所有节点，类似于PathChildrenCache和NodeCache的组合 4.8Zookeeper JavaAPI操作-Watch监听-NodeCache12345678910111213141516171819202122/*** 演示 NodeCache：给指定一个节点注册监听器*/@Testpublic void testNodeCache() throws Exception &#123; //1. 创建NodeCache对象 final NodeCache nodeCache = new NodeCache(client,&quot;/app1&quot;); //2. 注册监听 nodeCache.getListenable().addListener(new NodeCacheListener() &#123; @Override public void nodeChanged() throws Exception &#123; System.out.println(&quot;节点变化了~&quot;); //获取修改节点后的数据 byte[] data = nodeCache.getCurrentData().getData(); System.out.println(new String(data)); &#125; &#125;); //3. 开启监听.如果设置为true，则开启监听是，加载缓冲数据 nodeCache.start(true); while (true)&#123; &#125;&#125; 4.9)Zookeeper JavaAPI操作-Watch监听-PathChildrenCache12345678910111213141516171819202122232425@Testpublic void testPathChildrenCache() throws Exception &#123; //1.创建监听对象 PathChildrenCache pathChildrenCache = new PathChildrenCache(client,&quot;/app2&quot;,true); //2. 绑定监听器 pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; System.out.println(&quot;子节点变化了~&quot;); System.out.println(event); //监听子节点的数据变更，并且拿到变更后的数据 //1.获取类型 PathChildrenCacheEvent.Type type = event.getType(); //2.判断类型是否是update if(type.equals(PathChildrenCacheEvent.Type.CHILD_UPDATED))&#123; System.out.println(&quot;数据变了！！！&quot;); byte[] data = event.getData().getData(); System.out.println(new String(data)); &#125; &#125; &#125;); //3. 开启 pathChildrenCache.start(); while (true)&#123; &#125;&#125; 4.10)Zookeeper JavaAPI操作-Watch监听-TreeCache1234567891011121314151617181920/*** 演示 TreeCache：监听某个节点自己和所有子节点们*/@Testpublic void testTreeCache() throws Exception &#123; //1. 创建监听器 TreeCache treeCache = new TreeCache(client,&quot;/app2&quot;); //2. 注册监听 treeCache.getListenable().addListener(new TreeCacheListener() &#123; @Override public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123; System.out.println(&quot;节点变化了&quot;); System.out.println(event); &#125; &#125;); //3. 开启 treeCache.start(); while (true)&#123; &#125;&#125; 4.11)Zookeeper分布式锁-概念•在我们进行单机应用开发，涉及并发同步的时候，我们往往采用synchronized或者Lock的方式来解决多线程间的代码同步问题，这时多线程的运行都是在同一个JVM之下，没有任何问题。 •但当我们的应用是分布式集群工作的情况下，属于多JVM下的工作环境，跨JVM之间已经无法通过多线程的锁解决同步问题。 •那么就需要一种更加高级的锁机制，来处理种跨机器的进程之间的数据同步问题——这就是分布式锁。 4.12)Zookeeper 分布式锁-zookeeper分布式锁原理•核心思想：当客户端要获取锁，则创建节点，使用完锁，则删除该节点。 1.客户端获取锁时，在lock节点下创建临时顺序节点。 2.然后获取lock下面的所有子节点，客户端获取到所有的子节点之后，如果发现自己创建的子节点序号最小，那么就认为该客户端获取到了锁。使用完锁后，将该节点删除。 3.如果发现自己创建的节点并非lock所有子节点中最小的，说明自己还没有获取到锁，此时客户端需要找到比自己小的那个节点，同时对其注册事件监听器，监听删除事件。 4.如果发现比自己小的那个节点被删除，则客户端的 ​ Watcher会收到相应通知，此时再次判断自己创建的节点 ​ 是否是lock子节点中序号最小的，如果是则获取到了锁， ​ 如果不是则重复以上步骤继续获取到比自己小的一个节点 ​ 并注册监听。 4.13)Zookeeper 分布式锁-模拟12306售票案例Curator实现分布式锁API 在Curator中有五种锁方案： InterProcessSemaphoreMutex：分布式排它锁（非可重入锁） InterProcessMutex：分布式可重入排它锁 InterProcessReadWriteLock：分布式读写锁 InterProcessMultiLock：将多个锁作为单个实体管理的容器 InterProcessSemaphoreV2：共享信号量 1,创建线程进行加锁设置 12345678910111213141516171819202122232425262728public class Ticket12306 implements Runnable&#123; private int tickets = 10;//数据库的票数 private InterProcessMutex lock ; @Override public void run() &#123; while(true)&#123; //获取锁 try &#123; lock.acquire(3, TimeUnit.SECONDS); if(tickets &gt; 0)&#123; System.out.println(Thread.currentThread()+&quot;:&quot;+tickets); Thread.sleep(100); tickets--; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //释放锁 try &#123; lock.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 2,创建连接，并且初始化锁 123456789101112131415public Ticket12306()&#123; //重试策略 RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 10); //2.第二种方式 //CuratorFrameworkFactory.builder(); CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;192.168.149.135:2181&quot;) .sessionTimeoutMs(60 * 1000) .connectionTimeoutMs(15 * 1000) .retryPolicy(retryPolicy) .build(); //开启连接 client.start(); lock = new InterProcessMutex(client,&quot;/lock&quot;);&#125; 3,运行多个线程进行测试 12345678910public class LockTest &#123; public static void main(String[] args) &#123; Ticket12306 ticket12306 = new Ticket12306(); //创建客户端 Thread t1 = new Thread(ticket12306,&quot;携程&quot;); Thread t2 = new Thread(ticket12306,&quot;飞猪&quot;); t1.start(); t2.start(); &#125;&#125; 5)ZooKeeper 集群搭建5.1)Zookeeper集群介绍Leader选举： •Serverid：服务器ID 比如有三台服务器，编号分别是1,2,3。 编号越大在选择算法中的权重越大。 •Zxid：数据ID 服务器中存放的最大数据ID.值越大说明数据 越新，在选举算法中数据越新权重越大。 •在Leader选举的过程中，如果某台ZooKeeper ​ 获得了超过半数的选票， ​ 则此ZooKeeper就可以成为Leader了。 5.2)搭建要求真实的集群是需要部署在不同的服务器上的，但是在我们测试时同时启动很多个虚拟机内存会吃不消，所以我们通常会搭建伪集群，也就是把所有的服务都搭建在一台虚拟机上，用端口进行区分。 我们这里要求搭建一个三个节点的Zookeeper集群（伪集群）。 5.3)准备工作重新部署一台虚拟机作为我们搭建集群的测试服务器。 （1）安装JDK 【此步骤省略】。 （2）Zookeeper压缩包上传到服务器（3）将Zookeeper解压 ，建立&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster目录，将解压后的Zookeeper复制到以下三个目录 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-1 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-2 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-3 1234[root@localhost ~]# mkdir /usr/local/zookeeper-cluster[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-1[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-2[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-3 （4）创建data目录 ，并且将 conf下zoo_sample.cfg 文件改名为 zoo.cfg 1234567mkdir /usr/local/zookeeper-cluster/zookeeper-1/datamkdir /usr/local/zookeeper-cluster/zookeeper-2/datamkdir /usr/local/zookeeper-cluster/zookeeper-3/datamv /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgmv /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgmv /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg （5） 配置每一个Zookeeper 的dataDir 和 clientPort 分别为2181 2182 2183 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-1&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgclientPort=2181dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-2&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgclientPort=2182dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-3&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfgclientPort=2183dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data 5.4)配置集群（1）在每个zookeeper的 data 目录下创建一个 myid 文件，内容分别是1、2、3 。这个文件就是记录每个服务器的ID 123echo 1 &gt;/usr/local/zookeeper-cluster/zookeeper-1/data/myidecho 2 &gt;/usr/local/zookeeper-cluster/zookeeper-2/data/myidecho 3 &gt;/usr/local/zookeeper-cluster/zookeeper-3/data/myid （2）在每一个zookeeper 的 zoo.cfg配置客户端访问端口（clientPort）和集群服务器IP列表。 集群服务器IP列表如下 1234567vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgvim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgvim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfgserver.1=192.168.149.135:2881:3881server.2=192.168.149.135:2882:3882server.3=192.168.149.135:2883:3883 解释：server.服务器ID&#x3D;服务器IP地址：服务器之间通信端口：服务器之间投票选举端口 5.5)启动集群启动集群就是分别启动每个实例。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start 启动后我们查询一下每个实例的运行状态 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 先查询第一个服务 Mode为follower表示是跟随者（从） 再查询第二个服务Mod 为leader表示是领导者（主） 查询第三个为跟随者（从） 5.6)模拟集群异常（1）首先我们先测试如果是从服务器挂掉，会怎么样 把3号服务器停掉，观察1号和2号，发现状态并没有变化 1234/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status 由此得出结论，3个节点的集群，从服务器挂掉，集群正常 （2）我们再把1号服务器（从服务器）也停掉，查看2号（主服务器）的状态，发现已经停止运行了。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status 由此得出结论，3个节点的集群，2个从服务器都挂掉，主服务器也无法运行。因为可运行的机器没有超过集群总数量的半数。 （3）我们再次把1号服务器启动起来，发现2号服务器又开始正常工作了。而且依然是领导者。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status （4）我们把3号服务器也启动起来，把2号服务器停掉,停掉后观察1号和3号的状态。 12345/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 发现新的leader产生了~ 由此我们得出结论，当集群中的主服务器挂了，集群中的其他服务器会自动进行选举状态，然后产生新得leader （5）我们再次测试，当我们把2号服务器重新启动起来启动后，会发生什么？2号服务器会再次成为新的领导吗？我们看结果 1234/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 我们会发现，2号服务器启动后依然是跟随者（从服务器），3号服务器依然是领导者（主服务器），没有撼动3号服务器的领导地位。 由此我们得出结论，当领导者产生后，再次有新服务器加入集群，不会影响到现任领导者。 6)Zookeeper 核心理论Zookeepe集群角色 在ZooKeeper集群服中务中有三个角色： •Leader 领导者 ： ​ 1. 处理事务请求 ​ 2. 集群内部各服务器的调度者 •Follower 跟随者 ： ​ 1. 处理客户端非事务请求，转发事务请求给Leader服务器 ​ 2. 参与Leader选举投票 •Observer 观察者： 1. 处理客户端非事务请求，转发事务请求给Leader服务器 Spring Cloud Spring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和 反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） RestTemplate的使用1234567import org.springframework.web.client.RestTemplate;//注册bean//使用方法 发动请求,接收响应,并且帮我们对响应结果进行反序列化String url = &quot;http://localhost/user/8&quot;;User user = restTemplate.getForObject(url, User.class);System.out.println(user); Spring Cloud 实践1 编写消费者和生产者(无注册中心)生产者:常规后端 ​ spring boot应用, entity,service实现增删改查业务,controller接收请求调用业务进行处理,返回给请求者 消费者:简单controller使用restT请求生产者 网页访问消费者,消费者通过resttemplate(硬编码)请求生产者 存在的问题 url硬编码 如果变更,得不到通知地址将失效,不清楚服务状态,服务宕机也不知道 (采用注册中心对服务进行治理 ​ 自动注册和发现 ​ 服务注册到注册中心(服务类型,联系方式) ​ 消费者向服务中心订阅服务,选择服务类型,注册中心自动安排一个符合需求的服务 ​ 状态监管 “心跳(续约)机制” 定期通过http向Eureka刷新自己的状态 ​ 动态路由 一台生产者不具备高可用(集群解决,如何实现负载均衡) 容灾问题 服务如何实现统一配置 实践2 搭建Eureka-server工程Eureka注册中心EurekaServer主要功能:检查那些没有定期发送心跳的,记录服务,自我保护机制(todo) EurekaClient主要功能: 从Server拉取服务列表,基于负载均衡算法对远程服务进行调用,定时续约, 设置剔除时间(?) 搭建 添加依赖 添加Eureka的服务注解 @EnableEurekaServer 开启服务发现功能 123456789@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(EurekaApplication.class); &#125;&#125; 编写配置 123456789server: port: 6868eureka: client: register-with-eureka: false #是否将自己注册到eureka中 fetch-registry: false #是否从eureka中获取信息 service-url: defaultZone: http://127.0.0.1:$&#123;server.port&#125;/eureka/ 启动测试 客户端的注解是@EnableDiscoveryClient 客户端常用配置: 续约间隔 服务失效时间 获取服务地址列表间隔时间 &#96;&#96;&#96;javaeureka: client:service-url: defaultZone: http://127.0.0.1:6868/eureka instance:prefer-ip-address: true # 更倾向于使用ip，而不是host名 123456789101112131415161718192021222324 ## 实践3 搭建Eureka集群多机集群 只需要区分IP 端口号可相同在本机模拟 在启动时修改VM options defaultzone复制个项目 修改端口号启动多个服务![image-20220627103529460](../images/image-20220627103529460.png)客户端注册添加注册```javaeureka:client:service-url: # EurekaServer地址,多个地址以&#x27;,&#x27;隔开defaultZone: http://127.0.0.1:10086/eureka,http://127.0.0.1:10087/eureka 生产者会向Eureka服务发起一个Rest请求,并携带自己的元数据信息,Eureka 将信息保存到双层map 1Map&lt;name(服务ID),Map&lt;localhost:user-service:8081(实例ID), &gt; &gt; 默认注册使用主机名或localHost 如果想用ip注册 12345//生产者中eureka:instance:ip-address: 127.0.0.1 # ip地址prefer-ip-address: true # 更倾向于使用ip，而不是host名 服务续约 1234eureka:instance:lease-expiration-duration-in-seconds: 90 lease-renewal-interval-in-seconds: 30 服务失效 默认 90s 认为服务宕机 从服务列表中移除 心跳 30s一次 获取服务列表 当服务消费者启动时，会检测 eureka.client.fetch-registry&#x3D;true 参数的值，如果为true，则会从Eureka Server服务的列表拉取只读备份，然后缓存在本地。并且 每隔30秒 会重新拉取并更新数据。可以在 consumer-demo 项目中通过下面的参数来修改： 123eureka:client:registry-fetch-interval-seconds: 30 失效剔除和自我保护 服务下线 当服务进行正常关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉服务注册中心：“我要下线 了”。服务中心接受到请求之后，将该服务置为下线状态 失效剔除 有时我们的服务可能由于内存溢出或网络故障等原因使得服务不能正常的工作，而服务注册中心并未收到“服务下 线”的请求。相对于服务提供者的“服务续约”操作，服务注册中心在启动时会创建一个定时任务，默认每隔一段时间 （默认为60秒）将当前清单中超时（默认为90秒）没有续约的服务剔除，这个操作被称为失效剔除。 可以通过 eureka.server.eviction-interval-timer-in-ms 参数对其进行修改，单位是毫秒。 自我保护 当服务未按时进行心跳续约时，Eureka会统计服务实例最近15分钟心跳续约的 比例是否低于了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务 剔除列表并不妥当，因为服务可能没有宕机。Eureka在这段时间内不会剔除任何服务实例，直到网络恢复正常。生 产环境下这很有效，保证了大多数服务依然可用，不过也有可能获取到失败的服务实例，因此服务调用者必须做好服 务的失败容错。 可以通过下面的配置来关停自我保护： 123eureka:server:enable-self-preservation: false # 关闭自我保护模式（缺省为打开） 实践4 负载均衡Ribbon在实际环境中往往会开启很多个 user-service的集群,此时discoveryClient 获取的服务列表有多个,到底该访问哪一个呢 ​ 负载均衡算法就是在多个实例列表中进行选择 Eureka中集成了 ribbon 简单修改代码即可(提供例如轮询(默认),随机等) 1.首先 配置两个 user-service实例 启动两个,动态设置启动端口号 2.在RestTemplate的bean方法上添加 @LoadBalanced注解 3.不再手动获取IP和端口 而是直接通过服务名称调用 123456@GetMapping(&quot;&#123;id&#125;&quot;)public User queryById(@PathVariable(&quot;id&quot;) Long id)&#123;String url = &quot;http://user-service/user/&quot; + id;User user = restTemplate.getForObject(url, User.class);return user;&#125; 默认轮询,boot提供修改负载均衡配置入口 在消费者中加 1234user-service:ribbon:NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 实验5 熔断器Hystrix 服务降级延迟和容错库,用于隔离访问远程访问,第三方库,防止出现级联失败 雪崩问题:微服务间一个请求可能需要调用多个微服务接口才能实现 如果某个服务异常,请求不会得到响应,则tomcat的这个线程不会释放,于是越来越多的用户到来,越来越多的线程阻塞,导致服务器资源耗尽,其他所有服务都不可用,形成雪崩效应,一个微服务的阻塞导致整个服务的瘫痪 熔断器Hystrix的解决手段: 线程隔离 为每个依赖服务调用分配一个小的线程池 如果线程池已满调用将被立即拒绝,默认不采用排队,加速失败判定时间 服务降级 如果线程池满,或请求超时会进行服务降级:优先保证核心服务,非核心不可用或弱可用 服务熔断 断开服务,防止整个系统被拖垮 1.添加依赖 2.启动类开启熔断 @EnableCircuitBreaker 123456组合注解@SpringCloudApplication @SpringBootApplication @EnableDiscoveryClient @EnableCircuitBreaker 3.编写降级逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041package com.itheima.consumer.controller;import com.itheima.consumer.pojo.User;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;import java.util.List;@RestController@RequestMapping(&quot;/consumer&quot;)@Slf4jpublic class ConsumerController &#123;@Autowiredprivate RestTemplate restTemplate;@Autowiredprivate DiscoveryClient discoveryClient;@GetMapping(&quot;&#123;id&#125;&quot;)@HystrixCommand(fallbackMethod = &quot;queryByIdFallback&quot;)public String queryById(@PathVariable Long id)&#123;String url = &quot;http://localhost:9091/user/&quot; + id;//获取eureka中注册的user-service实例列表/*List&lt;ServiceInstance&gt; serviceInstanceList =discoveryClient.getInstances(&quot;user-service&quot;);ServiceInstance serviceInstance = serviceInstanceList.get(0);url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort()+ &quot;/user/&quot; + id;*/url = &quot;http://user-service/user/&quot; + id;return restTemplate.getForObject(url, String.class);&#125;public String queryByIdFallback(Long id)&#123;log.error(&quot;查询用户信息失败。id：&#123;&#125;&quot;, id);return &quot;对不起，网络太拥挤了！&quot;; //相同的参数列表和返回值声明&#125;&#125; 不写方法 默认fallbcak 实践6 服务熔断熔断器三个状态 关闭 所有请求正常访问 打开 所有请求都会被降级 ,请求次数不低于20次 失败比例 到达一半 触发熔断 半开 熔断器打开后会计时5s转为此状态(半开),释放部分请求,若都是健康的则关闭,否则保持打开,再次进行5s计时 源码阅读 配置 todo 实践7 Feigh(伪装)把rest的请求进行隐藏,伪装成类似springMVC的controller一样 不用自己拼接url参数等等,一切交给Feign 1234567891011package com.itheima.consumer.client;import com.itheima.consumer.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(&quot;user-service&quot;)public interface UserClient &#123;@GetMapping(&quot;/user/&#123;id&#125;&quot;)User queryById(@PathVariable(&quot;id&quot;) Long id);&#125; 首先这是一个接口 feign会通过动态代理,帮我们生成实现类,跟mybatis的mapper很像 编写新的控制器类 1234567891011121314151617181920package com.itheima.consumer.controller;import com.itheima.consumer.client.UserClient;import com.itheima.consumer.pojo.User;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/cf&quot;)public class ConsumerFeignController &#123;@Autowiredprivate UserClient userClient;@GetMapping(&quot;/&#123;id&#125;&quot;)public User queryById(@PathVariable Long id)&#123;return userClient.queryById(id);&#125;&#125; 在启动类开启feign功能 @EnableFeignClients 可以开启请求压缩 gzip减少通信过程中的性能损耗 123456feign:compression:request:enabled: true # 开启请求压缩response:enabled: true # 开启响应压缩 1234567feign:compression:request:enabled: true # 开启请求压缩mime-types: text/html,application/xml,application/json # 设置压缩的数据类型min-request-size: 2048 # 设置触发压缩的大小下限//默认 日志级别(了解)123logging:level:com.itheima: debug 12345678910111213141516package com.itheima.consumer.config;import feign.Logger;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FeignConfig &#123;@BeanLogger.Level feignLoggerLevel()&#123;//记录所有请求和响应的明细，包括头信息、请求体、元数据return Logger.Level.FULL; /*NONE：不记录任何日志信息，这是默认值。BASIC：仅记录请求的方法，URL以及响应状态码和执行时间HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。*/&#125;&#125; 观察日志 实践8 Gateway替代 netflix zuul核心是一系列的过滤器,通过这些过滤器可以将客户端发送的请求转发(路由)到对应微服务,是加在整个微服务最前沿的防火墙和代理器,隐藏微服务结点IP端口信息,从而加强安全保护,本身也是一个微服务,需要注册到中心 核心功能: ​ 路由:路由信息的组成:由一个ID,一个目的URL,一组断言工厂,一组Filter组成 如果路由断言为真,说明请求URL和配置路由匹配 ​ 断言: 网关中的断言函数输入类型是Spring5.0中的 ServerWebExchange,断言函数允许开发者去定义匹配来自于Http request中的任何信息比如请求头和参数 ​ 过滤器: 标准 spring WebFilter 网关中有两种filter 分别是 网关过滤器和 全局过滤器,过滤器将会对请求和响应进行修改处理 快速入门 创建项目(模块Maven) 1.引入依赖123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;parent&gt;&lt;artifactId&gt;heima-springcloud&lt;/artifactId&gt;&lt;groupId&gt;com.itheima&lt;/groupId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.itheima&lt;/groupId&gt;&lt;artifactId&gt;heima-gateway&lt;/artifactId&gt;&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;/dependencies&gt;&lt;/project&gt; 2.编写启动类123456789101112package com.itheima.gateway;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication@EnableDiscoveryClientpublic class GatewayApplication &#123;public static void main(String[] args) &#123;SpringApplication.run(GatewayApplication.class, args);&#125;&#125; 3.编写配置1234567891011server:port: 10010spring:application:name: api-gatewayeureka:client:service-url:defaultZone: http://127.0.0.1:10086/eurekainstance:prefer-ip-address: true 4.编写路由规则需要用网关来代理 User-service 看下当前 状态 修改网关配置 123456789101112131415161718192021server:port: 10010spring:application:name: api-gatewaycloud:gateway:routes:# 路由id，可以随意写- id: user-service-route# 代理的服务地址uri: http://127.0.0.1:9091# 路由断言，可以配置映射路径predicates:- Path=/user/** #将符合Path规则的一切请求都代理到uri参数指定的地址eureka:client:service-url:defaultZone: http://127.0.0.1:10086/eurekainstance:prefer-ip-address: true 5.启动测试 spring cloud config通过修改git中的配置文件实现所有微服务的配置文件的修改 添加配置中心依赖 配置 1234567891011121314server: port: 12000spring: application: name: config-server cloud: config: server: git: uri: https://gitee.com/goheima/heima-config.giteureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 修改service 删除application 创建 bootstrap.yml 12345678910111213141516171819spring: cloud: config: # 要与仓库中的配置文件的application保持一致 name: user # 要与仓库中的配置文件的profile保持一致 profile: dev # 要与仓库中的配置文件所属的版本（分支）一样 label: master discovery: # 使用配置中心 enabled: true # 配置中心服务名 service-id: config-servereureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka BusSpring Cloud Bus作用：将git仓库的配置文件更新，在不重启系统的情况下实现及时同步到各个微服务 Spring Cloud Alibaba","categories":[{"name":"微服务","slug":"微服务","permalink":"https://gouguoqiang.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://gouguoqiang.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"}]},{"title":"JUC","slug":"8JUC","date":"2022-09-01T03:51:56.000Z","updated":"2023-03-01T13:56:40.042Z","comments":true,"path":"2022/09/01/8JUC/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/8JUC/","excerpt":"","text":"使用线程线程常用方法1.Thread.currentThread() 类方法 获得当前进程， Java中任何一段代码都是执行在某个线程中的，执行当前代码的线程就是当前线程 ，返回的是在代码实际运行时候的线程 2.thread.setName(线程名称） 设置线程名称 对象方法thread.getName() 返回线程名称通过设置线程名称有助于程序调试，提高可读性 3.thread.isAlive() 判断线程是否处于活动状态 活动状态就是线程以启动且尚未终止 4.Thread.sleep(millis) 让当前线程休眠指定的毫秒数（currentThread返回的线程） 5.thread.getId()可以活的线程的唯一标识，某个编号的现成运行结束后，该编号可能被后续创建的线程使用，重启JVM后同一个线程的编号可能不一样 6.Thread.yield()方法的作用是放弃当前的CPU资源 7.thread.setPriority(num) ;设置线程的优先级Java的线程优先级取值范围是1~10 如果超出这个范围会抛出异常8.illegalArgumentException(非法参数异常）线程优先具有继承性，在线程A创建了线程B 则B线程的优先级与A线程是一样的 9.thread.interrupt() &#x2F;&#x2F;仅是给线程标记中断 线程有thread.isInterrupted()方法，该方法返回线程的中断标志 true or false 10.thread.setDaemon()设置守护线程 守护线程是为其他线程提供服务的线程，如GC就是一个典型的守护线程，守护线程不能单独运行，当JVM中没有其他用户线程，只有守护线程，守护线程会自动销毁，JVM会退出 设置守护线程应该在线程启动之前 12345678public class Test&#123; public static void main(String[] args)&#123; SubDaemonThread thread = new SubDaemonThread(); thread.setDaemon(true); //设置守护线程的代码应该在线程启动之前否则会报非法线程状态异常 thread.start(); //当main线程结束，守护线程thread也销毁了 &#125;&#125; 11.Thread.state 枚举类型可通过getState()方法获得 Java中的线程的生命周期1 NEW新建状态 在调用start（）启动之前的状态2 RUNNABLE 可运行状态 包含READY和RUNNING yield方法将RUNNABLE转为READY3 BLOCKED 线程发起阻塞的IO操作或者申请由其他线程占用的独占资源 转为阻塞满足条件转换为RUNNABLE4WAITING 等待状态 线程执行了object.wait()(可以使执行当前代码的线程等待，暂停执行，直到接到通知或被中断为止，只能由锁对象调用，调用wait（）方法当前线程会释放锁),thread.join方法（挂起调用线程的执行，直到被调用的对象完成他的执行，T1，T2，T3三个线程怎么保证按一定顺序执行， 按顺序try join 例如1-2-3 则在2中try T1.join,3中try T2.join）然后依次（代码）启动三个线程） 会把看成转为WAITING等待状态，执行object.notify() 方法 或者加入的线程执行完毕，当前线程都会转换为RUNNABLE状态5TIMED_WAITING状态，与WAITING类似 区别在于不会无限等待 如果线程没有在指定的时间范围内完成期望的操作，该线程自动转化为RUNNABLE 如sleep(long)wait(long) 6TERMINATED 终止状态 线程结束处于终止状态 基础线程机制ExecutorExecutor 管理多个异步任务的执行，而无需程序员显式地管理线程的生命周期。这里的异步是指多个任务的执行互不干扰，不需要进行同步操作。 主要有三种 Executor： CachedThreadPool：一个任务创建一个线程； FixedThreadPool：所有任务只能使用固定大小的线程； SingleThreadExecutor：相当于大小为 1 的 FixedThreadPool。 1234567public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; executorService.execute(new MyRunnable()); &#125; executorService.shutdown();&#125; 多线程二CPU与磁盘IO速度有差异 多线程时分复用减少CPU空闲 提高效率的方式：（1）CPU增加了缓存（2）操作系统增加了进程，线程以分时复用CPU 进而均衡CPU 与IO设备的速度差异 Java采用抢占式线程调度：如果一个线程申请IO 或者申请一个被其他线程占用的资源 就会进入阻塞状态 让出CPU 待准备完成OS会把这个休眠的线程唤醒，唤醒后就有机会重新获得CPU使用权（3）编译程序优化指令顺序 可见性：工作区和共享区数据更新应该立刻能被看到原子性：该操作要么已经执行完成要么尚未发生，其他线程不能得到操作的中间结果有序性：重排序可能会导致多线程程序出现非预期操作 （重排序）是对内存访问有序操作的一种优化 ：1 指令重排序 ：主要是由JIT编译器，处理器引起的，指程序顺序与执行顺序不一致 2存储子系统重排序：由高速缓存（是CPU中为了匹配与主内存处理速度不匹配而舍设计一个高速缓存），写缓冲器（用来提高高速缓存操作的效率）引起的，感知顺序与执行顺序不一致 重排序要保证单线程程序的正确性（貌似串行语义） 所以有数据依赖关系的指令不会重排 如果两个指令（操作）访问同一个变量，其中一个有写操作这两个指令就存在数据依赖关系 volatile synchronizevolatilevolatile 修饰变量关键字可以保证可见性与有序性（1）当对volatile变量执行写操作后JMM会把工作内存中的最新变量值强行刷新到主内存，写操作会导致其他线程里的缓存无效（CPU嗅探总线，主存中更改的数据地址与自己缓存对比，若一致则失效）（2） 防止指令重排 在volatile前后加上内存屏障 （各种屏障都是保证同步，简单来说在屏障之后的写操作必须等待屏障之前的写操作完成才可以执行，读操作则不受影响）缺点不具有原子性volatile的实现是轻量级的 性能优于 synchronized synchronize1 synchronized 隐性锁 依赖monitor2 每个对象会与一个monitor相关联 （1）当监视器被占用时，就会处于锁定状态，监视器的获得过程是排他的。如果某线程已经占用了监视器，则其他线程会进入阻塞状态等待锁的释放 （2）执行完成退出监视器修饰实例方法 修饰类方法 修饰代码块 （注意锁粒度） 同步器AQS底层CAS（抽象队列同步器）定义了一套多线程访问共享资源的同步器框架利用CLH队列锁实现 将获取不到线程的进程放入 JMM（Java内存模型)JVM中的共享数据可能被分配到CPU中的寄存器中，主内存RAM中若分配到寄存器中，每个CPU都有自己的 一个CPU不能读取其他CPU上的内容，如果两个线程分别运行在不同CPU上，无法看到数据的变化 CPU不直接从主存读取数据，先把RAM中数据读到Cache缓存中再把Cache的数据读到寄存器中，CPU中线程对数据更新，可能只是更新到写缓冲器，还没有到达Cache更不用说主存 分配到主存中 运行在另一个CPU中的线程无法看到共享数据的更新 CPU具有缓存同步 共享数据的更新必须被写入cache 这个过程就是冲刷处理缓存 JMM对这些进行规定 ：每个线程之间的共享数据都存储在主内存中 每个线程都有一个私有的工作内存（是一个抽象的概念，他涵盖寄存器，写缓冲器，其他硬件的优化） 每个线程从主内存中把数据读取到本地工作内存中，在工作内存中保存共享数据的副本，工作内存仅对当前线程可见 程序、进程、线程的理解 1、程序(programm)概念：是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码。 2、进程(process)概念：程序的一次执行过程，或是正在运行的一个程序。说明：进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域 3、线程(thread)概念：进程可进一步细化为线程，是一个程序内部的一条执行路径。说明：线程作为CPU调度和执行的单位，每个线程拥独立的运行栈和程序计数器(pc)，线程切换的开销小。 进程可以细化为多个线程。每个线程，拥有自己独立的：栈、程序计数器多个线程，共享同一个进程中的结构：方法区、堆。 并行与并发单核CPU与多核CPU的理解 单核CPU，其实是一种假的多线程，因为在一个时间单元内，也只能执行一个线程的任务。例如：虽然有多车道，但是收费站只有一个工作人员在收费，只有收了费才能通过，那么CPU就好比收费人员。如果某个人不想交钱，那么收费人员可以把他“挂起”（晾着他，等他想通了，准备好了钱，再去收费。）但是因为CPU时间单元特别短，因此感觉不出来。 如果是多核的话，才能更好的发挥多线程的效率。（现在的服务器都是多核的） 一个Java应用程序java.exe，其实至少三个线程：main()主线程，gc()垃圾回收线程，异常处理线程。当然如果发生异常，会影响主线程。 并行与并发的理解并行：多个CPU同时执行多个任务。比如：多个人同时做不同的事。并发：一个CPU(采用时间片)同时执行多个任务。比如：秒杀、多个人做同一件事 Runnable接口构造线程源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788JAVA/*下面是Thread类的部分源码*///1.用Runnable接口创建线程时会进入这个方法public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0); &#125;//2.接着调用这个方法private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true); &#125;//3.再调用这个方法private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name cannot be null&quot;); &#125; this.name = name; Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) &#123; /* Determine if it&#x27;s an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) &#123; g = security.getThreadGroup(); &#125; /* If the security doesn&#x27;t have a strong opinion of the matter use the parent thread group. */ if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); //4.最后在这里将Runnable接口(target)赋值给Thread自己的target成员属性 this.target = target; setPriority(priority); if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); &#125;/*如果你是实现了runnable接口，那么在上面的代码中target便不会为null，那么最终就会通过重写的规则去调用真正实现了Runnable接口(你之前传进来的那个Runnable接口实现类)的类里的run方法*/@Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 1、多线程的设计之中，使用了代理设计模式的结构，用户自定义的线程主体只是负责项目核心功能的实现，而所有的辅助实现全部交由Thread类来处理。2、在进行Thread启动多线程的时候调用的是start()方法，而后找到的是run()方法，但通过Thread类的构造方法传递了一个Runnable接口对象的时候，那么该接口对象将被Thread类中的target属性所保存，在start()方法执行的时候会调用Thread类中的run()方法。而这个run()方法去调用实现了Runnable接口的那个类所重写过run()方法，进而执行相应的逻辑。多线程开发的本质实质上是在于多个线程可以进行同一资源的抢占，那么Thread主要描述的是线程，而资源的描述是通过Runnable完成的。如下图所示： Thread类构造线程源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384JAVAMyThread t2 = new MyThread(); //这个构造函数会默认调用Super();也就是Thread类的无参构造JAVA//代码从上往下顺序执行public Thread() &#123; init(null, null, &quot;Thread-&quot; + nextThreadNum(), 0); &#125; private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true); &#125; private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name cannot be null&quot;); &#125; this.name = name; Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) &#123; /* Determine if it&#x27;s an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) &#123; g = security.getThreadGroup(); &#125; /* If the security doesn&#x27;t have a strong opinion of the matter use the parent thread group. */ if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; setPriority(priority); if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); &#125;/*由于这里是通过继承Thread类来实现的线程，那么target这个东西就是Null。但是因为你继承了Runnable接口并且重写了run()，所以最终还是调用子类的run()*/ @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 最直观的代码描述1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677JAVAclass Window extends Thread&#123; private int ticket = 100; @Override public void run() &#123; while(true)&#123; if(ticket &gt; 0)&#123; System.out.println(getName() + &quot;：卖票，票号为：&quot; + ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125;&#125;public class WindowTest &#123; public static void main(String[] args) &#123; Window t1 = new Window(); Window t2 = new Window(); Window t3 = new Window(); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125;JAVAclass Window1 implements Runnable&#123; private int ticket = 100; @Override public void run() &#123; while(true)&#123; if(ticket &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;:卖票，票号为：&quot; + ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125;&#125;public class WindowTest1 &#123; public static void main(String[] args) &#123; Window1 w = new Window1(); Thread t1 = new Thread(w); Thread t2 = new Thread(w); Thread t3 = new Thread(w); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125; 1、继承Thread类的方式，new了三个Thread，实际上是有300张票。 2、实现Runnable接口的方式，new了三个Thread，实际上是有100张票。 3、也就是说实现Runnable接口的线程中，成员属性是所有线程共有的。但是继承Thread类的线程中，成员属性是各个线程独有的，其它线程看不到，除非采用static的方式才能使各个线程都能看到。 4、就像上面说的Runnable相当于资源，Thread才是线程。用Runnable创建线程时，new了多个Thread，但是传进去的参数都是同一个Runnable（资源）。用Thread创建线程时，就直接new了多个线程，每个线程都有自己的Runnable（资源）。在Thread源码中就是用target变量（这是一个Runnable类型的变量）来表示这个资源。 5、同时因为这两个的区别，在并发编程中，继承了Thread的子类在进行线程同步时不能将成员变量当做锁，因为多个线程拿到的不是同一把锁，不过用static变量可以解决这个问题。而实现了Runnable接口的类在进行线程同步时没有这个问题。 策略模式在Thread和Runnable中的应用Runnable接口最重要的方法—–run方法，使用了策略者模式将执行的逻辑(run方法)和程序的执行单元(start0方法)分离出来，使用户可以定义自己的程序处理逻辑，更符合面向对象的思想。 Thread的构造方法 创建线程对象Thread，默认有一个线程名，以Thread-开头，从0开始计数，如“Thread-0、Thread-1、Thread-2 …” 如果没有传递Runnable或者没有覆写Thread的run方法，该Thread不会调用任何方法 如果传递Runnable接口的实例或者覆写run方法，则会执行该方法的逻辑单元（逻辑代码） 如果构造线程对象时，未传入ThreadGroup，Thread会默认获取父线程的ThreadGroup作为该线程的ThreadGroup，此时子线程和父线程会在同一个ThreadGroup中 stackSize可以提高线程栈的深度，放更多栈帧，但是会减少能创建的线程数目 stackSize默认是0，如果是0，代表着被忽略，该参数会被JNI函数调用，但是注意某些平台可能会失效，可以通过“-Xss10m”设置 具体的介绍可以看Java的API文档 123456789101112131415161718192021222324252627282930313233343536JAVA/*下面是Thread 的部分源码*/public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);&#125;public Thread(String name) &#123; init(null, null, name, 0);&#125; ↓ ↓ ↓ ↓ ↓ ↓ private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true);&#125; ↓ ↓ ↓ ↓ ↓ ↓ private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; //中间源码省略 this.target = target;//①&#125;/* What will be run. */private Runnable target; //Thread类中的target属性@Overridepublic void run() &#123; if (target != null) &#123; //② target.run(); &#125;&#125; 源码标记解读： 1、如果Thread类的构造方法传递了一个Runnable接口对象 ①那么该接口对象将被Thread类中的target属性所保存。 ②在start()方法执行的时候会调用Thread类中的run()方法。因为target不为null， target.run()就去调用实现Runnable接口的子类重写的run()。 2、如果Thread类的构造方传没传Runnable接口对象 ①Thread类中的target属性保存的就是null。 ②在start()方法执行的时候会调用Thread类中的run()方法。因为target为null，只能去调用继承Thread的子类所重写的run()。 JVM一旦启动，虚拟机栈的大小已经确定了。但是如果你创建Thread的时候传了stackSize（该线程占用的stack大小），该参数会被JNI函数去使用。如果没传这个参数，就默认为0，表示忽略这个参数。注：stackSize在有一些平台上是无效的。 start()源码12345678910111213141516171819202122232425262728293031323334JAVApublic synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException();//① group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125;&#125;private native void start0();@Overridepublic void run() &#123; if (target != null) &#123; target.run(); &#125;&#125; 源码标记解读： ①当多次调用start()，会抛出throw new IllegalThreadStateException()异常。也就是每一个线程类的对象只允许启动一次，如果重复启动则就抛出此异常。 为什么线程的启动不直接使用run()而必须使用start()呢?1、如果直接调用run()方法，相当于就是简单的调用一个普通方法。 2、run()的调用是在start0()这个Native C++方法里调用的 线程生命周期Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态，这几个状态在Java源码中用枚举来表示。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示。 图中 wait到 runnable状态的转换中，join实际上是Thread类的方法，但这里写成了Object。 1、由上图可以看出：线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 2、操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态，所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 3、调用sleep()方法，会进入Blocked状态。sleep()结束之后，Blocked状态首先回到的是Runnable状态中的Ready（也就是可运行状态，但并未运行）。只有拿到了cpu的时间片才会进入Runnable中的Running状态。 一个Java程序有哪些线程？1、当你调用一个线程start()方法的时候，此时至少有两个线程，一个是调用你的线程，还有一个是被你创建出来的线程。 例子： 12345678910JAVApublic static void main(String[] args) &#123; Thread t1 = new Thread() &#123; @Override public void run() &#123; System.out.println(&quot;==========&quot;); &#125; &#125;; t1.start();&#125; 这里面就是一个调用你的线程（main线程），一个被你创建出来的线程（t1，名字可能是Thread-0） 2、当JVM启动后，实际有多个线程，但是至少有一个非守护线程（比如main线程）。 Finalizer：GC守护线程 RMI：Java自带的远程方法调用（秋招面试，有个面试官问过） Monitor ：是一个守护线程，负责监听一些操作，也在main线程组中 其它：我用的是IDEA，其它的应该是IDEA的线程，比如鼠标监听啥的。 守护线程用处(发送心跳)A和B之间有一条网络连接，可以用守护线程来进行发送心跳，一旦A和B连接断开，非守护线程就结束了，守护线程（也就是心跳没有必要再发送了）也刚好断开。 123456789101112131415161718192021222324252627282930313233JAVApublic static void main(String[] args) &#123; Thread t = new Thread(() -&gt; &#123; Thread innerThread = new Thread(() -&gt; &#123; try &#123; while (true) &#123; System.out.println(&quot;Do some thing for health check.&quot;); Thread.sleep(1_000); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); // innerThread.setDaemon(true); innerThread.start(); try &#123; Thread.sleep(1_000); System.out.println(&quot;T thread finish done.&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); //t.setDaemon(true); t.start();&#125;/*设置该线程为守护线程必须在启动它之前。如果t.start()之后，再t.setDaemon(true);会抛出IllegalThreadStateException*/ 输出结果： Do some thing for health check.Do some thing for health check.T thread finish done. &#x2F;&#x2F;此时main线程已经结束，但是由于innerThread还在发送心跳，应用不会关闭Do some thing for health check.Do some thing for health check.Do some thing for health check.Do some thing for health check. 守护线程还有其它很多用处，在后面的文章里还会有出现。 join方法例子1 1234567891011121314151617181920JAVApublic static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); Thread t2 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); t1.start(); t2.start(); t1.join(); t2.join(); Optional.of(&quot;All of tasks finish done.&quot;).ifPresent(System.out::println); IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i));&#125; 默认传入的数字为0，这里是在main线程里调用了两个线程的join()，所以main线程会等到Thread-0和Thread-1线程执行完再执行它自己。 join必须在start方法之后，并且join()是对wait()的封装。（源码中可以清楚的看到） 也就是说，t.join()方法阻塞调用此方法的线程(calling thread)进入 TIMED_WAITING或WAITING 状态。直到线程t完成，此线程再继续。 join也有人理解成插队，比如在main线程中调用t.join()，就是t线程要插main线程的队，main线程要去等待。 例子2 12345678910111213141516171819202122232425JAVApublic static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); Thread t2 = new Thread(() -&gt; &#123; try &#123; t1.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); t1.start(); t2.start();// t1.join(); t2.join(); Optional.of(&quot;All of tasks finish done.&quot;).ifPresent(System.out::println); IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125; 这里是在t2（我们以后就都用变量名来称呼线程了）线程了。t1.join()了。所以t2线程会等待t1线程打印完，t2自己才会打印。然后t2.join()，main线程也要等待t2线程。总体执行顺序就是t1–&gt;t2–&gt;main 通过上方例子可以用join实现类似于CompletableFuture的异步任务编排。（后面会讲） 中断1、Java 中的中断和操作系统的中断还不一样，这里就按照状态来理解吧，不要和操作系统的中断联系在一起 2、记住中断只是一个状态，Java的方法可以选择对这个中断进行响应，也可以选择不响应。响应的意思就是写相对应的代码执行相对应的操作，不响应的意思就是什么代码都不写。 几个方法12345678910111213JAVA// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态public boolean isInterrupted() &#123;&#125;/*1、Thread 中的静态方法，检测调用这个方法的线程是否已经中断2、注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了*/public static boolean interrupted() &#123;&#125;// Thread 类中的实例方法，用于设置一个线程的中断状态为 truepublic void interrupt() &#123;&#125; 小tip1234JAVApublic static boolean interrupted()public boolean isInterrupted()//这个会清除中断状态 为什么要这么设置呢？原因在于： interrupted()是一个静态方法，可以在Runnable接口实例中使用 isInterrupted()是一个Thread的实例方法，在重写Thread的run方法时使用 12345678910111213141516JAVApublic class ThreadInterrupt &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(Thread.interrupted()); &#125;); //这个new Thread用的是runnable接口那个构造函数 Thread t2 = new Thread()&#123; @Override public void run() &#123; System.out.println(isInterrupted()); &#125; &#125;;//这个new Thread用的就是Thread的空参构造 &#125;&#125; 也就是说接口中不能调用Thread的实例方法，只能通过静态方法来判断是否发生中断 重难点当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。 初学者肯定以为 thread.interrupt() 方法是用来暂停线程的，主要是和它对应中文翻译的“中断”有关。中断在并发中是常用的手段，请大家一定好好掌握。可以将中断理解为线程的状态，它的特殊之处在于设置了中断状态为 true 后，这几个方法会感知到： wait(), wait(long), wait(long, int), join(), join(long), join(long, int), sleep(long), sleep(long, int) 这些方法都有一个共同之处，方法签名上都有throws InterruptedException，这个就是用来响应中断状态修改的。 如果线程阻塞在 InterruptibleChannel 类的 IO 操作中，那么这个 channel 会被关闭。 如果线程阻塞在一个 Selector 中，那么 select 方法会立即返回。 对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。然后执行相应的操作（通常就是跳到 catch 异常处）。 如果不是以上3种情况，那么，线程的 interrupt() 方法被调用，会将线程的中断状态设置为 true。 那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。 并发编程中的三个问题可见性可见性概念可见性（Visibility）：是指一个线程对共享变量进行修改，另一个先立即得到修改后的新值。 可见性演示1234567891011121314151617181920212223242526272829303132333435363738JAVA/* 笔记 * 1.当没有加Volatile的时候,while循环会一直在里面循环转圈 * 2.当加了之后Volatile,由于可见性,一旦num改了之后,就会通知其他线程 * 3.还有注意的时候不能用if,if不会重新拉回来再判断一次。(也叫做虚假唤醒) * 4.案例演示:一个线程对共享变量的修改,另一个线程不能立即得到新值 * */public class Video04_01 &#123; public static void main(String[] args) &#123; MyData myData = new MyData(); new Thread(() -&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;\\t come in &quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //睡3秒之后再修改num,防止A线程先修改了num,那么到while循环的时候就会直接跳出去了 myData.addTo60(); System.out.println(Thread.currentThread().getName() + &quot;\\t come out&quot;); &#125;,&quot;A&quot;).start(); while(myData.num == 0)&#123; //只有当num不等于0的时候,才会跳出循环 &#125; &#125;&#125;class MyData&#123; int num = 0; public void addTo60()&#123; this.num = 60; &#125;&#125; 由上面代码可以看出，并发编程时，会出现可见性问题，当一个线程对共享变量进行了修改，另外的线程并没有立即看到修改后的最新值。 原子性原子性概念原子性（Atomicity）：在一次或多次操作中，要么所有的操作都成功执行并且不会受其他因素干扰而中 断，要么所有的操作都不执行或全部执行失败。不会出现中间状态 原子性演示案例演示:5个线程各执行1000次 i++; 1234567891011121314151617181920212223242526272829303132333435363738JAVA/** * @Author: 吕 * @Date: 2019/9/23 15:50 * &lt;p&gt; * 功能描述: volatile不保证原子性的代码验证 */public class Video05_01 &#123; public static void main(String[] args) &#123; MyData03 myData03 = new MyData03(); for (int i = 0; i &lt; 20; i++) &#123; new Thread(() -&gt;&#123; for (int j = 0; j &lt; 1000; j++) &#123; myData03.increment(); &#125; &#125;,&quot;线程&quot; + String.valueOf(i)).start(); &#125; //需要等待上面的20个线程计算完之后再查看计算结果 while(Thread.activeCount() &gt; 2)&#123; Thread.yield(); &#125; System.out.println(&quot;20个线程执行完之后num:\\t&quot; + myData03.num); &#125;&#125;class MyData03&#123; static int num = 0; public void increment()&#123; num++; &#125;&#125; 1、控制台输出：（由于并发不安全，每次执行的结果都可能不一样） 20个线程执行完之后num: 19706 正常来说，如果保证原子性的话，20个线程执行完，结果应该是20000。控制台输出的值却不是这个，说明出现了原子性的问题。 2、使用javap反汇编class文件，对于num++可以得到下面的字节码指令： 12345CODE9: getstatic #12 // Field number:I 取值操作12: iconst_1 13: iadd 14: putstatic #12 // Field number:I 赋值操作 由此可见num++是由多条语句组成，以上多条指令在一个线程的情况下是不会出问题的，但是在多线程情况下就可能会出现问题。 比如num刚开始值是7。A线程在执行13: iadd时得到num值是8，B线程又执行9: getstatic得到前一个值是7。马上A线程就把8赋值给了num变量。但是B线程已经拿到了之前的值7，B线程是在A线程真正赋值前拿到的num值。即使A线程最终把值真正的赋给了num变量，但是B线程已经走过了getstaitc取值的这一步，B线程会继续在7的基础上进行++操作，最终的结果依然是8。本来两个线程对7进行分别进行++操作，得到的值应该是9，因为并发问题，导致结果是8。 3、并发编程时，会出现原子性问题，当一个线程对共享变量操作到一半时，另外的线程也有可能来操作共 享变量，干扰了前一个线程的操作。 有序性有序性概念有序性（Ordering）：是指程序中代码的执行顺序，Java在编译时和运行时会对代码进行优化（重排序）来加快速度，会导致程序终的执行顺序不一定就是我们编写代码时的顺序 12345JAVAinstance = new SingletonDemo() 是被分成以下 3 步完成 memory = allocate(); 分配对象内存空间 instance(memory); 初始化对象 instance = memory; 设置 instance 指向刚分配的内存地址，此时 instance != null 步骤2 和 步骤3 不存在数据依赖关系，重排与否的执行结果单线程中是一样的。这种指令重排是被 Java 允许的。当 3 在前时，instance 不为 null，但实际上初始化工作还没完成，会变成一个返回 null 的getInstance。这时候数据就出现了问题。 有序性演示jcstress是java并发压测工具。https://wiki.openjdk.java.net/display/CodeTools/jcstress 修改pom文件，添加依赖： 12345678910111213141516171819202122232425262728293031323334353637CODE&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jcstress&lt;/groupId&gt; &lt;artifactId&gt;jcstress-core&lt;/artifactId&gt; &lt;version&gt;$&#123;jcstress.version&#125;&lt;/version&gt; &lt;/dependency&gt;JAVAimport org.openjdk.jcstress.annotations.*;import org.openjdk.jcstress.infra.results.I_Result; @JCStressTest // @Outcome: 如果输出结果是1或4，我们是接受的(ACCEPTABLE)，并打印ok @Outcome(id = &#123;&quot;1&quot;, &quot;4&quot;&#125;, expect = Expect.ACCEPTABLE, desc = &quot;ok&quot;) //如果输出结果是0，我们是接受的并且感兴趣的，并打印danger @Outcome(id = &quot;0&quot;, expect = Expect.ACCEPTABLE_INTERESTING, desc = &quot;danger&quot;) @Statepublic class Test03Ordering &#123; int num = 0; boolean ready = false; // 线程1执行的代码 @Actor //@Actor：表示会有多个线程来执行这个方法 public void actor1(I_Result r) &#123; if (ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125; &#125; // 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; ready = true; &#125;&#125; 1、实际上上面两个方法会有很多线程来执行，为了讲解方便，我们只提出线程1和线程2来讲解。 2、I_Result 是一个保存int类型数据的对象，有一个属性 r1 用来保存结果，在多线程情况下可能出现几种结果？ 情况1：线 程1先执行actor1，这时ready &#x3D; false，所以进入else分支结果为1。 情况2：线程2执行到actor2，执行了num &#x3D; 2;和ready &#x3D; true，线程1执行，这回进入 if 分支，结果为 4。 情况3：线程2先执行actor2，只执行num &#x3D; 2；但没来得及执行 ready &#x3D; true，线程1执行，还是进入 else分支，结果为1。 情况4：0，发生了指令重排 12345678JAVA// 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; //pos_1 ready = true;//pos_2 &#125; pos_1处代码和pos_2处代码没有什么数据依赖关系，或者说没有因果关系。Java可能对其进行指令重排，排成下面的顺序。 1234567JAVA// 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; ready = true;//pos_2 num = 2; //pos_1 &#125; 此时如果线程2先执行到ready = true;还没来得及执行 num = 2; 。线程1执行，直接进入if分支，此时num默认值为0。 得到的结果也就是0。 volatile 1、关于可见性，重排序等等的硬件原理，MESI缓存一致性，内存屏障，JMM等等这些，请看我的后面文章。第一阶段只是介绍下用法，不涉及原理。 2、如果你在第一篇文章没有找到你想要的内容，请看我后面的内容。并发的体系，我自认为讲的还是比较全面的。 volatile保证可见性代码 读者可以把两个代码运行一下，就能明显看到不加volatile的死循环（就是程序一直显示没结束） 1234567891011121314151617181920212223242526272829303132333435363738JAVA/* 笔记 * 1.当没有加Volatile的时候,while循环会一直在里面转圈 * 2.当加了之后Volatile,由于可见性,一旦num改了之后,就会通知其他线程 * 3.还有注意的时候不能用if,if不会重新拉回来再判断一次 * */public class Video04_02 &#123; public static void main(String[] args) &#123; MyData2 myData = new MyData2(); new Thread(() -&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;\\t come in &quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //睡3秒之后再修改num,防止A线程先修改了num,那么到while循环的时候就会直接跳出去了 myData.addTo60(); System.out.println(Thread.currentThread().getName() + &quot;\\t come out&quot;); &#125;,&quot;A&quot;).start(); while(myData.num == 0)&#123; //只有当num不等于0的时候,才会跳出循环 &#125; &#125;&#125;class MyData2&#123; volatile int num = 0; public void addTo60()&#123; this.num = 60; &#125;&#125; volatile保证有序性代码12345678910111213141516171819202122232425262728293031JAVAimport org.openjdk.jcstress.annotations.*;import org.openjdk.jcstress.infra.results.I_Result; @JCStressTest // @Outcome: 如果输出结果是1或4，我们是接受的(ACCEPTABLE)，并打印ok @Outcome(id = &#123;&quot;1&quot;, &quot;4&quot;&#125;, expect = Expect.ACCEPTABLE, desc = &quot;ok&quot;) //如果输出结果是0，我们是接受的并且感兴趣的，并打印danger @Outcome(id = &quot;0&quot;, expect = Expect.ACCEPTABLE_INTERESTING, desc = &quot;danger&quot;) @Statepublic class Test03Ordering &#123; volatile int num = 0; volatile boolean ready = false; // 线程1执行的代码 @Actor //@Actor：表示会有多个线程来执行这个方法 public void actor1(I_Result r) &#123; if (ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125; &#125; // 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; ready = true; &#125;&#125; 读者可以将运行结果对比着来看，就能发现区别。 volatile只能保证可见性和有序性（禁止指令重排），无法保证原子性。 CASvolatile自己虽然不能保证原子性，但是和CAS结合起来就可以保证原子性了。CAS+volatile一起用就可以同时解决并发编程中的三个问题了，保证并发安全。 CAS 是什么？ CAS：比较并交换compareAndSet,它是一条 CPU 并发原语，它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子性的。 例: AtomicInteger 的 compareAndSet(‘期望值’,’设置值’) 方法，期望值与目标值一致时，修改目标变量为设置值，期望值与目标值不一致时，返回 false 和最新主存的变量值 CAS 的底层原理 1234例: AtomicInteger.getAndIncrement() 调用 Unsafe 类中的 CAS 方法，JVM 会帮我们实现出 CAS 汇编指令 这是一种完全依赖于硬件的功能，通过它实现原子操作。 原语的执行必须是连续的，在执行过程中不允许被中断，CAS 是 CPU 的一条原子指令。 CAS的思想就是乐观锁的思想 AtomicInteger在JUC并发包中，CAS和AtomicInteger（原子类的value值都被volatile修饰了）一起保证了并发安全。下面我们以AtomicInteger.getAndIncrement() 方法讲一下。 12345678910111213141516171819202122232425262728293031323334353637JAVA/** * unsafe: rt.jar/sun/misc/Unsafe.class * Unsafe 是 CAS 的核心类，由于 Java 无法直接访问底层系统，需要通过本地&lt;native&gt;方法来访问 * Unsafe 相当于一个后门，基于该类可以直接操作特定内存的数据 * Unsafe 其内部方法都是 native 修饰的，可以像 C 的指针一样直接操作内存 * Java 中的 CAS 操作执行依赖于 Unsafe 的方法，直接调用操作系统底层资源执行程序 * * this: 当前对象 * 变量 value 由 volatile 修饰，保证了多线程之间的内存可见性、禁止重排序 * * valueOffset: 内存地址 * 表示该变量值在内存中的偏移地址，因为 Unsafe 就是根据内存偏移地址获取数据 * * 1: 固定写死，原值加1 */public final int getAndIncrement()&#123; return unsafe.getAndAddInt(this,valueOffset,1);&#125;/** * Unsafe.getAndAddInt() * getIntVolatile: 通过内存地址去主存中取对应数据 * * while(!this.compareAndSwapInt(var1,var2,var5,var5 + var4)): * 将本地 value 与主存中取出的数据对比，如果相同，对其作运算， * 此时返回 true，取反后 while 结束，返回最终值。 * 如果不相同，此时返回 false，取反后 while 循环继续运行，此时为自旋锁&lt;重复尝试&gt; * 由于 value 是被 volatile 修饰的，所以拿到主存中最新值，再循环直至成功。 */public final int getAndAddInt(Object var1,long var2,int var4)&#123; int var5; do&#123; var5 = this.getIntVolatile(var1,var2); // 从主存中拷贝变量到本地内存 &#125; while(!this.compareAndSwapInt(var1,var2,var5,var5 + var4)); return var5;&#125; CAS 代码演示123456789JAVApublic class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger num = new AtomicInteger(5); // TODO System.out.println(num.compareAndSet(5, 1024) + &quot;\\t current num&quot; + num.get()); System.out.println(num.compareAndSet(5, 2019) + &quot;\\t current num&quot; + num.get()); &#125; CAS三大问题 如果 CAS 长时间一直不成功，会给 CPU 带来很大的开销，在Java的实现中是一直通过while循环自旋CAS获取锁。 只能保证一个共享变量的原子操作 引出了 ABA 问题 ABA问题什么是ABA问题？1234567891011121314151617181920212223242526272829JAVA/** * @Author: 吕 * @Date: 2019/9/24 16:43 * &lt;p&gt; * 功能描述: CAS引发的ABA问题 */public class Video19_01 &#123; static AtomicReference&lt;Integer&gt; num = new AtomicReference&lt;&gt;(100); public static void main(String[] args) &#123; new Thread(() -&gt;&#123; num.compareAndSet(100, 101); num.compareAndSet(101,100); &#125;,&quot;线程A&quot;).start(); new Thread(() -&gt;&#123; //保证A线程已经修改完 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean b = num.compareAndSet(100, 2019); System.out.println(b + &quot;\\t 当前最新值&quot; + num.get().toString()); &#125;,&quot;线程B&quot;).start(); &#125;&#125; CAS 会导致 ABA 问题： 例: A、B线程从主存取出变量 value -&gt; A 在 N次计算中改变 value 的值-&gt; A 最终计算结果还原 value 最初的值-&gt; B 计算后，比较主存值与自身 value 值一致，修改成功 尽管各个线程的 CAS 都操作成功，但是并不代表这个过程就是没有问题的。 ABA问题的解决123456789101112131415161718192021222324252627282930313233JAVA/** * @Author: 吕 * @Date: 2019/9/24 16:49 * &lt;p&gt; * 功能描述: ABA问题的解决 */public class Video19_02 &#123; static AtomicStampedReference&lt;Integer&gt; num = new AtomicStampedReference&lt;&gt;(100,1); public static void main(String[] args) &#123; int stamp = num.getStamp();//初始版本号 new Thread(() -&gt;&#123; num.compareAndSet(100,101,num.getStamp(),num.getStamp() + 1); System.out.println(Thread.currentThread().getName() + &quot;\\t 版本号&quot; + num.getStamp()); num.compareAndSet(101,100,num.getStamp(),num.getStamp() + 1); System.out.println(Thread.currentThread().getName() + &quot;\\t 版本号&quot; + num.getStamp()); &#125;,&quot;线程A&quot;).start(); new Thread(() -&gt;&#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean b = num.compareAndSet(100, 209, stamp, num.getStamp() + 1); System.out.println(b + &quot;\\t 当前版本号: \\t&quot; + num.getStamp()); System.out.println(&quot;当前最新值 \\t&quot; + num.getReference().toString()); &#125;,&quot;线程B&quot;).start(); &#125;&#125; 思想很简单，可以很明显的看出来用版本号的方式解决了ABA的问题。 除了对象值，AtomicStampedReference内部还维护了一个“状态戳”。 状态戳可类比为时间戳，是一个整数值，每一次修改对象值的同时，也要修改状态戳，从而区分相同对象值的不同状态。 当AtomicStampedReference设置对象值时，对象值以及状态戳都必须满足期望值，写入才会成功。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。还有一个方法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i&#x3D;2,j&#x3D;a合并一下ij&#x3D;2a，然后用CAS来操作ij。从java1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。 所以一般来说为了同时解决ABA问题和只能保证一个共享变量，原子类使用时大部分使用的是AtomicStampedReference UnSafeUnsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。 Java和C++语言的一个重要区别就是Java中我们无法直接操作一块内存区域，不能像C++中那样可以自己申请内存和释放内存。 Java中的Unsafe类为我们提供了类似C++手动管理内存的能力，同时也有了指针的问题。 首先，Unsafe类是”final”的，不允许继承。且构造函数是private的: 123456789101112JAVApublic final class Unsafe &#123; private static final Unsafe theUnsafe; public static final int INVALID_FIELD_OFFSET = -1; private static native void registerNatives(); private Unsafe() &#123; &#125; ...&#125; 因此我们无法在外部对Unsafe进行实例化。 获取UnsafeUnsafe无法实例化，那么怎么获取Unsafe呢？答案就是通过反射来获取Unsafe： 1234567JAVApublic Unsafe getUnsafe() throws IllegalAccessException &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); return unsafe;&#125; Unsafe的功能如下图： CAS相关JUC中大量运用了CAS操作，可以说CAS操作是JUC的基础，因此CAS操作是非常重要的。Unsafe中提供了int,long和Object的CAS操作： 123456JAVApublic final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 偏移量相关1234JAVApublic native long staticFieldOffset(Field var1);public native long objectFieldOffset(Field var1); staticFieldOffset方法用于获取静态属性Field在对象中的偏移量，读写静态属性时必须获取其偏移量。 objectFieldOffset方法用于获取非静态属性Field在对象实例中的偏移量，读写对象的非静态属性时会用到这个偏移量 类加载12345678910JAVApublic native Class&lt;?&gt; defineClass(String var1, byte[] var2, int var3, int var4, ClassLoader var5, ProtectionDomain var6);public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; var1, byte[] var2, Object[] var3);public native Object allocateInstance(Class&lt;?&gt; var1) throws InstantiationException;public native boolean shouldBeInitialized(Class&lt;?&gt; var1);public native void ensureClassInitialized(Class&lt;?&gt; var1); defineClass方法定义一个类，用于动态地创建类。 defineAnonymousClass用于动态的创建一个匿名内部类。 allocateInstance方法用于创建一个类的实例，但是不会调用这个实例的构造方法，如果这个类还未被初始化，则初始化这个类。 shouldBeInitialized方法用于判断是否需要初始化一个类。 ensureClassInitialized方法用于保证已经初始化过一个类。 举例 1234567891011121314151617181920212223242526272829303132333435363738JAVApublic class UnsafeFooTest &#123; private static Unsafe geUnsafe() &#123; try &#123; Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); f.setAccessible(true); return (Unsafe) f.get(null); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return null; &#125; static class Simple &#123; private long l = 0; public Simple() &#123; this.l = 1; System.out.println(&quot;我被初始化了&quot;); &#125; public long getL() &#123; return l; &#125; &#125; public static void main(String[] args) throws Exception &#123; Unsafe unsafe = geUnsafe(); Simple s = (Simple) unsafe.allocateInstance(Simple.class); System.out.println(s.getL()); &#125;&#125; 结果： 0 可以发现，利用Unsafe获取实例，不会调用构造方法 普通读写通过Unsafe可以读写一个类的属性，即使这个属性是私有的，也可以对这个属性进行读写。 读写一个Object属性的相关方法 1234JAVApublic native int getInt(Object var1, long var2);public native void putInt(Object var1, long var2, int var4); getInt用于从对象的指定偏移地址处读取一个int。 putInt用于在对象指定偏移地址处写入一个int。其他的primitive type也有对应的方法。 举例 123456789101112131415161718192021222324252627282930313233343536373839404142JAVApublic class UnsafeFooTest &#123; private static Unsafe geUnsafe() &#123; try &#123; Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); f.setAccessible(true); return (Unsafe) f.get(null); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return null; &#125; static class Guard&#123; private int ACCESS_ALLOWED = 1; private boolean allow()&#123; return 50 == ACCESS_ALLOWED; &#125; public void work()&#123; if (allow())&#123; System.out.println(&quot;我被允许工作....&quot;); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Unsafe unsafe = geUnsafe(); Guard guard = new Guard(); Field f = guard.getClass().getDeclaredField(&quot;ACCESS_ALLOWED&quot;); unsafe.putInt(guard,unsafe.objectFieldOffset(f),50); System.out.println(&quot;强行赋值...&quot;); guard.work(); &#125;&#125; 结果 强行赋值… 我被允许工作… 类加载12345678910JAVApublic native Class&lt;?&gt; defineClass(String var1, byte[] var2, int var3, int var4, ClassLoader var5, ProtectionDomain var6);public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; var1, byte[] var2, Object[] var3);public native Object allocateInstance(Class&lt;?&gt; var1) throws InstantiationException;public native boolean shouldBeInitialized(Class&lt;?&gt; var1);public native void ensureClassInitialized(Class&lt;?&gt; var1); defineClass方法定义一个类，用于动态地创建类。 defineAnonymousClass用于动态的创建一个匿名内部类。 allocateInstance方法用于创建一个类的实例，但是不会调用这个实例的构造方法，如果这个类还未被初始化，则初始化这个类。 shouldBeInitialized方法用于判断是否需要初始化一个类。 ensureClassInitialized方法用于保证已经初始化过一个类。 内存屏障123456JAVApublic native void loadFence();public native void storeFence();public native void fullFence(); loadFence：保证在这个屏障之前的所有读操作都已经完成。 storeFence：保证在这个屏障之前的所有写操作都已经完成。 fullFence：保证在这个屏障之前的所有读写操作都已经完成。 线程调度12345678910JAVApublic native void unpark(Object var1);public native void park(boolean var1, long var2);public native void monitorEnter(Object var1);public native void monitorExit(Object var1);public native boolean tryMonitorEnter(Object var1); park方法和unpark方法相信看过LockSupport类的都不会陌生，这两个方法主要用来挂起和唤醒线程。 LockSupport中的park和unpark方法正是通过Unsafe来实现的： 123456789101112JAVApublic static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null);&#125;public static void unpark(Thread thread) &#123; if (thread != null) UNSAFE.unpark(thread);&#125; monitorEnter方法和monitorExit方法用于加锁，Java中的synchronized锁就是通过这两个指令来实现的。 synchronized优化 synchronized可以同时保证可见性，有序性，原子性。这个东西就不讲了 从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化。synchronized说是锁，但是他的底层加锁的方式可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁 锁消除锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令。这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁 锁粗化12345678910111213141516JAVAsynchronized(this) &#123;&#125; synchronized(this) &#123; &#125; synchronized(this) &#123;&#125; 这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁 偏向锁这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS，性能会提升很多。但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好。可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小。如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好 轻量级锁如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁，如果是自己加的锁，那就执行代码就好了。如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁 适应性锁这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒。也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大。所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了 指令重排计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排。 为什么指令重排序可以提高性能？简单地说，每一个指令都会包含多个步骤，每个步骤可能使用不同的硬件。因此，流水线技术产生了，它的原理是指令1还没有执行完，就可以开始执行指令2，而不用等到指令1执行结束之后再执行指令2，这样就大大提高了效率。 但是，流水线技术最害怕中断，恢复中断的代价是比较大的，所以我们要想尽办法不让流水线中断。指令重排就是减少中断的一种技术。 我们分析一下下面这个代码的执行情况： 123JAVAa = b + c;d = e - f ; 先加载b、c（注意，即有可能先加载b，也有可能先加载c），但是在执行add(b,c)的时候，需要等待b、c装载结束才能继续执行，也就是增加了停顿，那么后面的指令也会依次有停顿,这降低了计算机的执行效率。 为了减少这个停顿，我们可以先加载e和f,然后再去加载add(b,c),这样做对程序（串行）是没有影响的,但却减少了停顿。既然add(b,c)需要停顿，那还不如去做一些有意义的事情。 综上所述，指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。 指令重排一般分为以下三种： 编译器优化重排 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令并行重排 现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序。 内存系统重排 由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。 指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致。所以在多线程下，指令重排序可能会导致一些问题。 as-if-serial语义as-if-serial语义的意思是：不管编译器和CPU如何重排序，必须保证在单线程情况下程序的结果是正确的。 以下数据有依赖关系，不能重排序。 写后读： 123CODEint a = 1; int b = a; 写后写： 123CODEint a = 1; int a = 2; 读后写： 1234CODEint a = 1; int b = a; int a = 2; 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 1234CODEint a = 1; int b = 2; int c = a + b; Java内存模型(JMM)在介绍Java内存模型之前，先来看一下到底什么是计算机内存模型。 计算机结构计算机结构简介冯诺依曼，提出计算机由五大组成部分，输入设备，输出设备存储器，控制器，运算器。 CPU中央处理器，是计算机的控制和运算的核心，我们的程序终都会变成指令让CPU去执行，处理程序中 的数据。 内存我们的程序都是在内存中运行的，内存会保存程序运行时的数据，供CPU处理。 缓存CPU的运算速度和内存的访问速度相差比较大。这就导致CPU每次操作内存都要耗费很多等待时间。内 存的读写速度成为了计算机运行的瓶颈。于是就有了在CPU和主内存之间增加缓存的设计。靠近CPU 的缓存称为L1，然后依次是 L2，L3和主内存，CPU缓存模型如图下图所示。 CPU Cache分成了三个级别: L1， L2， L3。级别越小越接近CPU，速度也更快，同时也代表着容量越小。速度越快的价格越贵。 1、L1是接近CPU的，它容量小，例如32K，速度快，每个核上都有一个L1 Cache。 2、L2 Cache 更大一些，例如256K，速度要慢一些，一般情况下每个核上都有一个独立的L2 Cache。 3、L3 Cache是三级缓存中大的一级，例如12MB，同时也是缓存中慢的一级，在同一个CPU插槽 之间的核共享一个L3 Cache。 [ 上面的图中有一个Latency指标。比如Memory这个指标为59.4ns，表示CPU在操作内存的时候有59.4ns的延迟，一级缓存最快只有1.2ns。 CPU处理数据的流程 Cache的出现是为了解决CPU直接访问内存效率低下问题的。 1、程序在运行的过程中，CPU接收到指令 后，它会先向CPU中的一级缓存（L1 Cache）去寻找相关的数据，如果命中缓存，CPU进行计算时就可以直接对CPU Cache中的数据进行读取和写人，当运算结束之后，再将CPUCache中的新数据刷新 到主内存当中，CPU通过直接访问Cache的方式替代直接访问主存的方式极大地提高了CPU 的吞吐能 力。 2、但是由于一级缓存（L1 Cache）容量较小，所以不可能每次都命中。这时CPU会继续向下一级的二 级缓存（L2 Cache）寻找，同样的道理，当所需要的数据在二级缓存中也没有的话，会继续转向L3 Cache、内存(主存)和硬盘。 Java内存模型1、Java Memory Molde (Java内存模型&#x2F;JMM)，千万不要和Java内存结构（JVM划分的那个堆，栈，方法区）混淆。关于“Java内存模型”的权威解释，参考 https://download.oracle.com/otn-pub/jcp/memory_model1.0-pfd-spec-oth-JSpec/memory_model-1_0-pfd-spec.pdf。 2、 Java内存模型，是Java虚拟机规范中所定义的一种内存模型，Java内存模型是标准化的，屏蔽掉了底层不同计算机的区别。 Java内存模型是一套规范，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量存储到内存和从内存中读取变量这样的底层细节，具体如下。 3、Java内存模型根据官方的解释，主要是在说两个关键字，一个是volatile，一个是synchronized。 主内存 主内存是所有线程都共享的，都能访问的。所有的共享变量都存储于主内存。 工作内存 每一个线程有自己的工作内存，工作内存只存储该线程对共享变量的副本。线程对变量的所有的操 作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量，不同线程之间也不能直接 访问对方工作内存中的变量。 Java的线程不能直接在主内存中操作共享变量。而是首先将主内存中的共享变量赋值到自己的工作内存中，再进行操作，操作完成之后，刷回主内存。 Java内存模型的作用 Java内存模型是一套在多线程读写共享数据时，对共享数据的可见性、有序性、和原子性的规则和保障。 synchronized,volatile CPU缓存，内存与Java内存模型的关系 通过对前面的CPU硬件内存架构、Java内存模型以及Java多线程的实现原理的了解，我们应该已经意识到，多线程的执行终都会映射到硬件处理器上进行执行。 但Java内存模型和硬件内存架构并不完全一致。 对于硬件内存来说只有寄存器、缓存内存、主内存的概念，并没有工作内存和主内存之分，也就是说Java内存模型对内存的划分对硬件内存并没有任何影响， 因为JMM只是一种抽象的概念，是一组规则，不管是工作内存的数据还是主内存的数据，对于计算机硬 件来说都会存储在计算机主内存中，当然也有可能存储到CPU缓存或者寄存器中，因此总体上来说， Java内存模型和计算机硬件内存架构是一个相互交叉的关系，是一种抽象概念划分与真实物理硬件的交叉。 JMM内存模型与CPU硬件内存架构的关系： 工作内存：可能对应CPU寄存器，也可能对应CPU缓存，也可能对应内存。 Java内存模型是一套规范，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量 存储到内存和从内存中读取变量这样的底层细节，Java内存模型是对共享数据的可见性、有序性、和原子性的规则和保障。 再谈可见性 1、图中所示是 个双核 CPU 系统架构 ，每个核有自己的控制器和运算器，其中控制器包含一组寄存器和操作控制器，运算器执行算术逻辅运算。每个核都有自己的1级缓存，在有些架构里面还有1个所有 CPU 共享的2级缓存。 那么 Java 内存模型里面的工作内存，就对应这里的 Ll 或者 L2 存或者 CPU 寄存器。 2、一个线程操作共享变量时，它首先从主内存复制共享变量到自己的工作内存，然后对工作内存里的变量进行处理，处理完后将变量值更新到主内存。 3、那么假如线程A和线程B同时处理一个共享变量，会出现什么情况?我们使用图所示CPU架构，假设线程A和线程B使用不同CPU执行，并且当前两级Cache都为空，那么这时候由于Cache的存在，将会导致内存不可见问题，具体看下面的分析。 线程A首先获取共享变量X的值，由于两级Cache都没有命中，所以加载主内存中X的值，假如为0。然后把X&#x3D;0的值缓存到两级缓存，线程A修改X的值为1，然后将其写入两级Cache，并且刷新到主内存。线程A操作完毕后，线程A所在的CPU的两级Cache 内和主内存里面的X的值都是1。 线程B获取X的值，首先一级缓存没有命中，然后看二级缓存，二级缓存命中了，所以返回X&#x3D;1;到这里一切都是正常的，因为这时候主内存中也是X&#x3D;1。然后线程B修改X的值为2，并将其存放到线程2所在的一级Cache和共享二级Cache中，最后更新主内存中X 的值为2;到这里一切都是好的。 线程A 这次又需要修改X的值，获取时一级缓存命中，并且X&#x3D;1，到这里问题就出现了，明明线程B已经把X的值修改为了2，为何线程A获取的还是1呢?这就是共享变量的内存不可见问题，也就是线程B写入的值对线程A不可见。那么如何解决共享变量内存不可见问题?使用Java中的volatile和synchronized关键字就可以解决这个问题，下面会有讲解。 主内存与工作内存之间的交互为了保证数据交互时数据的正确性，Java内存模型中定义了8种操作来完成这个交互过程，这8种操作本身都是原子性的。虚拟机实现时必须保证下面 提及的每一种操作都是原子的、不可再分的。 (1)lock:作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 (2)unlock:作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其它线程锁定。 (3)read:作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 (4)load:作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 (5)use:作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时都会执行这个操作。 (6)assign:作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 (7)store:作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write使用。 (8)write:作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 注意: 如果对一个变量执行lock操作，将会清空工作内存中此变量的值 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中 lock和unlock操作只有加锁才会有。synchronized就是通过这样来保证可见性的。 如果没有synchronized，那就是下面这样的 happens-before什么是happens-before?一方面，程序员需要JMM提供一个强的内存模型来编写代码；另一方面，编译器和处理器希望JMM对它们的束缚越少越好，这样它们就可以最可能多的做优化来提高性能，希望的是一个弱的内存模型。 JMM考虑了这两种需求，并且找到了平衡点，对编译器和处理器来说，只要不改变程序的执行结果（单线程程序和正确同步了的多线程程序），编译器和处理器怎么优化都行。 而对于程序员，JMM提供了happens-before规则（JSR-133规范），满足了程序员的需求——简单易懂，并且提供了足够强的内存可见性保证。换言之，程序员只要遵循happens-before规则，那他写的程序就能保证在JMM中具有强的内存可见性。 JMM使用happens-before的概念来定制两个操作之间的执行顺序。这两个操作可以在一个线程以内，也可以是不同的线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证。 happens-before关系的定义如下： 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序。 happens-before关系本质上和as-if-serial语义是一回事。 as-if-serial语义保证单线程内重排序后的执行结果和程序代码本身应有的结果是一致的，happens-before关系保证正确同步的多线程程序的执行结果不被重排序改变。 总之，如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的，不管它们在不在一个线程。 天然的happens-before关系在Java中，有以下天然的happens-before关系： 1、程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 2、锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作，比如说在代码里有先对一个lock.lock()，lock.unlock()，lock.lock() 3、volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个volatile变量的读操作，volatile变量写，再是读，必须保证是先写，再读 4、传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 5、线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作，thread.start()，thread.interrupt() 6、线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 7、线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 8、对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。 举例1： 12345JAVAint a = 1; // A操作int b = 2; // B操作int sum = a + b;// C 操作System.out.println(sum); 上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。 举例1： 12345JAVAint a = 1; // A操作int b = 2; // B操作int sum = a + b;// C 操作System.out.println(sum); 根据以上介绍的happens-before规则，假如只有一个线程，那么不难得出： 1234CODE1&gt; A happens-before B 2&gt; B happens-before C 3&gt; A happens-before C 注意，真正在执行指令的时候，其实JVM有可能对操作A &amp; B进行重排序，因为无论先执行A还是B，他们都对对方是可见的，并且不影响执行结果。 如果这里发生了重排序，这在视觉上违背了happens-before原则，但是JMM是允许这样的重排序的。 所以，我们只关心happens-before规则，不用关心JVM到底是怎样执行的。只要确定操作A happens-before操作B就行了。 重排序有两类，JMM对这两类重排序有不同的策略： 会改变程序执行结果的重排序，比如 A -&gt; C，JMM要求编译器和处理器都禁止这种重排序。 不会改变程序执行结果的重排序，比如 A -&gt; B，JMM对编译器和处理器不做要求，允许这种重排序。 举例2： 123456789101112131415JAVA//伪代码volatile boolean flag = false; //线程1 prepare(); flag = false; //线程2 while(!flag)&#123; sleep(); &#125; //基于准备好的资源进行操作 execute(); 这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。 比如这个例子，如果用volatile来修饰flag变量，一定可以让prepare()指令在flag &#x3D; true之前先执行，这就禁止了指令重排。 因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。 volatilevolatile不保证原子性，只保证可见性和禁止指令重排 CPU术语介绍 123456789101112131415161718JAVAprivate static volatile SingletonDemo instance = null; private SingletonDemo() &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 执行单例构造函数&quot;); &#125; public static SingletonDemo getInstance()&#123; if(instance == null)&#123; synchronized (SingletonDemo.class)&#123; if(instance == null)&#123; instance = new SingletonDemo(); //pos_1 &#125; &#125; &#125; return instance; &#125; pos_1处的代码转换成汇编代码如下 123SHELL0x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); volatile保证可见性原理有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架 构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情。 1）将当前处理器缓存行的数据写回到系统内存。 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 为了提高处理速度，处理器不直接和主内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的 变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现MESI缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 注意：lock前缀指令是同时保证可见性和有序性（也就是禁止指令重排）的 注意：lock前缀指令相当于一个内存屏障【后文讲】 volatile禁止指令重排的原理12345678910111213141516JAVApublic class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // step 1 flag = true; // step 2 &#125; public void reader() &#123; if (flag) &#123; // step 3 System.out.println(a); // step 4 &#125; &#125;&#125; 在JSR-133之前的旧的Java内存模型中，是允许volatile变量与普通变量重排序的。那上面的案例中，可能就会被重排序成下列时序来执行： 线程A写volatile变量，step 2，设置flag为true； 线程B读同一个volatile，step 3，读取到flag为true； 线程B读普通变量，step 4，读取到 a &#x3D; 0； 线程A修改普通变量，step 1，设置 a &#x3D; 1； 可见，如果volatile变量与普通变量发生了重排序，虽然volatile变量能保证内存可见性，也可能导致普通变量读取错误。 所以在旧的内存模型中，volatile的写-读就不能与锁的释放-获取具有相同的内存语义了。为了提供一种比锁更轻量级的线程间的通信机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序。 编译器还好说，JVM是怎么还能限制处理器的重排序的呢？它是通过内存屏障来实现的。 什么是内存屏障？硬件层面，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。内存屏障有两个作用： 阻止屏障两侧的指令重排序； 强制把写缓冲区&#x2F;高速缓存中的脏数据等写回主内存，或者让缓存中相应的数据失效。 注意这里的缓存主要指的是上文说的CPU缓存，如L1，L2等 保守策略下 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的前面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。编译器选择了一个比较保守的JMM内存屏障插入策略，但它可以保证在任意处理器平台，任意的程序中都能 得到正确的volatile内存语义。 再逐个解释一下这几个屏障。注：下述Load代表读操作，Store代表写操作 LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，这个屏障会吧Store1强制刷新到内存，保证Store1的写入操作对其它处理器可见。LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的（冲刷写缓冲器，清空无效化队列）。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能 对于连续多个volatile变量读或者连续多个volatile变量写，编译器做了一定的优化来提高性能，比如： 第一个volatile读; LoadLoad屏障； 第二个volatile读； LoadStore屏障 1、下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图 图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任 意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与 后面可能有的volatile读&#x2F;写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面 是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确 实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile 读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个 写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时， 选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率 2、下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图 图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。 LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 优化举例： 123456789101112131415JAVAclass VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; // 第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; // 普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; // 第二个 volatile写 &#125; // 其他方法 &#125; &#125; 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。 上面的优化针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以X86处理器为例，图中除最后的StoreLoad屏障外，其他的屏障都会被省略。 X86处理器优化前面保守策略下的volatile读和写，在X86处理器平台可以优化成如下图所示。 X86处理器仅会对写-读操作做重排序。X86不会对读-读、读-写和写-写操作 做重排序，因此在X86处理器中会省略掉这3种操作类型对应的内存屏障。在X86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在X86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 volatile的用途 下面的代码在前面可能已经写过了，这里总结一下 从volatile的内存语义上来看，volatile可以保证内存可见性且禁止重排序。 在保证内存可见性这一点上，volatile有着与锁相同的内存语义，所以可以作为一个“轻量级”的锁来使用。但由于volatile仅仅保证对单个volatile变量的读&#x2F;写具有原子性，而锁可以保证整个临界区代码的执行具有原子性。所以在功能上，锁比volatile更强大；在性能上，volatile更有优势。 在禁止重排序这一点上，volatile也是非常有用的。比如我们熟悉的单例模式，其中有一种实现方式是“双重锁检查”，比如这样的代码： 1234567891011121314151617JAVApublic class Singleton &#123; private static Singleton instance; // 不使用volatile关键字 // 双重锁检验 public static Singleton getInstance() &#123; if (instance == null) &#123; // 第7行 synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); // 第10行 &#125; &#125; &#125; return instance; &#125;&#125; 如果这里的变量声明不使用volatile关键字，是可能会发生错误的。它可能会被重排序： 123456789101112JAVAinstance = new Singleton(); // 第10行// 可以分解为以下三个步骤1 memory=allocate();// 分配内存 相当于c的malloc2 ctorInstanc(memory) //初始化对象3 s=memory //设置s指向刚分配的地址// 上述三个步骤可能会被重排序为 1-3-2，也就是：1 memory=allocate();// 分配内存 相当于c的malloc3 s=memory //设置s指向刚分配的地址2 ctorInstanc(memory) //初始化对象 而一旦假设发生了这样的重排序，比如线程A在第10行执行了步骤1和步骤3，但是步骤2还没有执行完。这个时候另一个线程B执行到了第7行，它会判定instance不为空，然后直接返回了一个未初始化完成的instance！ 所以JSR-133对volatile做了增强后，volatile的禁止重排序功能还是非常有用的。 中断什么是中断首先一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。所以，Thread.stop, Thread.suspend, Thread.resume 都已经被废弃了。 其次在Java中没有办法立即停止一条线程，然而停止线程却显得尤为重要，如取消一个耗时操作。因此，Java提供了一种用于停止线程的机制——中断。 中断只是一种协作机制，Java没有给中断增加任何语法，中断的过程完全需要程序员自己实现。若要中断一个线程，你需要手动调用该线程的interrupt方法，该方法也仅仅是将线程对象的中断标识设成true；接着你需要自己写代码不断地检测当前线程的标识位，如果为true，表示别的线程要求这条线程中断，此时究竟该做什么需要你自己写代码实现。 每个线程对象中都有一个标识，用于表示线程是否被中断；该标识位为true表示中断，为false表示未中断；通过调用线程对象的interrupt方法将该线程的标识位设为true；可以在别的线程中调用，也可以在自己的线程中调用。 中断API 如何停止线程使用中断标志位12345678910111213141516171819202122232425262728293031323334package com.atguigu.ggq.juc.interrupt;import java.util.concurrent.TimeUnit;public class ThreeApi &#123; public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(true)&#123;// System.out.println(&quot;线程执行&quot;); if(Thread.currentThread().isInterrupted())&#123; // 线程自己实现中断 System.out.println(&quot;线程中断&quot;); break; &#125; System.out.println(&quot;线程执行&quot;);// try &#123;// TimeUnit.SECONDS.sleep(1);// &#125; catch (InterruptedException e) &#123; //Thread.currentThread().interrupt(); 错误1 不加这行, 错误2 加// e.printStackTrace();// 抛出(注意这里代码是catch 异常已抛出过了)这个异常时 标志位变为false了 // &#125; &#125; &#125;,&quot;t1&quot;); t1.start(); System.out.println(&quot;*************&quot;+t1.isInterrupted()); try &#123; TimeUnit.MILLISECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t1.interrupt(); System.out.println(&quot;*************&quot;+t1.isInterrupted()); &#125;&#125; 12345678910111213正常结果*************false线程执行线程执行线程执行线程执行线程执行线程执行*************true线程中断Process finished with exit code 0 错误结果1(如果线程被block) + sleep 1抛出异常后 线程会一直执行 错误结果2 抛出异常 但线程关闭 添加捕获异常时的中断 12345678910111213141516171819202122232425262728//部分源码public boolean isInterrupted() &#123; return isInterrupted(false); &#125;private native boolean isInterrupted(boolean ClearInterrupted);public static boolean interrupted() &#123; return currentThread().isInterrupted(true); // 返回当前 状态 并设为 false&#125;// 此方法 若线程被处于阻塞 会抛出异常public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; interrupt方法源码注释 sleep (优雅的关闭)使用 volatile Boolean12345678910111213public class Vol &#123; private static volatile boolean flag; public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(!flag) &#123; //do &#125; &#125;,&quot;t1&quot;); t1.start(); flag = true; &#125;&#125; 使用AutomaticBoolean12345678910111213public class Ato &#123; private static final AtomicBoolean flag = new AtomicBoolean(true); public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(flag.get()) &#123; //do &#125; &#125;,&quot;t1&quot;); t1.start(); flag.set(false); &#125;&#125; LockSupport是什么LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。 下面这句话，后面详细说LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程 3种让线程等待和唤醒的方法方式1：使用Object中的wait()方法让线程等待，使用Object中的notify()方法唤醒线程 创建一个对象作为锁对象 Object类中的wait、notify、notifyAll用于线程等待和唤醒的方法，都必须在synchronized内部执行（必须用到关键字synchronized）。 先wait后notify才OK 方式2：使用JUC包中Condition的await()方法让线程等待，使用signal()方法唤醒线程 Lock lock &#x3D; new ReentrantLock(); lock.lock(); lock.unlock(); 线程先要获得并持有锁，必须在锁块(synchronized或lock)中 必须要先等待后唤醒，线程才能够被唤醒 以上两种 使用不当 会有非法monitor状态异常 方式3：LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程 LockSupport类使用了一种名为Permit（许可）的概念来做到阻塞和唤醒线程的功能， 每个线程都有一个许可(permit)，permit只有两个值1和零，默认是零。可以把许可看成是一种(0,1)信号量（Semaphore），但与 Semaphore 不同的是，许可的累加上限是1。默认0 park 把什么搁置 设为0 不许可 阻塞当前线程 unpark(t) 设为1 可执行 唤醒 t 不会抛什么异常 \\ 123456789101112131415161718public class T1&#123; public static void main(String[] args) &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()); LockSupport.park(); //执行park无效 System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()+&quot;---被叫醒&quot;); &#125;,&quot;t1&quot;); t1.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; LockSupport.unpark(t1); // 先执行 System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()+&quot;---unpark over&quot;); &#125;&#125; 聊聊ThreadLocalThreadLocal提供线程局部变量。这些变量与正常的变量不同，因为每一个线程在访问ThreadLocal实例的时候（通过其get或set方法）都有自己的、独立初始化的变量副本。ThreadLocal实例通常是类中的私有静态字段，使用它的目的是希望将状态（例如，用户ID或事务ID）与线程关联起来。 实现每一个线程都有自己专属的本地变量副本(自己用自己的变量不麻烦别人，不和其他人共享，人人有份，人各一份)， 主要解决了让每个线程绑定自己的值，通过使用get()和set()方法，获取默认值或将其值更改为当前线程所存的副本的值从而避免了线程安全问题。 使用例子1 群雄逐鹿起纷争 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.concurrent.TimeUnit;class MovieTicket&#123;int number = 50;public synchronized void saleTicket() &#123;if(number &gt; 0) &#123; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;号售票员卖出第： &quot;+(number--)); &#125;else&#123; System.out.println(&quot;--------卖完了&quot;); &#125; &#125;&#125;/** * @auther zzyy * @create 2021-03-23 15:03 * 三个售票员卖完50张票务，总量完成即可，吃大锅饭，售票员每个月固定月薪*/public class ThreadLocalDemo&#123;public static void main(String[] args) &#123; MovieTicket movieTicket = new MovieTicket(); //新建三个线程.start for (int i = 1; i 3; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j 20; j++) &#123; movieTicket.saleTicket(); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 2 凭本事拿提成(人手一份天下安) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.atguigu.juc.tl;class MovieTicket&#123;int number = 50;public synchronized void saleTicket() &#123; if(number &gt; 0) &#123; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;号售票员卖出第： &quot;+(number--)); &#125;else&#123; System.out.println(&quot;--------卖完了&quot;); &#125; &#125;&#125;class House&#123; ThreadLocal threadLocal = ThreadLocal.withInitial(() -&gt; 0);public void saleHouse() &#123; Integer value = threadLocal.get(); value++; threadLocal.set(value); &#125;&#125;/** * @auther zzyy * @create 2021-03-23 15:03 * 1 三个售票员卖完50张票务，总量完成即可，吃大锅饭，售票员每个月固定月薪* * 2 分灶吃饭，各个销售自己动手，丰衣足食*/public class ThreadLocalDemo&#123;public static void main(String[] args) &#123;/*MovieTicket movieTicket = new MovieTicket(); for (int i = 1; i new Thread(() -&gt; &#123; for (int j = 0; j movieTicket.saleTicket(); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,String.valueOf(i)).start(); &#125;*/ //===========================================House house = new House();new Thread(() -&gt; &#123;try &#123; for (int i = 1; i 3; i++) &#123; house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123;house.threadLocal.remove();//如果不清理自定义的 ThreadLocal 变量，可能会影响后续业务逻辑和造成内存泄露等问题&#125; &#125;,&quot;t1&quot;).start();new Thread(() -&gt; &#123;try &#123;for (int i = 1; i &lt; 2; i++) &#123;house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123; house.threadLocal.remove(); &#125; &#125;,&quot;t2&quot;).start();new Thread(() -&gt; &#123;try &#123;for (int i = 1; i &lt; 5; i++) &#123;house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123;house.threadLocal.remove(); &#125; &#125;,&quot;t3&quot;).start(); System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;&#125; 总结 统一设置初始值，但是每个线程对这个值的修改都是各自线程互相独立的 如何才能不争抢 ​ 1 加入synchronized或者Lock控制资源的访问顺序 ​ 2 人手一份，大家各自安好，没必要抢夺 ​ 由浅入深Java多线程第一章 进程与线程的基本概念程序有时会由于I&#x2F;O操作、网络等原因阻塞，所以批处理操作效率也不高。 进程解决了批处理系统时只能存在一个程序问题(单核并发) 线程解决了进程在一段时间只能做一件事情(如果一个进程有多个子任务时，只能逐个得执行这些子任务，很影响效率。) 进程让操作系统的并发性成为了可能,线程让进程的并发成为了可能 多进程也可以实现并发,为什么我们要使用多线程 进程间通信比较复杂,通常我们需要共享资源,这些资源在线程间通信比较简单 进程是重量级 CPU通过为每个线程分配CPU时间片来实现多线程机制。CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以任务从保存到再加载的过程就是一次上下文切换。上下文切换通常是计算密集型的，意味着此操作会消耗大量的 CPU 时间，故线程也不是越多越好。如何减少系统中上下文切换次数，是提升多线程性能的一个重点课题。 区别 是否单独占有内存地址空间及其它系统资源（比如I&#x2F;O） 进程间内存隔离,数据共享复杂,但是同步简单 线程则相反 一个进程出现问题不会影响其他进程,而线程则不同 进程创建不仅需要寄存器和栈信息,还需要资源的分配回收以及页调度,线程只需要保存寄存器和栈信息 另外一个重要区别是进程是操作系统进行资源分配的基本单位,而线程是os进行调度的基本单位即cpu分配时间的单位 第二章 Java多线程入门类和接口2.1 Thread类和Runnable接口 我们在程序里面调用了start()方法后，虚拟机会先为我们创建一个线程，然后等到这个线程第一次得到时间片时再调用run()方法。 注意不可多次调用start()方法。在第一次调用start()方法后，再次调用start()方法会抛出异常。 123456Runable可函数式编程@FunctionalInterfacepublic interface Runnable&#123;public abstract void run();&#125;new Thread(() -&gt; &#123; System.out.println(&quot;Java 8 匿名内部类&quot;); &#125;).start(); Thread类构造⽅法查看 Thread 类的构造⽅法，发现其实是简单调⽤⼀个私有的 init ⽅法来实现初 始化。 init 的⽅法签名： 1234567891011121314151617// Thread类源码// ⽚段1 - init⽅法private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc,boolean inheritThreadLocals)// ⽚段2 - 构造函数调⽤init⽅法public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);&#125;// ⽚段3 - 使⽤在init⽅法⾥初始化AccessControlContext类型的私有属性this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext();// ⽚段4 - 两个对⽤于⽀持ThreadLocal的私有属性ThreadLocal.ThreadLocalMap threadLocals = null;ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; inheritThreadLocals：可继承的 ThreadLocal ，⻅⽚段4， Thread 类⾥⾯有两 个私有属性来⽀持 ThreadLocal ，我们会在后⾯的章节介绍 ThreadLocal 的概 念。 12Thread(Runnable target)Thread(Runnable target, String name) Thread类的⼏个常⽤⽅法这⾥介绍⼀下Thread类的⼏个常⽤的⽅法： currentThread()：静态⽅法，返回对当前正在执⾏的线程对象的引⽤； start()：开始执⾏线程的⽅法，java虚拟机会调⽤线程内的run()⽅法； yield()：yield在英语⾥有放弃的意思，同样，这⾥的yield()指的是当前线程愿 意让出对当前处理器的占⽤。这⾥需要注意的是，就算当前线程调⽤了yield() ⽅法，程序在调度的时候，也还有可能继续运⾏这个线程的； sleep()：静态⽅法，使当前线程睡眠⼀段时间； join()：使当前线程等待另⼀个线程执⾏完毕之后再继续执⾏，内部调⽤的是 Object类的wait⽅法实现的； Thread类与Runnable接⼝的⽐较 由于Java“单继承，多实现”的特性，Runnable接⼝使⽤起来⽐Thread更灵活。 Runnable接⼝出现更符合⾯向对象，将线程单独进⾏对象的封装 Runnable接⼝出现，降低了线程对象和线程任务的耦合性。 如果使⽤线程时不需要使⽤Thread类的诸多⽅法，显然使⽤Runnable接⼝更 为轻量。 所以，我们通常优先使⽤“实现 Runnable 接⼝”这种⽅式来⾃定义线程类。 Callable、Future与FutureTask 异步模型Callable1234@FunctionalInterfacepublic interface Callable&lt;V&gt; &#123; V call() throws Exception;&#125; 那⼀般是怎么使⽤ Callable 的呢？ Callable ⼀般是配合线程池⼯ 具 ExecutorService 来使⽤的。我们会在后续章节解释线程池的使⽤。这⾥只介 绍 ExecutorService 可以使⽤ submit ⽅法来让⼀个 Callable 接⼝执⾏。它会返回 ⼀个 Future ，我们后续的程序可以通过这个 Future 的 get ⽅法得到结果。 12345678910111213141516171819// ⾃定义Callableclass Task implements Callable&lt;Integer&gt;&#123; @Override public Integer call() throws Exception &#123; // 模拟计算需要⼀秒 Thread.sleep(1000); return 2; &#125; public static void main(String args[])&#123; // 使⽤ ExecutorService executor = Executors.newCachedThreadPool(); Task task = new Task(); Future&lt;Integer&gt; result = executor.submit(task); // 注意调⽤get⽅法会阻塞当前线程，直到得到结果。 // 所以实际编码中建议使⽤可以设置超时时间的重载get⽅法。 System.out.println(result.get()); &#125;&#125; Future接口123456789public abstract interface Future&lt;V&gt; &#123; public abstract boolean cancel(boolean paramBoolean); public abstract boolean isCancelled(); public abstract boolean isDone(); public abstract V get() throws InterruptedException, ExecutionException; public abstract V get(long paramLong, TimeUnit paramTimeUnit) throws InterruptedException, ExecutionException, TimeoutException;&#125; cancel ⽅法是试图取消⼀个线程的执⾏。 注意是试图取消，并不⼀定能取消成功。因为任务可能已完成、已取消、或者⼀些 其它因素不能取消，存在取消失败的可能。 boolean 类型的返回值是“是否取消成 功”的意思。参数 paramBoolean 表示是否采⽤中断的⽅式取消线程执⾏。 所以有时候，为了让任务有能够取消的功能，就使⽤ Callable 来代替 Runnable 。 如果为了可取消性⽽使⽤ Future 但⼜不提供可⽤的结果，则可以声明 Future 形式类型、并返回 null 作为底层任务的结果。 FutureTask类上⾯介绍了 Future 接⼝。这个接⼝有⼀个实现类叫 FutureTask 。 FutureTask 是 实现的 RunnableFuture 接⼝的，⽽ RunnableFuture 接⼝同时继承了 Runnable 接⼝ 和 Future 接⼝： 在很多⾼并发的环境下，有可能Callable和FutureTask会创建多次。FutureTask能 够在⾼并发环境下确保任务只执⾏⼀次 第三章 线程组和线程优先级Java中的优先级来说不是特别的可靠，Java程序中对线程所设置的优先级只是给 操作系统⼀个建议，操作系统不⼀定会采纳。⽽真正的调⽤顺序，是由操作系统的 线程调度算法决定的 Java提供⼀个线程调度器来监视和控制处于RUNNABLE状态的线程。线程的调度 策略采⽤抢占式，优先级⾼的线程⽐优先级低的线程会有更⼤的⼏率优先执⾏。在 优先级相同的情况下，按照“先到先得”的原则。每个Java程序都有⼀个默认的主线 程，就是通过JVM启动的第⼀个线程main线程。 所以，如果某个线程优先级⼤于线程所在线程组的最⼤优先级，那么该线程的优先 级将会失效，取⽽代之的是线程组的最⼤优先级。 Thread.currentThread().getThreadGroup().getName() todo 第四章 Java线程的状态及主要转换方法线程的生命周期os五状态 新建 就绪 运行 等待 终止 java6状态 1. NEW 还未调用start方法 2. RUNABLE表示当前线程正在运行中. 线程在JVM运行,也有可能在等待其他系统资源如IO 包含了os的就绪与运行 3. BLOCKED 阻塞状态 等待锁的释放以进入同步区 4. WAITING 等待状态 被唤醒进入RUNABLE 5. TIMEED_WAITING 限定时间的等待,自动唤醒 6. 终止状态 线程执行完毕 4.3.4线程中断第五章 Java线程间的通信第六章 Java内存模型基础知识第七章 重排序与happens-before第八章 volatile第九章 synchronized与锁synchronized为什么是重量级锁 第十章 乐观锁和悲观锁第十一章 AQS第十二章 线程池原理12.1为什么要使用线程池 创建&#x2F;销毁线程需要消耗系统资源,线程池可复用已创建的线程 控制并发数量(主要原因),并发数量过多可能会导致资源消耗过多导致服务器崩溃 可以对线程做统一管理 12.2 线程池原理线程池顶层接口是Executor,TreadPoolExecutor是实现类 12.2.1 ThreadPoolExecutor提供的构造方法一共四个构造方法 涉及到5~7个参数 都有的 5 个 核心线程数 最大线程数 非核心线程闲置超时时长 时长单位 阻塞队列,维护等待执行的Runnable任务对象 两个非必须参数 指定线程工厂 默认defaultThreadFactory 拒绝处理策略 线程数量大于最大线程数就会采用拒绝处理策略 默认AbortPolicy 丢弃任务并抛出异常 第十三章 阻塞队列第十四章 锁接口和类第十五章 并发集合容器简介第十六章 CopyOnwrite第十七章 通信工具类第十八章 Fork&#x2F;Join框架第十九章 Java8 Stream并行计算原理第二十章 计划任务","categories":[{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"JVM","slug":"7JVM","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-28T08:14:28.500Z","comments":true,"path":"2022/09/01/7JVM/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/7JVM/","excerpt":"","text":"第一章 字节码篇分析一个对象的创建字节码D:\\IDEA\\projects\\Pinduo&gt;javap -verbose -p out&#x2F;production&#x2F;Pinduo&#x2F;P1&#x2F;djp1&#x2F;Main.class 123456789101112131415//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package P1.djp1;public class Main &#123; public Main() &#123; &#125; public static void main(String[] args) &#123; new Object(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869D:\\IDEA\\projects\\Pinduo&gt;javap -verbose -p out/production/Pinduo/P1/djp1/Main.classClassfile /D:/IDEA/projects/Pinduo/out/production/Pinduo/P1/djp1/Main.class Last modified 2022年9月23日; size 418 bytes MD5 checksum 339314e49862fe3b31f3c66703efb5ca Compiled from &quot;Main.java&quot;public class P1.djp1.Main minor version: 0 major version: 52 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #3 // P1/djp1/Main super_class: #2 // java/lang/Object interfaces: 0, fields: 0, methods: 2, attributes: 1Constant pool: #1 = Methodref #2.#19 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Class #20 // java/lang/Object #3 = Class #21 // P1/djp1/Main #4 = Utf8 &lt;init&gt; #5 = Utf8 ()V #6 = Utf8 Code #7 = Utf8 LineNumberTable #8 = Utf8 LocalVariableTable #9 = Utf8 this #10 = Utf8 LP1/djp1/Main; #11 = Utf8 main #12 = Utf8 ([Ljava/lang/String;)V #13 = Utf8 args #14 = Utf8 [Ljava/lang/String; #15 = Utf8 o #16 = Utf8 Ljava/lang/Object; #17 = Utf8 SourceFile #18 = Utf8 Main.java #19 = NameAndType #4:#5 // &quot;&lt;init&gt;&quot;:()V #20 = Utf8 java/lang/Object #21 = Utf8 P1/djp1/Main&#123; public P1.djp1.Main(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 7: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this LP1/djp1/Main; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: new #2 // class java/lang/Object 3: dup 4: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: return LineNumberTable: line 11: 0 line 13: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 args [Ljava/lang/String; 8 1 1 o Ljava/lang/Object;&#125;SourceFile: &quot;Main.java&quot; 123456789101112131415161718D:\\IDEA\\projects\\Pinduo&gt;javap -c out/production/Pinduo/P1/djp1/Main.classCompiled from &quot;Main.java&quot;public class P1.djp1.Main &#123; public P1.djp1.Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: new #2 // class java/lang/Object 3: dup 4: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: return&#125; NEW ：如果找不到Class对象，则进行类加载。加载成功后，则在堆中分配内存，从Object 开始到本类路径上的所有属性值都要分配内存。分配完毕之后，进行零值初始化。在分配过程中，注意引用是占据存储空间的，它是一个变量，占用4个字节。这个指令完毕后，将指向实例对象的引用变量压入虚拟机栈顶。DUP ：在栈顶复制该引用变量，这时的栈顶有两个指向堆内实例对象的引用变量。如果 方法有参数，还需要把参数压人操作栈中。两个引用变量的目的不同，其中压至底下的引用用于赋值，或者保存到局部变量表，另一个栈顶的引用变量作为句柄调用相关方法。INVOKESPECIAL ：调用对象实例方法，通过栈顶的引用变量调用＜init&gt; 方法。 第二章 类的加载1. 概述注意：方法区只有HotSpot虚拟机有，J9，JRockit都没有 如果自己想手写一个Java虚拟机的话，主要考虑哪些结构呢？ 类加载器 执行引擎 Bootstrap -&gt; Ext -&gt; App(也叫系统) 组合关系 但是 有一个GetParent方法 URLcl -&gt; Ext -&gt; App 1.1 类加载器子系统类加载器子系统作用： ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定。 加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射） 1.2 类加载器ClassLoader角色 class file存在于本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM当中来根据这个文件实例化出n个一模一样的实例。 class file加载到JVM中，被称为DNA元数据模板，放在方法区。 在.class文件–&gt;JVM–&gt;最终成为元数据模板，此过程就要一个运输工具（类装载器Class Loader），扮演一个快递员的角色。 2. 类加载过程(生命周期)概述12345678JAVApublic class HelloLoader &#123; public static void main(String[] args) &#123; System.out.println(&quot;谢谢ClassLoader加载我....&quot;); System.out.println(&quot;你的大恩大德，我下辈子再报！&quot;); &#125;&#125; 它加载(与对象创建区分)是怎么样的呢? 执行 main() 方法（静态方法）就需要先加载main方法所在类 HelloLoader 加载成功，则进行链接、初始化等操作。完成后调用 HelloLoader 类中的静态方法 main 加载失败则抛出异常 &#96;&#96;&#96;javaclass A {static { System.out.println(“1”);}A() { System.out.println(“a”);}}class B extends A {static { System.out.println(“2”);}B() { System.out.println(“b”);}}public class Main { public static void main(String[] args) &#123; B b = new B(); A a = new A(); // 12aba &#125; } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#### 2.1. 加载阶段**加载：**1. 通过一个类的全限定名获取定义此类的二进制字节流2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构3. **在内存中生成一个代表这个类的java.lang.Class对象**，作为方法区这个类的各种数据的访问入口**加载class文件的方式：**1. 从本地系统中直接加载2. 通过网络获取，典型场景：Web Applet3. 从zip压缩包中读取，成为日后jar、war格式的基础4. 运行时计算生成，使用最多的是：动态代理技术5. 由其他文件生成，典型场景：JSP应用从专有数据库中提取.class文件，比较少见6. 从加密文件中获取，典型的防Class文件被反编译的保护措施**数组类的加载**创建数组类的情况稍微有些特殊，因为数组类本身并不是由类加载器负责创建，而是由JVM在运行时根据需要而直接创建的，但数组的元素类型仍然需要依靠类加载器去创建。创建数组类（下述简称A）的过程：1. 如果数组的元素类型是引用类型，那么就遵循定义的加载过程递归加载和创建数组A的元素类型；2. JVM使用指定的元素类型和数组维度来创建新的数组类。3. 如果数组的元素类型是引用类型，数组类的可访问性就由元素类型的可访问性决定。否则数组类的可访问性将被缺省定义为public。int[] arrString[] arrObject[] arr#### 2.2. 链接阶段链接分为三个子阶段：验证 -&gt; 准备 -&gt; 解析##### 验证(Verify)1. 目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全2. 主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。链接过程之验证阶段(Verification)当类加载到系统后，就开始链接操作，验证是链接操作的第一步。它的目的是保证加载的字节码是合法、合理并符合规范的。验证的步骤比较复杂，实际要验证的项目也很繁多，大体上Java虚拟机需要做以下检査，如图所示。整体说明：验证的内容则涵盖了类数据信息的格式验证、语义检查、字节码验证，以及符号引用验证等。- 其中格式验证会和装载阶段一起执行。验证通过之后，类加载器才会成功将类的二进制数据信息加载到方法区中。- 格式验证之外的验证操作将会在方法区中进行。**举例**使用 BinaryViewer软件查看字节码文件，其开头均为 CAFE BABE ，如果出现不合法的字节码文件，那么将会验证不通过。##### 准备(Prepare)1. 为类变量（static变量）分配内存并且设置该类变量的默认初始值，即零值2. 这里不包含用final修饰的static，因为final在编译的时候就会分配好了默认值，准备阶段会显式初始化3. 注意：这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中链接过程之准备阶段(Preparation)简言之，为类的静态变量分配内存，并将其初始化为默认值。在这个阶段，虚拟机就会为这个类分配相应的内存空间，并设置默认初始值。Java虚拟机为各类型变量默认的初始值如表所示。注意：Java并不支持boolean类型，对于boolean类型，内部实现是int,由于int的默认值是0,故对应的，boolean的默认值就是false。**举例**代码：变量a在准备阶段会赋初始值，但不是1，而是0，在初始化阶段会被赋值为 1```javaJAVApublic class HelloApp &#123; private static int a = 1;//prepare：a = 0 ---&gt; initial : a = 1 public static void main(String[] args) &#123; System.out.println(a); &#125;&#125; 解析(Resolve) 将常量池内的符号引用转换为直接引用的过程 事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的CONSTANT Class info、CONSTANT Fieldref info、CONSTANT Methodref info等 符号引用 反编译 class 文件后可以查看符号引用，下面带# 的就是符号引用 链接过程之解析阶段(Resolution) 简言之，将类、接口、字段和方法的符号引用转为直接引用。 1.具体描述: 符号引用就是一些字面量的引用，和虚拟机的内部数据结构和和内存布局无关。比较容易理解的就是在Class类文件中，通过常量池进行了大量的符号引用。但是在程序实际运行时，只有符号引用是不够的，比如当如下println()方法被调用时，系统需要明确知道该方法的位置。 举例：输出操作System.out.println()对应的字节码： invokevirtual #24 以方法为例，Java虚拟机为每个类都准备了一张方法表，将其所有的方法都列在表中，当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法。通过解析操作，符号引用就可以转变为目标方法在类中方法表中的位置，从而使得方法被成功调用。 2.小结： 所谓解析就是将符号引用转为直接引用，也就是得到类、字段、方法在内存中的指针或者偏移量。因此，可以说，如果直接引用存在，那么可以肯定系统中存在该类、方法或者字段。但只存在符号引用，不能确定系统中一定存在该结构。 不过Java虚拟机规范并没有明确要求解析阶段一定要按照顺序执行。在HotSpot VM中，加载、验证、准备和初始化会按照顺序有条不紊地执行，但链接阶段中的解析操作往往会伴随着JVM在执行完初始化之后再执行。 2.3. 初始化阶段类的初始化时机 创建类的实例 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（比如：Class.forName(“com.atguigu.Test”)） 初始化一个类的子类 Java虚拟机启动时被标明为启动类的类 JDK7开始提供的动态语言支持：java.lang.invoke.MethodHandle实例的解析结果REF_getStatic、REF putStatic、REF_invokeStatic句柄对应的类没有初始化，则初始化 除了以上七种情况，其他使用Java类的方式都被看作是对类的被动使用，都不会导致类的初始化，即不会执行初始化阶段（不会调用 clinit() 方法和 init() 方法） clinit() 初始化阶段就是执行类构造器方法&lt;clinit&gt;()的过程 此方法不需定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。也就是说，当我们代码中包含static变量的时候，就会有clinit方法 &lt;clinit&gt;()方法中的指令按语句在源文件中出现的顺序执行 &lt;clinit&gt;()不同于类的构造器。（关联：构造器是虚拟机视角下的&lt;init&gt;()） 若该类具有父类，JVM会保证子类的&lt;clinit&gt;()执行前，父类的&lt;clinit&gt;()已经执行完毕 虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁 IDEA 中安装 JClassLib Bytecode viewer 插件，可以很方便的看字节码。安装过程可以自行百度 Java编译器并不会为所有的类都产生clinit()初始化方法。哪些类在编译为字节码后，字节码文件中将不会包含()方法？ 一个类中并没有声明任何的类变量，也没有静态代码块时 一个类中声明类变量，但是没有明确使用类变量的初始化语句以及静态代码块来执行初始化操作时 一个类中包含static final修饰的基本数据类型的字段，这些类字段初始化语句采用编译时常量表达式 查看下面这个代码的字节码，可以发现有一个&lt;clinit&gt;()方法。 123456789101112131415161718192021222324JAVApublic class ClassInitTest &#123; private static int num = 1; static&#123; num = 2; number = 20; System.out.println(num); //System.out.println(number);//报错：非法的前向引用。即在定义之前使用 &#125; /** * 1、linking之prepare: number = 0 --&gt; initial: 20 --&gt; 10 * 2、这里因为静态代码块出现在声明变量语句前面，所以之前被准备阶段为0的number变量会 * 首先被初始化为20，再接着被初始化成10（这也是面试时常考的问题哦） * */ private static int number = 10; public static void main(String[] args) &#123; System.out.println(ClassInitTest.num);//2 System.out.println(ClassInitTest.number);//10 &#125;&#125; 虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁 类的初始化情况：主动使用vs被动使用Java程序对类的使用分为两种：主动使用 和 被动使用。 主动使用的说明：Class只有在必须要首次使用的时候才会被装载，Java虚拟机不会无条件地装载Class类型。Java虚拟机规定，一个类或接口在初次使用前，必须要进行初始化。这里指的“使用”，是指主动使用。 主动使用只有下列几种情况：（即：如果出现如下的情况，则会对类进行初始化操作。而初始化操作之前的加载、验证、准备已经完成。） 当创建一个类的实例时，比如使用new关键字，或者通过反射、克隆、反序列化。 当调用类的静态方法时，即当使用了字节码invokestatic指令。 当使用类、接口的静态字段时(final修饰特殊考虑)，比如，使用getstatic或者putstatic指令。 当使用java.lang.reflect包中的方法反射类的方法时。比如：Class.forName(“com.atguigu.java.Test”) todo 当初始化子类时，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，该接口要在其之前被初始化。 todo 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。（涉及解析REF_getStatic、REF_putStatic、REF_invokeStatic方法句柄对应的类） todo 被动使用的情况除了以上的情况属于主动使用，其他的情况均属于被动使用。被动使用不会引起类的初始化。 也就是说：并不是在代码中出现的类，就一定会被加载或者初始化。如果不符合主动使用的条件，类就不会初始化。 当访问一个静态字段时，只有真正声明这个字段的类才会被初始化。 当通过子类引用父类的静态变量，不会导致子类初始化 通过数组定义类引用，不会触发此类的初始化 引用常量不会触发此类或接口的初始化。因为常量在链接阶段就已经被显式赋值了。 调用ClassLoader类的loadClass()方法加载一个类，并不是对类的主动使用，不会导致类的初始化。 被动的使用，意味着不需要执行初始化环节，意味着没有()的调用。 2.4. 类的使用任何一个类型在使用之前都必须经历过完整的加载、链接和初始化3个类加载步骤。一旦一个类型成功经历过这3个步骤之后，便“万事俱备，只欠东风”，就等着开发者使用了。 开发人员可以在程序中访问和调用它的静态类成员信息（比如：静态字段、静态方法），或者使用new关键字为其创建对象实例。 2.5. 类的卸载任何一个类型在使用之前都必须经历过完整的加载、链接和初始化3个类加载步骤。一旦一个类型成功经历过这3个步骤之后，便“万事俱备，只欠东风”，就等着开发者使用了。 开发人员可以在程序中访问和调用它的静态类成员信息（比如：静态字段、静态方法），或者使用new关键字为其创建对象实例。 当Sample类被加载、链接和初始化后，它的生命周期就开始了。当代表Sample类的Class对象不再被引用，即不可触及时，Class对象就会结束生命周期，Sample类在方法区内的数据也会被卸载，从而结束Sample类的生命周期。(因为双向引用所以除非类加载器被卸载) 类的卸载 (1) 启动类加载器加载的类型在整个运行期间是不可能被卸载的(jvm和jls规范) (2) 被系统类加载器和扩展类加载器加载的类型在运行期间不太可能被卸载，因为系统类加载器实例或者扩展类的实例基本上在整个运行期间总能直接或者间接的访问的到，其达到unreachable的可能性极小。 (3) 被开发者自定义的类加载器实例加载的类型只有在很简单的上下文环境中才能被卸载，而且一般还要借助于强制调用虚拟机的垃圾收集功能才可以做到。可以预想，稍微复杂点的应用场景中(比如：很多时候用户在开发自定义类加载器实例的时候采用缓存的策略以提高系统性能)，被加载的类型在运行期间也是几乎不太可能被卸载的(至少卸载的时间是不确定的)。 综合以上三点，一个已经加载的类型被卸载的几率很小至少被卸载的时间是不确定的。同时我们可以看的出来，开发者在开发代码时候，不应该对虚拟机的类型卸载做任何假设的前提下，来实现系统中的特定功能。 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。 HotSpot虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收。 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收。也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收。这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。 2.6. 总结1. 加载:将字节码静态数据装载到JVM中成为运行时数据 2. 链接阶段 1. 验证 1. 验证一些信息 2. 准备 1. 为类的静态变量分配内存，并将其初始化为默认值,final static 在编译时就初始化默认值了 在准备阶段直接赋予真正的值 3. 解析 1. 将符号引用转化为直接引用 3. 初始化 4. clinit()初始化.class 2. init()只是new时进行调用的方法与类加载无关 3. 类的加载器面试题 什么是类加载器，类加载器有哪些?（苏宁） 简单说说你了解的类加载器（拼多多） 类加载器都有哪些？（百度） 类加载器有哪些？ （腾讯） 什么是类加载器，类加载器有哪些？（字节跳动） 1. 类的加载分类：显式加载 vs 隐式加载class文件的显式加载与隐式加载的方式是指JVM加载class文件到内存的方式。 显式加载：指的是在代码中通过调用ClassLoader加载class对象，如直接使用Class.forName(name)或this.getClass().getClassLoader().loadClass()加载class对象。 隐式加载：则是不直接在代码中调用ClassLoader的方法加载class对象，而是通过虚拟机自动加载到内存中，如在加载某个类的class文件时，该类的class文件中引用了另外一个类的对象，此时额外引用的类将通过JVM自动加载到内存中。 在日常开发以上两种方式一般会混合使用。 2. 类加载机制的必要性一般情况下，Java开发人员并不需要在程序中显式地使用类加载器，但是了解类加载器的加载机制却显得至关重要。从以下几个方面说： 避免在开发中遇到 java.lang.ClassNotFoundException异常或java.lang.NoClassDefFoundError异常时，手足无措。只有了解类加载器的加载机制才能够在出现异常的时候快速地根据错误异常日志定位问题和解决问题 需要支持类的动态加载或需要对编译后的字节码文件进行加解密操作时，就需要与类加载器打交道了。 开发人员可以在程序中编写自定义类加载器来重新定义类的加载规则，以便实现一些自定义的处理逻辑。 3. 加载的类是唯一的吗?1.何为类的唯一性？ 对于任意一个类，都需要由加载它的类加载器和这个类本身一同确认其在Java虚拟机中的唯一性。每一个类加载器，都拥有一个独立的类名称空间：比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提下才有意义。否则，即使这两个类源自同一个Class文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这两个类就必定不相等。 2.命名空间 每个类加载器都有自己的命名空间，命名空间由该加载器及所有的父加载器所加载的类组成 在同一命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类 在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类 在大型应用中，我们往往借助这一特性，来运行同一个类的不同版本。 4. 类加载的特性通常类加载机制有三个基本特征： 双亲委派模型。但不是所有类加载都遵守这个模型，有的时候，启动类加载器所加载的类型，是可能要加载用户代码的，比如JDK内部的ServiceProvider&#x2F;ServiceLoader机制，用户可以在标准API框架上，提供自己的实现，JDK也需要提供些默认的参考实现。例如，Java 中JNDI、JDBC、文件系统、Cipher等很多方面，都是利用的这种机制，这种情况就不会用双亲委派模型去加载，而是利用所谓的上下文加载器。 todo 可见性。子类加载器可以访问父加载器加载的类型，但是反过来是不允许的。不然，因为缺少必要的隔离，我们就没有办法利用类加载器去实现容器的逻辑。 todo 单一性。由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，就不会在子加载器中重复加载。但是注意，类加载器“邻居”间，同一类型仍然可以被加载多次，因为互相并不可见。 todo 5. 类加载器的分类说明JVM支持两种类型的类加载器，分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器（User-Defined ClassLoader）。 从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 子父类的关系 除了顶层的启动类加载器外，其余的类加载器都应当有自己的“父类”加载器。 不同类加载器看似是继承（Inheritance）关系，实际上是包含关系。在下层加载器中，包含着上层加载器的引用。 启动类加载器（引导类加载器，Bootstrap ClassLoader） 这个类加载使用C&#x2F;C++语言实现的，嵌套在JVM内部。 它用来加载Java的核心库（JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar或sun.boot.class.path路径下的内容）。用于提供JVM自身需要的类。 并不继承自java.lang.ClassLoader，没有父加载器。 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 扩展类加载器（Extension ClassLoader） Java语言编写，由sun.misc.Launcher$ExtClassLoader实现。 继承于ClassLoader类 父类加载器为启动类加载器 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre&#x2F;lib&#x2F;ext子目录下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。 应用程序类加载器（系统类加载器，AppClassLoader） java语言编写，由sun.misc.Launcher$AppClassLoader实现 继承于ClassLoader类 父类加载器为扩展类加载器 它负责加载环境变量classpath或系统属性 java.class.path 指定路径下的类库 应用程序中的类加载器默认是系统类加载器。 它是用户自定义类加载器的默认父加载器 通过ClassLoader的getSystemClassLoader()方法可以获取到该类加载器 用户自定义类加载器 在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的。在必要时，我们还可以自定义类加载器，来定制类的加载方式。 体现Java语言强大生命力和巨大魅力的关键因素之一便是,Java开发者可以自定义类加载器来实现类库的动态加载，加载源可以是本地的JAR包，也可以是网络上的远程资源。 通过类加载器可以实现非常绝妙的插件机制，这方面的实际应用案例举不胜举。例如，著名的OSGI组件框架，再如Eclipse的插件机制。类加载器为应用程序提供了一种动态增加新功能的机制，这种机制无须重新打包发布应用程序就能实现。 同时，自定义加载器能够实现应用隔离，例如 Tomcat，Spring等中间件和组件框架都在内部实现了自定义的加载器，并通过自定义加载器隔离不同的组件模块。这种机制比C&#x2F;C++程序要好太多，想不修改C&#x2F;C++程序就能为其新增功能，几乎是不可能的，仅仅一个兼容性便能阻挡住所有美好的设想。 所有用户自定义类加载器通常需要继承于抽象类java.lang.ClassLoader。 待复习tomcat机制 4. cl源码剖析及相关机制第三章 运行时数据区 面试题 说一说JVM的内存结构是什么样子的,每个区域放什么，各有什么特点？（快手、搜狐） JVM的内存结构，及各个结构的内容。（vivo） 详细介绍一下内存结构（墨迹天气） JVM内存模型有哪些？（龙湖地产） Java虚拟机中内存划分为哪些区域（高德地图） JVM内存模型（中国计算机研究院、亚信） JVM内存结构（花旗银行） JVM 内存分哪几个区，每个区的作用是什么?（唯品会） 详解JVM内存模型（360） JVM有那些组成，堆，栈各放了什么东西？（搜狐、万达集团） JVM的内存模型，线程独有的放在哪里？哪些是线程共享的？哪些是线程独占的？（万达集团） 讲一下为什么JVM要分为堆、方法区等？原理是什么？（小米、搜狐） JVM的内存模型，线程独有的放在哪里？哪些是线程共享的？哪些是线程独占的？（菜鸟） 简单说一下JVM内存结构（浪潮） 说一下JVM内存模型吧，有哪些区？分别干什么的？ (百度) JVM的内存结构划分是什么样子的？ (支付宝) JVM 内存分哪几个区，每个区的作用是什么? (蚂蚁金服) Java虚拟机内存模型能说说吗？ (蚂蚁金服) JVM内存分布&#x2F;内存结构？ (蚂蚁金服) 讲讲JVM分区 (携程) 讲一下JVM内存布局 (滴滴) Java的内存分区 (字节跳动) 讲讲JVM运行时数据库区 (字节跳动) JVM内存模型以及分区，需要详细到每个区放什么。 (天猫) JVM 内存分哪几个区，每个区的作用是什么? (拼多多) JVM的内存布局以及垃圾回收原理及过程讲一下 (京东) 1. 程序计数寄存器（Program Counter Register）每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。它是唯一一个在Java 虚拟机规范中没有规定任何OutOtMemoryError 情况的区域。 使用PC寄存器存储字节码指令地址有什么用呢？ （为什么使用PC寄存器记录当前线程的执行地址呢？）线程切换 因为CPU需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行。 JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 PC寄存器为什么会被设定为线程私有？ 我们都知道所谓的多线程在一个特定的时间段内只会执行其中某一个线程的方法，CPU会不停地做任务切换，这样必然导致经常中断或恢复，如何保证分毫无差呢？为了能够准确地记录各个线程正在执行的当前字节码指令地址，最好的办法自然是为每一个线程都分配一个PC寄存器，这样一来各个线程之间便可以进行独立计算，从而不会出现相互干扰的情况。 2. JVM栈概述不存在GC ; 存在OOM StackOverFlowError？OutOfMemoryError？ Java 虚拟机规范允许Java栈的大小是动态的或者是固定不变的。 如果采用固定大小的Java虚拟机栈，那每一个线程的Java虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个 StackOverflowError 异常。 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那Java虚拟机将会抛出—个 OutOfMemoryError 异常。 如何设置栈内存的大小？ -Xss size (即：-XX:ThreadStackSize) 一般默认为512k-1024k，取决于操作系统。 栈的大小直接决定了函数调用的最大可达深度。 12345678910public class Main &#123; private static int count = 1; public static void main(String[] args) &#123; System.out.println(count++); main(args); &#125;&#125;SOF OOM: 死循环创建线程 方法和栈桢之间存在怎样的关系？ 在这个线程上正在执行的每个方法都各自对应一个栈帧（Stack Frame）。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息。 栈帧内部结构0. 概述每个栈帧中存储着： 局部变量表（Local Variables） 操作数栈（Operand Stack）（或表达式栈） 动态链接(Dynamic Linking) （或指向运行时常量池的方法引用） 方法返回地址（Return Address）（或方法正常退出或者异常退出的定义） 一些附加信息 1. 局部变量表局部变量表（local variables) 局部变量表也被称之为局部变量数组或本地变量表 定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各类基本数据类型(8种)、对象引用（reference），以及returnAddress类型。 局部变量表所需的容量大小是在编译期确定下来的，并保存在方法的Code属性的maximum local variables数据项中。在方法运行期间是不会改变局部变量表的大小的。 方法嵌套调用的次数由栈的大小决定。一般来说，栈越大，方法嵌套调用次数越多。对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，以满足方法调用所需传递的信息增大的需求。进而函数调用就会占用更多的栈空间，导致其嵌套调用次数就会减少。 局部变量表中的变量只在当前方法调用中有效。在方法执行时，虚拟机通过使用局部变量表完成参数值到参数变量列表的传递过程。当方法调用结束后，随着方法栈帧的销毁，局部变量表也会随之销毁。 slot 参数值的存放总是在局部变量数组的index为0开始，到数组长度-1的索引结束。 局部变量表，最基本的存储单元是Slot（变量槽） 在局部变量表里，32位以内的类型只占用一个slot（包括returnAddress类型），64位的类型（long和double)占用两个slot。 byte 、short 、char 在存储前被转换为int，boolean 也被转换为int，0 表示false ，非0 表示true。 long 和double 则占据两个Slot。 JVM会为局部变量表中的每一个Slot都分配一个访问索引，通过这个索引即可成功访问到局部变量表中指定的局部变量值 当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个Slot上 如果需要访问局部变量表中一个64bit的局部变量值时，只需要使用前一个索引即可。(比如：访问long或double类型变量） 如果当前帧是由构造方法或者实例方法创建的，那么该对象引用this将会存放在index为0的slot处，其余的参数按照参数表顺序继续排列。 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 参数表分配完毕之后，再根据方法体内定义的变量的顺序和作用域分配。 我们知道类变量表有两次初始化的机会，第一次是在“准备阶段”，执行系统初始化，对类变量设置零值，另一次则是在“初始化”阶段，赋予程序员在代码中定义的初始值。 和类变量初始化不同的是，局部变量表不存在系统初始化的过程，这意味着一旦定义了局部变量则必须人为的初始化，否则无法使用。 public void test() {int i; System.out.println(i); } 这样的代码是错误的，没有赋值不能够使用。 局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。 2. 操作数栈操作数栈（Operand Stack） 我们说Java虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 每一个独立的栈帧中除了包含局部变量表以外，还包含一个后进先出（Last-In-First-Out）的操作数栈，也可以称之为表达式栈（Expression Stack）。 操作数栈就是JVM执行引擎的一个工作区，当一个方法刚开始执行的时候，一个新的栈帧也会随之被创建出来，这个方法的操作数栈是空的。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期就定义好了，保存在方法的Code属性中，为max_stack的值。 栈中的任何一个元素都是可以任意的Java数据类型。 32bit的类型占用一个栈单位深度 64bit的类型占用两个栈单位深度 操作数栈，在方法执行过程中，根据字节码指令，并非采用访问索引的方式来进行数据访问的，而是只能通过标准的入栈（push）和出栈（pop）操作，往栈中写入数据或提取数据来完成一次数据访问。 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈。使用它们后再把结果压入栈。比如：执行复制、交换、求和等操作 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新PC寄存器中下一条需要执行的字节码指令。 栈顶缓存技术 前面提过，基于栈式架构的虚拟机所使用的零地址指令更加紧凑，但完成一项操作的时候必然需要使用更多的入栈和出栈指令，这同时也就意味着将需要更多的指令分派（instruction dispatch）次数和内存读&#x2F;写次数。 由于操作数是存储在内存中的，因此频繁地执行内存读&#x2F;写操作必然会影响执行速度。为了解决这个问题，HotSpot JVM的设计者们提出了栈顶缓存（ToS，Top-of-Stack Cashing）技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读&#x2F;写次数，提升执行引擎的执行效率。 3. 动态链接动态链接（或指向运行时常量池的方法引用） 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接（Dynamic Linking）。比如：invokedynamic指令 在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用（Symbolic Reference）保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 public void testGetSum(){**int i &#x3D; getSum();int j &#x3D; 10;} 4. 方法返回地址 存放调用该方法的pc寄存器的值。 一个方法的结束，有两种方式： 正常执行完成 出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的pc计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。 5. 一些附加信息栈帧中还允许携带与Java虚拟机实现相关的一些附加信息。例如，对程序调试提供支持的信息。 面试扩展问题问题一：栈溢出的情况? 栈溢出:StackOverflowError; 举个简单的例子:在main方法中调用main方法,就会不断压栈执行,直到栈溢出; 栈的大小可以是固定大小的,也可以是动态变化（动态扩展）的。 如果是固定的,可以通过-Xss设置栈的大小; 如果是动态变化的,当栈大小到达了整个内存空间不足了,就是抛出OutOfMemory异常(java.lang.OutOfMemoryError) 问题二：调整栈大小,就能保证不出现溢出吗? 不能。因为调整栈大小,只会减少出现溢出的可能,栈大小不是可以无限扩大的,所以不能保证不出现溢出 问题三：分配的栈内存越大越好吗? 不是,因为增加栈大小，会造成每个线程的栈都变的很大,使得一定的栈空间下,能创建的线程数量会变小 问题四：垃圾回收是否会涉及到虚拟机栈? 不会;垃圾回收只会涉及到方法区和堆中,方法区和堆也会存在溢出的可能; 程序计数器,只记录运行下一行的地址,不存在溢出和垃圾回收; 虚拟机栈和本地方法栈,都是只涉及压栈和出栈,可能存在栈溢出,不存在垃圾回收。 问题五：方法中定义的局部变量是否线程安全? 具体问题具体分析,见分析代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344/**方法中定义的局部变量是否线程安全? 具体问题具体分析 * @author shkstart * @create 15:53 */public class LocalVariableThreadSafe &#123;//s1的声明方式是线程安全的,因为线程私有，在线程内创建的s1 ，不会被其它线程调用public static void method1() &#123;//StringBuilder:线程不安全StringBuilder s1 = new StringBuilder(); s1.append(&quot;a&quot;); s1.append(&quot;b&quot;);//...&#125;//stringBuilder的操作过程：是线程不安全的， // 因为stringBuilder是外面传进来的，有可能被多个线程调用public static void method2(StringBuilder stringBuilder) &#123; stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);//...&#125;//stringBuilder的操作：是线程不安全的；因为返回了一个stringBuilder， // stringBuilder有可能被其他线程共享public static StringBuilder method3() &#123; StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);return stringBuilder; &#125;//stringBuilder的操作：是线程安全的；因为返回了一个stringBuilder.toString()相当于new了一个String， // 所以stringBuilder没有被其他线程共享的可能public static String method4() &#123; StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);return stringBuilder.toString();/** * 结论：如果局部变量在内部产生并在内部消亡的，那就是线程安全的 */&#125;&#125; 3. 本地方法接口与本地方法栈Java使用起来非常方便，然而有些层次的任务用Java实现起来不容易，或者我们对程序的效率很在意时，问题就来了。 与Java环境外交互： 有时Java应用需要与Java外面的环境交互，这是本地方法存在的主要原因。你可以想想Java需要与一些底层系统，如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口，而且我们无需去了解Java应用之外的繁琐的细节。 与操作系统交互： JVM支持着Java语言本身和运行时库，它是Java程序赖以生存的平台，它由一个解释器（解释字节码）和一些连接到本地代码的库组成。然而不管怎样，它毕竟不是一个完整的系统，它经常依赖于一些底层系统的支持。这些底层系统常常是强大的操作系统。通过使用本地方法，我们得以用Java实现了jre的与底层系统的交互，甚至JVM的一些部分就是用C写的。还有，如果我们要使用一些Java语言本身没有提供封装的操作系统的特性时，我们也需要使用本地方法。 Sun’s Java Sun的解释器是用C实现的，这使得它能像一些普通的C一样与外部交互。jre大部分是用Java实现的，它也通过一些本地方法与外界交互。例如：类java.lang.Thread 的 setPriority()方法是用Java实现的，但是它实现调用的是该类里的本地方法setPriority0()。这个本地方法是用C实现的，并被植入JVM内部，在Windows 95的平台上，这个本地方法最终将调用Win32 SetPriority() API。这是一个本地方法的具体实现由JVM直接提供，更多的情况是本地方法由外部的动态链接库（external dynamic link library）提供，然后被JVM调用。 4. 堆概述 《Java虚拟机规范》中对Java堆的描述是：所有的对象实例以及数组都应当在运行时分配在堆上。（The heap is the run-time data area from which memory for all class instances and arrays is allocated ) 数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 我要说的是：“几乎”所有的对象实例都在这里分配内存。——从实际使用角度看的。 所有的线程共享Java堆，在这里还可以划分线程私有的缓冲区（Thread Local Allocation Buffer, TLAB)。 堆的内部结构 几乎所有的Java对象都是在Eden区被new出来的。 绝大部分的Java对象的销毁都在新生代进行了。 IBM 公司的专门研究表明，新生代中 80% 的对象都是“朝生夕死”的。 如何设置堆内存大小？如何设置新生代与老年代比例？ 下面这参数开发中一般不会调： 配置新生代与老年代在堆结构的占比。 默认**-XX:NewRatio&#x3D;2**，表示新生代占1，老年代占2，新生代占整个堆的1&#x2F;3 可以修改-XX:NewRatio&#x3D;4，表示新生代占1，老年代占4，新生代占整个堆的1&#x2F;5 可以使用选项”**-Xmn**”设置新生代最大内存大小 这个参数一般使用默认值就可以了。 如何设置Eden、幸存者区比例？ 在HotSpot中，Eden空间和另外两个Survivor空间缺省所占的比例是8:1:1 当然开发人员可以通过选项“**-XX:SurvivorRatio”调整这个空间比例。比如-XX:SurvivorRatio&#x3D;8** OOM举例​ 参数设置小结 什么是空间分配担保策略？（渣打银行） 什么是空间分配担保策略？（顺丰） 什么是空间分配担保策略？（腾讯、百度） 1234567891011121314151617/** * 测试堆空间常用的jvm参数： * -XX:+PrintFlagsInitial : 查看所有的参数的默认初始值 * -XX:+PrintFlagsFinal ：查看所有的参数的最终值（可能会存在修改，不再是初始值） * 具体查看某个参数的指令： jps：查看当前运行中的进程 * jinfo -flag SurvivorRatio 进程id * * -Xms：初始堆空间内存 （默认为物理内存的1/64） * -Xmx：最大堆空间内存（默认为物理内存的1/4） * -Xmn：设置新生代的大小。(初始值及最大值) * -XX:NewRatio：配置新生代与老年代在堆结构的占比 * -XX:SurvivorRatio：设置新生代中Eden和S0/S1空间的比例 * -XX:MaxTenuringThreshold：设置新生代垃圾的最大年龄 * -XX:+PrintGCDetails：输出详细的GC处理日志 * 打印gc简要信息：① -XX:+PrintGC ② -verbose:gc * -XX:HandlePromotionFailure：是否设置空间分配担保 */ 对象分配1.new的对象先放伊甸园区。此区有大小限制。 2.当伊甸园的空间填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收(Minor GC&#x2F;YGC)，将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区 3.然后将伊甸园中的剩余对象移动到幸存者0区。 4**.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，如果没有回收，就会放到幸存者1区**。 5.如果再次经历垃圾回收，此时会重新放回幸存者0区，接着再去幸存者1区。 6.啥时候能去养老区呢？可以设置次数。默认是15次。 可以设置参数：**-XX:MaxTenuringThreshold&#x3D;** 设置对象晋升老年代的年龄阈值。 7.在养老区，相对悠闲。当养老区内存不足时，再次触发GC：Major GC，进行养老区的内存清理。 8.若养老区执行了Major GC之后发现依然无法进行对象的保存，就会产生OOM异常 空间分配担保1、在发生Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。 如果大于，则此次Minor GC是安全的 如果小于，则虚拟机会查看**-XX:HandlePromotionFailure**设置值是否允担保失败。 如果HandlePromotionFailure&#x3D;true，那么会继续检查 老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小 。 如果大于，则尝试进行一次Minor GC，但这次Minor GC依然是有风险的； 如果小于，则进行一次Full GC。 如果HandlePromotionFailure&#x3D;false，则进行一次Full GC。 5. 方法区1. 栈、堆、方法区的关系 2. 什么是TLAB？ 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每个线程分配了一个私有缓存区域，它包含在Eden空间内。 据我所知所有OpenJDK衍生出来的JVM都提供了TLAB的设计。 3. 方法区在哪里《Java虚拟机规范》中明确说明: “尽管所有的方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾收集或者进行压缩。” 但对于HotSpotJVM而言，方法区还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开。 所以，方法区看作是一块独立于Java 堆的内存空间。 4. 方法区的理解方法区（Method Area）与Java堆一样，是各个线程共享的内存区域。方法区在JVM启动的时候被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会抛出内存溢出错误：java.lang.OutOfMemoryError: PermGen space 或者 java.lang.OutOfMemoryError: Metaspace加载大量的第三方的jar包；Tomcat部署的工程过多（30-50个）；大量动态的生成反射类关闭JVM就会释放这个区域的内存。 5. 方法区的演进到了JDK 8，终于完全废弃了永久代的概念，改用与JRockit、J9一样在本地内存中实现的元空间（Metaspace）来代替￼元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代最大的区别在于：元空间不在虚拟机设置的内存中，而是使用本地内存。永久代、元空间二者并不只是名字变了，内部结构也调整了。根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OOM异常。 6. 设置方法区内存的大小 方法区的大小不必是固定的，jvm可以根据应用的需要动态调整。 jdk7及以前： 通过-XX:PermSize来设置永久代初始分配空间。默认值是20.75M -XX:MaxPermSize来设定永久代最大可分配空间。32位机器默认是64M，64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常OutOfMemoryError:PermGen space 。 jdk8及以后： 元数据区大小可以使用参数-XX:MetaspaceSize和-XX:MaxMetaspaceSize指定,替代上述原有的两个参数。 默认值依赖于平台。windows下，-XX:MetaspaceSize是21M，-XX:MaxMetaspaceSize 的值是-1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常OutOfMemoryError: Metaspace -XX:MetaspaceSize：设置初始的元空间大小。对于一个64位的服务器端JVM来说，其默认的-XX:MetaspaceSize值为21MB。这就是初始的高水位线，一旦触及这个水位线，Full GC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活），然后这个高水位线将会重置。新的高水位线的值取决于GC后释放了多少元空间。如果释放的空间不足，那么在不超过MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到Full GC多次调用。为了避免频繁地GC ，建议将-XX:MetaspaceSize设置为一个相对较高的值。 在JDK8 及以上版本中，设定MaxPermSize 参数， JVM在启动时并不会报错，但是会提示： Java HotSpot 64Bit Server VM warning: ignoring option MaxPermSize&#x3D;2560m; support was removed in 8.0 。 第四章 对象的内存布局1. 对象的实例化1.1 有几种创建对象的方式 实操 todo new 变形1: Xxx静态方法的new 变形2: XxxBuilder&#x2F;XxxFactory的静态方法 Class的newInstance() Constructor的newInstance() clone() 反序列化 第三发库Objenesis,利用了asm字节码技术,动态生成Constructor 典型用途： 需要在不调用构造函数的情况下实例化对象是一项相当特殊的任务，但是在某些情况下这是有用的： 序列化，远程调用和持久化-对象需要被实例化并恢复到特定的状态，而不需要调用代码 代理、 AOP 库和 mock 对象-类可以被子类继承而子类不用担心父类的构造器 容器框架-对象可以以非标准的方式动态地实例化 Spring中的封装的元数据读取器等就是利用了asm,需要注意的是，SimpleMetadataReader去解析类时，使用的ASM技术。 并不是等Java类加载到JVM在解析,而是直接读取字节码文件 为什么要使用ASM技术，Spring启动的时候需要去扫描，如果指定的包路径比较宽泛，那么扫描的类是非常多的，那如果在Spring启动时就把这些类全部加载进JVM了，这样不太好，所以使用了ASM技术。 1.2 创建对象的过程1.2.1 从字节码角度看待对象创建过程 : 见字节码篇1.2.2 从执行步骤角度分析 判断对象对应的类是否加载,链接,初始化 虚拟机遇到一条new指令，首先去检查这个指令的参数能否在Metaspace的常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化。（即判断类元信息是否存在）。 如果没有，那么在双亲委派模式下，使用当前类加载器以ClassLoader+包名+类名为Key进行查找对应的.class 文件。 如果没有找到文件，则抛出ClassNotFoundException 异常。 如果找到，则进行类加载，并生成对应的Class类对象。 为对象分配内存 指针碰撞 如果内存规整，使用指针碰撞 如果内存是规整的，那么虚拟机将采用的是指针碰撞法（Bump The Pointer）来为对象分配内存。意思是所有用过的内存在一边，空闲的内存在另外一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离罢了。 如果垃圾收集器选择的是Serial、ParNew这种基于压缩算法的，虚拟机采用这种分配方式。 一般使用带有compact（整理）过程的收集器时，使用指针碰撞。 空闲列表 如果内存不规整，虚拟机需要维护一个列表，使用空闲列表分配 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式称为“空闲列表（Free List）”。 处理并发安全问题 在分配内存空间时，另外一个问题是及时保证new对象时候的线程安全性：创建对象是非常频繁的操作，虚拟机需要解决并发问题。 虚拟机采用了两种方式解决并发问题: CAS ( Compare And Swap ）失败重试、区域加锁：保证指针更新操作的原子性; TLAB 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲区，（TLAB ，Thread Local Allocation Buffer）虚拟机是否使用TLAB，可以通过-XX:+&#x2F;-UseTLAB参数来设定。 初始化分配到的空间 内存分配结束，虚拟机将分配到的内存空间都初始化为零值（不包括对象头）。这一步保证了对象的实例字段在Java代码中可以不用赋初始值就可以直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象的对象头 将对象的所属类（即类的元数据信息）、对象的HashCode和对象的GC信息、锁信息等数据存储在对象的对象头中。这个过程的具体设置方式取决于JVM实现。 执行init方法进行初始化 在Java程序的视角看来，初始化才正式开始。初始化成员变量，执行实例化代码块，调用类的构造方法，并把堆内对象的首地址赋值给引用变量。 因此一般来说（由字节码中是否跟随有invokespecial指令所决定），new指令之后会接着就是执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全创建出来。 2. 对象的内存布局 对象头 对象头：它主要包括两部分。 一个是对象自身的运行时元数据(mark word)。 哈希值(hashcode)：对象在堆空间中都有一个首地址值，栈空间的引用根据这个地址指向堆中的对象，这就是哈希值起的作用 GC分代年龄：对象首先是在Eden中创建的，在经过多次GC后，如果没有被进行回收，就会在survivor中来回移动，其对应的年龄计数器会发生变化，达到阈值后会进入养老区 锁状态标志，在同步中判断该对象是否是锁 线程持有的锁 线程偏向ID 偏向时间戳 另一个是类型指针，指向元数据区的类元数据InstanceKlass，确定该对象所属的类型 此外，如果对象是一个数组，对象头中还必须有一块用于记录数组的长度的数据。 因为正常对象元数据就知道对象的确切大小。所以数组必须得知道长度。 实例数据 作用： 它是对象真正存储的有效信息，包括程序代码中定义的各种类型的字段（包括从父类继承下来的和本身拥有的字段）。 这里需要遵循的一些规则： 相同宽度的字段总是被分配在一起 父类中定义的变量会出现在子类之前（因为父类的加载是优先于子类加载的） 如果CompactFields参数为true(默认为true)：子类的窄变量可能插入到父类变量的空隙 对齐填充 对齐填充：不是必须的，也没特别含义，仅仅起到占位符的作用 3. 对象的访问定位 句柄访问 实现：堆需要划分出一块内存来做句柄池，reference中存储对象的句柄池地址，句柄中包含对象实例与类型数据各自具体的地址信息。 好处：reference中存储稳定句柄地址，对象被移动（垃圾收集时移动对象很普遍）时只会改变句柄中实例数据指针，reference本身不需要被修改。 直接使用指针访问 实现：reference中存储的就是对象的地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 好处：速度更快，java中对象访问频繁，每次访问都节省了一次指针定位的时间开销。 HotSpot这里主要使用第2种方式：直接指针访问 JVM可以通过对象引用准确定位到Java堆区中的instanceOopDesc对象，这样既可成功访问到对象的实例信息，当需要访问目标对象的具体类型时，JVM则会通过存储在instanceOopDesc中的元数据指针定位到存储在方法区中的instanceKlass对象上。 第五章 执行引擎篇第六章 垃圾回收篇从次数上讲：频繁收集Young区较少收集Old区基本不动Perm区（或元空间） 1.垃圾回收算法1.1标记算法1.1.1引用计数法原理： 对于一个对象A，只要有任何一个对象引用了A ，则A 的引用计数器就加1，当引用失效时，引用计数器就减1。只要对象A 的引用计数器的值为0，即表示对象A不可能再被使用，可进行回收。 优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性。 缺点： 缺点1：它需要单独的字段存储计数器，这样的做法增加了存储空间的开销。 缺点2：每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了时间开销。 缺点3：引用计数器有一个严重的问题，即无法处理循环引用的情况。这是一条致命缺陷，导致在Java 的垃圾回收器中没有使用这类算法。 引用计数算法，是很多语言的资源回收选择，例如因人工智能而更加火热的Python，它更是同时支持引用计数和垃圾收集机制。 具体哪种最优是要看场景的，业界有大规模实践中仅保留引用计数机制，以提高吞吐量的尝试。 Java并没有选择引用计数，是因为其存在一个基本的难题，也就是很难处理循环引用关系。 Python如何解决循环引用？ 手动解除：很好理解，就是在合适的时机，解除引用关系。 使用弱引用weakref， weakref是Python提供的标准库，旨在解决循环引用。 1.1.2可达性分析算法原理： 其原理简单来说，就是将对象及其引用关系看作一个图，选定活动的对象作为 GC Roots，然后跟踪引用链条，如果一个对象和GC Roots之间不可达，也就是不存在引用链条，那么即可认为是可回收对象。 优点： 实现简单，执行高效 ，有效的解决循环引用的问题，防止内存泄漏。 GC root在Java 语言中， GC Roots 包括以下几类元素： 虚拟机栈中引用的对象 比如：各个线程被调用的方法中使用到的参数、局部变量等。 本地方法栈内JNI(通常说的本地方法)引用的对象 类静态属性引用的对象 比如：Java类的引用类型静态变量 方法区中常量引用的对象 比如：字符串常量池（String Table）里的引用 所有被同步锁synchronized持有的对象 Java虚拟机内部的引用。 基本数据类型对应的Class对象，一些常驻的异常对象（如：NullPointerException、OutOfMemoryError），系统类加载器。 反映java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合。比如：分代收集和局部回收（Partial GC）。 不太了解细节 如果只针对Java堆中的某一块区域进行垃圾回收（比如：典型的只针对新生代），必须考虑到内存区域是虚拟机自己的实现细节，更不是孤立封闭的，这个区域的对象完全有可能被其他区域的对象所引用，这时候就需要一并将关联的区域对象也加入GC Roots集合中去考虑，才能保证可达性分析的准确性。 小技巧： 由于Root 采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个Root 。 STW 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。 这点也是导致GC进行时必须“Stop The World”的一个重要原因。 即使是号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 1.2.清除算法标记清除标记-清除（Mark - Sweep）算法 背景： 标记 - 清除算法（ Mark-Sweep ）是一种非常基础和常见的垃圾收集算法，该算法被J.McCarthy等人在1960年提出并并应用于Lisp语言。 执行过程： 当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被称为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。 标记：Collector从引用根节点开始遍历，标记所有被引用的对象。一般是在对象的Header中记录为可达对象。 清除：Collector对堆内存从头到尾进行线性的遍历，如果发现某个对象在其Header中没有标记为可达对象，则将其回收。 （很多书、视频讲错了！说是标记的垃圾对象。这里要注意了！） 缺点： 1、效率比较低：递归与全堆对象遍历两次 2、在进行GC的时候，需要停止整个应用程序，导致用户体验差 3、这种方式清理出来的空闲内存是不连续的，产生内存碎片。 注意：何为清除? 这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时，判断垃圾的位置空间是否够，如果够，就存放。 复制算法核心思想： 将活着的内存空间分为两块，每次只使用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后清除正在使用的内存块中的所有对象，交换两个内存的角色，最后完成垃圾回收。 优点： 没有标记和清除过程，实现简单，运行高效 复制过去以后保证空间的连续性，不会出现“碎片”问题。 缺点： 此算法的缺点也是很明显的，就是需要两倍的内存空间。 对于G1这种分拆成为大量region的GC，复制而不是移动，意味着GC需要维护region之间对象引用关系，不管是内存占用或者时间开销也不小。 特别的： 如果系统中的存活对象很多，复制算法不会很理想。因为复制算法需要复制的存活对象数量并不会太大,或者说非常低才行。 应用场景： 在新生代，对常规应用的垃圾回收，一次通常可以回收70%-99%的内存空间。回收性价比很高。所以现在的商业虚拟机都是用这种收集算法回收新生代。 比如：IBM 公司的专门研究表明，新生代中 80% 的对象都是“朝生夕死”的。 标记压缩标记-压缩（或标记-整理、Mark - Compact）算法 背景： 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法。 标记－清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以JVM 的设计者需要在此基础之上进行改进。标记 - 压缩（Mark - Compact）算法由此诞生。 执行过程： 第一阶段和标记-清除算法一样，从根节点开始标记所有被引用对象 第二阶段将所有的存活对象压缩到内存的一端，按顺序排放。 之后， 清理边界外所有的空间。 标记-压缩算法的最终效果等同于标记-清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为标记-清除-压缩(Mark-Sweep-Compact)算法。 二者的本质差异在于标记-清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策。 可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。 指针碰撞（Bump the Pointer） 如果内存空间以规整和有序的方式分布，即已用和未用的内存都各自一边，彼此之间维系着一个记录下一次分配起始点的标记指针，当为新对象分配内存时，只需要通过修改指针的偏移量将新对象分配在第一个空闲内存位置上，这种分配方式就叫做指针碰撞（Bump the Pointer）。 优点：（此算法消除了“标记-清除”和“复制”两个算法的弊端。） 消除了标记&#x2F;清除算法当中，内存区域分散的缺点，我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可。 消除了复制算法当中，内存减半的高额代价。 缺点： 从效率上来说，标记-压缩算法要低于复制算法。 效率不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。 对于老年代每次都有大量对象存活的区域来说，极为负重。 移动对象的同时，如果对象被其他对象引用，则还需要调整引用的地址。 移动过程中，需要全程暂停用户应用程序。即：STW 分代收集算法 三种算法的对比： 效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存。 而为了尽量兼顾上面提到的三个指标，标记-整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记的阶段，比标记-清除多了一个整理内存的阶段。 分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点使用不同的回收算法，以提高垃圾回收的效率。 在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的Session对象、线程、Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如：String对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。 目前几乎所有的GC都是采用分代收集（Generational Collecting）算法执行垃圾回收的。 在HotSpot中，基于分代的概念，GC所使用的内存回收算法必须结合年轻代和老年代各自的特点。 年轻代(Young Gen) 年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。 这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过hotspot中的两个survivor的设计得到缓解。 老年代(Tenured Gen) 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。 这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记-清除或者是标记-清除与标记-整理的混合实现。 Mark阶段的开销与存活对象的数量成正比。 Sweep阶段的开销与所管理区域的大小成正相关。 Compact阶段的开销与存活对象的数据成正比。 以HotSpot中的CMS回收器为例，CMS是基于Mark-Sweep实现的，对于对象的回收效率很高。而对于碎片问题，CMS采用基于Mark-Compact算法的Serial Old回收器作为补偿措施：当内存回收不佳（碎片导致的Concurrent Mode Failure时），将采用Serial Old执行Full GC以达到对老年代内存的整理。 分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代。 增量收集算法上述现有的算法，在垃圾回收过程中，应用软件将处于一种Stop the World 的状态。在Stop the World 状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收的完成。如果垃圾回收时间过长，应用程序会被挂起很久，将严重影响用户体验或者系统的稳定性。为了解决这个问题，即对实时垃圾收集算法的研究直接导致了增量收集（Incremental Collecting）算法的诞生。 基本思想 如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程和应用程序线程交替执行。每次，垃圾收集线程只收集一小片区域的内存空间，接着切换到应用程序线程。依次反复，直到垃圾收集完成。 总的来说，增量收集算法的基础仍是传统的标记-清除和复制算法。增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段的方式完成标记、清理或复制工作。 缺点： 使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 分区算法分区算法：—G1 GC使用的算法 分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同小区间。 每一个小区间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 一般来说，在相同条件下，堆空间越大，一次GC时所需要的时间就越长，有关GC产生的停顿也越长。为了更好地控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理地回收若干个小区间，而不是整个堆空间，从而减少一次GC所产生的停顿。 2.相关概念System.gc()System.gc()和Runtime.getRunTime().gc()会做什么事情？ (字节跳动) 在默认情况下，通过System.gc()或者Runtime.getRuntime().gc()的调用，会显式触发Full GC，同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。 然而System.gc()调用附带一个免责声明，无法保证对垃圾收集器的调用。 JVM实现者可以通过System.gc()调用来决定JVM的GC行为。而一般情况下，垃圾回收应该是自动进行的，无须手动触发，否则就太过于麻烦了。在一些特殊情况下，如我们正在编写一个性能基准，我们可以在运行之间调用System.gc()。 finalize()方法详解finalize()方法详解，前言，finalize()是Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 finalize的作用 (1)finalize()与C++中的析构函数不是对应的。C++中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性 (2)不建议用finalize方法完成“非内存资源”的清理工作，但建议用于：① **清理本地对象(通过JNI创建的对象)**；② 作为确保某些非内存资源(如Socket、文件等)释放的一个补充：在finalize方法中显式调用其他资源释放方法。 内存泄漏与内存溢出内存溢出 内存溢出相对于内存泄漏来说，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。 由于GC一直在发展，所有一般情况下，除非应用程序占用的内存增长速度非常快，造成垃圾回收已经跟不上内存消耗的速度，否则不太容易出现OOM的情况。 大多数情况下，GC会进行各种年龄段的垃圾回收，实在不行了就放大招，来一次独占式的Full GC操作，这时候会回收大量的内存，供应用程序继续使用。 javadoc中对OutOfMemoryError的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存。 OOM之前必回调用GC? 这里面隐含着一层意思是，在抛出OutOfMemoryError之前，通常垃圾收集器会被触发，尽其所能去清理出空间。 例如：在引用机制分析中，涉及到JVM会去尝试回收软引用指向的对象等。 在java.nio.BIts.reserveMemory()方法中，我们能清楚的看到，System.gc()会被调用，以清理空间。 当然，也不是在任何情况下垃圾收集器都会被触发的 比如，我们去分配一个超大对象，类似一个超大数组超过堆的最大值，JVM可以判断出垃圾收集并不能解决这个问题，所以直接抛出OutOfMemoryError。 内存泄漏 何为内存泄漏（memory leak） 可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。那么对于这种情况下，由于代码的实现不同就会出现很多种内存泄漏问题（让JVM误以为此对象还在引用中，无法回收，造成内存泄漏）。 是否还被使用？ 是 是否还被需要？ 否 内存泄漏（memory leak）的理解 严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。 但实际情况很多时候一些不太好的实践（或疏忽）会导致对象的生命周期变得很长甚至导致OOM，也可以叫做宽泛意义上的“内存泄漏”。 对象 X 引用对象 Y，X 的生命周期比 Y 的生命周期长； 那么当Y生命周期结束的时候，X依然引用着Y，这时候，垃圾回收期是不会回收对象Y的； 如果对象X还引用着生命周期比较短的A、B、C，对象A又引用着对象 a、b、c，这样就可能造成大量无用的对象不能被回收，进而占据了内存资源，造成内存泄漏，直到内存溢出。 内存泄漏与内存溢出的关系： 内存泄漏（memory leak ） 申请了内存用完了不释放，比如一共有 1024M 的内存，分配了 512M 的内存一直不回收，那么可以用的内存只有 512M 了，仿佛泄露掉了一部分； 通俗一点讲的话，内存泄漏就是【占着茅坑不拉shi】。 内存溢出（out of memory） 申请内存时，没有足够的内存可以使用； 通俗一点儿讲，一个厕所就三个坑，有两个站着茅坑不走的（内存泄漏），剩下最后一个坑，厕所表示接待压力很大，这时候一下子来了两个人，坑位（内存）就不够了，内存泄漏变成内存溢出了。 可见，内存泄漏和内存溢出的关系：内存泄漏的增多，最终会导致内存溢出。 泄漏的分类 经常发生：发生内存泄露的代码会被多次执行，每次执行，泄露一块内存； 偶然发生：在某些特定情况下才会发生； 一次性：发生内存泄露的方法只会执行一次； 隐式泄漏：一直占着内存不释放，直到执行结束；严格的说这个不算内存泄漏，因为最终释放掉了，但是如果执行时间特别长，也可能会导致内存耗尽。 Java中内存泄漏的8种情况 静态集合类，如HashMap、LinkedList等等。如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。 12345678public class MemoryLeak &#123;static List *list* = new ArrayList();public void oomTests() &#123; Object obj = new Object();//局部变量*list*.add(obj); &#125;&#125; 单例模式 单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和 JVM 的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。 内部类持有外部类 内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。 这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class HandlerDemoActivity extends Activity implements OnClickListener &#123; private static final int MESSAGE_INCRESE = 0; private static final int MESSAGE_DECRESE = 1; private TextView tv_demo_number; private Button btn_demo_increase; private Button btn_demo_decrease; private Button btn_demo_pause; private Handler handler = new Handler()&#123; //回调方法 public void handleMessage(android.os.Message msg) &#123; String strNum = tv_demo_number.getText().toString(); //转换为整型数据,获取当前显示的数值 int num = Integer.parseInt(strNum); switch(msg.what)&#123; case MESSAGE_INCRESE: num++; tv_demo_number.setText(num + &quot;&quot;); if(num == 20)&#123; Toast.makeText(HandlerDemoActivity.this, &quot;已达到最大值&quot;, 0).show(); btn_demo_pause.setEnabled(false); return; &#125; //发送延迟的+1的消息 sendEmptyMessageDelayed(MESSAGE_INCRESE, 300);//指的是延迟处理，而不是延迟发送 break; case MESSAGE_DECRESE: num--; tv_demo_number.setText(num + &quot;&quot;); if(num == 0)&#123; Toast.makeText(HandlerDemoActivity.this, &quot;已达到最小值&quot;, 0).show(); btn_demo_pause.setEnabled(false); return; &#125; //发送延迟的-1的消息 sendEmptyMessageDelayed(MESSAGE_DECRESE, 300);//指的是延迟处理，而不是延迟发送 break; &#125; &#125; &#125;; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_handler_demo); init(); &#125; private void init() &#123; tv_demo_number = (TextView) findViewById(R.id.tv_demo_number); btn_demo_increase = (Button) findViewById(R.id.btn_demo_increase); btn_demo_decrease = (Button) findViewById(R.id.btn_demo_decrease); btn_demo_pause = (Button) findViewById(R.id.btn_demo_pause); btn_demo_increase.setOnClickListener(this); btn_demo_decrease.setOnClickListener(this); btn_demo_pause.setOnClickListener(this); &#125; @Override public void onClick(View v) &#123; .... &#125;&#125; 各种连接，如数据库连接、网络连接和IO连接等 变量不合理的作用域 改变哈希值 缓存泄漏 监听器和回调 STW Stop-the-World ，简称STW，指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点像卡死的感觉，这个停顿称为STW。 可达性分析算法中枚举根节点（GC Roots）会导致所有Java执行线程停顿。 分析工作必须在一个能确保一致性的快照中进行 一致性指整个分析期间整个执行系统看起来像被冻结在某个时间点上 如果出现分析过程中对象引用关系还在不断变化，则分析结果的准确性无法保证 被STW中断的应用程序线程会在完成GC之后恢复，频繁中断会让用户感觉像是网速不快造成电影卡带一样，所以我们需要减少STW的发生。 STW事件和采用哪款GC无关，所有的GC都有这个事件。 哪怕是G1也不能完全避免Stop-the-world 情况发生，只能说垃圾回收器越来越优秀，回收效率越来越高，尽可能地缩短了暂停时间。 STW是JVM在后台自动发起和自动完成的。在用户不可见的情况下，把用户正常的工作线程全部停掉。 开发中不要用System.gc();会导致Stop-the-world的发生。 垃圾回收的并行与并发并发(Concurrent) 在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理器上运行。 并发不是真正意义上的“同时进行”，只是CPU把一个时间段划分成几个时间片段(时间区间)，然后在这几个时间区间之间来回切换，由于CPU处理的速度非常快，只要时间间隔处理得当，即可让用户感觉是多个应用程序同时在进行。 并行(Parallel) 当系统有一个以上CPU时，当一个CPU执行一个进程时，另一个CPU可以执行另一个进程，两个进程互不抢占CPU资源，可以同时进行，我们称之为并行(Parallel)。 其实决定并行的因素不是CPU的数量，而是CPU的核心数量，比如一个CPU多个核也可以并行。 适合科学计算，后台处理等弱交互场景 二者对比： 并发，指的是多个事情，在同一时间段内同时发生了。 并行，指的是多个事情，在同一时间点上同时发生了。 并发的多个任务之间是互相抢占资源的。 并行的多个任务之间是不互相抢占资源的。 只有在多CPU或者一个CPU多核的情况中，才会发生并行。 否则，看似同时发生的事情，其实都是并发执行的。 并发和并行，在谈论垃圾收集器的上下文语境中，它们可以解释如下： 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。 如ParNew、Parallel Scavenge、Parallel Old； 串行（Serial） 相较于并行的概念，单线程执行。 如果内存不够，则程序暂停，启动JVM垃圾回收器进行垃圾回收。回收完，再启动程序的线程。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），垃圾回收线程在执行时不会停顿用户程序的运行。 用户程序在继续运行，而垃圾收集程序线程运行于另一个CPU上； 如：CMS、G1 安全点与安全区域安全点(Safepoint) 程序执行时并非在所有地方都能停顿下来开始 GC，只有在特定的位置才能停顿下来开始GC，这些位置称为“安全点（Safepoint）”。 Safe Point的选择很重要，如果太少可能导致GC等待的时间太长，如果太频繁可能导致运行时的性能问题。大部分指令的执行时间都非常短暂，通常会根据“是否具有让程序长时间执行的特征”为标准。比如：选择一些执行时间较长的指令作为Safe Point，如方法调用、循环跳转和异常跳转等。 如何在GC发生时，检查所有线程都跑到最近的安全点停顿下来呢？ 抢先式中断：（目前没有虚拟机采用了） 首先中断所有线程。如果还有线程不在安全点，就恢复线程，让线程跑到安全点。 主动式中断： 设置一个中断标志，各个线程运行到Safe Point的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 安全区域(Safe Region) Safepoint 机制保证了程序执行时，在不太长的时间内就会遇到可进入 GC 的 Safepoint 。但是，程序“不执行”的时候呢？例如线程处于 Sleep 状态或 Blocked 状态，这时候线程无法响应 JVM 的中断请求，“走”到安全点去中断挂起，JVM 也不太可能等待线程被唤醒。对于这种情况，就需要安全区域（Safe Region）来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的。我们也可以把 Safe Region 看做是被扩展了的 Safepoint。 实际执行时： 1、当线程运行到Safe Region的代码时，首先标识已经进入了Safe Region，如果这段时间内发生GC，JVM会忽略标识为Safe Region状态的线程； 2、当线程即将离开Safe Region时，会检查JVM是否已经完成GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开Safe Region的信号为止； 5种引用强引用（Strong Reference）——不回收在Java程序中，最常见的引用类型是强引用（普通系统99%以上都是强引用），也就是我们最常见的普通对象引用，也是默认的引用类型。 当在Java语言中使用new操作符创建一个新的对象，并将其赋值给一个变量的时候，这个变量就成为指向该对象的一个强引用。 强引用的对象是可触及的，垃圾收集器就永远不会回收掉被引用的对象。 对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null，就是可以当做垃圾被收集了，当然具体回收时机还是要看垃圾收集策略。 相对的， 软引用、弱引用和虚引用的对象是软可触及、弱可触及和虚可触及的，在一定条件下，都是可以被回收的。所以，强引用是造成Java内存泄漏的主要原因之一。 强引用例子： StringBuffer str &#x3D; new StringBuffer (“Hello,尚硅谷”); 局部变量str指向StringBuffer实例所在堆空间，通过str可以操作该实例， 那么str就是StringBuffer实例的强引用 对应内存结构： 此时，如果再运行一个赋值语句： StringBuffer str1 &#x3D; str; 对应内存结构： 本例中的两个引用，都是强引用，强引用具备以下特点： 强引用可以直接访问目标对象。 强引用所指向的对象在任何时候都不会被系统回收，虚拟机宁愿抛出OOM异常，也不会回收强引用所指向对象。 强引用可能导致内存泄漏。 软引用（Soft Reference）— 内存不足即回收软引用是用来描述一些还有用，但非必需的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。 软引用通常用来实现内存敏感的缓存。比如：高速缓存就有用到软引用。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 垃圾回收器在某个时刻决定回收软可达的对象的时候，会清理软引用，并可选地把引用存放到一个引用队列（Reference Queue）。 类似弱引用，只不过Java虚拟机会尽量让软引用的存活时间长一些，迫不得已才清理。 在JDK 1.2版之后提供了java.lang.ref.SoftReference类来实现软引用。 Object obj &#x3D; new Object(); &#x2F;&#x2F;声明强引用 SoftReference sf &#x3D; new SoftReference(obj); obj &#x3D; null; &#x2F;&#x2F;销毁强引用 弱引用（Weak Reference）—发现即回收弱引用也是用来描述那些非必需对象，只被弱引用关联的对象只能生存到下一次垃圾收集发生为止。在系统GC时，只要发现弱引用，不管系统堆空间使用是否充足，都会回收掉只被弱引用关联的对象。 但是，由于垃圾回收器的线程通常优先级很低，因此, 并不一定能很快地发现持有弱引用的对象。在这种情况下，弱引用对象可以存在较长的时间。 弱引用和软引用一样，在构造弱引用时，也可以指定一个引用队列，当弱引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况。 弱引用非常适合来保存那些可有可无的缓存数据。如果这么做，当系统内存不足时，这些缓存数据会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。 在JDK 1.2版之后提供了java.lang.ref.WeakReference类来实现弱引用。 Object obj &#x3D; new Object(); &#x2F;&#x2F;声明强引用 WeakReference wr &#x3D; new WeakReference(obj); obj &#x3D; null; &#x2F;&#x2F;销毁强引用 弱引用对象与软引用对象的最大不同就在于，当GC在进行回收时，需要通过算法检查是否回收软引用对象，而对于弱引用对象，GC总是进行回收。弱引用对象更容易、更快被GC回收。 虚引用（Phantom Reference）—对象回收跟踪也称为“幽灵引用”或者“幻影引用”，是所有引用类型中最弱的一个。 一个对象是否有虚引用的存在，完全不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它和没有引用几乎是一样的，随时都可能被垃圾回收器回收。 它不能单独使用，也无法通过虚引用来获取被引用的对象。当试图通过虚引用的get()方法取得对象时，总是null。 为一个对象设置虚引用关联的唯一目的在于跟踪垃圾回收过程。比如：能在这个对象被收集器回收时收到一个系统通知。 虚引用必须和引用队列一起使用。虚引用在创建时必须提供一个引用队列作为参数。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象后，将这个虚引用加入引用队列，以通知应用程序对象的回收情况。 由于虚引用可以跟踪对象的回收时间，因此，也可以将一些资源释放操作放置在虚引用中执行和记录。 在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 Object obj &#x3D; new Object();ReferenceQueue phantomQueue &#x3D; new ReferenceQueue(); PhantomReference pf &#x3D; new PhantomReference(obj, phantomQueue); obj &#x3D; null; 终结器引用(Final reference) 不太懂 它用以实现对象的finalize()方法，也可以称为终结器引用。 无需手动编码，其内部配合引用队列使用。 在GC时，终结器引用入队。由Finalizer线程通过终结器引用找到被引用对象并调用它的finalize()方法，第二次GC时才能回收被引用对象。 3.垃圾回收器GC分类 按线程数分，可以分为串行垃圾回收器和并行垃圾回收器。 串行回收指的是在同一时间段内只允许有一个CPU用于执行垃圾回收操作，此时工作线程被暂停，直至垃圾收集工作结束。 在诸如单CPU处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以，串行回收默认被应用在客户端的Client模式下的JVM中 在并发能力比较强的CPU上，并行回收器产生的停顿时间要短于串行回收器。 和串行回收相反，并行收集可以运用多个CPU同时执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用了“Stop-the-world”机制。 按照工作模式分，可以分为并发式垃圾回收器和独占式垃圾回收器。 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间。 独占式垃圾回收器(Stop the world)一旦运行，就停止应用程序中的所有用户线程，直到垃圾回收过程完全结束。 按碎片处理方式分，可分为压缩式垃圾回收器和非压缩式垃圾回收器。 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。 再分配对象空间使用：指针碰撞 非压缩式的垃圾回收器不进行这步操作。 再分配对象空间使用：空闲列表 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器。 GC评估指标评估GC的性能指标：吞吐量(throughput) 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 &#x3D; 运行用户代码时间 &#x2F;（运行用户代码时间 + 垃圾收集时间）。 比如：虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 这种情况下，应用程序能容忍较高的暂停时间。因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的。 吞吐量优先，意味着在单位时间内，STW的时间最短：0.2 + 0.2 &#x3D; 0.4 评估GC的性能指标：暂停时间(pause time) “暂停时间”是指一个时间段内应用程序线程暂停，让GC线程执行的状态 例如，GC期间100毫秒的暂停时间意味着在这100毫秒期间内没有应用程序线程是活动的。 暂停时间优先，意味着尽可能让单次STW的时间最短：0.1 + 0.1 + 0.1 + 0.1 + 0.1 &#x3D; 0.5 评估GC的性能指标：吞吐量vs暂停时间 高吞吐量较好因为这会让应用程序的最终用户感觉只有应用程序线程在做“生产性”工作。直觉上，吞吐量越高程序运行越快。 低暂停时间（低延迟）较好因为从最终用户的角度来看不管是GC还是其他原因导致一个应用被挂起始终是不好的。这取决于应用程序的类型，有时候甚至短暂的200毫秒暂停都可能打断终端用户体验。因此，具有低的较大暂停时间是非常重要的，特别是对于一个交互式应用程序。 不幸的是”高吞吐量”和”低暂停时间”是一对相互竞争的目标（矛盾）。 因为如果选择以吞吐量优先，那么必然需要降低内存回收的执行频率，但是这样会导致GC需要更长的暂停时间来执行内存回收。 相反的，如果选择以低延迟优先为原则，那么为了降低每次执行内存回收时的暂停时间，也只能频繁地执行内存回收，但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。 在设计（或使用）GC算法时，我们必须确定我们的目标：一个GC算法只可能针对两个目标之一（即只专注于较大吞吐量或最小暂停时间），或尝试找到一个二者的折衷。 现在JVM调优标准：在最大吞吐量优先的情况下，降低停顿时间。 垃圾回收器都有哪些？GC发展史有了虚拟机，就一定需要收集垃圾的机制，这就是Garbage Collection，对应的产品我们称为Garbage Collector。 1999年随JDK1.3.1一起来的是串行方式的Serial GC ，它是第一款GC。ParNew垃圾收集器是Serial收集器的多线程版本 2002年2月26日，Parallel GC 和Concurrent Mark Sweep GC跟随JDK1.4.2一起发布 Parallel GC在JDK6之后成为HotSpot默认GC。 2012年，在JDK1.7u4版本中，G1可用。-XX:+UseG1GC 2017年，JDK9中G1变成默认的垃圾收集器，以替代CMS。 2018年3月，JDK 10中G1垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 2018年9月，JDK11发布。引入Epsilon 垃圾回收器，又被称为”No-Op（无操作）”回收器。同时，引入ZGC：可伸缩的低延迟垃圾回收器(Experimental)。 2019年3月，JDK12发布。增强G1，自动返回未用堆内存给操作系统。同时，引入Shenandoah GC：低停顿时间的GC(Experimental)。 2019年9月，JDK13发布。增强ZGC，自动返回未用堆内存给操作系统。 2020年3月，JDK14发布。删除CMS垃圾回收器。扩展ZGC在macOS和Windows上的应用 查看默认 垃圾回收期 -XX:+PrintCommandLineFlags：查看命令行相关参数（包含使用的垃圾收集器） 使用命令行指令：jinfo –flag 相关垃圾回收器参数 进程ID Serial GC：串行回收 Serial收集器是最基本、历史最悠久的垃圾收集器了。JDK1.3之前回收新生代唯一的选择。 Serial收集器作为HotSpot中Client模式下的默认新生代垃圾收集器。 Serial 收集器采用复制算法、串行回收和”Stop-the-World”机制的方式执行内存回收。 除了年轻代之外，Serial收集器还提供用于执行老年代垃圾收集的Serial Old收集器。Serial Old 收集器同样也采用了串行回收和”Stop the World”机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old是运行在Client模式下默认的老年代的垃圾回收器 Serial Old在Server模式下主要有两个用途：① 与新生代的Parallel Scavenge配合使用 ② 作为老年代CMS收集器的后备垃圾收集方案 这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个 CPU 或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束（Stop The World）。 优势：简单而高效（与其他收集器的单线程比），对于限定单个 CPU 的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 运行在Client模式下的虚拟机是个不错的选择。 在用户的桌面应用场景中，可用内存一般不大（几十MB至一两百MB），可以在较短时间内完成垃圾收集（几十ms至一百多ms）,只要不频繁发生，使用串行回收器是可以接受的 在HotSpot虚拟机中，使用 -XX:+UseSerialGC 参数可以指定年轻代和老年代都使用串行收集器。 等价于 新生代用Serial GC，且老年代用Serial Old GC 总 结： 这种垃圾收集器大家了解，现在已经不用串行的了。而且在限定单核cpu才可以用。现在都不是单核的了。 对于交互较强的应用而言，这种垃圾收集器是不能接受的。一般在Java web应用程序中是不会采用串行垃圾收集器的。 ParNew GC：并行回收 如果说Serial GC是年轻代中的单线程垃圾收集器，那么ParNew收集器则是Serial收集器的多线程版本。 Par是Parallel的缩写，New：只能处理的是新生代 ParNew 收集器除了采用并行回收的方式执行内存回收外，两款垃圾收集器之间几乎没有任何区别。ParNew收集器在年轻代中同样也是采用复制算法、”Stop-the-World”机制。 ParNew 是很多JVM运行在Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行方式高效。 对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于ParNew收集器是基于并行回收，那么是否可以断定ParNew收集器的回收效率在任何场景下都会比Serial收集器更高效？ ParNew 收集器运行在多CPU的环境下，由于可以充分利用多CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。 但是在单个CPU的环境下，ParNew收集器不比Serial 收集器更高效。虽然Serial收集器是基于串行回收，但是由于CPU不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。 因为除Serial外，目前只有ParNew GC能与CMS收集器配合工作 在程序中，开发人员可以通过选项”-XX:+UseParNewGC”手动指定使用ParNew收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。 -XX:ParallelGCThreads 限制线程数量，默认开启和CPU数据相同的线程数。 Parallel GC：吞吐量优先 HotSpot的年轻代中除了拥有ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法、并行回收和”Stop the World”机制。 那么Parallel收集器的出现是否多此一举？ 和ParNew收集器不同，Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput），它也被称为吞吐量优先的垃圾收集器。 自适应调节策略也是Parallel Scavenge与ParNew一个重要区别。 高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序。 Parallel 收集器在JDK1.6时提供了用于执行老年代垃圾收集的Parallel Old收集器，用来代替老年代的Serial Old收集器。 Parallel Old收集器采用了标记-压缩算法，但同样也是基于并行回收和”Stop-the-World”机制。 在程序吞吐量优先的应用场景中， Parallel 收集器和Parallel Old收集器的组合，在Server模式下的内存回收性能很不错。 在Java8中，默认是此垃圾收集器。 参数配置： -XX:+UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParallelOldGC 手动指定老年代都是使用并行回收收集器。 分别适用于新生代和老年代。默认jdk8是开启的。 上面两个参数，默认开启一个，另一个也会被开启。（互相激活） -XX:ParallelGCThreads 设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。 在默认情况下，当CPU 数量小于8个， ParallelGCThreads 的值等于CPU 数量。 当CPU数量大于8个，ParallelGCThreads 的值等于3+[5*CPU_Count]&#x2F;8] 。 -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间(即STW的时间)。单位是毫秒。 为了尽可能地把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。 对于用户来讲，停顿时间越短体验越好。但是在服务器端，我们注重高并发，整体的吞吐量。所以服务器端适合Parallel，进行控制。 该参数使用需谨慎。 -XX:GCTimeRatio垃圾收集时间占总时间的比例（&#x3D; 1 &#x2F; (N + 1))。用于衡量吞吐量的大小。 取值范围（0,100）。默认值99，也就是垃圾回收时间不超过1%。 与前一个-XX:MaxGCPauseMillis参数有一定矛盾性。暂停时间越长，Radio参数就容易超过设定的比例。 -XX:+UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小、Eden和Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量（GCTimeRatio）和停顿时间（MaxGCPauseMills），让虚拟机自己完成调优工作。 CMS：低延迟 在 JDK 1.5 时期，HotSpot 推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS (Concurrent-Mark-Sweep)收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的Java应用集中在互联网站或者B&#x2F;S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS的垃圾收集算法采用标记-清除算法，并且也会”Stop-the-world” 不幸的是，CMS 作为老年代的收集器，却无法与 JDK 1.4.0 中已经存在的新生代收集器Parallel Scavenge 配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。 在G1出现之前，CMS使用还是非常广泛的。一直到今天，仍然有很多系统使用CMS GC。 初始标记（STW）：暂时时间非常短，标记与GC Roots直接关联的对象。 并发标记（最耗时）：从GC Roots开始遍历整个对象图的过程。不会停顿用户线程 重新标记：（STW）：修复并发标记环节，因为用户线程的执行，导致数据的不一致性问题 并发清理（最耗时） CMS整个过程比之前的收集器要复杂,整个过程分为4个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段和并发清除阶段。 初始标记（Initial-Mark）阶段：在这个阶段中，程序中所有的工作线程都将会因为“Stop-the-World”机制而出现短暂的暂停，这个阶段的主要任务仅仅只是标记出GC Roots能直接关联到的对象。一旦标记完成之后就会恢复之前被暂停的所有应用线程。由于直接关联对象比较小，所以这里的速度非常快。 并发标记（Concurrent-Mark）阶段：从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记（Remark）阶段：由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或者交叉运行，因此为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（比如：由不可达变为可达对象的数据），这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 并发清除（Concurrent-Sweep）阶段：此阶段清理删除掉标记阶段判断的已经死亡的对象，释放内存空间。由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的 尽管CMS收集器采用的是并发回收（非独占式），但是在其初始化标记和再次标记这两个阶段中仍然需要执行“Stop-the-World”机制暂停程序中的工作线程，不过暂停时间并不会太长，因此可以说明目前所有的垃圾收集器都做不到完全不需要“Stop-the-World”，只是尽可能地缩短暂停时间。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 另外，由于在垃圾收集阶段用户线程没有中断，所以在CMS回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS收集器的垃圾收集算法采用的是标记—清除算法，这意味着每次执行完内存回收后，由于被执行内存回收的无用对象所占用的内存空间极有可能是不连续的一些内存块，不可避免地将会产生一些内存碎片。那么CMS在为新对象分配内存空间时，将无法使用指针碰撞（Bump the Pointer）技术，而只能够选择空闲列表（Free List）执行内存分配。 有人会觉得既然Mark Sweep会造成内存碎片,那么为什么不把算法换成Mark Compact呢? 答案其实很简答，因为当并发清除的时候，用Compact整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提的它运行的资源不受影响嘛。Mark-Compact更适合“Stop the World”这种场景下使用 参数 -XX:+UseConcMarkSweepGC 手动指定使用CMS 收集器执行内存回收任务。 开启该参数后会自动将-XX:+UseParNewGC打开。即：ParNew(Young区用)+CMS(Old区用)+Serial Old的组合。 -XX:CMSlnitiatingOccupanyFraction 设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。 JDK5及以前版本的默认值为68,即当老年代的空间使用率达到68%时，会执行一次CMS 回收。JDK6及以上版本默认值为92% 如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC 的执行次数。 -XX:+UseCMSCompactAtFullCollection 用于指定在执行完Full GC后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX:CMSFullGCsBeforeCompaction设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX:ParallelCMSThreads 设置CMS的线程数量。 CMS 默认启动的线程数是（ParallelGCThreads+3)&#x2F;4，ParallelGCThreads 是年轻代并行收集器的线程数。当CPU 资源比较紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 CMS的优点： 并发收集 低延迟 CMS的弊端： 1）会产生内存碎片，导致并发清除后，用户线程可用的空间不足。在无法分配大对象的情况下，不得不提前触发Full GC。 2）CMS收集器对CPU资源非常敏感。在并发阶段，它虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。 3）CMS收集器无法处理浮动垃圾。可能出现“Concurrent Mode Failure”失败而导致另一次 Full GC 的产生。在并发标记阶段由于程序的工作线程和垃圾收集线程是同时运行或者交叉运行的，那么在并发标记阶段如果产生新的垃圾对象，CMS将无法对这些垃圾对象进行标记，最终会导致这些新产生的垃圾对象没有被及时回收，从而只能在下一次执行GC时释放这些之前未被回收的内存空间。 小结： HotSpot有这么多的垃圾回收器，那么如果有人问，Serial GC、Parallel GC、Concurrent Mark Sweep GC这三个GC有什么不同呢？ 请记住以下口令： 如果你想要最小化地使用内存和并行开销，请选Serial GC； 如果你想要最大化应用程序的吞吐量，请选Parallel GC； 如果你想要最小化GC的中断或停顿时间，请选CMS GC。 JDK9新特性：CMS被标记为Deprecate了(JEP291) 如果对JDK 9及以上版本的HotSpot虚拟机使用参数-XX：+UseConcMarkSweepGC来开启CMS收集器的话，用户会收到一个警告信息，提示CMS未来将会被废弃。 JDK14新特性：删除CMS垃圾回收器(JEP363) 移除了CMS垃圾收集器，如果在JDK14中使用-XX:+UseConcMarkSweepGC的话，JVM不会报错，只是给出一个warning信息，但是不会exit。JVM会自动回退以默认GC方式启动JVM G1 GC：区域化分代式既然我们已经有了前面几个强大的GC，为什么还要发布Garbage First（G1）GC？ 原因就在于应用程序所应对的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序正常进行，而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。G1（Garbage-First）垃圾回收器是在Java7 update 4之后引入的一个新的垃圾回收器，是当今收集器技术发展的最前沿成果之一。 与此同时，为了适应现在不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量。 官方给G1设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，所以才担当起“全功能收集器”的重任与期望。 为什么名字叫做Garbage First（G1）呢？ 因为G1是一个并行回收器，它把堆内存分割为很多不相关的区域（Region）（物理上不连续的）。使用不同的Region来表示Eden、幸存者0区，幸存者1区，老年代等。 G1 GC有计划地避免在整个Java 堆中进行全区域的垃圾收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。 由于这种方式的侧重点在于回收垃圾最大量的区间（Region），所以我们给G1一个名字：垃圾优先（Garbage First）。 G1（Garbage-First）是一款面向服务端应用的垃圾收集器，主要针对配备多核CPU及大容量内存的机器，以极高概率满足GC停顿时间的同时，还兼具高吞吐量的性能特征。 在JDK1.7版本正式启用，移除了Experimental的标识，是JDK 9以后的默认垃圾回收器，取代了CMS 回收器以及Parallel + Parallel Old组合。被Oracle官方称为“全功能的垃圾收集器”。 与此同时，CMS已经在JDK 9中被标记为废弃（deprecated）。在jdk8中还不是默认的垃圾回收器，需要使用-XX:+UseG1GC来启用。 G1（Garbage-First）是一款面向服务端应用的垃圾收集器，兼顾吞吐量和停顿时间的GC实现。 在JDK1.7版本正式启用，是JDK 9以后的默认GC选项，取代了CMS 回收器。 与其他 GC 收集器相比，G1使用了全新的分区算法，其特点如下所示： 并行与并发 并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况 分代收集 从分代上看，G1依然属于分代型垃圾回收器，它会区分年轻代和老年代，年轻代依然有Eden区和Survivor区。但从堆的结构上看，它不要求整个Eden区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域（Region）,这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，它同时兼顾年轻代和老年代。对比其他回收器，或者工作在年轻代，或者工作在老年代； c 空间整合 CMS：“标记-清除”算法、内存碎片、若干次GC后进行一次碎片整理 G1将内存划分为一个个的region。内存的回收是以region作为基本单位的。Region之间是复制算法，但整体上实际可看作是标记-压缩（Mark-Compact）算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。尤其是当Java堆非常大的时候，G1的优势更加明显。 可预测的停顿时间模型（即：软实时soft real-time） 这是 G1 相对于 CMS 的另一大优势，G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 由于分区的原因，G1可以只选取部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。 G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 相比于CMS GC，G1未必能做到CMS在最好情况下的延时停顿，但是最差情况要好很多。 缺点： 相较于CMS，G1还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（Overload）都要比CMS要高。 从经验上来说，在小内存应用上CMS的表现大概率会优于G1，而G1在大内存应用上则发挥其优势。平衡点在6-8GB之间。 参数 -XX：+UseG1GC 手动指定使用G1收集器执行内存回收任务。 -XX:G1HeapRegionSize 设置每个Region的大小。值是2的幂，范围是1MB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1&#x2F;2000。 -XX:MaxGCPauseMillis 设置期望达到的最大GC停顿时间指标(JVM会尽力实现，但不保证达到)。默认值是200ms -XX:ParallelGCThread 设置STW时GC线程数的值。最多设置为8 -XX:ConcGCThreads 设置并发标记的线程数。将n设置为并行垃圾回收线程数(ParallelGCThreads)的1&#x2F;4左右。 -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的Java堆占用率阈值。超过此值，就触发GC。默认值是45。 G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优： 第一步：开启G1垃圾收集器 第二步：设置堆的最大内存 第三步：设置最大的停顿时间 G1中提供了三种垃圾回收模式：YoungGC、Mixed GC和Full GC，在不同的条件下被触发。 适用场景 面向服务端应用，针对具有大内存、多处理器的机器。(在普通大小的堆里表现并不惊喜) 最主要的应用是需要低GC延迟，并具有大堆的应用程序提供解决方案； 如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒；（G1通过每次只清理一部分而不是全部的Region的增量式清理来保证每次GC停顿时间不会过长）。 用来替换掉JDK1.5中的CMS收集器； 在下面的情况时，使用G1可能比CMS好： ① 超过50％的Java堆被活动数据占用； ② 对象分配频率或年代提升频率变化很大； ③ GC停顿时间过长（长于0.5至1秒）。 HotSpot 垃圾收集器里，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行GC的多线程操作，而G1 GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。 web应用，java进程最大堆4G,每分钟1500个请求，45s年轻代的垃圾回收。 31小时使用率达到了45%，则开发并发标记，进行混合回收。 分区Region：化整为零 使用 G1 收集器时，它将整个Java堆划分成约2048个大小相同的独立Region块，每个Region块大小根据堆空间的实际大小而定，整体被控制在1MB到32MB之间，且为2的N次幂，即1MB,2MB,4MB,8MB,16MB,32MB。可以通过-XX:G1HeapRegionSize设定。所有的Region大小相同，且在JVM生命周期内不会被改变。 虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。通过Region的动态分配方式实现逻辑上的连续。 一个 region 有可能属于 Eden，Survivor 或者 Old&#x2F;Tenured 内存区域。但是一个region只可能属于一个角色。图中的 E 表示该region属于Eden内存区域，S表示属于Survivor内存区域，O表示属于Old内存区域。图中空白的表示未使用的内存空间。 G1 垃圾收集器还增加了一种新的内存区域，叫做 Humongous 内存区域，如图中的 H 块。主要用于存储大对象，如果超过1.5个region，就放到H。 设置H的原因： 对于堆中的大对象，默认直接会被分配到老年代，但是如果它是一个短期存在的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放大对象。如果一个H区装不下一个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC。G1的大多数行为都把H区作为老年代的一部分来看待。 Bump – the – pointer 即：指针碰撞 TLAB 垃圾回收过程G1 GC的垃圾回收过程主要包括如下三个环节： 年轻代GC （Young GC） 老年代并发标记过程 （Concurrent Marking） 混合回收（Mixed GC） （如果需要，单线程、独占式、高强度的Full GC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。） 顺时针，young gc -&gt; young gc + concurrent mark-&gt; Mixed GC顺序，进行垃圾回收。 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值（默认45%）时，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描&#x2F;回收一小部分老年代的Region就可以了。同时，这个老年代Region是和年轻代一起被回收的。 举个例子：一个Web服务器，Java进程最大堆内存为4G，每分钟响应1500个请求，每45秒钟会新分配大约2G的内存。G1会每45秒钟进行一次年轻代回收，每31个小时整个堆的使用率会达到45%，会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。 G1回收器垃圾回收过程: Remembered Set 一个对象被不同区域引用的问题 一个Region不可能是孤立的，一个Region中的对象可能被其他任意Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题（而G1更突出） 回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率； 解决方法： 无论G1还是其他分代收集器，JVM都是使用Remembered Set来避免全局扫描： 每个Region都有一个对应的Remembered Set； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的Region（其他收集器：检查老年代对象是否引用了新生代对象）； 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在Region对应的Remembered Set中； 当进行垃圾收集时，在GC根节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。 G1回收过程一：年轻代GC JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。 年轻代垃圾回收只会回收Eden区和Survivor区。 YGC时，首先G1停止应用程序的执行（Stop-The-World），G1创建回收集（Collection Set），回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和Survivor区所有的内存分段。 然后开始如下回收过程： 第一阶段，扫描根。 根是指static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口。 第二阶段，更新RSet。 处理dirty card queue(见备注)中的card，更新RSet。此阶段完成后，RSet可以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet。 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。 第四阶段，复制对象。 此阶段，对象树被遍历，Eden区内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到Old区中空的内存分段。如果Survivor空间不够，Eden空间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft，Weak，Phantom，Final，JNI Weak 等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 G1回收过程二：并发标记过程 \\1. 初始标记阶段：标记从根节点直接可达的对象。这个阶段是STW的，并且会触发一次年轻代GC。 \\2. 根区域扫描（Root Region Scanning）：G1 GC扫描Survivor区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在young GC之前完成。 3.并发标记(Concurrent Marking)：在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 \\4. 再次标记(Remark)： 由于应用程序持续进行，需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 \\5. 独占清理(cleanup,STW)：计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。 这个阶段并不会实际上去做垃圾的收集 \\6. 并发清理阶段：识别并清理完全空闲的区域。 G1回收过程三：混合回收 当越来越多的对象晋升到老年代old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存region被回收了，部分为垃圾的内存region被计算了出来。默认情况下，这些老年代的内存分段会分8次（可以通过-XX:G1MixedGCCountTarget设置）被回收。 混合回收的回收集（Collection Set）包括八分之一的老年代内存分段，Eden区内存分段，Survivor区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent，默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 G1回收可选的过程四：Full GC G1的初衷就是要避免Full GC的出现。但是如果上述方式不能正常工作，G1会停止应用程序的执行（Stop-The-World），使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到full gc，这种情况可以通过增大内存解决。 导致G1Full GC的原因可能有两个： Evacuation（回收阶段）的时候没有足够的to-space来存放晋升的对象； 并发处理过程完成之前空间耗尽。 G1回收器优化建议 年轻代大小 避免使用-Xmn或-XX:NewRatio等相关选项显式设置年轻代大小 固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和10%的垃圾回收时间 评估G1 GC的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 补充： 从Oracle官方透露出来的信息可获知，回收阶段（Evacuation）其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分Region，停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器（即ZGC）中。另外，还考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 截止JDK 1.8，一共有7款不同的垃圾收集器。每一款不同的垃圾收集器都有不同的特点，在具体使用的时候，需要根据具体的情况选用不同的垃圾收集器。 Java垃圾收集器的配置对于JVM优化来说是一个很重要的选择，选择合适的垃圾收集器可以让JVM的性能有一个很大的提升。 怎么选择垃圾收集器？ 优先调整堆的大小让JVM自适应完成。 如果内存小于100M，使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器 如果是多CPU、需要高吞吐量、允许停顿时间超过1秒，选择并行收集器 如果是多CPU、追求低停顿时间，需快速响应（比如延迟不能超过1秒，如互联网应用），使用并发收集器 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1。 最后需要明确两个观点： 没有最好的收集器，更没有万能的收集； 调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器 GC新发展 Epsilon GC （http://openjdk.java.net/jeps/318），只做内存分配，不做垃圾回收的GC，对于运行完就退出的程序非常适合。称为无操作的垃圾收集器。 Open JDK12的Shenandoah GC：低停顿时间的GC（实验性） Shenandoah，无疑是众多GC中最孤独的一个。是第一款不由Oracle公司团队领导开发的HotSpot垃圾收集器。不可避免的受到官方的排挤。比如号称OpenJDK和OracleJDK没有区别的Oracle公司仍拒绝在OracleJDK12中支持Shenandoah。 Shenandoah垃圾回收器最初由RedHat进行的一项垃圾收集器研究项目Pauseless GC的实现，旨在针对JVM上的内存回收实现低停顿的需求。在2014年贡献给OpenJDK。 Red Hat研发Shenandoah团队对外宣称，Shenandoah垃圾回收器的暂停时间与堆大小无关，这意味着无论将堆设置为200 MB还是200GB，99.9%的目标都可以把垃圾收集的停顿时间限制在十毫秒以内。不过实际使用性能将取决于实际工作堆的大小和工作负载。 Shenandoah开发团队在实际应用中的测试数据 这是RedHat在2016年发表的论文数据，测试内容是使用ES对200GB的维基百科数据进行索引。从结果看： 停顿时间比其他几款收集器确实有了质的飞跃，但也未实现最大停顿时间控制在十毫秒以内的目标。 而吞吐量方面出现了明显的下降，总运行时间是所有测试收集器里最长的。 总结： Shenandoah GC的弱项：高运行负担下的吞吐量下降。 Shenandoah GC的强项：低延迟时间。 Shenandoah GC的工作过程大致分为九个阶段，这里就不再赘述。在之前Java12新特性视频里有过介绍。 令人震惊、革命性的ZGC ZGC与Shenandoah目标高度相似，在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。 ZGC：是一款基于Region内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-压缩算法的，以低延迟为首要目标的一款垃圾收集器。 ZGC的工作过程可以分为4个阶段：并发标记-并发预备重分配-并发重分配-并发重映射等。 ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。 测试数据： 在ZGC的强项停顿时间测试上，它毫不留情的将Parallel、G1拉开了两个数量级的差距。无论平均停顿、95%停顿、99%停顿、99.9%停顿，还是最大停顿时间，ZGC都能毫不费劲控制在10毫秒以内。 虽然ZGC还在试验状态，没有完成所有特性，但此时性能已经相当亮眼，用“令人震惊、革命性”来形容，不为过。 未来将在服务端、大内存、低延迟应用的首选垃圾收集器。 jdk14新特性 JEP 364：ZGC应用在macOS上 JEP 365：ZGC应用在Windows上 JDK14之前，ZGC仅Linux才支持。 尽管许多使用ZGC的用户都使用类Linux的环境，但在Windows和macOS上，人们也需要ZGC进行开发部署和测试。许多桌面应用也可以从ZGC中受益。因此，ZGC特性被移植到了Windows和macOS上。 现在mac或Windows上也能使用ZGC了，示例如下： -XX:+UnlockExperimentalVMOptions -XX:+UseZGC AliGC是阿里巴巴JVM团队基于G1算法， 面向大堆(LargeHeap)应用场景。 指定场景下的对比： 当然，其他厂商也提供了各种独具一格的GC实现，例如比较有名的低延迟GC，Zing（https://www.infoq.com/articles/azul_gc_in_detail），有兴趣可以参考提供的链接。 各GC使用场景如何选择？GC新发展4.分析日志JVM面试Java内存区域说一下 JVM 的主要组成部分及其作用？ JVM包含两个子系统和两个组件，两个子系统为Class loader(类装载)、Execution engine(执行引擎)；两个组件为Runtime data area(运行时数据区)、Native Interface(本地接口)。 Class loader(类装载)：根据给定的全限定名类名(如：java.lang.Object)来装载class文件到运行时数据区中的方法区。 Execution engine（执行引擎）：执行字节码中的指令。 Native Interface(本地接口)：与native libraries交互，是与其它编程语言交互的接口。 Runtime data area(运行时数据区域)：这就是我们常说的JVM的内存。 作用 ：首先通过编译器把 Java 代码转换成字节码，类加载器（ClassLoader）再把字节码加载到内存中，将其放在运行时数据区（Runtime data area）的方法区内，而字节码文件只是 JVM 的一套指令集规范，并不能直接交给底层操作系统去执行，因此需要特定的解释器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。 @$说一下 JVM 运行时数据区Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存区域划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有些区域随着虚拟机进程的启动而存在，有些区域则是依赖线程的启动和结束而建立和销毁。Java 虚拟机所管理的内存被划分为如下几个区域： 不同虚拟机的运行时数据区可能略微有所不同，但都会遵从 Java 虚拟机规范， Java 虚拟机规范规定的区域分为以下 5 个部分： Java 堆（Java Heap）：Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存； 方法区（Method Area）：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。 Java 虚拟机栈（Java Virtual Machine Stacks）：用于存储局部变量表、操作数栈、动态链接、方法出口等信息； 本地方法栈（Native Method Stack）：与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的； 程序计数器（Program Counter Register）：当前线程所执行的字节码的行号指示器，字节码解释器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成； 深拷贝和浅拷贝浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址 深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存， 使用深拷贝的情况下，不会出现浅拷贝时释放同一个内存的错误。 说一下堆栈的区别？ 堆 栈 物理地址 物理地址分配是不连续的，因此性能慢些。在GC的时候需要考虑到不连续的分配，所以有各种垃圾回收算法。 栈使用的是数据结构中的栈，具有先进后出的规则，物理地址分配是连续的，因此性能快 分配内存时机 堆因为是不连续的，所以分配的内存是在运行期确认的，因此大小不固定。一般堆大小远远大于栈。 栈是连续的，所以分配的内存大小要在编译期就确认，大小是固定的。 存放的内容 堆存放的是对象的实例和数组。此区域更关注的是数据的存储 栈存放：局部变量，操作数栈，返回结果。此区域更关注的是程序方法的执行。 程序的可见度 堆对于整个应用程序都是共享的、可见的。 栈对当前线程是可见的，是线程私有。他的生命周期和线程相同。 HotSpot虚拟机对象探秘@$对象的创建说到对象的创建，首先让我们看看 Java 中提供的几种对象创建方式： Header 解释 使用new关键字 调用了构造函数 使用Class的newInstance方法 调用了构造函数 使用Constructor类的newInstance方法 调用了构造函数 使用clone方法 没有调用构造函数 使用反序列化 没有调用构造函数 下面是对象创建的主要流程： 虚拟机遇到一条new指令时，先检查常量池是否已经加载相应的类，如果没有，必须先执行相应的类加载。类加载通过后，接下来分配内存。若Java堆中内存是绝对规整的，使用“指针碰撞“方式分配内存；如果不是规整的，就从空闲列表中分配，叫做”空闲列表“方式。划分内存时还需要考虑一个问题-并发，也有两种方式：CAS同步处理，或者本地线程分配缓冲(Thread Local Allocation Buffer, TLAB)。然后将分配到的内存空间都初始化为零值，接着是做一些必要的对象设置(元信息、哈希码…)，最后执行&lt;init&gt;方法。 为对象分配内存类加载完成后，接着会在Java堆中划分一块内存分配给对象。内存分配根据Java堆是否规整，有两种方式： 指针碰撞：如果Java堆的内存是规整，即所有用过的内存放在一边，而空闲的的放在另一边。分配内存时将位于中间的指针指示器向空闲的内存移动一段与对象大小相等的距离，这样便完成分配内存工作。 空闲列表：如果Java堆的内存不是规整的，则需要由虚拟机维护一个列表来记录哪些内存是可用的，这样在分配的时候可以从列表中查询到足够大的内存分配给对象，并在分配后更新列表记录。 选择哪种分配方式是由 Java 堆是否规整来决定的，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 处理并发安全问题在虚拟机中对象的创建是一个非常频繁的行为，哪怕只是修改一个指针所指向的位置，在并发情况下也是不安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案： 对内存分配的动作进行同步处理（采用 CAS + 失败重试来保障更新操作的原子性）； 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer, TLAB）。哪个线程要分配内存，就在哪个线程的 TLAB 上分配。只有 TLAB 用完并分配新的 TLAB 时，才需要同步锁。通过-XX:+UseTLAB参数来设定虚拟机是否使用TLAB。 对象的访问定位Java程序需要通过 JVM 栈上的引用访问堆中的具体对象。对象的访问方式取决于 JVM 虚拟机的实现。目前主流的访问方式有 句柄 和 直接指针 两种方式。 指针： 一种内存地址，代表一个对象在内存中的地址。 句柄： 可以理解为指向指针的指针，维护着对象的指针。句柄不直接指向对象，而是指向对象的指针（句柄不发生变化，指向固定内存地址），再由对象的指针指向对象的真实内存地址。 句柄访问Java堆中划分出一块内存来作为句柄池，引用中存储对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息，具体构造如下图所示： 优势：引用中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而引用本身不需要修改。 直接指针如果使用直接指针访问，引用 中存储的直接就是对象地址。 优势：速度更快，节省了一次指针定位的时间开销。由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是非常可观的执行成本。HotSpot 中采用的就是这种方式。 内存溢出异常@$Java会存在内存泄漏吗？请简单描述内存泄漏是指不再被使用的对象或者变量一直存在于内存中。理论上来说，Java是有GC垃圾回收机制的，也就是说，不再被使用的对象，会被GC自动回收掉，自动从内存中清除。 但是，即使这样，Java也还是存在着内存泄漏的情况，Java导致内存泄露的原因很明确：长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。 垃圾收集器简述Java垃圾回收机制，GC是什么？垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？Java垃圾回收机制：GC 是垃圾收集的意思（Gabage Collection），在java中，程序员是不需要显示的去释放一个对象的内存的，而是由虚拟机自行执行，这就是垃圾回收机制，垃圾回收机制有效的防止了内存泄露 垃圾回收器的基本原理：在JVM中，有一个垃圾回收线程，它是低优先级的，在正常情况下是不会执行的，只有在虚拟机空闲或者当前堆内存不足时，才会触发执行，扫描那些没有被引用的对象，并将它们添加到要回收的集合中，进行回收。 垃圾回收器可以马上回收内存吗？：程序员不能实时的对某个对象或所有对象调用垃圾回收器进行垃圾回收。但可以手动执行System.gc()主动通知虚拟机进行垃圾回收，但是Java语言规范并不能保证GC一定会执行。 Java 中都有哪些引用类型？强引用、软引用、弱引用、幻象引用有什么区别？具体使用场景是什么？在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用，根据其生命周期的长短，将引用分为4类。 不同的引用类型，主要体现的是对象不同的可达性状态和对垃圾收集的影响。 强引用：最常见的普通对象引用，通过关键字new创建的对象所关联的引用就是强引用，发生 gc 的时候不会被回收。 软引用：软引用的生命周期比强引用短一些。有用但不是必须的对象，在发生内存溢出之前会被回收。应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 弱引用：弱引用的生命周期比软引用短。有用但不是必须的对象，在下一次GC时会被回收。应用场景：弱应用同样可用于内存敏感的缓存。 虚引用（幽灵引用&#x2F;幻象引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用。应用场景：虚引用的用途是在这个对象被 gc 时返回一个系统通知。 怎么判断对象是否可以被回收？在Java中，对象什么时候可以被垃圾回收垃圾收集器在做垃圾回收的时候，首先需要判定的就是哪些内存是需要被回收的，哪些对象是「存活」的，是不可以被回收的；哪些对象已经「死掉」了，需要被回收。 一般有两种方法来判断： 引用计数器法：为每个对象创建一个引用计数器，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题； 可达性分析算法：当一个对象到GC Roots不可达时，在下一个垃圾回收周期中尝试回收该对象。定义一系列的 GC ROOT 为起点。从起点开始向下开始搜索，搜索走过的路径称为引用链。当一个对象到 GC ROOT没有任何引用链相连的话，则对象可以判定是可以被回收的。 可达性分析算法详答 当不能从GC Root寻找一条路径到达该对象时，将进行第一次标记。 第一次标记后检查对象是否重写了finalize() 和是否已经被调用了finalize()方法。若没有重写finalize()方法或已经被调用，则进行回收。 在已经重写finalize()方法且未调用的情况下，将对象加入一个F-Queue 的队列中，稍后进行第二次检查 在第二次标记之前，对象如果执行finalize()方法并完成自救，对象则不会被回收。否则完成第二次标记，进行回收。值得注意的是finalize()方法并不可靠。 虚拟机默认采用的是可达性分析算法。 可以作为 GC ROOT 的对象包括： 栈中引用的对象； 静态变量、常量引用的对象； 本地方法栈 native 方法引用的对象。 JVM中的永久代中会发生垃圾回收吗垃圾回收一般不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。通过查看垃圾收集器的输出信息，就会发现永久代也是被回收的。所以正确的设置永久代大小可以有效避免Full GC。 Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区，现在大多数的类元数据分配在本地化内存中。 @$说一下 JVM 有哪些垃圾回收算法？ 标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。 复制算法：按照容量划分两个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的那块内存空间清理掉。缺点：内存使用率不高，只有原来的一半。 标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后清除掉端边界以外的内存。 分代收集算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代采用复制算法，老年代采用标记整理算法。 标记-清除算法标记无用对象，然后进行清除回收。 标记-清除算法（Mark-Sweep）是一种常见的基础垃圾收集算法，它将垃圾收集分为两个阶段： 标记阶段：标记出可以回收的对象。 清除阶段：回收被标记的对象所占用的空间。 标记-清除算法之所以是基础的，是因为后面讲到的垃圾收集算法都是在此算法的基础上进行改进的。 优点：实现简单，不需要对象进行移动。 缺点：由于标记的过程需要遍历所有的 GC ROOT，清除的过程也要遍历堆中所有的对象，标记、清除过程效率低，产生大量不连续的内存碎片，提高了垃圾回收的频率。 标记-清除算法的执行的过程如下图所示 复制算法为了解决标记-清除算法的效率不高的问题，产生了复制算法。把内存空间划为两个相等的区域，每次只使用其中一个区域。垃圾收集时，遍历当前使用的区域，把存活对象复制到另外一个区域中，最后将已使用的内存空间一次清理掉。 优点：按顺序分配内存即可，实现简单、运行高效，不用考虑内存碎片。 缺点：可用的内存大小缩小为原来的一半，对象存活率高时会频繁进行复制。 复制算法的执行过程如下图所示 标记-整理算法在新生代中可以使用复制算法，但是在老年代就不能选择复制算法了，因为老年代的对象存活率会较高，这样会有较多的复制操作，导致效率变低。标记-清除算法可以应用在老年代中，但是它效率不高，在内存回收后容易产生大量内存碎片。因此就出现了一种标记-整理算法（Mark-Compact）算法，与标记-清除算法不同的是，在标记可回收的对象后将所有存活的对象压缩到内存的一端，使他们紧凑的排列在一起，然后对端边界以外的内存进行回收。回收后，已用和未用的内存都各自一边。 优点：解决了标记-清除算法存在的内存碎片问题。 缺点：需要进行局部对象移动，一定程度上降低了效率。 标记-整理算法的执行过程如下图所示 分代收集算法当前商业虚拟机都采用分代收集的垃圾收集算法。分代收集算法，顾名思义是根据对象的存活周期将内存划分为几块。一般包括新生代、老年代，新生代采用复制算法，老年代采用标记-整理算法，注：Java8中已经移除了永久代，添加了元数据区。如图所示： 垃圾收集算法小结 @$说一下 JVM 有哪些垃圾回收器？如果说垃圾收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。下图展示了7种作用于不同分代的收集器，其中用于回收新生代的收集器包括Serial、ParNew、Parallel Scavenge，回收老年代的收集器包括Serial Old、Parallel Old、CMS，还有用于回收整个Java堆的G1收集器。不同收集器之间的连线表示它们可以搭配使用。 Serial收集器(复制算法)：新生代单线程收集器，标记和清理都是单线程，优点是简单高效； ParNew收集器 (复制算法)：新生代并行收集器，实际上是Serial收集器的多线程版本，在多核CPU环境下有着比Serial更好的表现； Parallel Scavenge收集器 (复制算法)：新生代并行收集器，追求高吞吐量，高效利用 CPU。吞吐量 &#x3D; 用户线程时间&#x2F;(用户线程时间+GC线程时间)，高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，适合后台应用等对交互相应要求不高的场景； Serial Old收集器 (标记-整理算法)：老年代单线程收集器，Serial收集器的老年代版本； Parallel Old收集器 (标记-整理算法)：老年代并行收集器，吞吐量优先，Parallel Scavenge收集器的老年代版本； CMS(Concurrent Mark Sweep)收集器(标记-清除算法)：老年代并行收集器，追求最短GC回收停顿时间，具有高并发、低停顿的特点； G1(Garbage First)收集器 (标记-整理算法)：Java堆并行收集器，G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。此外，G1收集器不同于之前的收集器的一个重要特点是：G1回收的范围是整个Java堆(包括新生代，老年代)，而前六种收集器回收的范围仅限于新生代或老年代。 新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？ 新生代回收器：Serial、ParNew、Parallel Scavenge 老年代回收器：Serial Old、Parallel Old、CMS 整堆回收器：G1 新生代垃圾回收器一般采用的是复制算法 老年代垃圾回收器一般采用的是标记-整理的算法 详细介绍一下 CMS 垃圾回收器？CMS 是英文 Concurrent Mark-Sweep 的简称，是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。 在启动 JVM 的参数加上“-XX:+UseConcMarkSweepGC”来指定使用 CMS 垃圾回收器。 CMS 使用的是标记-清除的算法实现的，所以在 gc 的时候会产生大量的内存碎片，当剩余内存不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时 CMS 会采用 Serial Old 回收器进行垃圾清除，此时的性能将会被降低。 CMS 收集器是以获取最短停顿时间为目标的收集器。相对于其他的收集器 STW 的时间更短暂，可以并行收集是它的特点，同时它基于标记-清除算法。整个 GC 过程分为4步： 初始标记：标记 GC ROOT 能关联到的对象，需要 STW； 并发标记：从 GCRoots 的直接关联对象开始遍历整个对象图的过程，不需要 STW； 重新标记：为了修正并发标记期间，因用户程序继续运作而导致标记产生改变的标记，需要 STW； 并发清除：清理删除掉标记阶段判断的已经死亡的对象，不需要 STW。 从整个过程来看，并发标记和并发清除的耗时最长，但是不需要停止用户线程。而初始标记和重新标记的耗时较短，但是需要停止用户线程。总体而言，整个过程造成的停顿时间较短，大部分时候是可以和用户线程一起工作的。 G1垃圾回收器的原理了解吗？ G1 作为 JDK9 之后的服务端默认收集器，不再区分年轻代和老年代进行垃圾回收。 把内存划分为多个 Region，每个 Region 的大小可以通过 -XX:G1HeapRegionSize 设置，大小为1~32M。 对于大对象的存储则衍生出 Humongous 的概念。超过 Region 大小一半的对象会被认为是大对象，而超过整个 Region 大小的对象被认为是超级大对象，将会被存储在连续的 N 个 Humongous Region 中。 G1 在进行回收的时候会在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间优先回收收益最大的 Region。 G1 的回收过程分为以下四个步骤： 初始标记：标记 GC ROOT 能关联到的对象，需要 STW； 并发标记：从 GCRoots 的直接关联对象开始遍历整个对象图的过程，扫描完成后还会重新处理并发标记过程中产生变动的对象； 最终标记：短暂暂停用户线程，再处理一次，需要 STW； 筛选回收：更新 Region 的统计数据，对每个 Region 的回收价值和成本排序，根据用户设置的停顿时间制定回收计划。再把Region 中存活对象复制到空的 Region，同时清理旧的 Region。需要 STW。 总的来说除了并发标记之外，其他几个过程也还是需要短暂的 STW。G1 的目标是在停顿和延迟可控的情况下尽可能提高吞吐量。 简述分代垃圾回收器是怎么工作的？根据对象的存活周期将堆内存划分：老年代和新生代，新生代默认的空间占比总空间的 1&#x2F;3，老年代的默认占比是 2&#x2F;3。 新生代使用的是复制算法，新生代里有 3 个分区：Eden、From Survivor、To Survivor，它们的默认占比是 8:1:1，它的执行流程如下： 把 Eden + From Survivor 存活的对象放入 To Survivor 区； 清空 Eden 和 From Survivor 分区； From Survivor 和 To Survivor 分区交换，From Survivor 变 To Survivor，To Survivor 变 From Survivor。 每次在 From Survivor 到 To Survivor 移动存活的对象，年龄就 +1，当年龄到达 15（默认配置是 15）时，升级为老年代。大对象会直接进入老年代。 老年代当空间占用到达某个值之后就会触发全局垃圾回收，一般使用标记整理算法。 以上这些循环往复就构成了整个分代垃圾回收的整体执行流程。 内存分配策略@$简述java内存分配与回收策略以及Minor GC和Major GC所谓自动内存管理，最终要解决的也就是内存分配和内存回收两个问题。前面我们介绍了内存回收，这里我们再来聊聊内存分配。 对象的内存分配通常是在 Java 堆上分配（随着虚拟机优化技术的诞生，某些场景下也会在栈上分配，后面会详细介绍），对象主要分配在新生代的 Eden 区，如果启动了本地线程缓冲，则线程优先在 TLAB 上分配。少数情况下也会直接在老年代上分配。总的来说分配规则不是百分百固定的，其细节取决于哪一种垃圾收集器组合以及虚拟机相关参数有关，但是虚拟机对于内存的分配还是会遵循以下几种「普世」规则： 对象优先在 Eden 区分配多数情况，对象都在新生代 Eden 区分配。当 Eden 区没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC。如果本次 GC 后还是没有足够的空间，则将启用分配担保机制在老年代中分配内存。 这里我们提到 Minor GC，如果你仔细观察过 GC 日常，通常我们还能从日志中发现 Major GC&#x2F;Full GC。 Minor GC 是指发生在新生代的 GC，因为 Java 对象大多都是朝生夕死，所以 Minor GC 非常频繁，一般回收速度也非常快； Major GC&#x2F;Full GC 是指发生在老年代的 GC，出现了 Major GC 通常会伴随至少一次 Minor GC。Major GC 的速度通常会比 Minor GC 慢 10 倍以上。 大对象直接进入老年代所谓大对象是指需要大量连续内存空间的对象，频繁出现大对象是致命的，会导致在内存还有不少空间的情况下提前触发 GC，以获取足够的连续空间来安置新对象。 新生代使用的是复制算法，如果大对象直接在新生代分配，就会导致 Eden 区和两个 Survivor 区之间发生大量的内存复制，因此对于大对象都会直接在老年代进行分配。 长期存活对象将进入老年代虚拟机采用分代收集的思想来管理内存，会给每个对象定义了一个对象年龄的计数器，对象在 Eden 区出生，经过一次 Minor GC 对象年龄就会加 1，当年龄达到一定程度（默认 15） 就会被晋升到老年代，也就是长期存活对象将进入老年代。 虚拟机类加载机制简述Java类加载机制？类加载机制的原理Java中的所有类，都需要由类加载器装载到JVM中才能运行，同时对数据进行验证，准备，解析和初始化，最终形成可以被虚拟机直接使用的类型。 类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的需求，像是反射，就需要显式的加载所需要的类。 类装载方式，有两种 ： 隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中 显式装载， 通过class.forname()等方法，显式加载需要的类 Java中类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载，这是为了节省内存开销。 什么是类加载器，类加载器有哪些?类加载器负责将字节码文件的类加载到虚拟机内存中。 主要有一下四种类加载器: 启动类加载器（Bootstrap ClassLoader）：用来加载Java核心类库，无法被Java程序直接引用。即用来加载JAVA_HOME&#x2F;jre&#x2F;lib目录中的，或者被 -Xbootclasspath 参数所指定的路径中并且被虚拟机识别的类库； 扩展类加载器（Extension ClassLoader）：用来加载 Java 的扩展库。负责加载JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext目录或-Djava.ext.dir系统变量指定的路径中的所有类库； 应用程序类加载器（Application ClassLoader）：用来加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器，默认就是用这个加载器。 用户自定义类加载器：通过继承 java.lang.ClassLoader类的方式实现用户自定义类加载器。 说一下类装载的执行过程？ 类装载分为以下 5 个步骤： 加载：根据查找路径找到相应的 class 文件然后导入； 验证：检查加载的 class 文件的正确性； 准备：给类中的静态变量分配内存空间； 解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用可以理解为一个标识，而直接引用直接指向内存中的地址； 初始化：对静态变量和静态代码块执行初始化工作。 @$什么是双亲委派模型？双亲委派模型工作流程是怎样的？双亲委派模型的好处是什么？在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。 双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去加载，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载器无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载此类。 自下而上检查类是否已经被加载，自上而下尝试加载类 双亲委派模型工作流程： 当Application ClassLoader 收到一个类加载请求时，他首先不会自己去尝试加载这个类，而是将这个请求委派给父类加载器Extension ClassLoader去完成。 当Extension ClassLoader收到一个类加载请求时，他首先也不会自己去尝试加载这个类，而是将请求委派给父类加载器Bootstrap ClassLoader去完成。 Bootstrap ClassLoader尝试加载此类，如果Bootstrap ClassLoader加载失败，就会让Extension ClassLoader尝试加载。 Extension ClassLoader尝试加载此类，如果Extension ClassLoader也加载失败，就会让Application ClassLoader尝试加载。 Application ClassLoader尝试加载此类，如果Application ClassLoader也加载失败，就会让自定义加载器尝试加载。 如果均加载失败，就会抛出ClassNotFoundException异常。 双亲委派模型的好处：保证核心类库不被覆盖。如果没有使用双亲委派模型，由各个类加载器自行加载的话，如果用户自己编写了一个称为java.lang.Object的类，并放在程序的ClassPath中，那系统将会出现多个不同的Object类， Java类型体系中最基础的行为就无法保证，应用程序也将会变得一片混乱。 JVM调优说一下 JVM 调优的工具？JDK 自带了很多监控工具，都位于 JDK 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款可视化监控工具。 jconsole：JDK 自带的可视化管理工具，用于对 JVM 中的内存、线程和类等进行监控，对垃圾回收算法有很详细的跟踪，功能简单； jvisualvm：JDK 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等，功能强大。 常用的故障检测，监视，修理工具 工具名称 主要作用 jps (JVM Process Status Tool) 显示系统中所有的虚拟机进程 jstat (JVM Statistics Monitoring Tool) 收集虚拟机各方面的运行数据 jinfo (Configuration Info for Java) 显示虚拟机配置信息 jmap (Memory Map for Java) 生成虚拟机的内存转储快照 jhat (JVM Heap Dump Browser) 分析堆内存转储快照，不推荐使用，消耗资源而且慢 jstack (Stack Trace for Java） 显示线程堆栈快照 谈谈你的GC调优思路?谈到调优，这一定是针对特定场景、特定目的的事情， 对于 GC 调优来说，首先就需要清楚调优的目标是什么？从性能的角度看，通常关注三个方面，内存占用（footprint）、延时（latency）和吞吐量（throughput） 基本的调优思路可以总结为： 理解应用需求和问题，确定调优目标。假设，我们开发了一个应用服务，但发现偶尔会出现性能抖动，出现较长的服务停顿。评估用户可接受的响应时间和业务量，将目标简化为，希望 GC 暂停尽量控制在 200ms 以内，并且保证一定标准的吞吐量。 掌握 JVM 和 GC 的状态，定位具体问题，确定是否有 GC 调优的必要。具体有很多方法，比如，通过 jstat 等工具查看 GC 等相关状态，可以开启 GC 日志，或者是利用操作系统提供的诊断工具等。例如，通过追踪 GC 日志，就可以查找是不是 GC 在特定时间发生了长时间的暂停，进而导致了应用响应不及时。 接着需要思考选择的 GC 类型是否符合我们的应用特征，具体问题表现在哪里。是 Minor GC 过长，还是 Mixed GC 等出现异常停顿情况；如果不是，考虑切换到什么类型，如 CMS 和 G1 都是更侧重于低延迟的 GC 选项。 通过分析确定具体调整的参数或者软硬件配置。 验证是否达到调优目标，如果达到目标，即可以考虑结束调优；否则，重复进行分析、调整、验证。 @$常用的 JVM 调优的参数都有哪些？1、性能调优要做到有的放矢，根据实际业务系统的特点，以一定时间的JVM日志记录为依据，进行有针对性的调整、比较和观察。 2、性能调优是个无止境的过程，要综合权衡调优成本和更换硬件成本的大小，使用最经济的手段达到最好的效果。 3、性能调优不仅仅包括JVM的调优，还有服务器硬件配置、操作系统参数、中间件线程池、数据库连接池、数据库本身参数以及具体的数据库表、索引、分区等的调整和优化。 4、通过特定工具检查代码中存在的性能问题并加以修正是一种比较经济快捷的调优方法。 常用的 JVM 调优的参数 -Xms2g：初始化堆大小为 2g； -Xmx2g：堆最大内存为 2g； -Xmn1g：新生代内存大小为1g；-XX:NewSize 新生代大小，-XX:MaxNewSize 新生代最大值，-Xmn 则是相当于同时配置 -XX:NewSize 和 -XX:MaxNewSize 为一样的值； -XX:NewRatio&#x3D;2：设置新生代的和老年代的内存比例为 1:2，即新生代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3； -XX:SurvivorRatio&#x3D;8：设置新生代 Eden 和 两个Survivor 比例为 8:1:1； –XX:+UseParNewGC：对新生代使用并行垃圾回收器。 -XX:+UseParallelOldGC：对老年代并行垃圾回收器。 -XX:+UseConcMarkSweepGC：以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。 -XX:+PrintGC：开启打印 gc 信息； -XX:+PrintGCDetails：打印 gc 详细信息。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/tags/JVM/"}]},{"title":"图灵商城项目1","slug":"tuling","date":"2022-09-01T03:51:56.000Z","updated":"2023-02-26T07:47:50.614Z","comments":true,"path":"2022/09/01/tuling/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/tuling/","excerpt":"","text":"项目介绍前后端分离的网上商城后端微服务项目,在这个项目中 嗯，我做的是一个前后端分离的网上商城的微服务项目。我在我在本项目的工作主要是嗯对对实现一个商城的需求进行分析，就是你需要完成哪些功能，比如说你要实行商品的上架，然后你要一个管理员的界面，你必须就有普通用户的界面。管理员是来实现商品的上架呀，就是这是提供给这种工作人员使用的一个界面。嗯普通人员的话，需要去搜索商品，购买商品。购买商品，并且呃上作为商家也会有一些促销的手段，营销的手段来进行。提高交易额，促进消费，然后在这个项目中也会有一些，会有一些广告的。像广告，广告的模块。来来提高你的收入。嗯嗯，具体点来说的话，呃，使用了spring boot加呃，加s加s的。这基础框架，然后再使用了微服务的系列微服务，spring cloud微服务。嗯使用了服务的组件有优瑞卡注册注册中心新open原生调用。嗯，gateway网关等一系列的微服务系列，呃，微服务只是本科的项目的。就本科项目的一块儿内容吧，因为主要是为了熟悉恢复一个微服务项目需要怎么搭建呀，怎么去部署？包括你之间的调用，其实跟传统的项目区别也不是很大，只要你会用工具了之后。你只需要写一些配置，真正一些困难的地方是你去写一些具体的业务，就是你在分析好了你的需求之后，你的需求就是会一步步的，嗯。就是更加符合你你的实际，实际写代码。聊你要去设计库表啊之类的。 嗯，我做的是一个前后端分离的网上商城的微服务项目。 嗯，我做的是一个基于space比加mvc加实现的一个前后端分离的网上商城项目。嗯，晚上刷成微服务项使用使用了d收到视频cloud的各种组件，比如说因为他注册中心远程调用和大家都为网关等，嗯，也使用了一些中间键，比如说缓存呀，nx的。这些中间界比如来实现缓存NEX来进行负载均衡呀，等反向代理啊等，然后使用了也使用mk来进行异步操作，流量均分等。本是这样。 我做的是一个基于spring boot实现的网上商城项目。在linux做的系统环境下进行搭建。今天有x系统下进行垃圾使用了来进行缓存，使用了x来进行反向代理，负责均衡等，也有鱼。你学了x使用的是它里面会积一些路啊，使用了一些路啊，脚本等一些东西。完成的本地缓存，并且协同。是spring cloud的微服务各种主线来完成。没有服务的拆分呀，远程调用网关过滤之类的。嗯，采用来进行持久化。嗯的完成的功能呢主要就是一个商城的常见功能，比如说搜索呀，商品的上架，下架。就是有会有一个前前台的功能，会有一个提供给普通用户的一个界面，就是用户的登录啊。用户的灯注册，注册登录。给我搜索商品购买商品，下单，加入购物车的一系列基础的商城功能。也有一些高并发， 配置环境与启动项目Windows上传OSS镜像 导入镜像 创建实例 OSS与导入镜像地域要一致,否则会报invalid错 启动front项目 使用yarn即可 会报找不到Python路径错误 要提前安装Python2.7 (与项目的构建相关 我也不懂为啥) 然后设置Python路径全局配置 npm config set python “D:\\Python27\\python.exe” # 你安装的路径 启动后端项目 确认Maven settings 需要将pom里自带的设置Maven的信息注释掉 出现找不到等报错 可以去找到目标目录删了重新加载 本地域名解析配置 C:\\Windows\\System32\\drivers\\etc 127.0.0.1 tl.nacos.com tlshopdb.com tlshop.com 本机启动nacos 在黑窗体环境下切换目录到nacos&#x2F;bin下 startup.cmd -m standalone 配置数据库 1jdbc:mysql://tlshopdb.com:3306/micromall? root 123456 导入数据 SQL yog 新建同名数据库导入 执行SQL脚本 选择.sql文件执行 即可 速度贼快 Linux目的 学习docker指令 linux 实战,熟练掌握linux常用指令 docker部署各种中间件的一些配置 学会在linux跑起来一个项目使用docker 探明项目的功能 已配好docker环境 MySQL安装(已经安装好了) 下载MySQL5.7的docker镜像： docker pull mysql:5.7 使用如下命令启动MySQL服务： docker run -p 3306:3306 –name mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql \\ -e MYSQL_ROOT_PASSWORD&#x3D;123456 \\ -d mysql:5.7 docker run -p 3306:3306 –name mysql -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql -e MYSQL_ROOT_PASSWORD&#x3D;123456 -d mysql:5.7 docker run -p 3306:3306 –name mysql 参数说明 -p 3306:3306：将容器的3306端口映射到主机的3306端口 -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql：将配置文件夹挂在到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql：将日志文件夹挂载到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;：将数据文件夹挂载到主机 -e MYSQL_ROOT_PASSWORD&#x3D;root：初始化root用户的密码 进入运行MySQL的docker容器： docker exec -it mysql &#x2F;bin&#x2F;bash 使用MySQL命令打开客户端： mysql -uroot -proot –default-character-set&#x3D;utf8 创建mall数据库： create database mall character set utf8 安装上传下载插件，并将document&#x2F;sql&#x2F;mall.sql上传到Linux服务器上： yum -y install lrzsz 将mall.sql文件拷贝到mysql容器的&#x2F;目录下： docker cp &#x2F;mydata&#x2F;mall.sql mysql:&#x2F; 将sql文件导入到数据库： use mall; source &#x2F;mall.sql; 创建一个reader:123456帐号并修改权限，使得任何ip都能访问： grant all privileges on . to ‘reader’ @’%’ identified by ‘123456’; Redis安装 下载Redis5.0的docker镜像： docker pull redis:5 使用如下命令启动Redis服务： docker run -p 6379:6379 –name redis \\ -v &#x2F;mydata&#x2F;redis&#x2F;data:&#x2F;data \\ -d redis:5 redis-server –appendonly yes 进入Redis容器使用redis-cli命令进行连接： docker exec -it redis redis-cli ​ redis启动 redis-server &#x2F;usr&#x2F;local&#x2F;redis-5.0.2&#x2F;redis.conf redis-cli auth 123456 Nginx安装 下载Nginx1.10的docker镜像： docker pull nginx:1.10 先运行一次容器（为了拷贝配置文件）： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -d nginx:1.10 将容器内的配置文件拷贝到指定目录： docker container cp nginx:&#x2F;etc&#x2F;nginx &#x2F;mydata&#x2F;nginx&#x2F; 修改文件名称： mv nginx conf 终止并删除容器： docker stop nginx docker rm nginx 使用如下命令启动Nginx服务： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;conf:&#x2F;etc&#x2F;nginx \\ -d nginx:1.10 默认启动? 怎么查看启动的服务 RabbitMQ安装 下载rabbitmq3.7.15的docker镜像： docker pull rabbitmq:3.7.15 使用如下命令启动RabbitMQ服务： docker run -p 5672:5672 -p 15672:15672 –name rabbitmq -d rabbitmq:3.7.15 进入容器并开启管理功能： docker exec -it rabbitmq &#x2F;bin&#x2F;bash rabbitmq-plugins enable rabbitmq_management ​ 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;15672&#x2F;tcp –permanent firewall-cmd –reload 访问地址查看是否安装成功：http://192.168.3.101:15672 ​ 输入账号密码并登录：guest guest 创建帐号并设置其角色为管理员：mall mall ​ 创建一个新的虚拟host为：&#x2F;mall ​ 点击mall用户进入用户配置页面 ​ 给mall用户配置该虚拟host的权限 ​ Elasticsearch安装 下载Elasticsearch7.6.2的docker镜像： docker pull elasticsearch:7.6.2 修改虚拟内存区域大小，否则会因为过小而无法启动: sysctl -w vm.max_map_count&#x3D;262144 使用如下命令启动Elasticsearch服务： docker run -p 9200:9200 -p 9300:9300 –name elasticsearch \\ -e “discovery.type&#x3D;single-node” \\ -e “cluster.name&#x3D;elasticsearch” \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;plugins:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;plugins \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;data:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data \\ -d elasticsearch:7.6.2 启动时会发现&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data目录没有访问权限，只需要修改&#x2F;mydata&#x2F;elasticsearch&#x2F;data目录的权限，再重新启动即可； chmod 777 &#x2F;mydata&#x2F;elasticsearch&#x2F;data&#x2F; 安装中文分词器IKAnalyzer，并重新启动： docker exec -it elasticsearch &#x2F;bin&#x2F;bash #此命令需要在容器中运行 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip docker restart elasticsearch 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;9200&#x2F;tcp –permanent firewall-cmd –reload 访问会返回版本信息：http://192.168.3.101:9200 ​ Logstash安装 下载Logstash7.6.2的docker镜像： docker pull logstash:7.6.2 修改Logstash的配置文件logstash.conf中output节点下的Elasticsearch连接地址为es:9200，配置文件地址：&#x2F;document&#x2F;elk&#x2F;logstash.conf output { elasticsearch { hosts &#x3D;&gt; “es:9200” index &#x3D;&gt; “mall-%{type}-%{+YYYY.MM.dd}” } } 创建&#x2F;mydata&#x2F;logstash目录，并将Logstash的配置文件logstash.conf拷贝到该目录； mkdir &#x2F;mydata&#x2F;logstash 使用如下命令启动Logstash服务； docker run –name logstash -p 4560:4560 -p 4561:4561 -p 4562:4562 -p 4563:4563 \\ –link elasticsearch:es \\ -v &#x2F;mydata&#x2F;logstash&#x2F;logstash.conf:&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;pipeline&#x2F;logstash.conf \\ -d logstash:7.6.2 进入容器内部，安装json_lines插件。 logstash-plugin install logstash-codec-json_lines Kibana安装 下载Kibana7.6.2的docker镜像： docker pull kibana:7.6.2 使用如下命令启动Kibana服务： docker run –name kibana -p 5601:5601 \\ –link elasticsearch:es \\ -e “elasticsearch.hosts&#x3D;http://es:9200&quot; \\ -d kibana:7.6.2 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;5601&#x2F;tcp –permanent firewall-cmd –reload 访问地址进行测试：http://192.168.3.101:5601 ​ MongoDB安装 下载MongoDB4.2.5的docker镜像： docker pull mongo:4.2.5 使用docker命令启动： docker run -p 27017:27017 –name mongo \\ -v &#x2F;mydata&#x2F;mongo&#x2F;db:&#x2F;data&#x2F;db \\ -d mongo:4.2.5 Docker全部环境安装完成 所有下载镜像文件： REPOSITORY TAG IMAGE ID CREATED SIZE redis 5 071538dbbd71 2 weeks ago 98.3MB mongo 4.2.5 fddee5bccba3 3 months ago 388MB logstash 7.6.2 fa5b3b1e9757 4 months ago 813MB kibana 7.6.2 f70986bc5191 4 months ago 1.01GB elasticsearch 7.6.2 f29a1ee41030 4 months ago 791MB rabbitmq 3.7.15-management 6ffc11daa8d0 13 months ago 186MB mysql 5.7 7faa3c53e6d6 15 months ago 373MB registry 2 f32a97de94e1 17 months ago 25.8MB nginx 1.10 0346349a1a64 3 years ago 182MB java 8 d23bdf5b1b1b 3 years ago 643MB 所有运行在容器里面的应用： ​ SpringBoot应用部署构建所有Docker镜像并上传 修改项目根目录下的pom.xml中的docker.host属性： &lt;docker.host&gt;http://192.168.3.101:2375docker.host&gt; properties&gt; 如果项目根目录的pom.mxl中docker-maven-plugin的节点被注释掉了就打开注释，使项目在打包时直接构建Docker镜像； ​ 直接双击根项目mall的package命令可以一次性打包所有应用的Docker镜像； ​ REPOSITORY TAG IMAGE ID CREATED SIZE mall&#x2F;mall-portal 1.0-SNAPSHOT 70e0f76416a0 21 seconds ago 705MB mall&#x2F;mall-search 1.0-SNAPSHOT f3290bd1d0c7 41 seconds ago 725MB mall&#x2F;mall-admin 1.0-SNAPSHOT 26557b93a106 About a minute ago 705MB 部署mall-admin docker run -p 8080:8080 –name mall-admin \\ –link mysql:db \\ –link redis:redis \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;admin&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-admin:1.0-SNAPSHOT 注意：如果想使用Logstash收集日志的话，需要将应用容器连接到Logstsh，添加如下配置即可； –link logstash:logstash \\ 部署mall-search docker run -p 8081:8081 –name mall-search \\ –link elasticsearch:es \\ –link mysql:db \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;search&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-search:1.0-SNAPSHOT 部署mall-port docker run -p 8085:8085 –name mall-portal \\ –link mysql:db \\ –link redis:redis \\ –link mongo:mongo \\ –link rabbitmq:rabbit \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;portal&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-portal:1.0-SNAPSHOT 开启防火墙 firewall-cmd –zone&#x3D;public –add-port&#x3D;8080&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8081&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8085&#x2F;tcp –permanent firewall-cmd –reload 访问接口进行测试 mall-admin的api接口文档地址：http://192.168.3.101:8080/swagger-ui.html ​ mall-search的api接口文档地址：http://192.168.3.101:8081/swagger-ui.html ​ mall-portal的api接口文档地址：http://192.168.3.101:8085/swagger-ui.html ​ 总结psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD ​ docker run –name&#x3D;”容器新名字” 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p docker execdocker exec [OPTIONS] 容器名称 COMMAND [ARG…]OPTIONS说明： -d，以后台方式执行命令； -e，设置环境变量 -i，交互模式 -t，设置TTY -u，用户名或UID，例如myuser:myusergroup 通常COMMAND只能是一条语句，为了支持多个命令的执行，需要将多个命令连接起来交给Shell，docker exec命令的使用示例如下： sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject &amp;&amp; git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC} &amp;&amp; git checkout FETCH_HEAD”;sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject;git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC};git checkout FETCH_HEAD”; 注意：对于已经暂停或停止了的容器，无法执行docker exec命令，如下将抛出异常： docker pause myContainerdocker exec myContainer … options 作用 -d 在后台运行命令-i 即使没有附加也保持 STDIN 打开-t 设置TTY进入容器的 CLI 模式 -e 设置环境变量-w 需要执行命令的目录-u 指定访问容器的用户名备注：其实还有几个 options，但是目前还没用到，要用的时候再写吧 实际栗子执行 tomcat 容器的 startup.sh 脚本 docker exec -it tomcat7 startup.sh进入容器的 CLI 模式(最常用) docker exec -it tomcat7 bash执行普通命令 docker exec -it tomcat7 pwd 指定工作目录执行命令 docker exec -it -w &#x2F;usr tomcat7 pwd 以 root 用户身份进入容器(重点) docker exec -it -uroot jenkins1 bash linux入门：”&quot;的作用运行cellranger count，发现每行末尾有个\\，遂查了下\\的作用 123456cellranger count --id=XPBShm \\ --transcriptome=/home/rstudio/opt/refdata-gex-GRCh38-and-mm10-2020-A \\ --fastqs=/home/rstudio/data/rawx/xpbs \\ --sample=XPBS \\ --r1-length 26 \\ --r2-length 98 作用有2 1 作为转义符反斜线符号“ \\ ”在Bash中被解释为转义字符，用于去除一个单个字符的特殊意义，它保留了跟随在之后的字符的字面值，除了换行符（\\n,\\r）。 如果在反斜线之后一个换行字符立即出现，转义字符使行 得以继续，但是换行字符后必须紧跟命令，不能出现空格，遇到命令很长时使用反斜线很有效。 例一： 1234[linux@linux ~]$ echo $HOME/home/[linux@linux ~]$ echo \\$HOME$HOME 例子中，反斜线去除了“ $ ”字符的特殊意义，保留字面值，从而不输出home目录路径。 2. 作为换行符例二： 12345678910export PATH=\\/bin:\\/sbin:\\/usr/bin:\\/usr/sbin:\\/usr/local/bin:\\/apps/bin:\\/apps/tools:\\/apps/tslib/bin\\ 例子中，反斜线使行得以继续，命令可以正常输入。 例二（反） 12345678910export PATH=\\ /bin:\\ /sbin:\\ /usr/bin:\\ /usr/sbin:\\ /usr/local/bin:\\ /apps/bin:\\ /apps/tools:\\ /apps/tslib/bin\\ 例子中就会出现错误： &#x2F;bin:: bad variable name&#x2F;* &#x2F;bin：错误变量名 *&#x2F; 因为在”+换行符”之后必须紧跟命令，不能有空格。 笔记DevOps： DevOps的意思就是开发和运维不再是分开的两个团队，而是你中有我，我中有 你的一个团队。我们现在开发和运维已经是一个团队了，但是运维方面的知识和 经验还需要持续提高。 持续交付： 持续交付的意思就是在不影响用户使用服务的前提下频繁把新功能发布给用户使用，要做到 这点非常非常难。我们现在两周一个版本，每次上线之后都会给不同的用户造成不同程度的 影响。 容器化： 容器化的好处在于运维的时候不需要再关心每个服务所使用的技术栈了，每个服务都被无差 别地封装在容器里，可以被无差别地管理和维护，现在比较流行的工具是docker和k8s。 所以你也可以简单地把云原生理解为：云原生 &#x3D; 微服务 + DevOps + 持续交付 + 容器化 商品模块业务场景介绍： 商品模块业务详解 ​ ​ ​ 表的设计：打开游览器访问京东详细页问题： 商品这块的数据库如何更好的设计，商品详细页显示这么多信息，是一张表还是多张表更好了？ 这个问题到底是一张表还是多张表，我们判断依据是什么？我们判断商品详细页里面显示的这些信息他们的关系。通过他们的关系，我们才能知道到底是设计一张表还是多张表。 一张表： 如果是一张表存储所有数据的话，那么查询是非常方便的，这是其优点，但是你会发现存储的时候是不是很麻烦。不通类型不同大小不通商品等等都不一样，那这样的一张表设计起来实在是太复杂了。 多张表： 如果是多张表的话业务更加清晰，维护起来也更加方便，但是你会发现查询好像会非常的复杂，一个商品页面我们需要查很多的表和数据。 解决 我们正确的方式是根据不同的数据类型按不通的表进行存储 商品表的设计在设计这个表的时候，嗯，会画出它的实体关系图。嗯那你设计一张表的时候，你的查询十分方便，但是你的你设计这个表会很复杂，就是想要把各种各种情况，各种类型全部囊括进来，十分负责，所以选择按根据不同的类型。去跟显示不同的表进行存储。嗯商品有不同类型，就比如有吃的，穿的。还有其他用的。就可以根据分类来划分我们的礼品商品。嗯 需求分析为什么商品需要分类？ 我们知道商品是有不通类型的，比如有吃的 比如有穿的比如还有其他的用的。不通的商品用途不一样。我们一开始就可以按分类来进行划分我们的商品，这个就有点像我们去看论坛的分类是一样的。 第一个版本：商品+分类 ​ 问题：此时有什么问题？： 目前这个方案有什么问题了？我们慢慢发现一个问题，只有分类并不能适应所有的需求，比如nike鞋和nikeT恤，用户可能希望先看nike的所有商品，这个模型就不能满足。我们想在这个关系中，加入“品牌”概念 但是只有分类的时候并不能适应所有的需求，你只是比如说耐克鞋，耐克T恤，用户希望能先看耐克的所有商品，这个模型就不能满足，只要在加入品牌的概念。这样基本用户可以在通过分类或者品牌找到自己想要的商品。 第二个版本：商品+分类+品牌 ​ 这样基本用户可以在首页上通过分类或者品牌找到自己想要的商品，也可以直接查看热门的商品和新上架的商品。 问题：此时有什么问题？ 但是问题也来了，用户在进入分类后，展示在用户面前的是很多很多商品，用户希望再通过筛选查询出更接近他目标的商品？ 用户在进入分类之后展示在湖面前的是很多很多商品，用户希望通过筛选查询出更接近他目标的商品。其实就加入了属性。比如说图案呀！嗯，之类的。 ​ 加入属性： 于是优秀的产品设计师，设计出了类似这样的UI： ​ ​ 第三个版本：商品+分类+品牌+属性 用户可以通过这些筛选条件进一步缩小自己的目标范围，那么问题又来了，这样的产品需求排在程序员面前，怎么去实现它？经过分析，我们找出了一个方法，我们知道商品之间的属性可能存在着较大的差别，比如牛仔裤它有版型、腰型、裤长等属性；而电脑它有CPU、显卡等属性，各类商品的属性是不同的。再进一步想，休闲裤也版型、腰型、裤长等属性；台式电脑或者笔记本电脑都有CPU、显卡等属性。所以我们得出：一个分类对应若干属性，而一个属性，对应若干属性选项，而一个具体商品又对应若干属性选项（例如具体一条牛仔裤，他的裤长：7分，裤型：直筒）。有点绕，仔细品味一下。 通过分类与品牌查询到相关的商品，再根据属性嗯，在写。来进一步缩小范围，比如牛仔裤，它有版型，腰型，裤长等属性，而电脑有CPU，显卡的出现。一个分类对应若干属性来一个性对应若干属性选项。一个具体商品有对应活该属性决定。各分类对应若干属性，一个属性对应若干属性选项。一个具体商品有对应若干属性选项。月底一条牛仔裤，它的裤长裤型的。 ​ 从图上可以看出，分类和属性的关系（例如：“牛仔裤”分类下有裤型、裤长、版型等属性）、属性和属性选项的关系（例如：裤长属性有长款、九分裤、七分裤的选项）、商品和属性选项的关系（例如某条牛仔裤的裤长是7分裤）。至此，我们知道一个商品的分类、品牌以及它有什么属性和对应的属性值。那么通过筛选条件，自然就可以查询出指定的商品。这里特别说一句，价格也是属性，不要设想用商品表中的价格字段去做计算。这不利于查询也增加了复杂度，让商家编辑人员用属性来设置并保证他的正确性。 ​ ​ 这个页面展示商品的所有信息，按照之前的设计好像都可以满足。但是我们似乎感觉错过了什么，在图上右边我们发现该商品当前的颜色和尺寸，并且允许用户可以选择其他的颜色和尺寸。这给我们带来了疑惑，这里的“颜色”和“尺寸”是什么，一件商品的不同颜色不同尺寸是算一个商品还是多个商品。 ​ 为什么要加入规格： 第四个版本：商品+分类+品牌+属性+规格 经过思考后，我们发现我们混淆了两个概念——“商品”和“货品”。不同规格的货品作为独立的商品。比如一条裤子的有L尺寸、M尺寸、一个U盘有16G还是32G的，都是同样的货品，不同规格的商品。可以认为货品和商品是一对多的关系。弄清了这个概念，处理这个需求就容易多了，这里的“颜色”、“尺寸”我们就作为“规格”来处理，而红色、黑色；L号、M号我们视为规格的选项或者说规格值。一件货品对应若干规格，而具有某一规格值的货品就是商品。 spu：iphone12 sku：金色64 iphone12 ​ 好了，现在好像差不多了。基于这个模型可以满足基本的商品搜索、展示的需求。搜索引擎也可以根据这个模型数据生成对应的商品索引，达到准确搜索的目的。商品模块还会和其他模块一起协作，比如用户系统、订单系统、支付系统等。一般情况下我们会把商品业务独立出来做成“商品中心”的服务，集中处理商品查询、更新、发布等业务，支撑其他业务。 ​ \\ 嗯，总结商品表的设计，商品表的设计主要是根据呃，用户对商品进行搜索，并且商家对用户对商家对商品进行展示来设计的。首先你用户想搜索到商品它需要进行分类查询，就是会有衣服啊，电子商品啊这种。设计成一个分类，那么在分类之后你还嗯会显示态度很还是是太多，还是需要进行一些区分？嗯，就比如说你的需求是只查看特步的。鞋子啊，或者特步的特步鞋或者特步。特步的衣服，然后就是你需要加还有品牌的概念。然后用户再去根据分类还有品牌来缩小它的具体范围。但是这样还是有很多的，很多的区分就比如说你需要嗯，你是你需要看他的具体的属性就是如何一个牛仔裤它的裤长。裤长，腰宽这种属性，这个属性呢是跟它的分类挂钩的，比如说你的裤子它就是一个就能把它的属性选项给出来。然后 嗯，表的设计，表的设计主要是基于用户的搜索需求与商家的展示需求来进行设计。嗯比较重要的有五张表，然后其他还有一些。关联表啊，一些这种就这种关系表，嗯，比较重要的就是你首先要有一个商品表就是你对，但是你的商品会十分复杂，你就要把它拆分成分类，就是你一个啊，不对。商品货品 就是分类和品牌之后你可以添加，你可以根据商品的属性再进行搜索，就比如说你的CPU，CPU，CPU型号呀，机身内存呀这些对应，然后每一个，每一个这些属性要对应着每一个属性选项。嗯，属性之后呢你对应的属性选项就比如说16G呀，那机身内存16G这些。但是你在真正的我看看去 商品的搜索： ​ 搜索引擎elasticsearch 商品的展示： 三、商品模块展示技术难点商品详情页是展示商品详细信息的一个页面，承载在网站的大部分流量和订单的入口。京东商城目前有通用版、全球购、闪购、易车、惠买车、服装、拼购、今日抄底等许多套模板。各套模板的元数据是一样的，只是展示方式不一样。目前商品详情页个性化需求非常多，数据来源也是非常多的，而且许多基础服务做不了的都放我们这，因此我们需要一种架构能快速响应和优雅的解决这些需求问题。因此我们重新设计了商品详情页的架构，主要包括三部分：商品详情页系统、商品详情页统一服务系统和商品详情页动态服务系统；商品详情页系统负责静的部分，而统一服务负责动的部分，而动态服务负责给内网其他系统提供一些数据服务。 ​ 商品详情页前端结构 前端展示可以分为这么几个维度：商品维度(标题、图片、属性等)、主商品维度（商品介绍、规格参数）、分类维度、商家维度、店铺维度等；另外还有一些实时性要求比较高的如实时价格、实时促销、广告词、配送至、预售等是通过异步加载。 ​ ​ SPU： Standard Product Unit （标准化产品单元）,SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。 SKU： Stock keeping unit(库存量单位) SKU即库存进出计量的单位（买家购买、商家进货、供应商备货、工厂生产都是依据SKU进行的），在服装、鞋类商品中使用最多最普遍。 例如纺织品中一个SKU通常表示：规格、颜色、款式。SKU是物理上不可分割的最小存货单元。 单品页流量特点 热点少，各种爬虫、比价软件抓取。 静态处理 thymeleaf等模板引擎 架构方案的问题：问题一：我们知道数据新增分:增量和全量数据 如果后台的小二新增了很多的商品，那我们都要对这些商品进行静态化，但是现在有个问题。那这些数据如何同步了？这是一个新增商品同步的问题，那这个问题怎么解决比较好了？。 ​ 不同应用部署在不同服务器甚至在不同的机房不同的国家。 1、通过网络同步的方式 就是其中一台服务器静态化之后，然后把文件同步到其他应用服务器上去。比如我们的linux命令scp方式。这种方式虽然可行，但是我们发现问题还是蛮多的，有多少个节点就需要同步多少份，等于是商品的数量*服务器的应用数数。很显然这种办法不是最优的解决办法 如果上述办法无法解决，那我们就用另外的方案，同学们你们觉得还有其他的方案没有？ **2、定时任务:**可以在某个应用用一个定时任务，然后分别去执行数据库需要静态化的数据即可，可以解决上述1数据同步的问题，因为所有的任务都是在本机运行，就不需要数据同步了。但是也有一个问题。就是如何避免不通的机器跑的数据不要重复，也就是A和B定时任务都跑了一份商品。这个是这种方案需要解决的。（比较直观的就是上锁） 3、消息中间件：还有一种办法就是通过消息中间件来解决。订阅topic然后生成当前服务器静态化的页面。 问题二：我们的freemark它是数据要事先按我这个模板生产好的，那就是说一定你改了模板，如果要生效的话，需要重新在把数据取出来和我们这个模板进行匹配生产更多的的静态html文件。那这是一个比较大的问题 如果后台数据有变更呢?如何及时同步到其它服务端? 如果页面静态化了，我们搜索打开一个商品详细页，怎么知道要我需要的访问的静态页面？ 万一我们模板需要修改了怎么办？ 牵一发动全身。 ​ 3.1、压测测试 jmeter模板 ​ 1、换数据库 2、分库分表 3.2、后台 12345678910111213141516171819202122/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = portalProductDao.getProductInfo(id); if (null == productInfo) &#123; return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; return productInfo;&#125; 压测结果： 5000并发 ​ 后台优化：redis缓存： redis设置：RedisConifg》RedisOpsUtil 12345678910111213141516171819202122232425262728293031/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = null; //从缓存Redis里找 productInfo = redisOpsUtil.get(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, PmsProductParam.class); if(null!=productInfo)&#123; return productInfo; &#125; productInfo = portalProductDao.getProductInfo(id); if (null==productInfo) &#123; log.warn(&quot;没有查询到商品信息,id:&quot;+id); return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, productInfo, 3600, TimeUnit.SECONDS); return productInfo;&#125; 好处： 加入redis之后我们发现提高了可以把之前请求 数据库查询的商品都缓存到redis中，通过对redis的访问来减少对数据里的依赖，减少了依赖本质就是减少了磁盘IO。 问题：提高请求的吞吐量，除了减少磁盘IO，还有网络IO，我们可以发现，请求redis其实也会涉及到网络IO，我们所有的请求都要走xxx端口号。那有没有更好的优化思路了，来同学们你们鲜花在哪儿？ 读多写少有两种： 1、最终一致性方案： 设置超时时间来解决 ​ redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE+id,productInfo,360,TimeUnit.SECONDS); 2、实时一致性方案： ​ 课程讲到 交易canal binlog 两个问题(高并发)、压缩的问题》减少内存 现在有什么问题了？ 跟我们预期只set一次redis 是有出入，为何会这样子了？并发问题 当我第二次再去访问，此时此刻没有日志输出，说明全部走了缓存： 并发问题：并发编程》并发问题》锁的方式来实现 java并发 加锁方式（不适合》特殊》分布式） 分布式锁：redis、zookeeper QPS立马就提高了很多。 加入分布式锁: 1234567&lt;!--加入redisson--&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; setnx 缓存应用场景： 1、访问量大、QPS高、更新频率不是很高的业务 2、数据一致性要求不高 缓存和数据库双写一致性问题： 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。 答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。 我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 zk&gt;临时顺序节点》原子性 线程创建如果可以创建成功，是否第一个 拿到了锁 业务场景介绍： 熟悉秒杀系统的业务和技术核心点、以及流程等 正常电商流程: 秒杀场景演示： 完毕 活动和场次关系 秒杀活动表：sms_flash_promotion 秒杀场次表：sms_flash_promotion_session 场次商品关系表：sms_flash_promotion_product_relation 一个活动可以有多个场次，每个场次可以有多个商品进行秒-杀。 秒杀系统设计 分两部分内容；秒杀业务设计和秒杀技术实现。 3215186661 11秒杀链路优化三 图灵：楼兰 秒杀系统核心交易链路优化 三 今天要处理的问题：秒杀场景下如何进行限流。 今天要做的内容： 解决的问题：1、在秒杀页面，客户点击秒杀后，在前台弹出一个验证码，需要用户 输入验证码才能往后端发送请求，这样能够错开秒杀下单的时间。 2、通过验证码，对后台下单请求进行保护，防止刷单，即绕开前端，直接往后端发 送请求。 在秒杀页面开始秒杀后，客户点击秒杀按钮，要在前台弹出一个验证 码，需要用户输入验证码才能往后端发请求，这样能够错开秒杀下单时 间。 在我们的实现中，是要将memberId、producrId和验证码的值一起传入 后台，后台返回一个token。然后再根据这个tokne拼接一个后台秒杀地 址。这个token会存入到redis中。实际秒杀时，会增加一个判断，检测 这个token是不是在redis中存在。如果不存在，就是机器刷单 一、电商项目中秒杀的实现流程 1、在tmll-admin中添加秒杀活动，在秒杀活动中先设置活动的开始日期和结束日 期，然后添加商品。 这个秒杀活动信息会保存到mysql中。sms_flash_promotion_product_relation 表。 同时，添加商品后会将活动商品保存到ZK中。(路 径&#x2F;ZkLock&#x2F;load_db&#x2F;{productId})。然后，当访问到商城前端商品页时， http://lo calhost:8080&#x2F;#&#x2F;product&#x2F;{productId}，会检查Redis中的产品信息缓存。如果 Redis中没有产品信息，就会重建Redis缓存。key为product:detail:cache:{ProdId} 然后进入商城的单品页 http://localhost:8080/#/product/32 ， product.vue 那个”立即购买”的按钮就会变成”立即秒杀” 点击立即秒杀就会进入秒杀页。secKillDetail.vue 代码实现机制： 1、从Redis判断商品是否有秒杀活动。 一个商品要么就只能秒杀，要么就只能普通购买，这样是否合理？ 这就 是为什么要单独独立出一套秒杀服务集群。 2、发送后台请求申请验证码。后台返回验证码图片，并将验证码的计算结果保存 到Redis。 验证码的请求路径里header里的memeberId是怎么进去的。有什么用？ 生成验证码图片的这个请求要怎么防刷？ 3、保护后台请求接口。 输入验证码后，先验证输入的验证码结果，返回一个Token。这个Token会传 入到接下来的商品确认页面，同时会保存到Redis当中，表示当前用户有购买秒杀商 品的资格。有效期300秒，300秒内必须完成下单，否则就要重新申请秒杀资格。 在后续的下单过程中，需要传入这个Token才能正常下单。 验证码如果输入错误，是如何判断的？ 二、如何加强限流方案的安全性 了解整理流程后，要继续深入思考下我们这个限流方案的安全性。 1&gt; 针对验证码 针对验证码的安全性，可以加上之前的验证码内容。 1、我们做了这一套机制后，到底有多安全？ 下单请求依然是可以用机器人模拟 的。 用户ID是存在Cookie当中的，可以拿到。 图形验证码是随机的，那就总有可能产生容易被机器识别的验证码。 2、怎么加强验证码本身的安全性 这个问题也是必须要前后台配合来思考的，而不是单独靠前端或者后端能够解决 的。这个方案要如何设计？ 提高验证码安全性的措施：1、加干扰线或者干扰 点，2，将关键字符变形并且在图形上串到一起。3、增加更多的前端交互，行为 验证。 验证码的内容最好是一个比较复杂的题目，而不是简单的输入数字。这样可以有 效延长下单请求的时长，更好的分散请求峰值。 图形验证码可以篡改。可以用PostMan另外访问生成图形验证码的接口，这时 Redis里的值就被篡改了，不再是页面上看到的计算结果了。如何处理？1、增加 更多的判断因素，例如IP。2、前端签名，后端验证签名。 输入了验证码之后，存在Redis中的验证码要及时删除。同时生成一个Token， 代表当前用户有购买权限。这个Token有效期是非常短的。 针对验证机制的安全性，可以增加一些安全机制。 换一种验证码 我们动手来换一种复杂一点的验证码，HappyCaptcha 官网地址： https://gitee.c om&#x2F;ramostear&#x2F;Happy-Captcha 换的方式比较简单，首先在pom.xml中加入HappyCaptcha的依赖 com.ramostear Happy-Captcha 1.0.1 1 2 3 4 5 然后在OmsPortalOrderController中getVerifyCode方法，将生成验证码的部分修 改一下： 这样，前台的验证码就变成了一闪一闪的动画。并且是中文的加减法，更难破解。 可以看到整个HappyCaptcha的实现机制跟我们自己的实现机制是差不多的，也是 使用session来存储答案。 其他还有哪些更难以破解的验证码？ 2&gt; 针对下单请求 我们的实现机制是要求将token拼凑到请求路径上来。这跟把token作为参数传递有 什么区别？ 如果一个模拟程序需要使用机器来参与秒杀抢单，首先需要根据其他用户的请求来 分析获取下单路径。如果是同一个请求路径，只是带的参数不同，那机器完全可以 尝试用暴力破解的方式来尝试进行下单。如果碰巧传入了一个Redis中的token值， 那他就下单成功了。但是现在把参数隐藏到了请求路径当中，动态的请求路径对于 下单的机器来说，就比较难试探出请求的地址，这样就增加了他下单的难度。 三、电商整体的秒杀限流方案： try { &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;这个步骤就会完成生成图片并且往response发送的步骤。 HappyCaptcha.require(request,response).style(CaptchaStyle.ANIM) .type(CaptchaType.ARITHMETIC_ZH) .build().finish(); Object captcha &#x3D; request.getSession().getAttribute(“happycaptcha”); &#x2F;&#x2F;HappyCaptcha生成的验证码是String类型 int code &#x3D; Integer.parseInt(captcha.toString()); log.info(“验证码答案:{}”,captcha); redisOpsUtil.set(RedisKeyPrefixConst.MIAOSHA_VERIFY_CODE_PREFIX + memberId + “:” + productId ,code ,300 ,TimeUnit.SECONDS); &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码结束 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; return null; }catch(Exception e) { e.printStackTrace(); return CommonResult.failed(“秒杀失败”); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 我们天天都在说三高，高并发、高可用、高可扩展，那到底应该如何去落地一个三 高的设计方案？ 构建大并发、高性能、高可用系统中几种通用的优化思路，可以抽象总结为“4 要 1 不要”原则。也就是：数据要尽量少、请求数要尽量少、路径要尽量短、依赖要尽 量少，以及不要有单点。当然，这几点是你要努力的方向，具体操作时还是要密切 结合实际的场景和具体条件来进行。 针对秒杀这个场景，其实方案设计往往比技术细节更为重要。因为你可以想象，每 一个秒杀环节的经典问题，都意味着互联网的秒杀业务出现过大的问题，这都是实 打实买来的教训。发现了问题之后才会有针对性的方案设计。那现在，我们整体来 回顾下电商的秒杀限流方案。 错峰1：动静分离的本质是将包含浏览者信息的动态数据和不包含浏览者信息的静态 资源区分开。例如在商品单品页，商品信息是不包含浏览者信息的，这部分就可以 抽象出静态资源。而用户登录状态、cookie等这些动态数据也尽可能缓存起来，并 且使缓存能够离用户更近。 错峰2：秒杀答题的形式可以是多种多样的，目的是防止机器刷单，以及错开用户的 下单时长。在秒杀场景下，答题速度靠后的请求自然就没有库存了，也可以减少系 统的请求量。 错峰3：缓存的作用主要有两个，一是快速扣减库存，保护数据库流量，并且库存扣 减完成后，快速通知Nginx，屏蔽后续请求；二是提前识别热点数据，并且针对热 点数据提供优化处理。处理的方案主要是三个，一是优化，二是限制，三是隔离， 包括业务隔离、系统隔离、数据隔离。 错峰4：单独提供秒杀服务集群，有利于减少秒杀商品的超大流量对普通商品的性能 冲击，不要让1%的商品影响到另外的99%。 后台错峰：这一部分是我们实战课程的重点。之前monkey老师带大家在后端针对 秒杀场景做了非常多的设计与实战。我们这个图中每一个错峰点虽然在图上就是比 较简单的一个点，但是深入进去，每个地方要考虑的细节都还是非常多的，大家可 以回顾下之前的几节课，体会下如何在后端对秒杀服务做针对性的优化。 首先想到的是使用MQ进行削峰。但是实际上，后端需要考虑的三高问题也远不止 MQ削峰这一步。每一个环节都需要考虑后端组件是否能够承载得住。例如秒杀服务 集群，到底应该部署多大的集群？部署多少台机器呢？显然为了顶住秒杀的大流 量，秒杀集群就需要部署得非常大。但是，如果在大部分没有秒杀服务的时间内， 这个集群的资源就闲置得非常厉害。所以，虚拟化+云计算进行弹性部署也是非常重 要的。在我们的项目实战课后面就会由诸葛老师给大家带来k8s和云部署的实战课 程。 然后：在后端系统中，添加了Redis、MQ这样的一些中间产品。而这些产品集群本 身，也存在效率低下、服务崩溃的风险。这样也就给系统整体带来了更多的风险 点。那要怎么去屏蔽这些产品给系统带来的风险呢？大家可以思考一下，下一节课 将会由fox老师给大家进行系统降级方面的设计。 题外话 方案优先 &gt; 技术优先。学习技术的同时，都要增加对软件问题的思考，很多同学 技术学得很快，但是缺乏思考。秒杀这种超大并发场景下的限流问题，不是任何一 个技术或者任何一个步骤可以限制住的，需要一个完整全面的方案才能保证业务稳 定性。所以我们在开发过程中，不能只埋头于技术点，要站在更高的角度，整体来 理解解决方案，这样才能更深入的理解自己在做的事情，也才能真正来解决问题。 这才是高级程序员与普通程序员真正的区别。 例如针对前端验证问题，还有哪些优化方案？ 提前发Token。可以在秒杀前设置一个预约活动。 在活动中提前发放 token。例如一个秒杀活动有20W个商品，那就可以预先准备200W个 token。用户进行预约时，只发放200W个Token，其他人也能预约成 功，但是其实没有获得token，那后面的秒杀，直接通过这个token就可 以过滤掉一大部分人。相当于没有token的人都只预约了个寂寞。这也是 互联网常用的一个套路。 例如针对超卖问题，在之前的课程中，介绍了如何使用Redis分布式锁防 超卖。针对同一个商品ID，使用一把分布式锁，确实可以很快很方便的 处理超卖问题。但是如果同时进行秒杀的商品多了呢？像京东、淘宝一 场大型的秒杀活动，同时有成千上万个商品要进行秒杀，那就意味着同 一时间Redis上锁解锁的操作会要执行成千上万次，这对Redis的性能消 耗是相当巨大的，Redis就有可能升级成为新的性能瓶颈。这时该怎么 办？ 当然具体问题的解决方案从来不止一个，这里我们可以选择一种返璞归 真的方案，把秒杀超卖的问题从分布式降级到本地JVM中，来获取极限 性能。例如将秒杀服务接入配置中心，然后在秒杀服务开始前，由配置 中心给每个应用服务实例下发一个库存数量。然后每次下单，每个服务 器只管自己的库存数量，与其他应用服务器完全不进行库存同步，在各 自的内存里扣减库存，这样就不会有超卖的情况发生。减少了网络消 耗，性能也能够进一步提升。 这种方案可不可行呢？当然也会有一些问题需要去处理。有可能某给服 务器上的库存很快消耗完了，而其他的服务器上仍有库存。整个服务就 会表现为你抢不到商品，但是在你后面抢商品的人却能抢到商品。(你们 在参与秒杀时有没有过这样的经历？)但是这在秒杀这种场景下，完全是 可以接受的。另外，如果某一个应用服务器挂了，那给他分配的库存就 会丢失。这时候又要怎么办？其实也没必要再去设置什么复杂的逻辑， 大不了少卖一点出去。反正都是售罄了，全卖完了，和卖了99%，其实 没什么区别。这时只需要统计好订单的数量(可以通过MQ来统计，也可 以通过Redis统计)，等秒杀活动的30分钟等待支付期过去后，再将没卖 出去的库存重新丢回库存池，与没有付款而被取消的订单商品一起返场 售卖就可以了。这也是很多互联网公司目前采用的方案。 最后虽然我们是后台开发工程师，但是前端也必须要有所了解。今天我们关注的 这个问题，也不能只关注后端，需要前后端一起才能理解他的作用。","categories":[{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"图灵商城项目2","slug":"tulingnote","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:46:42.511Z","comments":true,"path":"2022/09/01/tulingnote/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/tulingnote/","excerpt":"","text":"项目介绍前后端分离的网上商城后端微服务项目,在这个项目中 嗯，我做的是一个前后端分离的网上商城的微服务项目。我在我在本项目的工作主要是嗯对对实现一个商城的需求进行分析，就是你需要完成哪些功能，比如说你要实行商品的上架，然后你要一个管理员的界面，你必须就有普通用户的界面。管理员是来实现商品的上架呀，就是这是提供给这种工作人员使用的一个界面。嗯普通人员的话，需要去搜索商品，购买商品。购买商品，并且呃上作为商家也会有一些促销的手段，营销的手段来进行。提高交易额，促进消费，然后在这个项目中也会有一些，会有一些广告的。像广告，广告的模块。来来提高你的收入。嗯嗯，具体点来说的话，呃，使用了spring boot加呃，加s加s的。这基础框架，然后再使用了微服务的系列微服务，spring cloud微服务。嗯使用了服务的组件有优瑞卡注册注册中心新open原生调用。嗯，gateway网关等一系列的微服务系列，呃，微服务只是本科的项目的。就本科项目的一块儿内容吧，因为主要是为了熟悉恢复一个微服务项目需要怎么搭建呀，怎么去部署？包括你之间的调用，其实跟传统的项目区别也不是很大，只要你会用工具了之后。你只需要写一些配置，真正一些困难的地方是你去写一些具体的业务，就是你在分析好了你的需求之后，你的需求就是会一步步的，嗯。就是更加符合你你的实际，实际写代码。聊你要去设计库表啊之类的。 嗯，我做的是一个基于space比加mvc加实现的一个前后端分离的网上商城项目。嗯，晚上刷成微服务项使用使用了d收到视频cloud的各种组件，比如说因为他注册中心远程调用和大家都为网关等，嗯，也使用了一些中间键，比如说缓存呀，nx的。这些中间界比如来实现缓存NEX来进行负载均衡呀，等反向代理啊等，然后使用了也使用mk来进行异步操作，流量均分等。本是这样。 我做的是一个基于spring boot实现的网上商城项目。在linux做的系统环境下进行搭建。今天有x系统下进行垃圾使用了来进行缓存，使用了x来进行反向代理，负责均衡等，也有鱼。你学了x使用的是它里面会积一些路啊，使用了一些路啊，脚本等一些东西。完成的本地缓存，并且协同。是spring cloud的微服务各种主线来完成。没有服务的拆分呀，远程调用网关过滤之类的。嗯，采用来进行持久化。嗯的完成的功能呢主要就是一个商城的常见功能，比如说搜索呀，商品的上架，下架。就是有会有一个前前台的功能，会有一个提供给普通用户的一个界面，就是用户的登录啊。用户的灯注册，注册登录。给我搜索商品购买商品，下单，加入购物车的一系列基础的商城功能。也有一些高并发， 配置环境与启动项目Windows上传OSS镜像 导入镜像 创建实例 OSS与导入镜像地域要一致,否则会报invalid错 启动front项目 使用yarn即可 会报找不到Python路径错误 要提前安装Python2.7 (与项目的构建相关 我也不懂为啥) 然后设置Python路径全局配置 npm config set python “D:\\Python27\\python.exe” # 你安装的路径 启动后端项目 确认Maven settings 需要将pom里自带的设置Maven的信息注释掉 出现找不到等报错 可以去找到目标目录删了重新加载 本地域名解析配置 C:\\Windows\\System32\\drivers\\etc 127.0.0.1 tl.nacos.com tlshopdb.com tlshop.com 本机启动nacos 在黑窗体环境下切换目录到nacos&#x2F;bin下 startup.cmd -m standalone 配置数据库 1jdbc:mysql://tlshopdb.com:3306/micromall? root 123456 导入数据 SQL yog 新建同名数据库导入 执行SQL脚本 选择.sql文件执行 即可 速度贼快 Linux目的 学习docker指令 linux 实战,熟练掌握linux常用指令 docker部署各种中间件的一些配置 学会在linux跑起来一个项目使用docker 探明项目的功能 已配好docker环境 MySQL安装(已经安装好了) 下载MySQL5.7的docker镜像： docker pull mysql:5.7 使用如下命令启动MySQL服务： docker run -p 3306:3306 –name mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql \\ -e MYSQL_ROOT_PASSWORD&#x3D;123456 \\ -d mysql:5.7 docker run -p 3306:3306 –name mysql -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql -e MYSQL_ROOT_PASSWORD&#x3D;123456 -d mysql:5.7 docker run -p 3306:3306 –name mysql 参数说明 -p 3306:3306：将容器的3306端口映射到主机的3306端口 -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql：将配置文件夹挂在到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql：将日志文件夹挂载到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;：将数据文件夹挂载到主机 -e MYSQL_ROOT_PASSWORD&#x3D;root：初始化root用户的密码 进入运行MySQL的docker容器： docker exec -it mysql &#x2F;bin&#x2F;bash 使用MySQL命令打开客户端： mysql -uroot -proot –default-character-set&#x3D;utf8 创建mall数据库： create database mall character set utf8 安装上传下载插件，并将document&#x2F;sql&#x2F;mall.sql上传到Linux服务器上： yum -y install lrzsz 将mall.sql文件拷贝到mysql容器的&#x2F;目录下： docker cp &#x2F;mydata&#x2F;mall.sql mysql:&#x2F; 将sql文件导入到数据库： use mall; source &#x2F;mall.sql; 创建一个reader:123456帐号并修改权限，使得任何ip都能访问： grant all privileges on . to ‘reader’ @’%’ identified by ‘123456’; Redis安装 下载Redis5.0的docker镜像： docker pull redis:5 使用如下命令启动Redis服务： docker run -p 6379:6379 –name redis \\ -v &#x2F;mydata&#x2F;redis&#x2F;data:&#x2F;data \\ -d redis:5 redis-server –appendonly yes 进入Redis容器使用redis-cli命令进行连接： docker exec -it redis redis-cli ​ redis启动 redis-server &#x2F;usr&#x2F;local&#x2F;redis-5.0.2&#x2F;redis.conf redis-cli auth 123456 Nginx安装 下载Nginx1.10的docker镜像： docker pull nginx:1.10 先运行一次容器（为了拷贝配置文件）： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -d nginx:1.10 将容器内的配置文件拷贝到指定目录： docker container cp nginx:&#x2F;etc&#x2F;nginx &#x2F;mydata&#x2F;nginx&#x2F; 修改文件名称： mv nginx conf 终止并删除容器： docker stop nginx docker rm nginx 使用如下命令启动Nginx服务： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;conf:&#x2F;etc&#x2F;nginx \\ -d nginx:1.10 默认启动? 怎么查看启动的服务 RabbitMQ安装 下载rabbitmq3.7.15的docker镜像： docker pull rabbitmq:3.7.15 使用如下命令启动RabbitMQ服务： docker run -p 5672:5672 -p 15672:15672 –name rabbitmq -d rabbitmq:3.7.15 进入容器并开启管理功能： docker exec -it rabbitmq &#x2F;bin&#x2F;bash rabbitmq-plugins enable rabbitmq_management ​ 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;15672&#x2F;tcp –permanent firewall-cmd –reload 访问地址查看是否安装成功：http://192.168.3.101:15672 ​ 输入账号密码并登录：guest guest 创建帐号并设置其角色为管理员：mall mall ​ 创建一个新的虚拟host为：&#x2F;mall ​ 点击mall用户进入用户配置页面 ​ 给mall用户配置该虚拟host的权限 ​ Elasticsearch安装 下载Elasticsearch7.6.2的docker镜像： docker pull elasticsearch:7.6.2 修改虚拟内存区域大小，否则会因为过小而无法启动: sysctl -w vm.max_map_count&#x3D;262144 使用如下命令启动Elasticsearch服务： docker run -p 9200:9200 -p 9300:9300 –name elasticsearch \\ -e “discovery.type&#x3D;single-node” \\ -e “cluster.name&#x3D;elasticsearch” \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;plugins:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;plugins \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;data:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data \\ -d elasticsearch:7.6.2 启动时会发现&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data目录没有访问权限，只需要修改&#x2F;mydata&#x2F;elasticsearch&#x2F;data目录的权限，再重新启动即可； chmod 777 &#x2F;mydata&#x2F;elasticsearch&#x2F;data&#x2F; 安装中文分词器IKAnalyzer，并重新启动： docker exec -it elasticsearch &#x2F;bin&#x2F;bash #此命令需要在容器中运行 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip docker restart elasticsearch 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;9200&#x2F;tcp –permanent firewall-cmd –reload 访问会返回版本信息：http://192.168.3.101:9200 ​ Logstash安装 下载Logstash7.6.2的docker镜像： docker pull logstash:7.6.2 修改Logstash的配置文件logstash.conf中output节点下的Elasticsearch连接地址为es:9200，配置文件地址：&#x2F;document&#x2F;elk&#x2F;logstash.conf output { elasticsearch { hosts &#x3D;&gt; “es:9200” index &#x3D;&gt; “mall-%{type}-%{+YYYY.MM.dd}” } } 创建&#x2F;mydata&#x2F;logstash目录，并将Logstash的配置文件logstash.conf拷贝到该目录； mkdir &#x2F;mydata&#x2F;logstash 使用如下命令启动Logstash服务； docker run –name logstash -p 4560:4560 -p 4561:4561 -p 4562:4562 -p 4563:4563 \\ –link elasticsearch:es \\ -v &#x2F;mydata&#x2F;logstash&#x2F;logstash.conf:&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;pipeline&#x2F;logstash.conf \\ -d logstash:7.6.2 进入容器内部，安装json_lines插件。 logstash-plugin install logstash-codec-json_lines Kibana安装 下载Kibana7.6.2的docker镜像： docker pull kibana:7.6.2 使用如下命令启动Kibana服务： docker run –name kibana -p 5601:5601 \\ –link elasticsearch:es \\ -e “elasticsearch.hosts&#x3D;http://es:9200&quot; \\ -d kibana:7.6.2 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;5601&#x2F;tcp –permanent firewall-cmd –reload 访问地址进行测试：http://192.168.3.101:5601 ​ MongoDB安装 下载MongoDB4.2.5的docker镜像： docker pull mongo:4.2.5 使用docker命令启动： docker run -p 27017:27017 –name mongo \\ -v &#x2F;mydata&#x2F;mongo&#x2F;db:&#x2F;data&#x2F;db \\ -d mongo:4.2.5 Docker全部环境安装完成 所有下载镜像文件： REPOSITORY TAG IMAGE ID CREATED SIZE redis 5 071538dbbd71 2 weeks ago 98.3MB mongo 4.2.5 fddee5bccba3 3 months ago 388MB logstash 7.6.2 fa5b3b1e9757 4 months ago 813MB kibana 7.6.2 f70986bc5191 4 months ago 1.01GB elasticsearch 7.6.2 f29a1ee41030 4 months ago 791MB rabbitmq 3.7.15-management 6ffc11daa8d0 13 months ago 186MB mysql 5.7 7faa3c53e6d6 15 months ago 373MB registry 2 f32a97de94e1 17 months ago 25.8MB nginx 1.10 0346349a1a64 3 years ago 182MB java 8 d23bdf5b1b1b 3 years ago 643MB 所有运行在容器里面的应用： ​ SpringBoot应用部署构建所有Docker镜像并上传 修改项目根目录下的pom.xml中的docker.host属性： &lt;docker.host&gt;http://192.168.3.101:2375docker.host&gt; properties&gt; 如果项目根目录的pom.mxl中docker-maven-plugin的节点被注释掉了就打开注释，使项目在打包时直接构建Docker镜像； ​ 直接双击根项目mall的package命令可以一次性打包所有应用的Docker镜像； ​ REPOSITORY TAG IMAGE ID CREATED SIZE mall&#x2F;mall-portal 1.0-SNAPSHOT 70e0f76416a0 21 seconds ago 705MB mall&#x2F;mall-search 1.0-SNAPSHOT f3290bd1d0c7 41 seconds ago 725MB mall&#x2F;mall-admin 1.0-SNAPSHOT 26557b93a106 About a minute ago 705MB 部署mall-admin docker run -p 8080:8080 –name mall-admin \\ –link mysql:db \\ –link redis:redis \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;admin&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-admin:1.0-SNAPSHOT 注意：如果想使用Logstash收集日志的话，需要将应用容器连接到Logstsh，添加如下配置即可； –link logstash:logstash \\ 部署mall-search docker run -p 8081:8081 –name mall-search \\ –link elasticsearch:es \\ –link mysql:db \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;search&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-search:1.0-SNAPSHOT 部署mall-port docker run -p 8085:8085 –name mall-portal \\ –link mysql:db \\ –link redis:redis \\ –link mongo:mongo \\ –link rabbitmq:rabbit \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;portal&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-portal:1.0-SNAPSHOT 开启防火墙 firewall-cmd –zone&#x3D;public –add-port&#x3D;8080&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8081&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8085&#x2F;tcp –permanent firewall-cmd –reload 访问接口进行测试 mall-admin的api接口文档地址：http://192.168.3.101:8080/swagger-ui.html ​ mall-search的api接口文档地址：http://192.168.3.101:8081/swagger-ui.html ​ mall-portal的api接口文档地址：http://192.168.3.101:8085/swagger-ui.html ​ 总结psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD ​ docker run –name&#x3D;”容器新名字” 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p docker execdocker exec [OPTIONS] 容器名称 COMMAND [ARG…]OPTIONS说明： -d，以后台方式执行命令； -e，设置环境变量 -i，交互模式 -t，设置TTY -u，用户名或UID，例如myuser:myusergroup 通常COMMAND只能是一条语句，为了支持多个命令的执行，需要将多个命令连接起来交给Shell，docker exec命令的使用示例如下： sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject &amp;&amp; git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC} &amp;&amp; git checkout FETCH_HEAD”;sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject;git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC};git checkout FETCH_HEAD”; 注意：对于已经暂停或停止了的容器，无法执行docker exec命令，如下将抛出异常： docker pause myContainerdocker exec myContainer … options 作用 -d 在后台运行命令-i 即使没有附加也保持 STDIN 打开-t 设置TTY进入容器的 CLI 模式 -e 设置环境变量-w 需要执行命令的目录-u 指定访问容器的用户名备注：其实还有几个 options，但是目前还没用到，要用的时候再写吧 实际栗子执行 tomcat 容器的 startup.sh 脚本 docker exec -it tomcat7 startup.sh进入容器的 CLI 模式(最常用) docker exec -it tomcat7 bash执行普通命令 docker exec -it tomcat7 pwd 指定工作目录执行命令 docker exec -it -w &#x2F;usr tomcat7 pwd 以 root 用户身份进入容器(重点) docker exec -it -uroot jenkins1 bash linux入门：”&quot;的作用运行cellranger count，发现每行末尾有个\\，遂查了下\\的作用 123456cellranger count --id=XPBShm \\ --transcriptome=/home/rstudio/opt/refdata-gex-GRCh38-and-mm10-2020-A \\ --fastqs=/home/rstudio/data/rawx/xpbs \\ --sample=XPBS \\ --r1-length 26 \\ --r2-length 98 作用有2 1 作为转义符反斜线符号“ \\ ”在Bash中被解释为转义字符，用于去除一个单个字符的特殊意义，它保留了跟随在之后的字符的字面值，除了换行符（\\n,\\r）。 如果在反斜线之后一个换行字符立即出现，转义字符使行 得以继续，但是换行字符后必须紧跟命令，不能出现空格，遇到命令很长时使用反斜线很有效。 例一： 1234[linux@linux ~]$ echo $HOME/home/[linux@linux ~]$ echo \\$HOME$HOME 例子中，反斜线去除了“ $ ”字符的特殊意义，保留字面值，从而不输出home目录路径。 2. 作为换行符例二： 12345678910export PATH=\\/bin:\\/sbin:\\/usr/bin:\\/usr/sbin:\\/usr/local/bin:\\/apps/bin:\\/apps/tools:\\/apps/tslib/bin\\ 例子中，反斜线使行得以继续，命令可以正常输入。 例二（反） 12345678910export PATH=\\ /bin:\\ /sbin:\\ /usr/bin:\\ /usr/sbin:\\ /usr/local/bin:\\ /apps/bin:\\ /apps/tools:\\ /apps/tslib/bin\\ 例子中就会出现错误： &#x2F;bin:: bad variable name&#x2F;* &#x2F;bin：错误变量名 *&#x2F; 因为在”+换行符”之后必须紧跟命令，不能有空格。 笔记DevOps： DevOps的意思就是开发和运维不再是分开的两个团队，而是你中有我，我中有 你的一个团队。我们现在开发和运维已经是一个团队了，但是运维方面的知识和 经验还需要持续提高。 持续交付： 持续交付的意思就是在不影响用户使用服务的前提下频繁把新功能发布给用户使用，要做到 这点非常非常难。我们现在两周一个版本，每次上线之后都会给不同的用户造成不同程度的 影响。 容器化： 容器化的好处在于运维的时候不需要再关心每个服务所使用的技术栈了，每个服务都被无差 别地封装在容器里，可以被无差别地管理和维护，现在比较流行的工具是docker和k8s。 所以你也可以简单地把云原生理解为：云原生 &#x3D; 微服务 + DevOps + 持续交付 + 容器化 商品模块业务场景介绍： 商品模块业务详解 ​ ​ ​ 表的设计：打开游览器访问京东详细页问题： 商品这块的数据库如何更好的设计，商品详细页显示这么多信息，是一张表还是多张表更好了？ 这个问题到底是一张表还是多张表，我们判断依据是什么？我们判断商品详细页里面显示的这些信息他们的关系。通过他们的关系，我们才能知道到底是设计一张表还是多张表。 一张表： 如果是一张表存储所有数据的话，那么查询是非常方便的，这是其优点，但是你会发现存储的时候是不是很麻烦。不通类型不同大小不通商品等等都不一样，那这样的一张表设计起来实在是太复杂了。 多张表： 如果是多张表的话业务更加清晰，维护起来也更加方便，但是你会发现查询好像会非常的复杂，一个商品页面我们需要查很多的表和数据。 解决 我们正确的方式是根据不同的数据类型按不通的表进行存储 商品表的设计在设计这个表的时候，嗯，会画出它的实体关系图。嗯那你设计一张表的时候，你的查询十分方便，但是你的你设计这个表会很复杂，就是想要把各种各种情况，各种类型全部囊括进来，十分负责，所以选择按根据不同的类型。去跟显示不同的表进行存储。嗯商品有不同类型，就比如有吃的，穿的。还有其他用的。就可以根据分类来划分我们的礼品商品。嗯 需求分析为什么商品需要分类？ 我们知道商品是有不通类型的，比如有吃的 比如有穿的比如还有其他的用的。不通的商品用途不一样。我们一开始就可以按分类来进行划分我们的商品，这个就有点像我们去看论坛的分类是一样的。 第一个版本：商品+分类 ​ 问题：此时有什么问题？： 目前这个方案有什么问题了？我们慢慢发现一个问题，只有分类并不能适应所有的需求，比如nike鞋和nikeT恤，用户可能希望先看nike的所有商品，这个模型就不能满足。我们想在这个关系中，加入“品牌”概念 但是只有分类的时候并不能适应所有的需求，你只是比如说耐克鞋，耐克T恤，用户希望能先看耐克的所有商品，这个模型就不能满足，只要在加入品牌的概念。这样基本用户可以在通过分类或者品牌找到自己想要的商品。 第二个版本：商品+分类+品牌 ​ 这样基本用户可以在首页上通过分类或者品牌找到自己想要的商品，也可以直接查看热门的商品和新上架的商品。 问题：此时有什么问题？ 但是问题也来了，用户在进入分类后，展示在用户面前的是很多很多商品，用户希望再通过筛选查询出更接近他目标的商品？ 用户在进入分类之后展示在湖面前的是很多很多商品，用户希望通过筛选查询出更接近他目标的商品。其实就加入了属性。比如说图案呀！嗯，之类的。 ​ 加入属性： 于是优秀的产品设计师，设计出了类似这样的UI： ​ ​ 第三个版本：商品+分类+品牌+属性 用户可以通过这些筛选条件进一步缩小自己的目标范围，那么问题又来了，这样的产品需求排在程序员面前，怎么去实现它？经过分析，我们找出了一个方法，我们知道商品之间的属性可能存在着较大的差别，比如牛仔裤它有版型、腰型、裤长等属性；而电脑它有CPU、显卡等属性，各类商品的属性是不同的。再进一步想，休闲裤也版型、腰型、裤长等属性；台式电脑或者笔记本电脑都有CPU、显卡等属性。所以我们得出：一个分类对应若干属性，而一个属性，对应若干属性选项，而一个具体商品又对应若干属性选项（例如具体一条牛仔裤，他的裤长：7分，裤型：直筒）。有点绕，仔细品味一下。 通过分类与品牌查询到相关的商品，再根据属性嗯，在写。来进一步缩小范围，比如牛仔裤，它有版型，腰型，裤长等属性，而电脑有CPU，显卡的出现。一个分类对应若干属性来一个性对应若干属性选项。一个具体商品有对应活该属性决定。各分类对应若干属性，一个属性对应若干属性选项。一个具体商品有对应若干属性选项。月底一条牛仔裤，它的裤长裤型的。 ​ 从图上可以看出，分类和属性的关系（例如：“牛仔裤”分类下有裤型、裤长、版型等属性）、属性和属性选项的关系（例如：裤长属性有长款、九分裤、七分裤的选项）、商品和属性选项的关系（例如某条牛仔裤的裤长是7分裤）。至此，我们知道一个商品的分类、品牌以及它有什么属性和对应的属性值。那么通过筛选条件，自然就可以查询出指定的商品。这里特别说一句，价格也是属性，不要设想用商品表中的价格字段去做计算。这不利于查询也增加了复杂度，让商家编辑人员用属性来设置并保证他的正确性。 ​ ​ 这个页面展示商品的所有信息，按照之前的设计好像都可以满足。但是我们似乎感觉错过了什么，在图上右边我们发现该商品当前的颜色和尺寸，并且允许用户可以选择其他的颜色和尺寸。这给我们带来了疑惑，这里的“颜色”和“尺寸”是什么，一件商品的不同颜色不同尺寸是算一个商品还是多个商品。 ​ 为什么要加入规格： 第四个版本：商品+分类+品牌+属性+规格 经过思考后，我们发现我们混淆了两个概念——“商品”和“货品”。不同规格的货品作为独立的商品。比如一条裤子的有L尺寸、M尺寸、一个U盘有16G还是32G的，都是同样的货品，不同规格的商品。可以认为货品和商品是一对多的关系。弄清了这个概念，处理这个需求就容易多了，这里的“颜色”、“尺寸”我们就作为“规格”来处理，而红色、黑色；L号、M号我们视为规格的选项或者说规格值。一件货品对应若干规格，而具有某一规格值的货品就是商品。 spu：iphone12 sku：金色64 iphone12 ​ 好了，现在好像差不多了。基于这个模型可以满足基本的商品搜索、展示的需求。搜索引擎也可以根据这个模型数据生成对应的商品索引，达到准确搜索的目的。商品模块还会和其他模块一起协作，比如用户系统、订单系统、支付系统等。一般情况下我们会把商品业务独立出来做成“商品中心”的服务，集中处理商品查询、更新、发布等业务，支撑其他业务。 ​ \\ 嗯，总结商品表的设计，商品表的设计主要是根据呃，用户对商品进行搜索，并且商家对用户对商家对商品进行展示来设计的。首先你用户想搜索到商品它需要进行分类查询，就是会有衣服啊，电子商品啊这种。设计成一个分类，那么在分类之后你还嗯会显示态度很还是是太多，还是需要进行一些区分？嗯，就比如说你的需求是只查看特步的。鞋子啊，或者特步的特步鞋或者特步。特步的衣服，然后就是你需要加还有品牌的概念。然后用户再去根据分类还有品牌来缩小它的具体范围。但是这样还是有很多的，很多的区分就比如说你需要嗯，你是你需要看他的具体的属性就是如何一个牛仔裤它的裤长。裤长，腰宽这种属性，这个属性呢是跟它的分类挂钩的，比如说你的裤子它就是一个就能把它的属性选项给出来。然后 嗯，表的设计，表的设计主要是基于用户的搜索需求与商家的展示需求来进行设计。嗯比较重要的有五张表，然后其他还有一些。关联表啊，一些这种就这种关系表，嗯，比较重要的就是你首先要有一个商品表就是你对，但是你的商品会十分复杂，你就要把它拆分成分类，就是你一个啊，不对。商品货品 就是分类和品牌之后你可以添加，你可以根据商品的属性再进行搜索，就比如说你的CPU，CPU，CPU型号呀，机身内存呀这些对应，然后每一个，每一个这些属性要对应着每一个属性选项。嗯，属性之后呢你对应的属性选项就比如说16G呀，那机身内存16G这些。但是你在真正的我看看去 商品的搜索： ​ 搜索引擎elasticsearch 商品的展示： 三、商品模块展示技术难点商品详情页是展示商品详细信息的一个页面，承载在网站的大部分流量和订单的入口。京东商城目前有通用版、全球购、闪购、易车、惠买车、服装、拼购、今日抄底等许多套模板。各套模板的元数据是一样的，只是展示方式不一样。目前商品详情页个性化需求非常多，数据来源也是非常多的，而且许多基础服务做不了的都放我们这，因此我们需要一种架构能快速响应和优雅的解决这些需求问题。因此我们重新设计了商品详情页的架构，主要包括三部分：商品详情页系统、商品详情页统一服务系统和商品详情页动态服务系统；商品详情页系统负责静的部分，而统一服务负责动的部分，而动态服务负责给内网其他系统提供一些数据服务。 ​ 商品详情页前端结构 前端展示可以分为这么几个维度：商品维度(标题、图片、属性等)、主商品维度（商品介绍、规格参数）、分类维度、商家维度、店铺维度等；另外还有一些实时性要求比较高的如实时价格、实时促销、广告词、配送至、预售等是通过异步加载。 ​ ​ SPU： Standard Product Unit （标准化产品单元）,SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。 SKU： Stock keeping unit(库存量单位) SKU即库存进出计量的单位（买家购买、商家进货、供应商备货、工厂生产都是依据SKU进行的），在服装、鞋类商品中使用最多最普遍。 例如纺织品中一个SKU通常表示：规格、颜色、款式。SKU是物理上不可分割的最小存货单元。 单品页流量特点 热点少，各种爬虫、比价软件抓取。 静态处理 thymeleaf等模板引擎 架构方案的问题：问题一：我们知道数据新增分:增量和全量数据 如果后台的小二新增了很多的商品，那我们都要对这些商品进行静态化，但是现在有个问题。那这些数据如何同步了？这是一个新增商品同步的问题，那这个问题怎么解决比较好了？。 ​ 不同应用部署在不同服务器甚至在不同的机房不同的国家。 1、通过网络同步的方式 就是其中一台服务器静态化之后，然后把文件同步到其他应用服务器上去。比如我们的linux命令scp方式。这种方式虽然可行，但是我们发现问题还是蛮多的，有多少个节点就需要同步多少份，等于是商品的数量*服务器的应用数数。很显然这种办法不是最优的解决办法 如果上述办法无法解决，那我们就用另外的方案，同学们你们觉得还有其他的方案没有？ **2、定时任务:**可以在某个应用用一个定时任务，然后分别去执行数据库需要静态化的数据即可，可以解决上述1数据同步的问题，因为所有的任务都是在本机运行，就不需要数据同步了。但是也有一个问题。就是如何避免不通的机器跑的数据不要重复，也就是A和B定时任务都跑了一份商品。这个是这种方案需要解决的。（比较直观的就是上锁） 3、消息中间件：还有一种办法就是通过消息中间件来解决。订阅topic然后生成当前服务器静态化的页面。 问题二：我们的freemark它是数据要事先按我这个模板生产好的，那就是说一定你改了模板，如果要生效的话，需要重新在把数据取出来和我们这个模板进行匹配生产更多的的静态html文件。那这是一个比较大的问题 如果后台数据有变更呢?如何及时同步到其它服务端? 如果页面静态化了，我们搜索打开一个商品详细页，怎么知道要我需要的访问的静态页面？ 万一我们模板需要修改了怎么办？ 牵一发动全身。 ​ 3.1、压测测试 jmeter模板 ​ 1、换数据库 2、分库分表 3.2、后台 12345678910111213141516171819202122/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = portalProductDao.getProductInfo(id); if (null == productInfo) &#123; return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; return productInfo;&#125; 压测结果： 5000并发 ​ 后台优化：redis缓存： redis设置：RedisConifg》RedisOpsUtil 12345678910111213141516171819202122232425262728293031/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = null; //从缓存Redis里找 productInfo = redisOpsUtil.get(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, PmsProductParam.class); if(null!=productInfo)&#123; return productInfo; &#125; productInfo = portalProductDao.getProductInfo(id); if (null==productInfo) &#123; log.warn(&quot;没有查询到商品信息,id:&quot;+id); return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, productInfo, 3600, TimeUnit.SECONDS); return productInfo;&#125; 好处： 加入redis之后我们发现提高了可以把之前请求 数据库查询的商品都缓存到redis中，通过对redis的访问来减少对数据里的依赖，减少了依赖本质就是减少了磁盘IO。 问题：提高请求的吞吐量，除了减少磁盘IO，还有网络IO，我们可以发现，请求redis其实也会涉及到网络IO，我们所有的请求都要走xxx端口号。那有没有更好的优化思路了，来同学们你们鲜花在哪儿？ 读多写少有两种： 1、最终一致性方案： 设置超时时间来解决 ​ redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE+id,productInfo,360,TimeUnit.SECONDS); 2、实时一致性方案： ​ 课程讲到 交易canal binlog 两个问题(高并发)、压缩的问题》减少内存 现在有什么问题了？ 跟我们预期只set一次redis 是有出入，为何会这样子了？并发问题 当我第二次再去访问，此时此刻没有日志输出，说明全部走了缓存： 并发问题：并发编程》并发问题》锁的方式来实现 java并发 加锁方式（不适合》特殊》分布式） 分布式锁：redis、zookeeper QPS立马就提高了很多。 加入分布式锁: 1234567&lt;!--加入redisson--&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; setnx 缓存应用场景： 1、访问量大、QPS高、更新频率不是很高的业务 2、数据一致性要求不高 缓存和数据库双写一致性问题： 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。 答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。 我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 zk&gt;临时顺序节点》原子性 线程创建如果可以创建成功，是否第一个 拿到了锁 业务场景介绍： 熟悉秒杀系统的业务和技术核心点、以及流程等 正常电商流程: 秒杀场景演示： 完毕 活动和场次关系 秒杀活动表：sms_flash_promotion 秒杀场次表：sms_flash_promotion_session 场次商品关系表：sms_flash_promotion_product_relation 一个活动可以有多个场次，每个场次可以有多个商品进行秒-杀。 秒杀系统设计 分两部分内容；秒杀业务设计和秒杀技术实现。 3215186661 11秒杀链路优化三 图灵：楼兰 秒杀系统核心交易链路优化 三 今天要处理的问题：秒杀场景下如何进行限流。 今天要做的内容： 解决的问题：1、在秒杀页面，客户点击秒杀后，在前台弹出一个验证码，需要用户 输入验证码才能往后端发送请求，这样能够错开秒杀下单的时间。 2、通过验证码，对后台下单请求进行保护，防止刷单，即绕开前端，直接往后端发 送请求。 在秒杀页面开始秒杀后，客户点击秒杀按钮，要在前台弹出一个验证 码，需要用户输入验证码才能往后端发请求，这样能够错开秒杀下单时 间。 在我们的实现中，是要将memberId、producrId和验证码的值一起传入 后台，后台返回一个token。然后再根据这个tokne拼接一个后台秒杀地 址。这个token会存入到redis中。实际秒杀时，会增加一个判断，检测 这个token是不是在redis中存在。如果不存在，就是机器刷单 一、电商项目中秒杀的实现流程 1、在tmll-admin中添加秒杀活动，在秒杀活动中先设置活动的开始日期和结束日 期，然后添加商品。 这个秒杀活动信息会保存到mysql中。sms_flash_promotion_product_relation 表。 同时，添加商品后会将活动商品保存到ZK中。(路 径&#x2F;ZkLock&#x2F;load_db&#x2F;{productId})。然后，当访问到商城前端商品页时， http://lo calhost:8080&#x2F;#&#x2F;product&#x2F;{productId}，会检查Redis中的产品信息缓存。如果 Redis中没有产品信息，就会重建Redis缓存。key为product:detail:cache:{ProdId} 然后进入商城的单品页 http://localhost:8080/#/product/32 ， product.vue 那个”立即购买”的按钮就会变成”立即秒杀” 点击立即秒杀就会进入秒杀页。secKillDetail.vue 代码实现机制： 1、从Redis判断商品是否有秒杀活动。 一个商品要么就只能秒杀，要么就只能普通购买，这样是否合理？ 这就 是为什么要单独独立出一套秒杀服务集群。 2、发送后台请求申请验证码。后台返回验证码图片，并将验证码的计算结果保存 到Redis。 验证码的请求路径里header里的memeberId是怎么进去的。有什么用？ 生成验证码图片的这个请求要怎么防刷？ 3、保护后台请求接口。 输入验证码后，先验证输入的验证码结果，返回一个Token。这个Token会传 入到接下来的商品确认页面，同时会保存到Redis当中，表示当前用户有购买秒杀商 品的资格。有效期300秒，300秒内必须完成下单，否则就要重新申请秒杀资格。 在后续的下单过程中，需要传入这个Token才能正常下单。 验证码如果输入错误，是如何判断的？ 二、如何加强限流方案的安全性 了解整理流程后，要继续深入思考下我们这个限流方案的安全性。 1&gt; 针对验证码 针对验证码的安全性，可以加上之前的验证码内容。 1、我们做了这一套机制后，到底有多安全？ 下单请求依然是可以用机器人模拟 的。 用户ID是存在Cookie当中的，可以拿到。 图形验证码是随机的，那就总有可能产生容易被机器识别的验证码。 2、怎么加强验证码本身的安全性 这个问题也是必须要前后台配合来思考的，而不是单独靠前端或者后端能够解决 的。这个方案要如何设计？ 提高验证码安全性的措施：1、加干扰线或者干扰 点，2，将关键字符变形并且在图形上串到一起。3、增加更多的前端交互，行为 验证。 验证码的内容最好是一个比较复杂的题目，而不是简单的输入数字。这样可以有 效延长下单请求的时长，更好的分散请求峰值。 图形验证码可以篡改。可以用PostMan另外访问生成图形验证码的接口，这时 Redis里的值就被篡改了，不再是页面上看到的计算结果了。如何处理？1、增加 更多的判断因素，例如IP。2、前端签名，后端验证签名。 输入了验证码之后，存在Redis中的验证码要及时删除。同时生成一个Token， 代表当前用户有购买权限。这个Token有效期是非常短的。 针对验证机制的安全性，可以增加一些安全机制。 换一种验证码 我们动手来换一种复杂一点的验证码，HappyCaptcha 官网地址： https://gitee.c om&#x2F;ramostear&#x2F;Happy-Captcha 换的方式比较简单，首先在pom.xml中加入HappyCaptcha的依赖 com.ramostear Happy-Captcha 1.0.1 1 2 3 4 5 然后在OmsPortalOrderController中getVerifyCode方法，将生成验证码的部分修 改一下： 这样，前台的验证码就变成了一闪一闪的动画。并且是中文的加减法，更难破解。 可以看到整个HappyCaptcha的实现机制跟我们自己的实现机制是差不多的，也是 使用session来存储答案。 其他还有哪些更难以破解的验证码？ 2&gt; 针对下单请求 我们的实现机制是要求将token拼凑到请求路径上来。这跟把token作为参数传递有 什么区别？ 如果一个模拟程序需要使用机器来参与秒杀抢单，首先需要根据其他用户的请求来 分析获取下单路径。如果是同一个请求路径，只是带的参数不同，那机器完全可以 尝试用暴力破解的方式来尝试进行下单。如果碰巧传入了一个Redis中的token值， 那他就下单成功了。但是现在把参数隐藏到了请求路径当中，动态的请求路径对于 下单的机器来说，就比较难试探出请求的地址，这样就增加了他下单的难度。 三、电商整体的秒杀限流方案： try { &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;这个步骤就会完成生成图片并且往response发送的步骤。 HappyCaptcha.require(request,response).style(CaptchaStyle.ANIM) .type(CaptchaType.ARITHMETIC_ZH) .build().finish(); Object captcha &#x3D; request.getSession().getAttribute(“happycaptcha”); &#x2F;&#x2F;HappyCaptcha生成的验证码是String类型 int code &#x3D; Integer.parseInt(captcha.toString()); log.info(“验证码答案:{}”,captcha); redisOpsUtil.set(RedisKeyPrefixConst.MIAOSHA_VERIFY_CODE_PREFIX + memberId + “:” + productId ,code ,300 ,TimeUnit.SECONDS); &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码结束 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; return null; }catch(Exception e) { e.printStackTrace(); return CommonResult.failed(“秒杀失败”); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 我们天天都在说三高，高并发、高可用、高可扩展，那到底应该如何去落地一个三 高的设计方案？ 构建大并发、高性能、高可用系统中几种通用的优化思路，可以抽象总结为“4 要 1 不要”原则。也就是：数据要尽量少、请求数要尽量少、路径要尽量短、依赖要尽 量少，以及不要有单点。当然，这几点是你要努力的方向，具体操作时还是要密切 结合实际的场景和具体条件来进行。 针对秒杀这个场景，其实方案设计往往比技术细节更为重要。因为你可以想象，每 一个秒杀环节的经典问题，都意味着互联网的秒杀业务出现过大的问题，这都是实 打实买来的教训。发现了问题之后才会有针对性的方案设计。那现在，我们整体来 回顾下电商的秒杀限流方案。 错峰1：动静分离的本质是将包含浏览者信息的动态数据和不包含浏览者信息的静态 资源区分开。例如在商品单品页，商品信息是不包含浏览者信息的，这部分就可以 抽象出静态资源。而用户登录状态、cookie等这些动态数据也尽可能缓存起来，并 且使缓存能够离用户更近。 错峰2：秒杀答题的形式可以是多种多样的，目的是防止机器刷单，以及错开用户的 下单时长。在秒杀场景下，答题速度靠后的请求自然就没有库存了，也可以减少系 统的请求量。 错峰3：缓存的作用主要有两个，一是快速扣减库存，保护数据库流量，并且库存扣 减完成后，快速通知Nginx，屏蔽后续请求；二是提前识别热点数据，并且针对热 点数据提供优化处理。处理的方案主要是三个，一是优化，二是限制，三是隔离， 包括业务隔离、系统隔离、数据隔离。 错峰4：单独提供秒杀服务集群，有利于减少秒杀商品的超大流量对普通商品的性能 冲击，不要让1%的商品影响到另外的99%。 后台错峰：这一部分是我们实战课程的重点。之前monkey老师带大家在后端针对 秒杀场景做了非常多的设计与实战。我们这个图中每一个错峰点虽然在图上就是比 较简单的一个点，但是深入进去，每个地方要考虑的细节都还是非常多的，大家可 以回顾下之前的几节课，体会下如何在后端对秒杀服务做针对性的优化。 首先想到的是使用MQ进行削峰。但是实际上，后端需要考虑的三高问题也远不止 MQ削峰这一步。每一个环节都需要考虑后端组件是否能够承载得住。例如秒杀服务 集群，到底应该部署多大的集群？部署多少台机器呢？显然为了顶住秒杀的大流 量，秒杀集群就需要部署得非常大。但是，如果在大部分没有秒杀服务的时间内， 这个集群的资源就闲置得非常厉害。所以，虚拟化+云计算进行弹性部署也是非常重 要的。在我们的项目实战课后面就会由诸葛老师给大家带来k8s和云部署的实战课 程。 然后：在后端系统中，添加了Redis、MQ这样的一些中间产品。而这些产品集群本 身，也存在效率低下、服务崩溃的风险。这样也就给系统整体带来了更多的风险 点。那要怎么去屏蔽这些产品给系统带来的风险呢？大家可以思考一下，下一节课 将会由fox老师给大家进行系统降级方面的设计。 题外话 方案优先 &gt; 技术优先。学习技术的同时，都要增加对软件问题的思考，很多同学 技术学得很快，但是缺乏思考。秒杀这种超大并发场景下的限流问题，不是任何一 个技术或者任何一个步骤可以限制住的，需要一个完整全面的方案才能保证业务稳 定性。所以我们在开发过程中，不能只埋头于技术点，要站在更高的角度，整体来 理解解决方案，这样才能更深入的理解自己在做的事情，也才能真正来解决问题。 这才是高级程序员与普通程序员真正的区别。 例如针对前端验证问题，还有哪些优化方案？ 提前发Token。可以在秒杀前设置一个预约活动。 在活动中提前发放 token。例如一个秒杀活动有20W个商品，那就可以预先准备200W个 token。用户进行预约时，只发放200W个Token，其他人也能预约成 功，但是其实没有获得token，那后面的秒杀，直接通过这个token就可 以过滤掉一大部分人。相当于没有token的人都只预约了个寂寞。这也是 互联网常用的一个套路。 例如针对超卖问题，在之前的课程中，介绍了如何使用Redis分布式锁防 超卖。针对同一个商品ID，使用一把分布式锁，确实可以很快很方便的 处理超卖问题。但是如果同时进行秒杀的商品多了呢？像京东、淘宝一 场大型的秒杀活动，同时有成千上万个商品要进行秒杀，那就意味着同 一时间Redis上锁解锁的操作会要执行成千上万次，这对Redis的性能消 耗是相当巨大的，Redis就有可能升级成为新的性能瓶颈。这时该怎么 办？ 当然具体问题的解决方案从来不止一个，这里我们可以选择一种返璞归 真的方案，把秒杀超卖的问题从分布式降级到本地JVM中，来获取极限 性能。例如将秒杀服务接入配置中心，然后在秒杀服务开始前，由配置 中心给每个应用服务实例下发一个库存数量。然后每次下单，每个服务 器只管自己的库存数量，与其他应用服务器完全不进行库存同步，在各 自的内存里扣减库存，这样就不会有超卖的情况发生。减少了网络消 耗，性能也能够进一步提升。 这种方案可不可行呢？当然也会有一些问题需要去处理。有可能某给服 务器上的库存很快消耗完了，而其他的服务器上仍有库存。整个服务就 会表现为你抢不到商品，但是在你后面抢商品的人却能抢到商品。(你们 在参与秒杀时有没有过这样的经历？)但是这在秒杀这种场景下，完全是 可以接受的。另外，如果某一个应用服务器挂了，那给他分配的库存就 会丢失。这时候又要怎么办？其实也没必要再去设置什么复杂的逻辑， 大不了少卖一点出去。反正都是售罄了，全卖完了，和卖了99%，其实 没什么区别。这时只需要统计好订单的数量(可以通过MQ来统计，也可 以通过Redis统计)，等秒杀活动的30分钟等待支付期过去后，再将没卖 出去的库存重新丢回库存池，与没有付款而被取消的订单商品一起返场 售卖就可以了。这也是很多互联网公司目前采用的方案。 最后虽然我们是后台开发工程师，但是前端也必须要有所了解。今天我们关注的 这个问题，也不能只关注后端，需要前后端一起才能理解他的作用。","categories":[{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]}],"categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"2搜索","slug":"算法/2搜索","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"},{"name":"3数据结构","slug":"算法/3数据结构","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"6数学知识","slug":"算法/6数学知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"name":"7刷题笔记","slug":"算法/7刷题笔记","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/7%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"},{"name":"6基础知识","slug":"算法/6基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/categories/suibi/"},{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/categories/ES/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"},{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/categories/Net/"},{"name":"前端","slug":"前端","permalink":"https://gouguoqiang.github.io/categories/%E5%89%8D%E7%AB%AF/"},{"name":"Python","slug":"Python","permalink":"https://gouguoqiang.github.io/categories/Python/"},{"name":"云原生","slug":"云原生","permalink":"https://gouguoqiang.github.io/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"大数据","slug":"大数据","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"微服务","slug":"微服务","permalink":"https://gouguoqiang.github.io/categories/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/categories/JVM/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"位运算","slug":"位运算","permalink":"https://gouguoqiang.github.io/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"搜索","slug":"搜索","permalink":"https://gouguoqiang.github.io/tags/%E6%90%9C%E7%B4%A2/"},{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"拓扑排序","slug":"拓扑排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"name":"最小生成树","slug":"最小生成树","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"},{"name":"最短路","slug":"最短路","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E7%9F%AD%E8%B7%AF/"},{"name":"二分图","slug":"二分图","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86%E5%9B%BE/"},{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"力扣","slug":"力扣","permalink":"https://gouguoqiang.github.io/tags/%E5%8A%9B%E6%89%A3/"},{"name":"区间dp","slug":"区间dp","permalink":"https://gouguoqiang.github.io/tags/%E5%8C%BA%E9%97%B4dp/"},{"name":"状态机","slug":"状态机","permalink":"https://gouguoqiang.github.io/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/"},{"name":"方法论","slug":"方法论","permalink":"https://gouguoqiang.github.io/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"背包","slug":"背包","permalink":"https://gouguoqiang.github.io/tags/%E8%83%8C%E5%8C%85/"},{"name":"单链表","slug":"单链表","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"},{"name":"双链表","slug":"双链表","permalink":"https://gouguoqiang.github.io/tags/%E5%8F%8C%E9%93%BE%E8%A1%A8/"},{"name":"队列","slug":"队列","permalink":"https://gouguoqiang.github.io/tags/%E9%98%9F%E5%88%97/"},{"name":"单调栈","slug":"单调栈","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E8%B0%83%E6%A0%88/"},{"name":"leetCode","slug":"leetCode","permalink":"https://gouguoqiang.github.io/tags/leetCode/"},{"name":"kmp","slug":"kmp","permalink":"https://gouguoqiang.github.io/tags/kmp/"},{"name":"字典树","slug":"字典树","permalink":"https://gouguoqiang.github.io/tags/%E5%AD%97%E5%85%B8%E6%A0%91/"},{"name":"堆","slug":"堆","permalink":"https://gouguoqiang.github.io/tags/%E5%A0%86/"},{"name":"数学知识","slug":"数学知识","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"name":"容斥原理","slug":"容斥原理","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"},{"name":"python","slug":"python","permalink":"https://gouguoqiang.github.io/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"二叉树","slug":"二叉树","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"name":"杂记","slug":"杂记","permalink":"https://gouguoqiang.github.io/tags/%E6%9D%82%E8%AE%B0/"},{"name":"数学","slug":"数学","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6/"},{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/tags/suibi/"},{"name":"数据库","slug":"数据库","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/tags/ES/"},{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/tags/OS/"},{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/tags/Net/"},{"name":"Linux","slug":"Linux","permalink":"https://gouguoqiang.github.io/tags/Linux/"},{"name":"前端","slug":"前端","permalink":"https://gouguoqiang.github.io/tags/%E5%89%8D%E7%AB%AF/"},{"name":"实践","slug":"实践","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%9E%E8%B7%B5/"},{"name":"云原生","slug":"云原生","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"大数据","slug":"大数据","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Spring","slug":"Spring","permalink":"https://gouguoqiang.github.io/tags/Spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gouguoqiang.github.io/tags/Mybatis/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gouguoqiang.github.io/tags/SpringBoot/"},{"name":"微服务","slug":"微服务","permalink":"https://gouguoqiang.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/tags/JVM/"},{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]}