{"meta":{"title":"ggq","subtitle":"","description":"","author":"ggq","url":"https://gouguoqiang.github.io","root":"/"},"pages":[{"title":"about","date":"2022-10-10T02:10:55.000Z","updated":"2022-10-10T02:11:45.270Z","comments":true,"path":"about/index.html","permalink":"https://gouguoqiang.github.io/about/index.html","excerpt":"","text":""},{"title":"archives","date":"2022-10-10T01:30:43.000Z","updated":"2022-10-10T01:31:11.074Z","comments":true,"path":"archives/index.html","permalink":"https://gouguoqiang.github.io/archives/index.html","excerpt":"","text":""},{"title":"tags","date":"2022-10-10T05:33:05.000Z","updated":"2022-10-10T05:33:05.796Z","comments":true,"path":"tags/index.html","permalink":"https://gouguoqiang.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"算法/3数据结构/堆排序","date":"2022-10-17T11:43:13.031Z","updated":"2022-10-13T14:38:02.088Z","comments":true,"path":"2022/10/17/算法/3数据结构/堆排序/","link":"","permalink":"https://gouguoqiang.github.io/2022/10/17/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%A0%86%E6%8E%92%E5%BA%8F/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"算法/3数据结构/二叉树","date":"2022-10-17T11:43:13.029Z","updated":"2022-10-13T14:28:48.119Z","comments":true,"path":"2022/10/17/算法/3数据结构/二叉树/","link":"","permalink":"https://gouguoqiang.github.io/2022/10/17/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"","slug":"算法/3数据结构/trie","date":"2022-10-17T11:43:13.027Z","updated":"2022-10-14T03:13:38.711Z","comments":true,"path":"2022/10/17/算法/3数据结构/trie/","link":"","permalink":"https://gouguoqiang.github.io/2022/10/17/%E7%AE%97%E6%B3%95/3%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/trie/","excerpt":"","text":"Trie字符串统计维护一个字符串集合，支持两种操作： I x 向集合中插入一个字符串 xx； Q x 询问一个字符串在集合中出现了多少次。 共有 N 个操作，输入的字符串总长度不超过 105，字符串仅包含小写英文字母。 输入格式第一行包含整数 N，表示操作数。 接下来 N 行，每行包含一个操作指令，指令为 I x 或 Q x 中的一种。 输出格式对于每个询问指令 Q x，都要输出一个整数作为结果，表示 x 在集合中出现的次数。 每个结果占一行。 数据范围1≤N≤2∗1041≤N≤2∗104 输入样例：1234565I abcQ abcQ abI abQ ab 输出样例：123101 思路前缀树,字典要内置多叉树(用) 二维数组 N个结点 每个结点 26个分支, 对结点标记是否为单词尾部 实现insert字符串 : 代码一: java类写法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778//Trie() 初始化前缀树对象。void insert(String word) 向前缀树中插入字符串 word 。boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。来源：力扣（LeetCode）链接：https://leetcode.cn/problems/implement-trie-prefix-tree著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。class TrieNode&#123; TrieNode[] children; boolean isWord; public TrieNode()&#123; children = new TrieNode[26]; isWord = true; &#125; public TrieNode(boolean isWord)&#123; children = new TrieNode[26]; this.isWord = isWord; &#125;&#125;class Trie &#123; // 多叉树 // 附加是否单词结尾属性 TrieNode root; public Trie() &#123; root = new TrieNode(); &#125; public void insert(String word) &#123; int len = word.length(); TrieNode current = root; for(int i = 0 ; i &lt; len ; i++)&#123; int index = word.charAt(i)-97; if(current.children[index] == null)&#123; current.children[index] = new TrieNode(false); &#125; current = current.children[index]; &#125; current.isWord = true; &#125; public boolean search(String word) &#123; TrieNode p = find(word); if(p == null ) return false; return p.isWord; &#125; public boolean startsWith(String prefix) &#123; return find(prefix) !=null; &#125; public TrieNode find(String word)&#123; TrieNode current = root; int len = word.length(); for(int i = 0 ; i &lt; len ; i++)&#123; int index = word.charAt(i) - 97; if(current.children[index]!=null)&#123; current = current.children[index]; &#125;else&#123; return null; &#125; &#125; return current; &#125;&#125;/** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */","categories":[],"tags":[]},{"title":"基础知识-二分","slug":"算法/1基础知识/二分","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:10:43.881Z","comments":true,"path":"2022/09/02/算法/1基础知识/二分/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E4%BA%8C%E5%88%86/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"二分","slug":"二分","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86/"}]},{"title":"基础知识-排序","slug":"算法/1基础知识/排序","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:10:52.694Z","comments":true,"path":"2022/09/02/算法/1基础知识/排序/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/%E6%8E%92%E5%BA%8F/","excerpt":"","text":"排序快速排序第K个数均分纸牌有N堆纸牌，编号分别为 1,2,…,N 每堆上有若干张，但纸牌总数必为 N 的倍数。 可以在任一堆上取若干张纸牌，然后移动。 移牌规则为：在编号为 1 的堆上取的纸牌，只能移到编号为 2 的堆上；在编号为 N 的堆上取的纸牌，只能移到编号为 N−1 的堆上；其他堆上取的纸牌，可以移到相邻左边或右边的堆上。 现在要求找出一种移动方法，用最少的移动次数使每堆上纸牌数都一样多。 例如 N&#x3D;4，4 堆纸牌数分别为：(9,8,17,6) 移动 3 次可达到目的： 从第三堆取四张牌放入第四堆，各堆纸牌数量变为:(9,8,13,10) 从第三堆取三张牌放入第二堆，各堆纸牌数量变为:(9,11,10,10) 从第二堆取一张牌放入第一堆，各堆纸牌数量变为:(10,10,10,10) 输入格式第一行包含整数 N。 第二行包含 N 个整数，A1,A2,…,AN 表示各堆的纸牌数量。 输出格式输出使得所有堆的纸牌数量都相等所需的最少移动次数。 数据范围1≤N≤1001≤N≤100,1≤Ai≤100001≤Ai≤10000 输入样例：1249 8 17 6 输出样例：13 思路:模拟多进少补 处理每一个数 多进少补 代码:123456789int t = 0; for (int i = 1; i &lt;= n; i++) t += arr[i]; t /= n; for (int i = 1; i &lt;= n; i++) arr[i] -= t; int tnt = 0, ans = 0; for (int i = 1; i &lt;= n; i++) &#123; tnt += arr[i]; ans++; if(tnt == 0) ans--; &#125; 七夕祭七夕节因牛郎织女的传说而被扣上了「情人节」的帽子。 于是 TYVJ 今年举办了一次线下七夕祭。 Vani 同学今年成功邀请到了 cl 同学陪他来共度七夕，于是他们决定去 TYVJ 七夕祭游玩。 TYVJ 七夕祭和 11 区的夏祭的形式很像。 矩形的祭典会场由 N 排 M 列共计 N×M 个摊点组成。 虽然摊点种类繁多，不过 cl 只对其中的一部分摊点感兴趣，比如章鱼烧、苹果糖、棉花糖、射的屋……什么的。 Vani 预先联系了七夕祭的负责人 zhq，希望能够通过恰当地布置会场，使得各行中 cl 感兴趣的摊点个数一样多，并且各列中 cl 感兴趣的摊点数也一样多。 不过 zhq 告诉 Vani，摊点已经随意布置完毕了，如果想满足 cl 的要求，唯一的调整方式就是交换两个相邻的摊点。 两个摊点相邻，当且仅当他们处在同一行或者同一列的相邻位置上。 由于 zhq 率领的 TYVJ 开发小组成功地扭曲了空间，每一行或每一列的第一个位置和最后一个位置也算作相邻。 现在 Vani 想知道他的两个要求最多能满足多少个。 在此前提下，至少需要交换多少次摊点。 输入格式第一行包含三个整数 N 和 M 和 T，T 表示 cl 对多少个摊点感兴趣。 接下来 T 行，每行两个整数 x,y，表示 cl 对处在第 x行第 y 列的摊点感兴趣。 输出格式首先输出一个字符串。 如果能满足 Vani 的全部两个要求，输出 both； 如果通过调整只能使得各行中 cl 感兴趣的摊点数一样多，输出 row； 如果只能使各列中 cl 感兴趣的摊点数一样多，输出 column； 如果均不能满足，输出 impossible。 如果输出的字符串不是 impossible， 接下来输出最小交换次数，与字符串之间用一个空格隔开。 数据范围1≤N,M≤1000000≤T≤min(N∗M,100000),1≤x≤N,1≤y≤M 输入样例：123452 3 41 32 12 22 3 输出样例：1row 1 思路: 1 2 3 4 5 只交换相邻两个人的牌 需要交换多少次每个人都一样 设平均值为m &#x3D;3, 去一个通俗的就可以带入所有的 需要借: 1: m-1 2: 2-(m-1) &#x3D; 3-m, m - (3-m) &#x3D; 2m -3 3: 3 - (2m-3) &#x3D; 6 - 2m, m - (6-2m) &#x3D; 3m - 6 4: 4-(3m-6) &#x3D; 10 -3m, 4m - 10; 5: 5m - 15; 观察常数为前缀和, 如果不取首尾只能相邻 那就是前缀和的 每项绝对值相加 但是现在可以选取一个点做为分割点 转化为前缀和货仓选址问题(设置一个仓库位置 到其他所有点的总和距离最短,求中位数(要先排序)) 最少交换次数 排成一个环 取每个数减去中位数的新数组的前缀和 求出都减去平均值数组的前缀和做为新数组, 对新数组做为(“货仓选址”); 代码:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import java.util.*;public class Main&#123; static int N = 100005; static long[] row = new long[N], col = new long[N]; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int T = in.nextInt(); // 统计每行每列 for (int i = 1; i &lt;= T; i ++) &#123; int x = in.nextInt(); int y = in.nextInt(); row[x] ++; col[y] ++; &#125; for (int i = 1; i &lt;= n; i++) &#123; row[0] += row[i]; &#125; for (int i = 1; i &lt;= m; i++) &#123; col[0] += col[i]; &#125; if (T % n == 0 &amp;&amp; T % m == 0) &#123; long ans = get(row,n)+get(col,m); System.out.println(&quot;both&quot; + &quot; &quot; + ans); &#125; else if (T % n == 0) &#123; System.out.println(&quot;row&quot; + &quot; &quot; + get(row,n)); &#125; else if (T % m == 0) &#123; System.out.println(&quot;column&quot; + &quot; &quot; + get(col,m)); &#125; else &#123; System.out.println(&quot;impossible&quot;); &#125; &#125; public static long get(long[] a, int n) &#123; long avg = a[0] / n; long[] s = new long[N]; for (int i = 1; i &lt;= n; i++) &#123; a[i] -= avg; s[i] = s[i-1] + a[i]; &#125; long ans = 0; Arrays.sort(s,1,n); long mid = s[(1 + n) / 2]; for (int i = 1; i &lt;= n; i++) &#123; ans += Math.abs(s[i] - mid); &#125; return ans; &#125;&#125; 归并排序归并排序给定你一个长度为 n 的整数数列。 请你使用归并排序对这个数列按照从小到大进行排序。 并将排好序的数列按顺序输出。 输入格式输入共两行，第一行包含整数 n。 第二行包含 n 个整数（所有整数均在 1∼109范围内），表示整个数列。 输出格式输出共一行，包含 n 个整数，表示排好序的数列。 数据范围1≤n≤100000 输入样例：1253 1 2 4 5 输出样例：11 2 3 4 5 思路归并排序思路: 创建一个新数组 每次排好两边,再将两边合并 代码1234567891011121314151617181920212223242526272829303132import java.util.*;public class Main&#123; static int N = 100005; static int[] tmp = new int[N], q = new int[N]; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); for (int i = 1; i &lt;= n; i++) &#123; q[i] = in.nextInt(); &#125; mergeSort(q,1,n); for (int i = 1; i &lt;= n; i++) &#123; System.out.println(q[i]); &#125; &#125; public static void mergeSort(int[] q, int l, int r) &#123; if (l &gt;= r) return ; int mid = (l + r) / 2; mergeSort(q,l,mid); mergeSort(q,mid+1,r); int k = 0, i = l, j = mid + 1; while (i &lt;= mid &amp;&amp; j &lt;= r) &#123; if (q[i] &lt;= q[j]) tmp[k++] = q[i++]; else tmp[k++] = q[j++]; &#125; while (i &lt;= mid) tmp[k++] = q[i++]; while (j &lt;= r) tmp[k++] = q[j++]; for (i = l, j = 0; i &lt;= r; i++,j++) q[i] = tmp[j]; &#125;&#125; 逆序对数量给定一个长度为 n 的整数数列，请你计算数列中的逆序对的数量。 逆序对的定义如下：对于数列的第 i 个和第 j 个元素，如果满足 i&lt;j 且 a[i]&gt;a[j]，则其为一个逆序对；否则不是。 输入格式第一行包含整数 n，表示数列的长度。 第二行包含 n 个整数，表示整个数列。 输出格式输出一个整数，表示逆序对的个数。 数据范围1≤n≤100000，数列中的元素的取值范围 [1,109][1,109]。 输入样例：1262 3 4 5 6 1 输出样例：15 思路:分析左右两半部分，如果左半部分 q[i] 大于右半部分的 q[j]，那么从 i 到 mid 都可以和 j 组成逆序对，逆序对个数 res +&#x3D; mid - i + 1 代码:12345678910111213141516171819202122232425262728293031323334class Main&#123; static int N = 100010; static long res = 0; static int[] q = new int[N]; static int[] tmp = new int[N]; public static void main(String[] args) throws IOException&#123; BufferedReader br = new BufferedReader(new InputStreamReader(System.in)); int n = Integer.parseInt(br.readLine()); String[] s = br.readLine().split(&quot; &quot;); for(int i = 0; i &lt; n; i++)&#123; q[i] = Integer.parseInt(s[i]); &#125; mergeSort(0, n - 1); System.out.println(res); &#125; public static void mergeSort(int l, int r)&#123; if(l &gt;= r) return; int mid = l + r &gt;&gt; 1; mergeSort(l, mid); mergeSort(mid + 1, r); int k = 0, i = l, j = mid + 1; while(i &lt;= mid &amp;&amp; j &lt;= r)&#123; if(q[i] &lt;= q[j]) tmp[k++] = q[i++]; else &#123; res += mid - i + 1; tmp[k++] = q[j++]; &#125; &#125; while(i &lt;= mid) tmp[k++] = q[i++]; while(j &lt;= r) tmp[k++] = q[j++]; for(i = l, j = 0; i &lt;= r; i++, j++) q[i] = tmp[j]; &#125;&#125; 选择排序从头到尾找最小的跟第一个未排序交换 n方 插入排序在已排好序的中插入应该插的地方 12345678910111213int[] arr = &#123;1,3,5,6,2,1,23,5,6&#125;int n = arr.length;for (int i = 1; i &lt; n; i++) &#123; int t = arr[i]; int j = i - 1; for (; j &gt;= 0; j--) &#123; if (arr[j] &lt;= t) break; arr[j+1] = arr[j]; &#125; arr[j+1] = t; &#125; 希尔排序插入排序的优化 堆排序从最后一个根节点开始后层序遍历 上浮 为一个大顶堆 之后在交换第一个与最后一个为排序的","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"搜索","slug":"算法/2搜索/搜索","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:11:17.658Z","comments":true,"path":"2022/09/02/算法/2搜索/搜索/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/%E6%90%9C%E7%B4%A2/","excerpt":"","text":"BFSflood fill 洪水覆盖算法形成的池塘农夫约翰有一片 $N*M$ 的矩形土地。 最近，由于降雨的原因，部分土地被水淹没了。 现在用一个字符矩阵来表示他的土地。 每个单元格内，如果包含雨水，则用”W”表示，如果不含雨水，则用”.”表示。 现在，约翰想知道他的土地中形成了多少片池塘。 每组相连的积水单元格集合可以看作是一片池塘。 每个单元格视为与其上、下、左、右、左上、右上、左下、右下八个邻近单元格相连。 请你输出共有多少片池塘，即矩阵中共有多少片相连的”W”块。 输入格式第一行包含两个整数 $N$ 和 $M$。 接下来 $N$ 行，每行包含 $M$ 个字符，字符为”W”或”.”，用以表示矩形土地的积水状况，字符之间没有空格。 输出格式输出一个整数，表示池塘数目。 数据范围$1 \\le N,M \\le 1000$ 输入样例：123456789101110 12W........WW..WWW.....WWW....WW...WW..........WW..........W....W......W...W.W.....WW.W.W.W.....W..W.W......W...W.......W. 输出样例：13 山峰山谷 BFS最短路迷宫 最短路 给定一个 n×mn×m 的二维整数数组，用来表示一个迷宫，数组中只包含 00 或 11，其中 00 表示可以走的路，11 表示不可通过的墙壁。 最初，有一个人位于左上角 (1,1)(1,1) 处，已知该人每次可以向上、下、左、右任意一个方向移动一个位置。 请问，该人从左上角移动至右下角 (n,m)(n,m) 处，至少需要移动多少次。 数据保证 (1,1)(1,1) 处和 (n,m)(n,m) 处的数字为 00，且一定至少存在一条通路。 输入格式第一行包含两个整数 nn 和 mm。 接下来 nn 行，每行包含 mm 个整数（00 或 11），表示完整的二维数组迷宫。 输出格式输出一个整数，表示从左上角移动至右下角的最少移动次数。 数据范围1≤n,m≤1001≤n,m≤100 输入样例：1234565 50 1 0 0 00 1 0 1 00 0 0 0 00 1 1 1 00 0 0 1 0 输出样例：18 代码123456789101112131415161718192021222324252627282930313233343536373839import java.util.*;class Main&#123; static int n,m; static int[][] g; static int[][] d; // 起点到ij的最短距离 static int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); m = in.nextInT(); g = new int[n][m]; d = new int[n][m]; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; g[i][j] = in.nextInt(); d[i][j] = -1; &#125; &#125; bfs(); System.out(dp[n-1][m-1]); &#125; static public void bfs() &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); d[0][0] = 0; q.offer(new int[]&#123;0,0&#125;); while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int a = t[0] + dx[i], b = t[0] + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) continue; if (g[a][b] == 1 || d[a][b] != -1) continue; q.offer(new int[]&#123;a,b&#125;); d[a][b] = d[t[0]][t[1]] + 1; &#125; &#125; &#125;&#125; BFS记录路径12345678910111213int maze[5][5] = &#123;0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,&#125;; 它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 数据保证至少存在一条从左上角走到右下角的路径。 输入格式第一行包含整数 n。 接下来 nn 行，每行包含 nn 个整数 0 或 1，表示迷宫。 输出格式输出从左上角到右下角的最短路线，如果答案不唯一，输出任意一条路径均可。 按顺序，每行输出一个路径中经过的单元格的坐标，左上角坐标为 (0,0)(0,0)，右下角坐标为 (n−1,n−1)(n−1,n−1)。 数据范围0≤n≤10000≤n≤1000 输入样例：12345650 1 0 0 00 1 0 1 00 0 0 0 00 1 1 1 00 0 0 1 0 输出样例：1234567890 01 02 02 12 22 32 43 44 4 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.*;class Main&#123; static int n; static int[][] g; static int[][][] d; // 起点到ij的最短距离 static int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); g = new int[n][n]; d = new int[n][n][3]; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; g[i][j] = in.nextInt(); d[i][j][0] = -1; &#125; &#125; bfs(); int[] end = d[0][0]; while (true) &#123; System.out.println(end[1] + &quot; &quot; + end[2]); if (end[1] == n-1 &amp;&amp; end[2] == n-1) break; end = d[end[1]][end[2]]; &#125; &#125; static public void bfs() &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); d[n-1][n-1][0] = 0; q.offer(new int[]&#123;n-1,n-1&#125;); while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int a = t[0] + dx[i], b = t[1] + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= n) continue; if (g[a][b] == 1 || d[a][b][0] != -1) continue; q.offer(new int[]&#123;a,b&#125;); d[a][b][0] = d[t[0]][t[1]][0] + 1; d[a][b][1] = t[0]; d[a][b][2] = t[1]; &#125; &#125; &#125;&#125; 一维最短路模型抓住那头牛农夫知道一头牛的位置，想要抓住它。 农夫和牛都位于数轴上，农夫起始位于点 N，牛位于点 K。 农夫有两种移动方式： 从 X 移动到 X−1或 X+1，每次移动花费一分钟 从 X移动到 2∗X，每次移动花费一分钟 假设牛没有意识到农夫的行动，站在原地不动。 农夫最少要花多少时间才能抓住牛？ 输入格式共一行，包含两个整数N和K。 输出格式输出一个整数，表示抓到牛所花费的最少时间。 数据范围0≤N,K≤105 输入样例：15 17 输出样例：14 代码1234567891011121314151617181920212223242526272829303132333435363738// 想象成 0-(K)2*10的五次方个点 每个点有三种走法public class Main&#123; static int n, k,N; static int[] d; static int[] dx = &#123;-1,1&#125;; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); n = in.nextInt(); k = in.nextInt(); N = Math.max(k,n) * 2 + 10; d = new int[N]; Arrays.fill(d,-1); d[n] = 0; bfs(); System.out.println(d[k]); &#125; public static void bfs() &#123; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); q.offer(n); while (!q.isEmpty()) &#123; int cur = q.poll(); for (int i = 0; i &lt; 3; i++) &#123; int a = cur; if (i != 2) &#123; a += dx[i]; &#125;else &#123; a *= 2; &#125; if (a &lt; 0 || a &gt;= N ) continue; if (d[a] != -1) continue; d[a] = d[cur] + 1; q.offer(a); &#125; &#125; &#125;&#125; Leetcode200 岛屿数量123456789101112131415161718192021222324252627282930313233343536 boolean[][] st; int n; int m; int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,1,0,-1&#125;; public int numIslands(char[][] grid) &#123; n = grid.length; m = grid[0].length; st = new boolean[n][m]; int cnt = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; if (grid[i][j] == &#x27;1&#x27; &amp;&amp; !st[i][j]) &#123; bfs(grid,i,j); cnt++; &#125; &#125; &#125; return cnt; &#125; public void bfs(char[][] g, int a, int b ) &#123; Deque&lt;int[]&gt; q = new ArrayDeque&lt;&gt;(); q.offer(new int[]&#123;a,b&#125;); st[a][b] = true; while (!q.isEmpty()) &#123; int[] t = q.poll(); for (int i = 0; i &lt; 4; i++) &#123; int x = t[0] + dx[i], y = t[1] + dy[i]; if (x &gt;= n || x &lt; 0 || y &lt; 0 || y &gt;= m || st[x][y] || g[x][y] != &#x27;1&#x27;) continue; q.offer(new int[]&#123;x,y&#125;); st[x][y] = true; &#125; &#125; &#125;&#125; DFSdfs连通性迷宫一天Extense在森林里探险的时候不小心走入了一个迷宫，迷宫可以看成是由 n∗n 的格点组成，每个格点只有2种状态，.和#，前者表示可以通行后者表示不能通行。 同时当Extense处在某个格点时，他只能移动到东南西北(或者说上下左右)四个方向之一的相邻格点上，Extense想要从点A走到点B，问在不走出迷宫的情况下能不能办到。 如果起点或者终点有一个不能通行(为#)，则看成无法办到。 注意：A、B不一定是两个不同的点。 输入格式第1行是测试数据的组数 k，后面跟着 k 组输入。 每组测试数据的第1行是一个正整数 n，表示迷宫的规模是 n∗n的。 接下来是一个 n∗n的矩阵，矩阵中的元素为.或者#。 再接下来一行是 4 个整数 ha,la,hb,lb，描述 AA 处在第 ha 行, 第 la 列，BB 处在第 hb 行, 第 lb 列。 注意到 ha,la,hb,lb全部是从 0 开始计数的。 输出格式k行，每行输出对应一个输入。 能办到则输出“YES”，否则输出“NO”。 数据范围1≤n≤1001≤n≤100 输入样例：1234567891011121323.##..##..0 0 2 25.....###.#..#..###.....#.0 0 4 0 输出样例:12YESNO 代码123456789101112131415import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int T = in.nextInt(); while (T-- &gt; 0) &#123; int n = in.nextInt(); char[][] w = new char[][]; for(int i = 0; i &lt; n; i++) &#123; w[i] = in.nextChar(); &#125; &#125; &#125;&#125; 搜索顺序马走日 马在中国象棋以日字形规则移动。 请编写一段程序，给定 n∗mn∗m 大小的棋盘，以及马的初始位置 (x，y)(x，y)，要求不能重复经过棋盘上的同一个点，计算马可以有多少途径遍历棋盘上的所有点。 输入格式第一行为整数 TT，表示测试数据组数。 每一组测试数据包含一行，为四个整数，分别为棋盘的大小以及初始位置坐标 n,m,x,yn,m,x,y。 输出格式每组测试数据包含一行，为一个整数，表示马能遍历棋盘的途径总数，若无法遍历棋盘上的所有点则输出 0。 数据范围1≤T≤91≤T≤9,1≤m,n≤91≤m,n≤9,1≤n×m≤281≤n×m≤28,0≤x≤n−10≤x≤n−1,0≤y≤m−10≤y≤m−1 输入样例：1215 4 0 0 输出样例：132 代码1 Leetcode79 单词搜索123456789101112131415161718192021222324252627282930class Solution &#123; int[] dx = &#123;-1,0,1,0&#125;, dy = &#123;0,-1,0,1&#125;; int n, m; public boolean exist(char[][] board, String word) &#123; //dfs n = board.length; m = board[0].length; for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; m; j++) &#123; if (dfs(board,i,j,word,0)) return true; &#125; &#125; return false; &#125; public boolean dfs(char[][] board, int x, int y, String word, int u) &#123; if (board[x][y] != word.charAt(u)) return false; if (u == word.length()-1) return true; board[x][y] = &#x27;.&#x27;; for (int i = 0; i &lt; 4; i++) &#123; int a = x + dx[i], b = y + dy[i]; if (a &lt; 0 || a &gt;= n || b &lt; 0 || b &gt;= m) continue; if (dfs(board,a,b,word,u+1)) return true; &#125; board[x][y] = word.charAt(u); return false; &#125;&#125; 46 全排列 无重复数顺序一: 枚举每个位置放哪个数 顺序二: 枚举每个数放到那个位置上 12345678910111213141516171819202122232425262728293031//第一种搜索顺序class Solution &#123; int n; List&lt;List&lt;Integer&gt;&gt; ans; List&lt;Integer&gt; path; boolean[] st; public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; //dfs 顺序 n = nums.length; st = new boolean[n]; ans = new ArrayList&lt;&gt;(); path = new ArrayList&lt;&gt;(); dfs(nums,0); return ans; &#125; public void dfs(int[] nums, int u) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; for (int i = 0; i &lt; n; i++) &#123; if (!st[i]) &#123; st[i] = true; path.add(nums[i]); dfs(nums,u+1); path.remove(path.size()-1); st[i] = false; &#125; &#125; &#125;&#125; 47 有重复数 全排列 123456789101112131415161718192021222324252627282930313233class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; boolean[] st; public List&lt;List&lt;Integer&gt;&gt; permuteUnique(int[] nums) &#123; n = nums.length; st = new boolean[n]; ans = new ArrayList&lt;&gt;(); path = new ArrayList&lt;&gt;(n); Arrays.sort(nums); for (int i = 0; i &lt; n; i++) &#123; path.add(null); &#125; dfs(nums,0,0); return ans; &#125; public void dfs(int[] nums, int u, int start) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; for (int i = start; i &lt; n; i++) &#123; if (!st[i]) &#123; st[i] = true; path.set(i,nums[u]); dfs(nums,u+1,u+1 &lt; n &amp;&amp; nums[u] == nums[u+1] ? i+1 : 0); st[i] = false; &#125; &#125; &#125;&#125; 78 子集二进制枚举 1234567891011121314151617181920212223242526class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; //dfs 搜索顺序,枚举每个数选还是不选 n = nums.length; path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(nums,0); return ans; &#125; public void dfs(int[] nums, int u) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; int k = 1; for (int i = 0; i &lt;= k; i++) &#123; dfs(nums,u+1); path.add(nums[u]); &#125; for (int i = 0; i &lt;= k; i++) path.remove(path.size() - 1); &#125;&#125; 12345678910111213141516171819202122232425class Solution &#123; int n; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; //dfs 搜索顺序,枚举每个数选还是不选 n = nums.length; path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(nums,0); return ans; &#125; public void dfs(int[] nums, int u) &#123; if (u == n) &#123; ans.add(new ArrayList&lt;&gt;(path)); return ; &#125; dfs(nums,u+1); path.add(nums[u]); dfs(nums,u+1); path.remove(path.size() - 1); &#125;&#125; 90 子集II 先枚举每个数字,然后枚举每个数字选多少个 1234567891011121314151617181920212223242526272829class Solution &#123; List&lt;List&lt;Integer&gt;&gt; res; List&lt;Integer&gt; path; int n; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; res = new ArrayList&lt;&gt;(); path = new ArrayList&lt;&gt;(); n = nums.length; Arrays.sort(nums); dfs(nums,0); return res; &#125; private void dfs(int[] nums, int u)&#123; if (u == n) &#123; res.add(new ArrayList&lt;&gt;(path)); return ; &#125; int k = 0; while(u + k &lt; n &amp;&amp; nums[u+k] == nums[u]) k++; for (int i = 0; i &lt;= k; i++) &#123; dfs(nums,u+k); path.add(nums[u]); &#125; for (int i = 0; i &lt;= k; i++) path.remove(path.size()-1); &#125;&#125; 216 组合总和III12345678910111213141516171819202122232425262728293031class Solution &#123; List&lt;Integer&gt; path; List&lt;List&lt;Integer&gt;&gt; ans; public List&lt;List&lt;Integer&gt;&gt; combinationSum3(int k, int n) &#123; // 组合 搜索顺序: 依次枚举每个数 是否选 path = new ArrayList&lt;&gt;(); ans = new ArrayList&lt;&gt;(); dfs(1,k,n); return ans; &#125; public void dfs(int u, int k,int n) &#123; if (k == 0) &#123; if (n == 0) &#123; ans.add(new ArrayList&lt;&gt;(path)); &#125; return ; &#125; if (u == 10) return ; dfs(u+1,k,n); path.add(u); dfs(u+1,k-1,n-u); path.remove(path.size() - 1); &#125;&#125;// 依次枚举每个位置 可以选哪些数 提高A*DFS剪枝与优化双向DFS多源BFS双端队列广搜迭代加深IDA*","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"2搜索","slug":"算法/2搜索","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"搜索","slug":"搜索","permalink":"https://gouguoqiang.github.io/tags/%E6%90%9C%E7%B4%A2/"}]},{"title":"图论-Floyd","slug":"算法/4图论/Floyd","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:42.216Z","comments":true,"path":"2022/09/02/算法/4图论/Floyd/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/Floyd/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"Floyd","slug":"Floyd","permalink":"https://gouguoqiang.github.io/tags/Floyd/"}]},{"title":"图论-二分图","slug":"算法/4图论/二分图","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:00.136Z","comments":true,"path":"2022/09/02/算法/4图论/二分图/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E4%BA%8C%E5%88%86%E5%9B%BE/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"二分图","slug":"二分图","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86%E5%9B%BE/"}]},{"title":"图论-拓扑排序","slug":"算法/4图论/拓扑排序","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:12.570Z","comments":true,"path":"2022/09/02/算法/4图论/拓扑排序/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"拓扑排序","slug":"拓扑排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"}]},{"title":"图论-最小生成树","slug":"算法/4图论/最小生成树","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:31.873Z","comments":true,"path":"2022/09/02/算法/4图论/最小生成树/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"最小生成树","slug":"最小生成树","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"}]},{"title":"图论-最近公共祖先","slug":"算法/4图论/最近公共祖先","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:17.992Z","comments":true,"path":"2022/09/02/算法/4图论/最近公共祖先/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"最近公共祖先","slug":"最近公共祖先","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/"}]},{"title":"图论-单源最短路","slug":"算法/4图论/单源最短路","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:57.331Z","comments":true,"path":"2022/09/02/算法/4图论/单源最短路/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"单源最短路","slug":"单源最短路","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF/"}]},{"title":"图论-负环","slug":"算法/4图论/负环","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:09:06.968Z","comments":true,"path":"2022/09/02/算法/4图论/负环/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/%E8%B4%9F%E7%8E%AF/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"}],"tags":[{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"负环","slug":"负环","permalink":"https://gouguoqiang.github.io/tags/%E8%B4%9F%E7%8E%AF/"}]},{"title":"dp-区间dp","slug":"算法/5dp/区间DP","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:06:52.217Z","comments":true,"path":"2022/09/02/算法/5dp/区间DP/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E5%8C%BA%E9%97%B4DP/","excerpt":"","text":"区间DP模板 合并石子 设有 NN 堆石子排成一排，其编号为 1，2，3，…，N1，2，3，…，N。 每堆石子有一定的质量，可以用一个整数来描述，现在要将这 NN 堆石子合并成为一堆。 每次只能合并相邻的两堆，合并的代价为这两堆石子的质量之和，合并后与这两堆石子相邻的石子将和新堆相邻，合并时由于选择的顺序不同，合并的总代价也不相同。 例如有 44 堆石子分别为 1 3 5 2， 我们可以先合并 1、21、2 堆，代价为 44，得到 4 5 2， 又合并 1，21，2 堆，代价为 99，得到 9 2 ，再合并得到 1111，总代价为 4+9+11&#x3D;244+9+11&#x3D;24； 如果第二步是先合并 2，32，3 堆，则代价为 77，得到 4 7，最后一次合并代价为 1111，总代价为 4+7+11&#x3D;224+7+11&#x3D;22。 问题是：找出一种合理的方法，使总的代价最小，输出最小代价。 输入格式第一行一个数 NN 表示石子的堆数 NN。 第二行 NN 个数，表示每堆石子的质量(均不超过 10001000)。 输出格式输出一个整数，表示最小代价。 数据范围1≤N≤3001≤N≤300 输入样例：1241 3 5 2 输出样例：122 代码123456789101112131415161718192021222324252627282930import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int N = n + 10; int[] w = new int[N]; int[] s = new int[N]; int[][] f = new int[N][N]; for (int i = 1; i &lt;= n ; i++) &#123; w[i] = in.nextInt(); &#125; for (int i = 1; i &lt;= n; i++) s[i] = s[i-1] + w[i]; for (int len = 1; len &lt;= n; len++) &#123; for (int l == 1; i + len - 1 &lt;= n; l++) &#123; int r = i + len - 1; if (len == 1) f[l][r] = 0; else &#123; for (int k = l; k &lt; r; k++) &#123; f[l][r] = Math.max(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); &#125; &#125; &#125; &#125; System.out.println(f[1][n]) &#125;&#125; 环形石子合并 可以首尾n的四次方枚举缺口 优化 环形DP 123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.util.*;class Main&#123; static final int INF = Integer.MAX_VALUE / 2; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int N = 2 * n + 10; int[] w = new int[N]; int[] s = new int[N]; int[][] f = new int[N][N]; int[][] g = new int[N][N]; for (int i = 1; i &lt;= n ; i++) &#123; w[i] = in.nextInt(); w[i+n] = w[i]; &#125; for (int i = 1; i &lt;= 2 * n; i++) s[i] = s[i-1] + w[i]; for (int i = 0; i &lt; N; i++) &#123; Arrays.fill(f[i],-INF); Arrays.fill(g[i],INF); &#125; for (int len = 1; len &lt;= n; len++) &#123; for (int l == 1; i + len - 1 &lt;= 2 * n; l++) &#123; int r = i + len - 1; if (len == 1) f[l][r] = 0; else &#123; for (int k = l; k &lt; r; k++) &#123; f[l][r] = Math.max(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); g[l][r] = Math.min(f[l][r],f[l][k] + f[k+1][r] + s[r] - s[l-1]); &#125; &#125; &#125; &#125; int maxv = -INF, minv = INF; for (int i = 1; i &lt;= n; i++) &#123; maxv = Math.max(f[i][i+n-1]); minv = Math.min(g[i][i+n-1]); &#125; System.out.println(minv); System.out.println(maxv); &#125;&#125; leetcode最长回文子串1234567891011121314151617181920212223242526272829class Solution &#123; public String longestPalindrome(String s) &#123; int n = s.length(); int N = n + 10; boolean[][] f = new boolean[N][N]; int max = 0; int ll = 1; int rr = 1; for (int len = 1; len &lt;= n; len++) &#123; for (int l = 1; l + len - 1 &lt;= n; l++) &#123; int r = l + len -1; if (len == 1) f[l][r] = true; else if (len == 2) &#123; if (s.charAt(l-1) == s.charAt(r-1)) f[l][r] = true; &#125; else &#123; f[l][r] = f[l+1][r-1] &amp;&amp; s.charAt(l-1) == s.charAt(r-1); &#125; if (f[l][r] &amp;&amp; (r - l + 1 &gt; max)) &#123; max = r - l + 1; ll = l; rr = r; &#125; &#125; &#125; return s.substring(ll-1,rr); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"区间dp","slug":"区间dp","permalink":"https://gouguoqiang.github.io/tags/%E5%8C%BA%E9%97%B4dp/"}]},{"title":"dp-数字三角形模型","slug":"算法/5dp/数字三角形模型","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:07:07.353Z","comments":true,"path":"2022/09/02/算法/5dp/数字三角形模型/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E6%95%B0%E5%AD%97%E4%B8%89%E8%A7%92%E5%BD%A2%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"数字三角形模型组合取数","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"数字三角形模型","slug":"数字三角形模型","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%97%E4%B8%89%E8%A7%92%E5%BD%A2%E6%A8%A1%E5%9E%8B/"}]},{"title":"dp-最长上升子序列模型","slug":"算法/5dp/最长上升子序列模型","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:07:56.168Z","comments":true,"path":"2022/09/02/算法/5dp/最长上升子序列模型/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"最长上升子序列模型","slug":"最长上升子序列模型","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"}]},{"title":"dp-背包","slug":"算法/5dp/背包","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:06:32.958Z","comments":true,"path":"2022/09/02/算法/5dp/背包/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E8%83%8C%E5%8C%85/","excerpt":"","text":"模板 优化空间后 除了 完全背包可从前往后 ,其他都是从后往前 123for 物品 for 体积 for 决策 01背包12//不选第i个物品和 选一个最后一个物品dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w[i]] + v[i]); 板子题1234567输入 3 7071 10069 11 2 // 给定背包大小 m,给定 物品个数 n,w[i] v[i] 求背包能装物品的最大值 123456789101112131415import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int[] dp = new int[m+10]; for (int i =0; i &lt; n; i++) &#123; int w = in.nextInt(); int v = in.nextInt(); for (int j = m; j &gt;= w; j--) dp[j] = Math.max(dp[j],dp[j-w]+v); &#125; System.out.println(dp[m]); &#125;&#125; 装箱问题123456789101112输入 24 // 箱子容量 6 // 物品个数 8 // 物品体积 3 12 7 9 7 // 求任取若干 使箱子剩余空间最小 //思路 体积也看成价值 二维费用的背包问题有 NN 件物品和一个容量是 VV 的背包，背包能承受的最大重量是 MM。 每件物品只能用一次。体积是 vivi，重量是 mimi，价值是 wiwi。 求解将哪些物品装入背包，可使物品总体积不超过背包容量，总重量不超过背包可承受的最大重量，且价值总和最大。输出最大价值。 输入格式第一行三个整数，N,V,MN,V,M，用空格隔开，分别表示物品件数、背包容积和背包可承受的最大重量。 接下来有 NN 行，每行三个整数 vi,mi,wi, vi,mi,wi，用空格隔开，分别表示第 i 件物品的体积、重量和价值。 输出格式输出一个整数，表示最大价值。 数据范围0&lt;N≤10000&lt;N≤10000&lt;V,M≤1000&lt;V,M≤1000&lt;vi,mi≤1000&lt;vi,mi≤1000&lt;wi≤10000&lt;wi≤1000 输入样例123454 5 61 2 32 4 43 4 54 5 6 输出样例：18 代码:1f[i][j][k] = Math.max(dp[i-1][j][k],dp[i][j-w1][k-w2] + v) 1234567891011121314151617181920import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int W1 = in.nextInt(); int W2 = in.nextInt(); for (int i = 0; i &lt; n; i++) &#123; int w1 = in.nextInt(); int w2 = in.nextInt(); int v = nextInt(); for (int j = W1; j &gt;= 0; j--) &#123; for (int k = W2; k&gt;= 0; k--) &#123; dp[j][k] = Math.max(dp[i][k],dp[j-w1][k-w2]+v); &#125; &#125; &#125; System.out.println(dp[W1][W2]); &#125;&#125; 数字组合 给定 N 个正整数 A1,A2,…,AN ,从中选出若干个数，使它们的和为 M，求有多少种选择方案。 输入格式第一行包含两个整数 N 和 M。 第二行包含 N 个整数，表示 A1,A2,…,AN。 输出格式包含一个整数，表示可选方案数。 数据范围1≤N≤1001≤N≤100,1≤M≤100001≤M≤10000,1≤Ai≤10001≤Ai≤1000,答案保证在 int 范围内。 输入样例：124 41 1 2 2 输出样例：13 代码1import 完全背包123456//不选第i个物品和,选一个,选两个,选n个直到不能选dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w]+v,dp[i-1][j-2w]+2v...dp[i-1][j-nw] + nv);dp[i][j-w] = Math.max( dp[i-1][j-w] ,dp[i-1][j-2w]+v ...dp[i-1][j-nw] + (n-1)v);//最终dp[i][j] = Math.max(dp[i-1][j],dp[i][j-w[i]] + v[i]); 零钱兑换I给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins &#x3D; [1, 2, 5], amount &#x3D; 11输出：3解释：11 &#x3D; 5 + 5 + 1示例 2： 输入：coins &#x3D; [2], amount &#x3D; 3输出：-1示例 3： 输入：coins &#x3D; [1], amount &#x3D; 0输出：0 提示： 1 &lt;&#x3D; coins.length &lt;&#x3D; 121 &lt;&#x3D; coins[i] &lt;&#x3D; 231 - 10 &lt;&#x3D; amount &lt;&#x3D; 104 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/coin-change著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 123456789101112131415161718192021class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int coinChange(int[] coins, int amount) &#123; // dp[i] // 刚好装满的最少个数(价值为1) int m = amount; int n = coins.length; int[]dp = new int[amount+10]; Arrays.fill(dp,INF); dp[0] = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = coins[i]; j &lt;= m; j++) &#123; dp[j] = Math.min(dp[j],dp[j-coins[i]]+1); &#125; &#125; return dp[amount] == INF ? -1 : dp[amount]; &#125;&#125; 零钱兑换II买书&#x2F;货币系统&#x2F;零钱兑换II货币系统II多重背包 r &#x3D; j % v; 物品有限s[i]个 12345678910//不选第i个物品,选一个第i个物品,选两个,选到不能选(体积之内 || 个数之内)dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-w]+v,dp[i-1][j-2w]+2v...dp[i-1][j-nw] + nv);dp[i][j-w] = Math.max( dp[i-1][j-w] ,dp[i-1][j-2w]+v ...dp[i-1][j-nw] + (n-1)v,dp[i][j-(n+1)w] + v); //在多重背包里会多一项不能直接替换 (在j-w的体积基础上选s[i](n个),所以要-(n+1)w)//最简单的 for(int j = 1; j &lt;= V; j++)&#123; for(int k = 0; k &lt;= s &amp;&amp; j &gt;= k * v; k++)&#123; dp[i][j] = Math.max(dp[i][j], dp[i - 1][j - k * v] + k * w); &#125; &#125; 多重背包I有 N 种物品和一个容量是 V 的背包。 第 i 种物品最多有 si 件，每件体积是 vi，价值是 wi。 求解将哪些物品装入背包，可使物品体积总和不超过背包容量，且价值总和最大。输出最大价值。 输入格式第一行两个整数，N，V用空格隔开，分别表示物品种数和背包容积。 接下来有 NN行，每行三个整数 vi,wi,si，用空格隔开，分别表示第 i种物品的体积、价值和数量。 输出格式输出一个整数，表示最大价值。 数据范围0&lt;N,V≤1000&lt;N,V≤1000&lt;vi,wi,si≤1000&lt;vi,wi,si≤100 输入样例123454 5 // n m1 2 3 //体积、价值和数量2 4 13 4 34 5 2 输出样例：110 代码:12345678910111213import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner.nextInt(); int n = in.nextInt(); int m = in.nextInt(); int[] w = new int[n+10]; int[] v = new int[n+10]; int[] s = new int[n+10]; for (int i = 0; i &lt; n; i++) &#125;&#125; 混合背包板子题最优选法 背包问题求方案数 有 N件物品和一个容量是 V的背包。每件物品只能使用一次。 第 i 件物品的体积是 v_i，价值是 w_i。 求解将哪些物品装入背包，可使这些物品的总体积不超过背包容量，且总价值最大。 输出 最优选法的方案数。注意答案可能很大，请输出答案模 10^9 + 7 的结果。 输入格式第一行两个整数，N，V，用空格隔开，分别表示物品数量和背包容积。 接下来有 N 行，每行两个整数 v_i, w_i，用空格隔开，分别表示第 i 件物品的体积和价值。 输出格式输出一个整数，表示 方案数 模 10^9 + 7 的结果。 数据范围$0 \\lt N, V \\le 1000$$0\\lt v_i, w_i \\le 1000$ 输入样例123454 51 22 43 44 6 输出样例：12 代码1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.*;class Main&#123; static final int mod = (int)1e9 + 7; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); //恰好装满的最大价值 int[] f = new int[m+10]; Arrays.fill(f,-1010); // 最优解的方案数 int[] g = new int[m+10]; f[0] = 0; g[0] = 1; for (int i = 0; i &lt; n; i++) &#123; int w = in.nextInt(); int v = in.nextInt(); for (int j = m; j &gt;= w; j--) &#123; int maxv = Math.max(f[j],f[j-w] + v); int cnt = 0; if (maxv == f[j]) cnt += g[j]; if (maxv == f[j-w] + v) cnt += g[j-w]; g[j] = cnt % mod; f[j] = maxv; &#125; &#125; int res = 0; for (int i = 1; i &lt;= m; i++) &#123; res = Math.max(res,f[i]); &#125; int cnt = 0; for (int i = 1; i &lt;= m; i++) &#123; if (res == f[i]) cnt = (cnt + g[i]) % mod; &#125; System.out.println(cnt == 0 ? 1 : cnt); &#125; &#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"背包","slug":"背包","permalink":"https://gouguoqiang.github.io/tags/%E8%83%8C%E5%8C%85/"}]},{"title":"dp-状态机","slug":"算法/5dp/状态机","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:07:33.614Z","comments":true,"path":"2022/09/02/算法/5dp/状态机/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/5dp/%E7%8A%B6%E6%80%81%E6%9C%BA/","excerpt":"","text":"123456789import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); &#125;&#125; 状态机模型抢家劫舍为例 阿福是一名经验丰富的大盗。趁着月黑风高，阿福打算今晚洗劫一条街上的店铺。 这条街上一共有 N 家店铺，每家店中都有一些现金。 阿福事先调查得知，只有当他同时洗劫了两家相邻的店铺时，街上的报警系统才会启动，然后警察就会蜂拥而至。 作为一向谨慎作案的大盗，阿福不愿意冒着被警察追捕的风险行窃。 他想知道，在不惊动警察的情况下，他今晚最多可以得到多少现金？ 输入格式输入的第一行是一个整数 T，表示一共有 T 组数据。 接下来的每组数据，第一行是一个整数 N ，表示一共有 N 家店铺。 第二行是 N 个被空格分开的正整数，表示每一家店铺中的现金数量。 每家店铺中的现金数量均不超过1000。 输出格式对于每组数据，输出一行。 该行包含一个整数，表示阿福在不惊动警察的情况下可以得到的现金数量。 数据范围$1 \\le T \\le 50$,$1 \\le N \\le 10^5$ 输入样例：12345231 8 2410 7 6 14 输出样例：12824 样例解释对于第一组样例，阿福选择第2家店铺行窃，获得的现金数量为8。 对于第二组样例，阿福选择第1和4家店铺行窃，获得的现金数量为10+14&#x3D;24。 代码12345678910111213141516171819import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); for (int i = 0; i &lt; n; i++) &#123; int m = nextInt(); int[][] dp = new int[m+10][2]; // 0 不偷i家 for (int j = 1; j &lt;= m; j++) &#123; int v = in.nextInt(); dp[j][0] = Math.max(dp[j-1][0],dp[j-1][1]); dp[j][1] = Math.max(dp[j-1][0] + v,dp[j-1][1]); &#125; System.out(Math.max(dp[m][0],dp[m][1])); &#125; &#125;&#125; 1234567891011class Solution &#123; public int rob(int[] nums) &#123; int n = nums.length; int[][] dp = new int[n+10][2]; for (int i = 1; i &lt;= n; i++) &#123; dp[i][0] = Math.max(dp[i-1][0],dp[i-1][1]); dp[i][1] = Math.max(dp[i-1][1],dp[i-1][0] + nums[i-1]); &#125; return Math.max(dp[n][0],dp[n][1]); &#125;&#125; 股票状态机 股票买卖IV &#x2F; leetcode 188123456789101112131415161718192021222324252627class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int maxProfit(int k, int[] prices) &#123; int n = prices.length; int[][][] dp = new int[n+10][k+10][2]; // 买入算半次交易 卖出 为一次完整交易 // 入口为手中无货 for (int i = 0; i &lt; n+10; i++) &#123; for (int j = 0; j &lt; k+10; j++) &#123; dp[i][j][0] = -INF; dp[i][j][1] = -INF; &#125; &#125; for (int i = 0; i &lt; n; i++) dp[i][0][0] = 0; for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= k; j++) &#123; dp[i][j][1] = Math.max(dp[i-1][j][1],dp[i-1][j-1][0] - prices[i-1]); dp[i][j][0] = Math.max(dp[i-1][j][0],dp[i-1][j][1] + prices[i-1]); // 买入则交易次数+1 &#125; &#125; int res = 0; for (int i = 1; i &lt;= k; i++) res = Math.max(dp[n][i][0],res); return res; &#125;&#125; 股票买卖含冷冻期 &#x2F; 309 12345678910111213141516171819class Solution &#123; public final int INF = Integer.MAX_VALUE / 2; public int maxProfit(int[] prices) &#123; int n = prices.length; // 手中有货 手中无货的第 1天 手中无货的 &gt;=2天 // 1 0 2 int[][] dp = new int[n+10][3]; dp[0][1] = dp[0][0] = -INF; dp[0][2] = 0; for (int i = 1; i &lt;= n; i++) &#123; dp[i][1] = Math.max(dp[i-1][2] - prices[i-1],dp[i-1][1]); dp[i][2] = Math.max(dp[i-1][2],dp[i-1][0]); dp[i][0] = dp[i-1][1] + prices[i-1]; &#125; int res = 0; res = Math.max(dp[n][0],dp[n][2]); return res &lt; 0 ? 0 : res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"}],"tags":[{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"状态机","slug":"状态机","permalink":"https://gouguoqiang.github.io/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/"}]},{"title":"数学知识","slug":"算法/6数学知识/数学","date":"2022-09-02T03:51:56.000Z","updated":"2022-10-17T13:11:58.112Z","comments":true,"path":"2022/09/02/算法/6数学知识/数学/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/02/%E7%AE%97%E6%B3%95/6%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/%E6%95%B0%E5%AD%A6/","excerpt":"","text":"容斥原理&#x2F;&#x2F;代码 二进制枚举 选1个 选两个 组合 能被整除的数给定一个整数 n 和 m 个不同的质数 p1,p2,…,pm 请你求出 1∼n 中能被 p1,p2,…,pm 中的至少一个数整除的整数有多少个。 输入格式第一行包含整数 nn 和 mm。 第二行包含 mm 个质数。 输出格式输出一个整数，表示满足条件的整数的个数。 数据范围1≤m≤161≤m≤16,1≤n,pi≤1091≤n,pi≤109 输入样例：1210 22 3 输出样例：17 代码1234567891011121314151617181920212223242526272829303132333435import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int[] p = new int[m+10]; for (int i = 0; i &lt; m; i++) &#123; p[i] = in.nextInt(); &#125; //枚举每个质数 算集合个数 int res = 0; for (int i = 0; i &lt; 1 &lt;&lt; m; i++) &#123; int t = 1, cnt = 0; for (int j = 0; j &lt; m; j++) &#123; if (((i &gt;&gt; j) &amp; 1) != 0) &#123; cnt++; if ((long)t * p[j] &gt; n) &#123; t = -1; break; &#125; t *= p[j]; &#125; &#125; if (t != -1) &#123; if (cnt % 2 == 0) res -= n / t; else res += n / t; &#125; &#125; Sytem.out.println(res); &#125;&#125; 博弈论先手必胜 先手必败 集合 移棋子游戏有 N个节点的有向无环图，图中某些节点上有棋子，两名玩家交替移动棋子。 玩家每一步可将任意一颗棋子沿一条有向边移动到另一个点，无法移动者输掉游戏。 对于给定的图和棋子初始位置，双方都会采取最优的行动，询问先手必胜还是先手必败。 输入格式第一行，三个整数 N,M,K，N 表示图中节点总数，M 表示图中边的条数，K 表示棋子的个数。 接下来 M 行，每行两个整数 X,Y 表示有一条边从点 X 出发指向点 Y。 接下来一行， K 个空格间隔的整数，表示初始时，棋子所在的节点编号。 节点编号从 1 到 N。 输出格式若先手胜，输出 win，否则输出 lose。 数据范围1≤N≤20001≤N≤2000,1≤M≤60001≤M≤6000,1≤K≤N1≤K≤N 输入样例：123456789106 8 42 12 41 41 54 51 33 53 61 2 4 6 输出样例：1win 代码1234567891011import java.util.*;class Main&#123; public static void main(String[] args) &#123; Scanner in = new Scanner(System.in); int n = in.nextInt(); int m = in.nextInt(); int k = in.nextInt(); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"6基础知识","slug":"算法/6基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"数学知识","slug":"数学知识","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"name":"容斥原理","slug":"容斥原理","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"}]},{"title":"suibi","slug":"0suibi","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:47:45.184Z","comments":true,"path":"2022/09/01/0suibi/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/0suibi/","excerpt":"","text":"Java类型转换一定都要转换运算之后一定要转化 Java中 a+&#x3D;b和a&#x3D;a+b有什么区别？+&#x3D; 是java中的一个运算符，而不是两个，所以在运算时 会进行自动类型转换 Serializable接口为什么需要定义serialVersionUID常量验证加载的类与序列化对象是否兼容 序列化运行时使用一个称为 serialVersionUID 的版本号与每个可序列化类相关联，该序列号在反序列化过程中用于验证序列化对象的发送者和接收者是否为该对象加载了与序列化兼容的类。如果接收者加载的该对象的类的 serialVersionUID 与对应的发送者的类的版本号不同，则反序列化将会导致 InvalidClassException HashMap扩容初始化如果不指定则为null 添加之后初始化数组为16 每次put会检查元素总量超过阈值 0.75乘当前数组大小扩容为两倍 如果链表长度大于8 数组长度小于64则会进行数组扩容 两个都满足 则会变为红黑树 扩容方法是用一个新数组代替了原来的数组 将原数据迁移到新数组中使用尾插法 框架谈谈 spring IOCSpring提供了一个IOC容器 ,用于管理对象的创建与对象之间的依赖关系,有各种子接口和实现类以AnnotationApplicationContext为例 有三类后置处理器以提供干涉对象创建的过程 有一些具体的实现类来完成Spring的功能 比如各种Aware接口 生命周期接口 等等 Bean定义,BeanFactoy,Bean IOC容器作用 有准备对象信息阶段,内置了defaultListableBeanFactory 他的内部有各种Map作为各种信息的池,IOC容器来读取我们告诉他的信息生成Bean定义信息(有机会更改Bean信息的生成)存到map里以备后用,Sping内部有一些默认的Bean也会自动的加载进来 创建对象等等 对于mybatis中#和$绑定参数的区别总结＃{}将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号。如：order by #{id}，如果传入的值是111，那么解析成sql时的值为order by “111”， 如果传入的值是id，则解析成的sql为order by “id”。 ${}将传入的数据直接显示生成在sql中。如：order by ${id}，如果传入的值是111，那么解析成sql时的值为order by 111， 如果传入的值是id，则解析成的sql为order by id。 Spring Boot自动装配原理 从@SpringBootApplication说起,内含了@EnableAutoConfiguration注解,其内含了@import()引入了一个选择器 选择器导入组件SpringFactoriesLoader.loadFactoryNanme;去类路径下找META-INF&#x2F;spring.factory文件SPI机制找到@EnableAutoConfiguration全类名对应的所有配置的值 SprignBoot写好的全场景的自动配置类全部导入进来,配置类就是给容器放组件Bean SPI机制其实就是根据Servlet厂商（服务提供商）提供要求的一个接口， 在固定的目录（META-INF&#x2F;services）放上以接口全类名 为命名的文件， 文件中放入接口的实现的全类名，该类由我们自己实现，按照这种约定的方式（即SPI规范），服务提供商会调用文件中实现类的方法， 从而完成扩展。 SPI演示案例： 假设我们自己是服务提供商： 现在要求的一个接口 IUserDao 1.在固定的目录放上接口的文件名 ​ 2.文件中放入实现类（该实现类由你实现）： 一行一个实现类。 ​ 3.通过java.util.ServiceLoader提供的ServiceLoader就可以完成SPI的实现类加载 ​ public class App { public static void main(String[] args) { ServiceLoader daos &#x3D; ServiceLoader.load(IUserDao.class); for (IUserDao dao : daos) { dao.save(); } } Mysql谈谈MVCC Mysql在读已提交和可重复读隔离级别下都实现了MVCC机制 可重复读 实际上就是一个copyOrwrite 写的是副本数据,写完在提交,读的是旧数据,所以写的可能是脏写,所以加CAS 读操作会有一个版本号,写的时候必须要和持有的版本号一致, 这个读的是最新数据而不是快照数据用来做更新 读已提交读的是最新数据 查询事务需要事务吗 具体分析 (别的情况会更改数据) Undo日志版本链通俗: 每次修改都会记录一次undo日志,行记录有两个隐藏字段 一行数据被多个事务依次修改过后，在每个事务修改完后，Mysql会保留修改前的数据undo回滚日志,串联起来形成一个历史记录版本链 可重复读级别 快照读(生成当前事务的一致性视图)该视图在事务结束前不会变化 读已提交 则每次查询重新生成 DATA_TRX_ID 事务ID 记录最近更新这条行记录的事务 ID，大小为 6 个字节 DATA_ROLL_PTR 回滚指针 表示指向该行回滚段（rollback segment）的指针，大小为 7 个字节，InnoDB 便是通过这个指针找到之前版本的数据。该行记录上所有旧版本，在 undo 中都通过链表的形式组织。 图片解释 roll_point指向 insert undo(即一条delete id &#x3D; 1 的指令) 操作系统项目作者 | 张建飞 阿里巴巴高级技术专家 了解我的人都知道，我一直在致力于应用架构和代码复杂度的治理。 这两天在看零售通商品域的代码。面对零售通如此复杂的业务场景，如何在架构和代码层面进行应对，是一个新课题。针对该命题，我进行了比较细致的思考和研究。结合实际的业务场景，我沉淀了一套“如何写复杂业务代码”的方法论，在此分享给大家。 我相信，同样的方法论可以复制到大部分复杂业务场景。 一个复杂业务的处理过程业务背景简单的介绍下业务背景，零售通是给线下小店供货的 B2B 模式，我们希望通过数字化重构传统供应链渠道，提升供应链效率，为新零售助力。阿里在中间是一个平台角色，提供的是 Bsbc 中的 service 的功能。 商品力是零售通的核心所在，一个商品在零售通的生命周期如下图所示： 在上图中红框标识的是一个运营操作的“上架”动作，这是非常关键的业务操作。上架之后，商品就能在零售通上面对小店进行销售了。因为上架操作非常关键，所以也是商品域中最复杂的业务之一，涉及很多的数据校验和关联操作。 针对上架，一个简化的业务流程如下所示： 过程分解像这么复杂的业务，我想应该没有人会写在一个 service 方法中吧。一个类解决不了，那就分治吧。 说实话，能想到分而治之的工程师，已经做的不错了，至少比没有分治思维要好很多。我也见过复杂程度相当的业务，连分解都没有，就是一堆方法和类的堆砌。 不过，这里存在一个问题：即很多同学过度的依赖工具或是辅助手段来实现分解。比如在我们的商品域中，类似的分解手段至少有 3 套以上，有自制的流程引擎，有依赖于数据库配置的流程处理： 本质上来讲，这些辅助手段做的都是一个 pipeline 的处理流程，没有其它。因此，我建议此处最好保持 KISS（Keep It Simple and Stupid），即最好是什么工具都不要用，次之是用一个极简的 Pipeline 模式，最差是使用像流程引擎这样的重方法。 除非你的应用有极强的流程可视化和编排的诉求，否则我非常不推荐使用流程引擎等工具。第一，它会引入额外的复杂度，特别是那些需要持久化状态的流程引擎；第二，它会割裂代码，导致阅读代码的不顺畅。大胆断言一下，全天下估计 80% 对流程引擎的使用都是得不偿失的。 回到商品上架的问题，这里问题核心是工具吗？是设计模式带来的代码灵活性吗？显然不是，问题的核心应该是如何分解问题和抽象问题，知道金字塔原理的应该知道，此处，我们可以使用结构化分解将问题解构成一个有层级的金字塔结构： 按照这种分解写的代码，就像一本书，目录和内容清晰明了。 以商品上架为例，程序的入口是一个上架命令（OnSaleCommand）, 它由三个阶段（Phase）组成。 每个 Phase 又可以拆解成多个步骤（Step），以 OnSaleProcessPhase 为例，它是由一系列 Step 组成的： 看到了吗，这就是商品上架这个复杂业务的业务流程。需要流程引擎吗？不需要；需要设计模式支撑吗？也不需要。对于这种业务流程的表达，简单朴素的组合方法模式（Composed Method）是再合适不过的了。 因此，在做过程分解的时候，我建议工程师不要把太多精力放在工具上，放在设计模式带来的灵活性上。而是应该多花时间在对问题分析，结构化分解，最后通过合理的抽象，形成合适的阶段（Phase）和步骤（Step）上。 过程分解后的两个问题的确，使用过程分解之后的代码，已经比以前的代码更清晰、更容易维护了。不过，还有两个问题值得我们去关注一下： 1、领域知识被割裂肢解什么叫被肢解？因为我们到目前为止做的都是过程化拆解，导致没有一个聚合领域知识的地方。每个 Use Case 的代码只关心自己的处理流程，知识没有沉淀。 相同的业务逻辑会在多个 Use Case 中被重复实现，导致代码重复度高，即使有复用，最多也就是抽取一个 util，代码对业务语义的表达能力很弱，从而影响代码的可读性和可理解性。 2、代码的业务表达能力缺失试想下，在过程式的代码中，所做的事情无外乎就是取数据 – 做计算 – 存数据，在这种情况下，要如何通过代码显性化的表达我们的业务呢？ 说实话，很难做到，因为我们缺失了模型，以及模型之间的关系。脱离模型的业务表达，是缺少韵律和灵魂的。 举个例子，在上架过程中，有一个校验是检查库存的，其中对于组合品（CombineBackOffer）其库存的处理会和普通品不一样。原来的代码是这么写的： 然而，如果我们在系统中引入领域模型之后，其代码会简化为如下： 有没有发现，使用模型的表达要清晰易懂很多，而且也不需要做关于组合品的判断了，因为我们在系统中引入了更加贴近现实的对象模型（CombineBackOffer 继承 BackOffer），通过对象的多态可以消除我们代码中的大部分的 if-else。 过程分解+对象模型通过上面的案例，我们可以看到有过程分解要好于没有分解，过程分解+对象模型要好于仅仅是过程分解。对于商品上架这个 case，如果采用过程分解+对象模型的方式，最终我们会得到一个如下的系统结构： 写复杂业务的方法论通过上面案例的讲解，我想说，我已经交代了复杂业务代码要怎么写：即自上而下的结构化分解+自下而上的面向对象分析。 接下来，让我们把上面的案例进行进一步的提炼，形成一个可落地的方法论，从而可以泛化到更多的复杂业务场景。 上下结合所谓上下结合，是指我们要结合自上而下的过程分解和自下而上的对象建模，螺旋式的构建我们的应用系统。这是一个动态的过程，两个步骤可以交替进行、也可以同时进行。 这两个步骤是相辅相成的，上面的分析可以帮助我们更好的理清模型之间的关系，而下面的模型表达可以提升我们代码的复用度和业务语义表达能力。 其过程如下图所示： 使用这种上下结合的方式，我们就有可能在面对任何复杂的业务场景，都能写出干净整洁、易维护的代码。 能力下沉一般来说实践 DDD 有两个过程： 套概念阶段了解了一些 DDD 的概念，然后在代码中“使用”Aggregation Root，Bounded Context，Repository 等等这些概念。更进一步，也会使用一定的分层策略。然而这种做法一般对复杂度的治理并没有多大作用。 融会贯通阶段术语已经不再重要，理解 DDD 的本质是统一语言、边界划分和面向对象分析的方法。 大体上而言，我大概是在 1.7 的阶段，因为有一个问题一直在困扰我，就是哪些能力应该放在 Domain 层，是不是按照传统的做法，将所有的业务都收拢到 Domain 上，这样做合理吗？说实话，这个问题我一直没有想清楚。 因为在现实业务中，很多的功能都是用例特有的（Use case specific）的，如果“盲目”的使用 Domain 收拢业务并不见得能带来多大的益处。相反，这种收拢会导致 Domain 层的膨胀过厚，不够纯粹，反而会影响复用性和表达能力。 鉴于此，我最近的思考是我们应该采用能力下沉的策略。 所谓的能力下沉，是指我们不强求一次就能设计出 Domain 的能力，也不需要强制要求把所有的业务功能都放到 Domain 层，而是采用实用主义的态度，即只对那些需要在多个场景中需要被复用的能力进行抽象下沉，而不需要复用的，就暂时放在 App 层的 Use Case 里就好了。 注：Use Case 是《架构整洁之道》里面的术语，简单理解就是响应一个 Request 的处理过程。 通过实践，我发现这种循序渐进的能力下沉策略，应该是一种更符合实际、更敏捷的方法。因为我们承认模型不是一次性设计出来的，而是迭代演化出来的。**下沉的过程如下图所示，假设两个 use case 中，我们发现 uc1 的 step3 和 uc2 的 step1 有类似的功能，我们就可以考虑让其下沉到 Domain 层，从而增加代码的复用性。 指导下沉有两个关键指标：代码的复用性和内聚性。 复用性是告诉我们 When（什么时候该下沉了），即有重复代码的时候。内聚性是告诉我们 How（要下沉到哪里），功能有没有内聚到恰当的实体上，有没有放到合适的层次上（因为 Domain 层的能力也是有两个层次的，一个是 Domain Service 这是相对比较粗的粒度，另一个是 Domain 的 Model 这个是最细粒度的复用）。 比如，在我们的商品域，经常需要判断一个商品是不是最小单位，是不是中包商品。像这种能力就非常有必要直接挂载在 Model 上。 之前，因为老系统中没有领域模型，没有 CSPU 这个实体。你会发现像判断单品是否为最小单位的逻辑是以 StringUtils.equals(code, baseCode) 的形式散落在代码的各个角落。这种代码的可理解性是可想而知的，至少我在第一眼看到这个代码的时候，是完全不知道什么意思。 业务技术要怎么做写到这里，我想顺便回答一下很多业务技术同学的困惑，也是我之前的困惑：即业务技术到底是在做业务，还是做技术？业务技术的技术性体现在哪里？ 通过上面的案例，我们可以看到业务所面临的复杂性并不亚于底层技术，要想写好业务代码也不是一件容易的事情。业务技术和底层技术人员唯一的区别是他们所面临的问题域不一样。 业务技术面对的问题域变化更多、面对的人更加庞杂。而底层技术面对的问题域更加稳定、但对技术的要求更加深。比如，如果你需要去开发 Pandora，你就要对 Classloader 有更加深入的了解才行。 但是，不管是业务技术还是底层技术人员，有一些思维和能力都是共通的。比如，分解问题的能力，抽象思维，结构化思维等等。 用我的话说就是：“做不好业务开发的，也做不好技术底层开发，反之亦然。业务开发一点都不简单，只是我们很多人把它做“简单”了 因此，如果从变化的角度来看，业务技术的难度一点不逊色于底层技术，其面临的挑战甚至更大。因此，我想对广大的从事业务技术开发的同学说：沉下心来，夯实自己的基础技术能力、OO 能力、建模能力… 不断提升抽象思维、结构化思维、思辨思维… 持续学习精进，写好代码。我们可以在业务技术岗做的很“技术”","categories":[{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/categories/suibi/"}],"tags":[{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/tags/suibi/"}]},{"title":"MQ","slug":"13MQ","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:53:26.135Z","comments":true,"path":"2022/09/01/13MQ/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/13MQ/","excerpt":"","text":"0. 学习目标 能够说出什么是消息中间件 能够安装RabbitMQ 能够编写RabbitMQ的入门程序 能够说出RabbitMQ的5种模式特征 能够使用Spring整合RabbitMQ 1. 消息中间件概述1.1. 什么是消息中间件MQ全称为Message Queue，消息队列是应用程序和应用程序之间的通信方法。 为什么使用MQ 在项目中，可将一些无需即时返回且耗时的操作提取出来，进行异步处理，而这种异步处理的方式大大的节省了服务器的请求响应时间，从而提高了系统的吞吐量。 疑惑1:另起一个线程(是MQ吗)来处理,MQ的作用是将消息写到消息队列,减少了服务器将消息写入缓冲区的同步操作? 开发中消息队列通常有如下应用场景： 1、任务异步处理 将不需要同步处理的并且耗时长的操作由消息队列通知消息接收方进行异步处理。提高了应用程序的响应时间。 接疑惑1:另起的”线程”是一个消费方,来进行处理吗 解惑: 另一个消费端相当于另一个线程 来做主线程本该做的事 1.并发编程又叫多线程编程。 在程序中，往往有很多很耗时的工作，比如上传文件、下载文件、跟客户聊天需要长时间建立连接。这种时候，一个线程是服务不了多个用户的，会产生因为资源独占产生的等待问题。并发的实质是一个物理CPU(也可以多个物理CPU) 在若干道程序之间的多路复用，并发性是对有限物理资源强制行使多用户共享以提高效率（买票问题并发进行）。 并发当有多个线程在操作时,如果系统只有一个CPU,则它根本不可能真正同时进行一个以上的线程，它只能把CPU运行时间划分成若干个时间段,再将时间 段分配给各个线程执行，在一个时间段的线程代码运行时，其它线程处于挂起状。.这种方式我们称之为并发(Concurrent)。 2.“并行”指两个或两个以上事件或活动在同一时刻发生。在多道程序环境下，并行性使多个程序同一时刻可在不同CPU上同时执行。（hadoop集群就是并行计算的） 当系统有一个以上CPU时,则线程的操作有可能非并发。当一个CPU执行一个线程时，另一个CPU可以执行另一个线程，两个线程互不抢占CPU资源，可以同时进行，这种方式我们称之为并行(Parallel)。 并发和并行 并发和并行是即相似又有区别的两个概念，并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔内发生。在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。倘若在计算机系统中有多个处理机，则这些可以并发执行的程序便可被分配到多个处理机上，实现并行执行，即利用每个处理机来处理一个可并发执行的程序，这样，多个程序便可以同时执行。 3.串行、并行： 并行和串行指的是任务的执行方式。串行是指多个任务时，各个任务按顺序执行，完成一个之后才能进行下一个。并行指的是多个任务可以同时执行，异步是多个任务并行的前提条件。 4.同步、异步： 指的是能否开启新的线程。同步不能开启新的线程，异步可以。 异步：异步和同步是相对的，同步就是顺序执行，执行完一个再执行下一个，需要等待、协调运行。异步就是彼此独立,在等待某事件的过程中继续做自己的事，不需要等待这一事件完成后再工作。线程就是实现异步的一个方式。异步是让调用方法的主线程不需要同步等待另一线程的完成，从而可以让主线程干其它的事情。 异步和多线程并不是一个同等关系,异步是最终目的,多线程只是我们实现异步的一种手段。异步是当一个调用请求发送给被调用者,而调用者不用等待其结果的返回而可以做其它的事情。实现异步可以采用多线程技术或则交给另外的进程来处理。 5.多线程 多线程是程序设计的逻辑层概念，它是进程中并发运行的一段代码。多线程可以实现线程间的切换执行。 rabbitmq一个典型的使用场景就是异步处理 场景说明：用户注册后，向用户发注册邮件和注册短信。 我们在进行以上场景开发时，通常会使用两种方式实现。 串行的方式 在没有mq中间件之前，我们通常使用这种方式实现，实现起来很容易，比如先将用户信息插入数据库，然后发送成功注册的邮件、短信。以上三个任务完成后才会给用户响应，但我们应该都知道，对于邮件、短信，对于系统核心业务来说这都不是必须马上发送的，这样的实现方式无非会增加系统的响应时间，甚至给用户带来不好的体验。可以认为就是一个线程在串行执行三个任务。 并行的方式 将注册信息写入数据库成功后，发送注册邮件的同时，发送注册短信。以上三个任务完成后，返回给客户端。与串行的差别是，并行的方式可以提高处理的时间。可以认为是三个线程在同时处理不同的任务，并行执行。 有了以上的应用场景，我们可以联想到其他的应用场景，比如一个用户同时下单了多个商品，此时系统要向该用户及商家发送下单消息。这个场景主要分为两个任务：1.生成订单 2.向用户和服务商发送下单消息。而生成订单任务是系统的核心业务，发送消息的任务就可以异步执行，否则每次都要访问一次数据库插入一条消息记录，增加了下单操作的响应时间。 下面就是此应用场景的实战。 原文链接：https://blog.csdn.net/qq_41519304/article/details/120432790 2、应用程序解耦合 MQ相当于一个中介，生产方通过MQ与消费方交互，它将应用程序进行解耦合。 3、削峰填谷 如订单系统，在下单的时候就会往数据库写数据。但是数据库只能支撑每秒1000左右的并发写入，并发量再高就容易宕机。低峰期的时候并发也就100多个，但是在高峰期时候，并发量会突然激增到5000以上，这个时候数据库肯定卡死了。 消息被MQ保存起来了，然后系统就可以按照自己的消费能力来消费，比如每秒1000个数据，这样慢慢写入数据库，这样就不会卡死数据库了。 但是使用了MQ之后，限制消费消息的速度为1000，但是这样一来，高峰期产生的数据势必会被积压在MQ中，高峰就被“削”掉了。但是因为消息积压，在高峰期过后的一段时间内，消费消息的速度还是会维持在1000QPS，直到消费完积压的消息,这就叫做“填谷” 1.2. AMQP 和 JMSMQ是消息通信的模型；实现MQ的大致有两种主流方式：AMQP、JMS。 1.2.1. AMQPAMQP是一种协议，更准确的说是一种binary wire-level protocol（链接协议）。这是其和JMS的本质差别，AMQP不从API层进行限定，而是直接定义网络交换的数据格式。 1.2.2. JMSJMS即Java消息服务（JavaMessage Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 1.2.3. AMQP 与 JMS 区别 JMS是定义了统一的接口，来对消息操作进行统一；AMQP是通过规定协议来统一数据交互的格式 JMS限定了必须使用Java语言；AMQP只是协议，不规定实现方式，因此是跨语言的。 JMS规定了两种消息模式；而AMQP的消息模式更加丰富 1.3. 消息队列产品市场上常见的消息队列有如下： ActiveMQ：基于JMS ZeroMQ：基于C语言开发 RabbitMQ：基于AMQP协议，erlang语言开发，稳定性好 RocketMQ：基于JMS，阿里巴巴产品 Kafka：类似MQ的产品；分布式消息系统，高吞吐量 1.4. RabbitMQRabbitMQ是由erlang语言开发，基于AMQP（Advanced Message Queue 高级消息队列协议）协议实现的消息队列，它是一种应用程序之间的通信方法，消息队列在分布式系统开发中应用非常广泛。 RabbitMQ官方地址：http://www.rabbitmq.com/ RabbitMQ提供了6种模式：简单模式，work模式，Publish&#x2F;Subscribe发布与订阅模式，Routing路由模式，Topics主题模式，RPC远程调用模式（远程调用，不太算MQ；暂不作介绍）； 官网对应模式介绍：https://www.rabbitmq.com/getstarted.html 2. 安装及配置RabbitMQ详细查看 资料/软件/安装RabbitMQ.md 文档。 3. RabbitMQ入门3.1. 搭建示例工程3.1.1. 创建工程 3.1.2. 添加依赖往heima-rabbitmq的pom.xml文件中添加如下依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt; &lt;version&gt;5.6.0&lt;/version&gt;&lt;/dependency&gt; 3.2. 编写生产者编写消息生产者com.itheima.rabbitmq.simple.Producer 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.itheima.rabbitmq.simple;import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class Producer &#123; static final String QUEUE_NAME = &quot;simple_queue&quot;; public static void main(String[] args) throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(&quot;localhost&quot;); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(&quot;/itcast&quot;); //连接用户名；默认为guest connectionFactory.setUsername(&quot;heima&quot;); //连接密码；默认为guest connectionFactory.setPassword(&quot;heima&quot;); //创建连接 Connection connection = connectionFactory.newConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(QUEUE_NAME, true, false, false, null); // 要发送的信息 String message = &quot;你好；小兔子！&quot;; /** * 参数1：交换机名称，如果没有指定则使用默认Default Exchage * 参数2：路由key,简单模式可以传递队列名称 * 参数3：消息其它属性 * 参数4：消息内容 */ channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot;已发送消息：&quot; + message); // 关闭资源 channel.close(); connection.close(); &#125;&#125; 在执行上述的消息发送之后；可以登录rabbitMQ的管理控制台，可以发现队列和其消息： 3.3. 编写消费者抽取创建connection的工具类com.itheima.rabbitmq.util.ConnectionUtil； 123456789101112131415161718192021222324252627package com.itheima.rabbitmq.util;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class ConnectionUtil &#123; public static Connection getConnection() throws Exception &#123; //创建连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //主机地址;默认为 localhost connectionFactory.setHost(&quot;localhost&quot;); //连接端口;默认为 5672 connectionFactory.setPort(5672); //虚拟主机名称;默认为 / connectionFactory.setVirtualHost(&quot;/itcast&quot;); //连接用户名；默认为guest connectionFactory.setUsername(&quot;heima&quot;); //连接密码；默认为guest connectionFactory.setPassword(&quot;heima&quot;); //创建连接 return connectionFactory.newConnection(); &#125;&#125; 编写消息的消费者com.itheima.rabbitmq.simple.Consumer 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package com.itheima.rabbitmq.simple;import com.itheima.rabbitmq.util.ConnectionUtil;import com.rabbitmq.client.*;import java.io.IOException;public class Consumer &#123; public static void main(String[] args) throws Exception &#123; Connection connection = ConnectionUtil.getConnection(); // 创建频道 Channel channel = connection.createChannel(); // 声明（创建）队列 /** * 参数1：队列名称 * 参数2：是否定义持久化队列 * 参数3：是否独占本次连接 * 参数4：是否在不使用的时候自动删除队列 * 参数5：队列其它参数 */ channel.queueDeclare(Producer.QUEUE_NAME, true, false, false, null); //创建消费者；并设置消息处理 DefaultConsumer consumer = new DefaultConsumer(channel)&#123; @Override /** * consumerTag 消息者标签，在channel.basicConsume时候可以指定 * envelope 消息包的内容，可从中获取消息id，消息routingkey，交换机，消息和重传标志(收到消息失败后是否需要重新发送) * properties 属性信息 * body 消息 */ public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; //路由key System.out.println(&quot;路由key为：&quot; + envelope.getRoutingKey()); //交换机 System.out.println(&quot;交换机为：&quot; + envelope.getExchange()); //消息id System.out.println(&quot;消息id为：&quot; + envelope.getDeliveryTag()); //收到的消息 System.out.println(&quot;接收到的消息为：&quot; + new String(body, &quot;utf-8&quot;)); &#125; &#125;; //监听消息 /** * 参数1：队列名称 * 参数2：是否自动确认，设置为true为表示消息接收到自动向mq回复接收到了，mq接收到回复会删除消息，设置为false则需要手动确认 * 参数3：消息接收到后回调 */ channel.basicConsume(Producer.QUEUE_NAME, true, consumer); //不关闭资源，应该一直监听消息 //channel.close(); //connection.close(); &#125;&#125; 3.4. 小结上述的入门案例中中其实使用的是如下的简单模式： 在上图的模型中，有以下概念： P：生产者，也就是要发送消息的程序 C：消费者：消息的接受者，会一直等待消息到来。 queue：消息队列，图中红色部分。类似一个邮箱，可以缓存消息；生产者向其中投递消息，消费者从其中取出消息。 4. AMQP4.1. 相关概念介绍AMQP 一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。 AMQP是一个二进制协议，拥有一些现代化特点：多信道、协商式，异步，安全，扩平台，中立，高效。 RabbitMQ是AMQP协议的Erlang的实现。 概念 说明 连接Connection 一个网络连接，比如TCP&#x2F;IP套接字连接。 会话Session 端点之间的命名对话。在一个会话上下文中，保证“恰好传递一次”。 信道Channel 多路复用连接中的一条独立的双向数据流通道。为会话提供物理传输介质。 客户端Client AMQP连接或者会话的发起者。AMQP是非对称的，客户端生产和消费消息，服务器存储和路由这些消息。 服务节点Broker 消息中间件的服务节点；一般情况下可以将一个RabbitMQ Broker看作一台RabbitMQ 服务器。 端点 AMQP对话的任意一方。一个AMQP连接包括两个端点（一个是客户端，一个是服务器）。 消费者Consumer 一个从消息队列里请求消息的客户端程序。 生产者Producer 一个向交换机发布消息的客户端应用程序。 4.2. RabbitMQ运转流程在入门案例中： 生产者发送消息 生产者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker； 声明队列并设置属性；如是否排它，是否持久化，是否自动删除； 将路由键（空字符串）与队列绑定起来； 发送消息至RabbitMQ Broker； 关闭信道； 关闭连接； 消费者接收消息 消费者创建连接（Connection），开启一个信道（Channel），连接到RabbitMQ Broker 向Broker 请求消费相应队列中的消息，设置相应的回调函数； 等待Broker回应闭关投递响应队列中的消息，消费者接收消息； 确认（ack，自动确认）接收到的消息； RabbitMQ从队列中删除相应已经被确认的消息； 关闭信道； 关闭连接； 4.3. 生产者流转过程说明 客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start&#x2F;.Start-OK 、Connection.Tune&#x2F;.Tune-Ok ，Connection.Open&#x2F; .Open-Ok 这6 个命令的交互。 客户端调用connection.createChannel方法。此方法开启信道，其包装的channel.open命令发送给Broker,等待channel.basicPublish方法，对应的AMQP命令为Basic.Publish,这个命令包含了content Header 和content Body()。content Header 包含了消息体的属性，例如:投递模式，优先级等，content Body 包含了消息体本身。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 4.4. 消费者流转过程说明 消费者客户端与代理服务器Broker建立连接。会调用newConnection() 方法,这个方法会进一步封装Protocol Header 0-9-1 的报文头发送给Broker ，以此通知Broker 本次交互采用的是AMQPO-9-1 协议，紧接着Broker 返回Connection.Start 来建立连接，在连接的过程中涉及Connection.Start&#x2F;.Start-OK 、Connection.Tune&#x2F;.Tune-Ok ，Connection.Open&#x2F; .Open-Ok 这6 个命令的交互。 消费者客户端调用connection.createChannel方法。和生产者客户端一样，协议涉及Channel . Open&#x2F;Open-Ok命令。 在真正消费之前，消费者客户端需要向Broker 发送Basic.Consume 命令(即调用channel.basicConsume 方法〉将Channel 置为接收模式，之后Broker 回执Basic . Consume - Ok 以告诉消费者客户端准备好消费消息。 Broker 向消费者客户端推送(Push) 消息，即Basic.Deliver 命令，这个命令和Basic.Publish 命令一样会携带Content Header 和Content Body。 消费者接收到消息并正确消费之后，向Broker 发送确认，即Basic.Ack 命令。 客户端发送完消息需要关闭资源时，涉及到Channel.Close和Channl.Close-Ok 与Connetion.Close和Connection.Close-Ok的命令交互。 RabbitMQ高级内容介绍RabbitMQ高级特性 消息可靠性投递Consumer ACK消费端限流TTL死信队列延迟队列日志与监控消息可靠性分析与追踪管理 RabbitMQ应用问题 消息可靠请保障消息幂等性处理 RabbitMQ集群搭建 RabbitMQ高可用集群 1、RabbitMQ高级特性1.1、消息的可靠投递1、定义在使用RabbitMQ的时候，作为消息发送方希望杜绝任何消息丢失或投递失败场景。RabbitMQ为我们提供了两种方式用来控制消息的投递可靠性模式。 confirm 确认模式return退回模式rabbitMQ整个消息投递的路径为：pruducer —&gt;rabbitMQ broker —-&gt;exchange——&gt;queue —-&gt; consumer 消息从producer到exchange则会返回一个confirmCallback。消息从exchange–&gt;queue投递失败则会返回一个returnCallback。我们将利用这两个callback控制消息的可靠性投递。 我们都知道，消息从生产端到消费端消费要经过3个步骤： 生产端发送消息到RabbitMQ； RabbitMQ发送消息到消费端； 消费端消费这条消息； 这3个步骤中的每一步都有可能导致消息丢失，消息丢失不可怕，可怕的是丢失了我们还不知道，所以要有一些措施来保证系统的可靠性。这里的可靠并不是一定就100%不丢失了，磁盘损坏，机房爆炸等等都能导致数据丢失，当然这种都是极小概率发生，能做到99.999999%消息不丢失，就是可靠的了。下面来具体分析一下问题以及解决方案。 生产端可靠性投递生产端可靠性投递，即生产端要确保将消息正确投递到RabbitMQ中。生产端投递的消息丢失的原因有很多，比如消息在网络传输的过程中发生网络故障消息丢失，或者消息投递到RabbitMQ时RabbitMQ挂了，那消息也可能丢失，而我们根本不知道发生了什么。针对以上情况，RabbitMQ本身提供了一些机制。 事务消息机制事务消息机制由于会严重降低性能，所以一般不采用这种方法，我就不介绍了，而采用另一种轻量级的解决方案——confirm消息确认机制。 confirm消息确认机制什么是confirm消息确认机制？顾名思义，就是生产端投递的消息一旦投递到RabbitMQ后，RabbitMQ就会发送一个确认消息给生产端，让生产端知道我已经收到消息了，否则这条消息就可能已经丢失了，需要生产端重新发送消息了。 通过下面这句代码来开启确认模式： 1channel.confirmSelect();// 开启发送方确认模式 然后异步监听确认和未确认的消息： 123456789101112131415channel.addConfirmListener(new ConfirmListener() &#123; //消息正确到达broker @Override public void handleAck(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println(&quot;已收到消息&quot;); //做一些其他处理 &#125; //RabbitMQ因为自身内部错误导致消息丢失，就会发送一条nack消息 @Override public void handleNack(long deliveryTag, boolean multiple) throws IOException &#123; System.out.println(&quot;未确认消息，标识：&quot; + deliveryTag); //做一些其他处理，比如消息重发等 &#125;&#125;); 这样就可以让生产端感知到消息是否投递到RabbitMQ中了，当然这样还不够，稍后我会说一下极端情况。 消息持久化那消息持久化呢？我们知道，RabbitMQ收到消息后将这个消息暂时存在了内存中，那这就会有个问题，如果RabbitMQ挂了，那重启后数据就丢失了，所以相关的数据应该持久化到硬盘中，这样就算RabbitMQ重启后也可以到硬盘中取数据恢复。那如何持久化呢？ message消息到达RabbitMQ后先是到exchange交换机中，然后路由给queue队列，最后发送给消费端。 所有需要给exchange、queue和message都进行持久化： exchange持久化： 12//第三个参数true表示这个exchange持久化channel.exchangeDeclare(EXCHANGE_NAME, &quot;direct&quot;, true); queue持久化： 12//第二个参数true表示这个queue持久化channel.queueDeclare(QUEUE_NAME, true, false, false, null); message持久化： 12//第三个参数MessageProperties.PERSISTENT_TEXT_PLAIN表示这条消息持久化channel.basicPublish(EXCHANGE_NAME, ROUTING_KEY, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(StandardCharsets.UTF_8)); 这样，如果RabbitMQ收到消息后挂了，重启后会自行恢复消息。 到此，RabbitMQ提供的几种机制都介绍完了，但这样还不足以保证消息可靠性投递RabbitMQ中，上面我也提到了会有极端情况，比如RabbitMQ收到消息还没来得及将消息持久化到硬盘时，RabbitMQ挂了，这样消息还是丢失了，或者RabbitMQ在发送确认消息给生产端的过程中，由于网络故障而导致生产端没有收到确认消息，这样生产端就不知道RabbitMQ到底有没有收到消息，就不好做接下来的处理。 所以除了RabbitMQ提供的一些机制外，我们自己也要做一些消息补偿机制，以应对一些极端情况。接下来我就介绍其中的一种解决方案——消息入库。 消息入库消息入库，顾名思义就是将要发送的消息保存到数据库中。 首先发送消息前先将消息保存到数据库中，有一个状态字段status&#x3D;0，表示生产端将消息发送给了RabbitMQ但还没收到确认；在生产端收到确认后将status设为1，表示RabbitMQ已收到消息。这里有可能会出现上面说的两种情况，所以生产端这边开一个定时器，定时检索消息表，将status&#x3D;0并且超过固定时间后（可能消息刚发出去还没来得及确认这边定时器刚好检索到这条status&#x3D;0的消息，所以给个时间）还没收到确认的消息取出重发（第二种情况下这里会造成消息重复，消费者端要做幂等性），可能重发还会失败，所以可以做一个最大重发次数，超过就做另外的处理。 这样消息就可以可靠性投递到RabbitMQ中了，而生产端也可以感知到了。 消费端消息不丢失既然已经可以让生产端100%可靠性投递到RabbitMQ了，那接下来就改看看消费端的了，如何让消费端不丢失消息。 默认情况下，以下3种情况会导致消息丢失： 在RabbitMQ将消息发出后，消费端还没接收到消息之前，发生网络故障，消费端与RabbitMQ断开连接，此时消息会丢失； 在RabbitMQ将消息发出后，消费端还没接收到消息之前，消费端挂了，此时消息会丢失； 消费端正确接收到消息，但在处理消息的过程中发生异常或宕机了，消息也会丢失。 其实，上述3中情况导致消息丢失归根结底是因为RabbitMQ的自动ack机制，即默认RabbitMQ在消息发出后就立即将这条消息删除，而不管消费端是否接收到，是否处理完，导致消费端消息丢失时RabbitMQ自己又没有这条消息了。 所以就需要将自动ack机制改为手动ack机制。 消费端手动确认消息： 1234567891011DeliverCallback deliverCallback = (consumerTag, delivery) -&gt; &#123; try &#123; //接收到消息，做处理 //手动确认 channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false); &#125; catch (Exception e) &#123; //出错处理，这里可以让消息重回队列重新发送或直接丢弃消息 &#125;&#125;;//第二个参数autoAck设为false表示关闭自动确认机制，需手动确认channel.basicConsume(QUEUE_NAME, false, deliverCallback, consumerTag -&gt; &#123;&#125;); 这样，当autoAck参数置为false，对于RabbitMQ服务端而言，队列中的消息分成了两个部分：一部分是等待投递给消费端的消息；一部分是已经投递给消费端，但是还没有收到消费端确认信号的消息。如果RabbitMQ一直没有收到消费端的确认信号，并且消费此消息的消费端已经断开连接或宕机（RabbitMQ会自己感知到），则RabbitMQ会安排该消息重新进入队列（放在队列头部），等待投递给下一个消费者，当然也有能还是原来的那个消费端，当然消费端也需要确保幂等性。 好了，到此从生产端到RabbitMQ再到消费端的全链路，就可以保证数据的不丢失。 简单实践官网get start 要先将RabbitMQ 已安装并运行在标准端口( 5672 )上的localhost上。如果您使用不同的主机、端口或凭据，则需要调整连接设置。 其他操作没有实际意义就不写了,主要是使用Java客户端与服务端进行连接 6.2 发送支付状态(1)集成RabbitMQ 修改支付微服务，集成RabbitMQ，添加如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 这里我们建议在后台手动创建队列，并绑定队列。如果使用程序创建队列，可以按照如下方式实现。 修改application.yml，配置支付队列和交换机信息，代码如下： 123456789#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order routing: key: queue.order 创建队列以及交换机并让队列和交换机绑定，修改com.changgou.WeixinPayApplication,添加如下代码： 1234567891011121314151617181920212223242526/*** * 创建DirectExchange交换机 * @return */@Beanpublic DirectExchange basicExchange()&#123; return new DirectExchange(env.getProperty(&quot;mq.pay.exchange.order&quot;), true,false);&#125;/*** * 创建队列 * @return */@Bean(name = &quot;queueOrder&quot;)public Queue queueOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.order&quot;), true);&#125;/**** * 队列绑定到交换机上 * @return */@Beanpublic Binding basicBinding()&#123; return BindingBuilder.bind(queueOrder()).to(basicExchange()).with(env.getProperty(&quot;mq.pay.routing.key&quot;));&#125; 6.2.2 发送MQ消息修改回调方法，在接到支付信息后，立即将支付信息发送给RabbitMQ，代码如下： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Value(&quot;$&#123;mq.pay.exchange.order&#125;&quot;)private String exchange;@Value(&quot;$&#123;mq.pay.queue.order&#125;&quot;)private String queue;@Value(&quot;$&#123;mq.pay.routing.key&#125;&quot;)private String routing;@Autowiredprivate WeixinPayService weixinPayService;@Autowiredprivate RabbitTemplate rabbitTemplate;/*** * 支付回调 * @param request * @return */@RequestMapping(value = &quot;/notify/url&quot;)public String notifyUrl(HttpServletRequest request)&#123; InputStream inStream; try &#123; //读取支付回调数据 inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); // 将支付回调数据转换成xml字符串 String result = new String(outSteam.toByteArray(), &quot;utf-8&quot;); //将xml字符串转换成Map结构 Map&lt;String, String&gt; map = WXPayUtil.xmlToMap(result); //将消息发送给RabbitMQ rabbitTemplate.convertAndSend(exchange,routing, JSON.toJSONString(map)); //响应数据设置 Map respMap = new HashMap(); respMap.put(&quot;return_code&quot;,&quot;SUCCESS&quot;); respMap.put(&quot;return_msg&quot;,&quot;OK&quot;); return WXPayUtil.mapToXml(respMap); &#125; catch (Exception e) &#123; e.printStackTrace(); //记录错误日志 &#125; return null;&#125; 6.3 监听MQ消息处理订单在订单微服务中，我们需要监听MQ支付状态消息，并实现订单数据操作。 6.3.1 集成RabbitMQ在订单微服务中，先集成RabbitMQ，再监听队列消息。 在pom.xml中引入如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml中配置rabbitmq配置，代码如下： 在application.yml中配置队列名字，代码如下： 12345#位置支付交换机和队列mq: pay: queue: order: queue.order 6.3.2 监听消息修改订单在订单微服务于中创建com.changgou.order.consumer.OrderPayMessageListener，并在该类中consumeMessage方法，用于监听消息，并根据支付状态处理订单，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041@Component@RabbitListener(queues = &#123;&quot;$&#123;mq.pay.queue.order&#125;&quot;&#125;)public class OrderPayMessageListener &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private OrderService orderService; /*** * 接收消息 */ @RabbitHandler public void consumeMessage(String msg)&#123; //将数据转成Map Map&lt;String,String&gt; result = JSON.parseObject(msg,Map.class); //return_code=SUCCESS String return_code = result.get(&quot;return_code&quot;); //业务结果 String result_code = result.get(&quot;result_code&quot;); //业务结果 result_code=SUCCESS/FAIL，修改订单状态 if(return_code.equalsIgnoreCase(&quot;success&quot;) )&#123; //获取订单号 String outtradeno = result.get(&quot;out_trade_no&quot;); //业务结果 if(result_code.equalsIgnoreCase(&quot;success&quot;))&#123; if(outtradeno!=null)&#123; //修改订单状态 out_trade_no orderService.updateStatus(outtradeno,result.get(&quot;transaction_id&quot;)); &#125; &#125;else&#123; //订单删除 orderService.deleteOrder(outtradeno); &#125; &#125; &#125;&#125; 7 定时处理订单状态(学员完成)7.1 业务分析在现实场景中，可能会出现这么种情况，就是用户支付后，有可能畅购服务网络不通或者服务器挂了，此时会导致回调地址无法接收到用户支付状态，这时候我们需要取微信服务器查询。所以我们之前订单信息的ID存入到了Redis队列，主要用于解决这种网络不可达造成支付状态无法回调获取的问题。 实现思路如下： 123451.每次下单，都将订单存入到Reids List队列中2.定时每5秒检查一次Redis 队列中是否有数据，如果有，则再去查询微信服务器支付状态3.如果已支付，则修改订单状态4.如果没有支付，是等待支付，则再将订单存入到Redis队列中，等会再次检查5.如果是支付失败，直接删除订单信息并修改订单状态 消息中间件&amp;RabbitMQ面试什么是RabbitMQ？RabbitMQ是一款开源的，Erlang编写的，基于AMQP协议的消息中间件 为什么使用MQ？MQ的优点 异步处理 - 相比于传统的串行、并行方式，提高了系统的吞吐量。 应用解耦 - 系统间通过消息通信，不用关心其他系统的处理。 流量削锋 - 可以通过消息队列长度控制请求量，可以缓解短时间内的高并发请求。 消息通讯 - 消息队列一般都内置了高效的通信机制，因此也可以用在纯消息通讯上。比如实现点对点消息队列，或者聊天室等。 日志处理 - 解决大量日志传输。(todo) @$你们公司生产环境用的是什么消息中间件？这个首先你可以说下你们公司选用的是什么消息中间件，比如用的是RabbitMQ，然后可以初步给一些你对不同MQ中间件技术的选型分析。 举个例子：比如说ActiveMQ是老牌的消息中间件，国内很多公司过去运用的还是非常广泛的，功能很强大。 但是问题在于ActiveMQ没法支撑互联网公司的高并发、高负载以及高吞吐的复杂场景，现在在国内互联网公司落地较少。而且使用较多的是一些传统企业，用ActiveMQ做异步调用和系统解耦。 然后你可以说说RabbitMQ，他的好处在于可以支撑高并发、高吞吐量、性能很高，同时有非常完善便捷的后台管理界面可以使用。 另外，他还支持集群化、高可用部署架构、消息高可靠支持，功能较为完善。 而且经过调研，国内各大互联网公司落地RabbitMQ集群支撑自身业务的case较多，国内各种中小型互联网公司使用RabbitMQ的实践也比较多。 除此之外，RabbitMQ的开源社区很活跃，较高频率的版本迭代，来修复发现的bug以及进行各种优化，因此综合考虑过后，公司采取了RabbitMQ。 但是RabbitMQ也有一点缺陷，就是他自身是基于erlang语言开发的，所以导致较为难以分析里面的源码，也较难进行深层次的源码定制和改造，需要较为扎实的erlang语言功底。 然后可以聊聊RocketMQ，是阿里开源的，经过阿里生产环境的超高并发、高吞吐的考验，性能卓越，同时还支持分布式事务等特殊场景。 而且RocketMQ是基于Java语言开发的，适合深入阅读源码，有需要可以站在源码层面解决线上问题，包括源码的二次开发和改造。 另外就是Kafka。Kafka提供的消息中间件的功能明显较少一些，相对上述几款MQ中间件要少很多。 但是Kafka的优势在于专为超高吞吐量的实时日志采集、实时数据同步、实时数据计算等场景。 因此Kafka在大数据领域中配合实时计算技术（比如Spark Streaming、Storm、Flink）使用的较多。但是在传统的MQ中间件使用场景中较少采用。 ActiveMQ、RabbitMQ、RocketMQ、Kafka有什么优缺点？ ActiveMQ RabbitMQ RocketMQ Kafka ZeroMQ 单机吞吐量 比RabbitMQ低 2.6w&#x2F;s（消息做持久化） 11.6w&#x2F;s 17.3w&#x2F;s 29w&#x2F;s 开发语言 Java Erlang Java Scala&#x2F;Java C 主要维护者 Apache Mozilla&#x2F;Spring Alibaba Apache iMatix，创始人已去世 成熟度 成熟 成熟 开源版本不够成熟 比较成熟 只有C、PHP等版本成熟 订阅形式 点对点(p2p)、广播（发布-订阅） 提供了4种：direct, topic ,Headers和fanout。fanout就是广播模式 基于topic&#x2F;messageTag以及按照消息类型、属性进行正则匹配的发布订阅模式 基于topic以及按照topic进行正则匹配的发布订阅模式 点对点(p2p) 持久化 支持少量堆积 支持少量堆积 支持大量堆积 支持大量堆积 不支持 顺序消息 不支持 不支持 支持 支持 不支持 性能稳定性 好 好 一般 较差 很好 集群方式 支持简单集群模式，比如’主-备’，对高级集群模式支持不好。 支持简单集群，’复制’模式，对高级集群模式支持不好。 常用 多对’Master-Slave’ 模式，开源版本需手动切换Slave变成Master 天然的‘Leader-Slave’无状态集群，每台服务器既是Master也是Slave 不支持 管理界面 一般 较好 一般 无 无 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 @$MQ 有哪些常见问题？如何解决这些问题？MQ 的常见问题有： 消息的顺序问题 消息的重复问题 消息的顺序问题 消息有序指的是可以按照消息的发送顺序来消费。 假如生产者产生了 2 条消息：M1、M2，假定 M1 发送到 S1，M2 发送到 S2，如果要保证 M1 先于 M2 被消费，怎么做？ 解决方案： 保证生产者 - MQServer - 消费者是一对一对一的关系 RabbitMQ：拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。 缺陷： 并行度就会成为消息系统的瓶颈（吞吐量不够） 更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。通过合理的设计或者将问题分解来规避。 不关注顺序的应用实际大量存在 队列无序并不意味着消息无序，所以从业务层面来保证消息的顺序而不仅仅是依赖于消息系统，是一种更合理的方式。 其他解决方案 方案一：消费端增加消息记录表，暂存不满足业务条件的消息，并采用定时器进行补偿处理，补偿超次进行预警；（该方案对技术营运友好，目前DMS正在使用，同样该方案可以用来解决重复消费问题） 方案二：消费端对不满足业务条件的消息不进行确认，多次消费失败进入死信队列，监听死信队列进行补偿，补偿超次或失败进行预警； 方案三：采用RocketMQ顺序消费机制；（不建议使用，会降低系统吞吐量） todo 消息记录表 消息的重复问题 造成消息重复的根本原因是：网络不可达。 所以解决这个问题的办法就是绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑需要保持幂等性。只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。保证每条消息都有唯一编号和添加一张日志表来记录已经处理成功的消息的 ID，如果新到的消息 ID 已经在日志表中，那么就不再处理这条消息。 @$消息积压怎么处理消息积压的原因 消息积压的直接原因，一定是系统中某个部分出现了性能问题，来不及处理上游发送的消息，才会导致消息积压。 如果日常系统正常运转的时候，没有积压或者只有少量积压很快就消费掉了，但是某一个时刻，突然就开始积压消息并且积压持续上涨。这种情况下需要你在短时间内找到消息积压的原因，迅速解决问题才不至于影响业务。 消息积压的处理 排查消息积压原因的方法：能导致积压突然增加，最粗粒度的原因，只有两种：要么是发送变快了，要么是消费变慢了。 大部分消息队列都内置了监控的功能，只要通过监控数据，很容易确定是哪种原因。如果是单位时间发送的消息增多，比如说是赶上大促或者抢购，短时间内不太可能优化消费端的代码来提升消费性能，唯一的方法是通过扩容消费端的实例数来提升总体的消费能力。如果短时间内没有足够的服务器资源进行扩容，没办法的办法是，将系统降级，通过关闭一些不重要的业务，减少发送方发送的数据量，最低限度让系统还能正常运转，服务一些重要业务。 还有一种不太常见的情况，你通过监控发现，无论是发送消息的速度还是消费消息的速度和原来都没什么变化，这时候你需要检查一下你的消费端，是不是消费失败导致的一条消息反复消费这种情况比较多，这种情况也会拖慢整个系统的消费速度。 如果监控到消费变慢了，你需要检查你的消费实例，分析一下是什么原因导致消费变慢。优先检查一下日志是否有大量的消费错误，如果没有错误的话，可以通过打印堆栈信息，看一下你的消费线程是不是卡在什么地方不动了，比如触发了死锁或者卡在等待某些资源上了。 @$如何保证RabbitMQ消息的可靠传输？消息丢失怎么办？消息不可靠的情况可能是消息丢失，劫持等原因； 丢失又分为：生产者丢失消息、消息列表丢失消息、消费者丢失消息； 生产者丢失消息：从生产者弄丢数据这个角度来看，RabbitMQ提供transaction和confirm模式来确保生产者不丢消息； transaction机制就是说：发送消息前，开启事务（channel.txSelect()），然后发送消息，如果发送过程中出现什么异常，事务就会回滚（channel.txRollback()），如果发送成功则提交事务（channel.txCommit()）。然而，这种方式有个缺点：吞吐量下降； confirm模式用的居多：一旦channel进入confirm模式，所有在该信道上发布的消息都将会被指派一个唯一的ID（从1开始），一旦消息被投递到所有匹配的队列之后，rabbitMQ就会发送一个ACK给生产者（包含消息的唯一ID），这就使得生产者知道消息已经正确到达目的队列了；如果rabbitMQ没能处理该消息，则会发送一个Nack消息给你，生产者可以进行重试操作。 消息队列丢数据：消息持久化。 处理消息队列丢数据的情况，一般是开启持久化磁盘的配置。 这个持久化配置可以和confirm机制配合使用，你可以在消息持久化磁盘后，再给生产者发送一个Ack信号。 这样，如果消息持久化磁盘之前，rabbitMQ阵亡了，那么生产者收不到Ack信号，生产者会自动重发。 那么如何持久化呢？ 这里顺便说一下吧，其实也很容易，就下面两步 将queue的持久化标识durable设置为true，则代表是一个持久的队列 发送消息的时候将deliveryMode&#x3D;2 这样设置以后，即使rabbitMQ挂了，重启后也能恢复数据 消费者丢失消息：消费者丢数据一般是因为采用了自动确认消息模式，改为手动确认消息即可！ 消费者在收到消息之后，处理消息之前，会自动回复RabbitMQ已收到消息； 如果这时处理消息失败，就会丢失该消息； 解决方案：处理消息成功后，手动回复确认消息。 todo 手动回复确认消息 @$RabbitMQ 常见工作模式和应用场景 交换机todo 一、简单模式原理：一个生产者，一个消费者。生产者将消息发送到队列，消费者监听消息队列，如果队列中有消息，就进行消费，消费后消息从队列中删除 场景：聊天；有一个oa系统，用户通过接收手机验证码进行注册，页面上点击获取验证码后，将验证码放到消息队列，然后短信服务从队列中获取到验证码，并发送给用户。 二、工作模式原理：一个生产者，多个消费者，一条消息只能被一个消费者消费。生产者将消息发送到消息队列，多个消费者同时监听一个队列，谁先抢到消息谁负责消费。这样就形成了资源竞争，谁的资源空闲大，争抢到的可能性就大。 场景：红包；有一个电商平台，有两个订单服务，用户下单的时候，任意一个订单服务消费用户的下单请求生成订单即可。不用两个订单服务同时消费用户的下单请求。 三、发布订阅模式原理：一个生产者，多个消费者，每个消费者都可以收到相同的消息。生产者将消息发送到交换机，交换机类型是fanout，不同的队列注册到交换机上，不同的消费者监听不同的队列，所有消费者都会收到消息。 场景：邮件群发，群聊天，广播(广告)；有一个商城，我们新添加一个商品后，可能同时需要去更新缓存和数据库。 四、路由模式原理：生产者将消息发送给交换机，消息携带具体的routingkey。交换机类型是direct，交换机匹配与之绑定的队列的routingkey，分发到不同的队列上。 场景：还是一样，有一个商城，新添加了一个商品，实时性不是很高，只需要添加到数据库即可，不用刷新缓存。 五、主题模式原理：路由模式的一种，交换机类型是topic，路由功能添加了模糊匹配。星号（*）代表1个单词，#号（#）代表一个或多个单词。 场景：还是一样，有一个商城，新添加了一个商品，实时性不是很高，只需要添加到数据库即可，数据库包含了主数据库mysql1和从数据库mysql2的内容，不用刷新缓存。 六、RPC1、首先客户端发送一个reply_to和corrention_id的请求，发布到RPC队列中； 2、服务器端处理这个请求，并把处理结果发布到一个回调Queue,此Queue的名称应当与reply_to的名称一致 3、客户端从回调Queue中得到先前corrention_id设定的值的处理结果。如果碰到和先前不一样的corrention_id的值，将会忽略而不是抛出异常。 @$如何保证高可用的？RabbitMQ 的集群RabbitMQ 是比较有代表性的，因为是基于主从（非分布式）做高可用性的，我们就以 RabbitMQ 为例子讲解第一种 MQ 的高可用性怎么实现。RabbitMQ 有三种模式：单机模式、普通集群模式、镜像集群模式。 单机模式，就是 Demo 级别的，一般就是你本地启动了玩玩儿的?，没人生产用单机模式 普通集群模式，意思就是在多台机器上启动多个 RabbitMQ 实例，每个机器启动一个。你创建的 queue，只会放在一个 RabbitMQ 实例上，但是每个实例都同步 queue 的元数据（元数据可以认为是 queue 的一些配置信息，通过元数据，可以找到 queue 所在实例）。你消费的时候，实际上如果连接到了另外一个实例，那么那个实例会从 queue 所在实例上拉取数据过来。这方案主要是提高吞吐量的，就是说让集群中多个节点来服务某个 queue 的读写操作。 类似 nginx vip(queue) 只是大家可以直接找到queue被连接也能提供服务,而不是消费端直连vip 镜像集群模式：这种模式，才是所谓的 RabbitMQ 的高可用模式。跟普通集群模式不一样的是，在镜像集群模式下，你创建的 queue，无论元数据还是 queue 里的消息都会存在于多个实例上，就是说，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据的意思。然后每次你写消息到 queue 的时候，都会自动把消息同步到多个实例的 queue 上。RabbitMQ 有很好的管理控制台，就是在后台新增一个策略，这个策略是镜像集群模式的策略，指定的时候是可以要求数据同步到所有节点的，也可以要求同步到指定数量的节点，再次创建 queue 的时候，应用这个策略，就会自动将数据同步到其他的节点上去了。这样的话，好处在于，你任何一个机器宕机了，没事儿，其它机器（节点）还包含了这个 queue 的完整数据，别的 consumer 都可以到其它节点上去消费数据。坏处在于，第一，这个性能开销也太大了吧，消息需要同步到所有机器上，导致网络带宽压力和消耗很重！RabbitMQ 一个 queue 的数据都是放在一个节点里的，镜像集群下，也是每个节点都放这个 queue 的完整数据。","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"ES","slug":"14ES","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:53:17.753Z","comments":true,"path":"2022/09/01/14ES/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/14ES/","excerpt":"","text":"1-今日内容 初识 ElasticSearch 安装 ElasticSearch ElasticSearch 核心概念 操作 ElasticSearch ElasticSearch JavaAPI 2-初识ElasticSearch2.1-基于数据库查询的问题![1580888245982](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1580888245982.png) 2.2-倒排索引倒排索引：将文档进行分词，形成词条和id的对应关系即为反向索引。 以唐诗为例，所处包含“前”的诗句 正向索引：由《静夜思》–&gt;窗前明月光—&gt;“前”字 反向索引：“前”字–&gt;窗前明月光–&gt;《静夜思》 反向索引的实现就是对诗句进行分词，分成单个的词，由词推据，即为反向索引 “床前明月光”–&gt; 分词 将一段文本按照一定的规则，拆分为不同的词条（term） ![1580887683510](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1580887683510.png) ![1580887667417](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1580887667417.png) 2.3-ES存储和查询的原理index（索引）：相当于mysql的库 映射：相当于mysql 的表结构 **document(文档)**：相当于mysql的表中的数据 数据库查询存在的问题： 性能低：使用模糊查询，左边有通配符，不会走索引，会全表扫描，性能低 功能弱：如果以”华为手机“作为条件，查询不出来数据 Es使用倒排索引，对title 进行分词 ![1581143412491](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1581143412491.png) 使用“手机”作为关键字查询 生成的倒排索引中，词条会排序，形成一颗树形结构，提升词条的查询速度 待学习 使用“华为手机”作为关键字查询 华为：1,3 手机：1,2,3 ![1581143489911](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1581143489911.png) 2.4-ES概念详解•ElasticSearch是一个基于Lucene的搜索服务器 ![1580887955947](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;6.第六章 搜索引擎-ElasticSearch-V10.0&#x2F;ElasticSearch-1&#x2F;img&#x2F;1580887955947.png) •是一个分布式、高扩展、高实时的搜索与数据分析引擎 •基于RESTful web接口 •Elasticsearch是用Java语言开发的，并作为Apache许可条款下的开放源码发布，是一种流行的企业级搜索引擎 •官网：https://www.elastic.co/ 应用场景 •搜索：海量数据的查询 •日志数据分析 •实时数据分析 3-安装ElasticSearch3.1-ES安装 参见ElasticSearch-ES安装.md 查看elastic是否启动 1ps -ef|grep elastic 3.2-ES辅助工具安装 参见ElasticSearch-ES安装.md 后台启动 1nohup ../bin/kibana &amp; 4-ElasticSearch核心概念索引（index） ElasticSearch存储数据的地方，可以理解成关系型数据库中的数据库概念。 映射（mapping） mapping定义了每个字段的类型、字段所使用的分词器等。相当于关系型数据库中的表结构。 文档（document） Elasticsearch中的最小数据单元，常以json格式显示。一个document相当于关系型数据库中的一行数据。 倒排索引 一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，对应一个包含它的文档id列表。 类型（type） 一种type就像一类表。如用户表、角色表等。在Elasticsearch7.X默认type为_doc 12345\\- ES 5.x中一个index可以有多种type。 \\- ES 6.x中一个index只能有一种type。 \\- ES 7.x以后，将逐步移除type这个概念，现在的操作已经不再使用，默认_doc 5-脚本操作ES5.1-RESTful风格介绍1.ST（Representational State Transfer），表述性状态转移，是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是RESTful。就是一种定义接口的规范。 2.基于HTTP。 3.使用XML格式定义或JSON格式定义。 4.每一个URI代表1种资源。 5.客户端使用GET、POST、PUT、DELETE 4个表示操作方式的动词对服务端资源进行操作： GET：用来获取资源 POST：用来新建资源（也可以用于更新资源） PUT：用来更新资源 DELETE：用来删除资源 ​ 5.2-操作索引​ PUT 1http://ip:端口/索引名称 查询 123GET http://ip:端口/索引名称 # 查询单个索引信息GET http://ip:端口/索引名称1,索引名称2... # 查询多个索引信息GET http://ip:端口/_all # 查询所有索引信息 •删除索引 1DELETE http://ip:端口/索引名称 •关闭、打开索引 12POST http://ip:端口/索引名称/_close POST http://ip:端口/索引名称/_open 5.3-ES数据类型​ 1. **简单数据类型** 字符串 聚合：相当于mysql 中的sum（求和） 123text：会分词，不支持聚合keyword：不会分词，将全部内容作为一个词条，支持聚合 数值 布尔：boolean 二进制：binary 范围类型 1integer_range, float_range, long_range, double_range, date_range 日期:date 复杂数据类型 •数组：[ ] Nested: nested (for arrays of JSON objects 数组类型的JSON对象) •对象：{ } Object: object(for single JSON objects 单个JSON对象) 5.4-操作映射​ 123456789101112131415161718PUT personGET person#添加映射PUT /person/_mapping&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;text&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;:&quot;integer&quot; &#125; &#125;&#125; ​ #创建索引并添加映射 1234567891011121314151617 #创建索引并添加映射 PUT /person1&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125; &#125;&#125;GET person1/_mapping 添加字段 123456789101112#添加字段PUT /person1/_mapping&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125;&#125; 5.5-操作文档​ •添加文档，指定id 12345678910POST /person1/_doc/2&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京&quot;&#125;GET /person1/_doc/1 •添加文档，不指定id 12345678910#添加文档，不指定idPOST /person1/_doc/&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京&quot;&#125;#查询所有文档GET /person1/_search 12#删除指定id文档DELETE /person1/_doc/1 6-分词器6.1分词器-介绍•IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包 •是一个基于Maven构建的项目 •具有60万字&#x2F;秒的高速处理能力 •支持用户词典扩展定义 •下载地址：https://github.com/medcl/elasticsearch-analysis-ik/archive/v7.4.0.zip 安装包在资料文件夹中提供 6.2-ik分词器安装 参见 ik分词器安装.md 执行如下命令时如果出现 打包失败（501码）将maven镜像换成阿里云的 1mvn package /opt/apache-maven-3.1.1/conf/setting.xml 123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; 6.3-ik分词器使用 IK分词器有两种分词模式：ik_max_word和ik_smart模式。 1、ik_max_word 会将文本做最细粒度的拆分，比如会将“乒乓球明年总冠军”拆分为“乒乓球、乒乓、球、明年、总冠军、冠军。 123456#方式一ik_max_wordGET /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;乒乓球明年总冠军&quot;&#125; ik_max_word分词器执行如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;乒乓球&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;乒乓&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;球&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;明年&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;总冠军&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;冠军&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125; ]&#125; 2、ik_smart 会做最粗粒度的拆分，比如会将“乒乓球明年总冠军”拆分为乒乓球、明年、总冠军。 123456#方式二ik_smartGET /_analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;乒乓球明年总冠军&quot;&#125; ik_smart分词器执行如下： 1234567891011121314151617181920212223242526&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;乒乓球&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;明年&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;总冠军&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125; ]&#125; 由此可见 使用ik_smart可以将文本”text”: “乒乓球明年总冠军”分成了【乒乓球】【明年】【总冠军】 这样看的话，这样的分词效果达到了我们的要求。 ​ 6.4使用IK分词器-查询文档•词条查询：term ​ 词条查询不会分析查询条件，只有当词条和查询字符串完全匹配时才匹配搜索 •全文查询：match ​ 全文查询会分析查询条件，先将查询条件进行分词，然后查询，求并集 1.创建索引，添加映射，并指定分词器为ik分词器 1234567891011121314PUT person2&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot; &#125; &#125; &#125;&#125; 2.添加文档 123456789101112131415161718192021POST /person2/_doc/1&#123; &quot;name&quot;:&quot;张三&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京海淀区&quot;&#125;POST /person2/_doc/2&#123; &quot;name&quot;:&quot;李四&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京朝阳区&quot;&#125;POST /person2/_doc/3&#123; &quot;name&quot;:&quot;王五&quot;, &quot;age&quot;:18, &quot;address&quot;:&quot;北京昌平区&quot;&#125; 3.查询映射 1GET person2 4.查看分词效果 123456GET _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;北京海淀&quot;&#125; 5.词条查询：term 查询person2中匹配到”北京”两字的词条 12345678910GET /person2/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; &quot;address&quot;: &#123; &quot;value&quot;: &quot;北京&quot; &#125; &#125; &#125;&#125; 6.全文查询：match ​ 全文查询会分析查询条件，先将查询条件进行分词，然后查询，求并集 12345678GET /person2/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;address&quot;:&quot;北京昌平&quot; &#125; &#125;&#125; 7-ElasticSearch JavaApi-​ 7.1SpringBoot整合ES①搭建SpringBoot工程 ②引入ElasticSearch相关坐标 12345678910111213141516&lt;!--引入es的坐标--&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-client&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt; &lt;version&gt;7.4.0&lt;/version&gt; &lt;/dependency&gt; ③测试 ElasticSearchConfig 123456789101112131415161718192021222324252627282930313233343536@Configuration@ConfigurationProperties(prefix=&quot;elasticsearch&quot;)public class ElasticSearchConfig &#123; private String host; private int port; public String getHost() &#123; return host; &#125; public void setHost(String host) &#123; this.host = host; &#125; public int getPort() &#123; return port; &#125; public void setPort(int port) &#123; this.port = port; &#125; @Bean public RestHighLevelClient client()&#123; return new RestHighLevelClient(RestClient.builder( new HttpHost(host,port,&quot;http&quot;) )); &#125;&#125; ElasticsearchDay01ApplicationTests 注意：使用@Autowired注入RestHighLevelClient 如果报红线，则是因为配置类所在的包和测试类所在的包，包名不一致造成的 123456789101112131415@SpringBootTestclass ElasticsearchDay01ApplicationTests &#123; @Autowired RestHighLevelClient client; /** * 测试 */ @Test void contextLoads() &#123; System.out.println(client); &#125;&#125; 7.2-创建索引 1.添加索引 1234567891011121314151617/** * 添加索引 * @throws IOException */ @Test public void addIndex() throws IOException &#123; //1.使用client获取操作索引对象 IndicesClient indices = client.indices(); //2.具体操作获取返回值 //2.1 设置索引名称 CreateIndexRequest createIndexRequest=new CreateIndexRequest(&quot;itheima&quot;); CreateIndexResponse createIndexResponse = indices.create(createIndexRequest, RequestOptions.DEFAULT); //3.根据返回值判断结果 System.out.println(createIndexResponse.isAcknowledged()); &#125; 2.添加索引，并添加映射 1234567891011121314151617181920212223242526272829303132/** * 添加索引，并添加映射 */ @Test public void addIndexAndMapping() throws IOException &#123; //1.使用client获取操作索引对象 IndicesClient indices = client.indices(); //2.具体操作获取返回值 //2.具体操作，获取返回值 CreateIndexRequest createIndexRequest = new CreateIndexRequest(&quot;itcast&quot;); //2.1 设置mappings String mapping = &quot;&#123;\\n&quot; + &quot; \\&quot;properties\\&quot; : &#123;\\n&quot; + &quot; \\&quot;address\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;text\\&quot;,\\n&quot; + &quot; \\&quot;analyzer\\&quot; : \\&quot;ik_max_word\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;age\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;long\\&quot;\\n&quot; + &quot; &#125;,\\n&quot; + &quot; \\&quot;name\\&quot; : &#123;\\n&quot; + &quot; \\&quot;type\\&quot; : \\&quot;keyword\\&quot;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;\\n&quot; + &quot; &#125;&quot;; createIndexRequest.mapping(mapping,XContentType.JSON); CreateIndexResponse createIndexResponse = indices.create(createIndexRequest, RequestOptions.DEFAULT); //3.根据返回值判断结果 System.out.println(createIndexResponse.isAcknowledged()); &#125; ​ 7.3-查询、删除、判断索引 查询索引 1234567891011121314151617181920 /** * 查询索引 */@Testpublic void queryIndex() throws IOException &#123; IndicesClient indices = client.indices(); GetIndexRequest getRequest=new GetIndexRequest(&quot;itcast&quot;); GetIndexResponse response = indices.get(getRequest, RequestOptions.DEFAULT); Map&lt;String, MappingMetaData&gt; mappings = response.getMappings(); //iter 提示foreach for (String key : mappings.keySet()) &#123; System.out.println(key+&quot;===&quot;+mappings.get(key).getSourceAsMap()); &#125;&#125; 删除索引 1234567891011/** * 删除索引 */ @Test public void deleteIndex() throws IOException &#123; IndicesClient indices = client.indices(); DeleteIndexRequest deleteRequest=new DeleteIndexRequest(&quot;itheima&quot;); AcknowledgedResponse delete = indices.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(delete.isAcknowledged()); &#125; 索引是否存在 123456789101112131415/** * 索引是否存在 */ @Test public void existIndex() throws IOException &#123; IndicesClient indices = client.indices(); GetIndexRequest getIndexRequest=new GetIndexRequest(&quot;itheima&quot;); boolean exists = indices.exists(getIndexRequest, RequestOptions.DEFAULT); System.out.println(exists); &#125; 7.4-添加文档 1.添加文档,使用map作为数据 12345678910@Test public void addDoc1() throws IOException &#123; Map&lt;String, Object&gt; map=new HashMap&lt;&gt;(); map.put(&quot;name&quot;,&quot;张三&quot;); map.put(&quot;age&quot;,&quot;18&quot;); map.put(&quot;address&quot;,&quot;北京二环&quot;); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(&quot;1&quot;).source(map); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId()); &#125; 2.添加文档,使用对象作为数据 123456789101112@Testpublic void addDoc2() throws IOException &#123; Person person=new Person(); person.setId(&quot;2&quot;); person.setName(&quot;李四&quot;); person.setAge(20); person.setAddress(&quot;北京三环&quot;); String data = JSON.toJSONString(person); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(person.getId()).source(data,XContentType.JSON); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId());&#125; ​ 7.5-修改、查询、删除文档 1.修改文档：添加文档时，如果id存在则修改，id不存在则添加 123456789101112131415161718/** * 修改文档：添加文档时，如果id存在则修改，id不存在则添加 */ @Testpublic void UpdateDoc() throws IOException &#123; Person person=new Person(); person.setId(&quot;2&quot;); person.setName(&quot;李四&quot;); person.setAge(20); person.setAddress(&quot;北京三环车王&quot;); String data = JSON.toJSONString(person); IndexRequest request=new IndexRequest(&quot;itcast&quot;).id(person.getId()).source(data,XContentType.JSON); IndexResponse response = client.index(request, RequestOptions.DEFAULT); System.out.println(response.getId());&#125; 2.根据id查询文档 123456789101112/** * 根据id查询文档 */@Testpublic void getDoc() throws IOException &#123; //设置查询的索引、文档 GetRequest indexRequest=new GetRequest(&quot;itcast&quot;,&quot;2&quot;); GetResponse response = client.get(indexRequest, RequestOptions.DEFAULT); System.out.println(response.getSourceAsString());&#125; 3.根据id删除文档 123456789101112/** * 根据id删除文档 */ @Test public void delDoc() throws IOException &#123; //设置要删除的索引、文档 DeleteRequest deleteRequest=new DeleteRequest(&quot;itcast&quot;,&quot;1&quot;); DeleteResponse response = client.delete(deleteRequest, RequestOptions.DEFAULT); System.out.println(response.getId()); &#125; ​","categories":[{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/categories/ES/"}],"tags":[]},{"title":"OS","slug":"15os","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:52:46.447Z","comments":true,"path":"2022/09/01/15os/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/15os/","excerpt":"","text":"第一章 操作系统概述abstractWhat’s OS按软件工程的观点分析OS的结构操作系统的发展，类型及特征现代操作系统体系结构基础知识 1.1What’s OS1.User&#x2F;Computer Interface (用户角度)OS是用户使用计算机系统的接口，为用户提供了方便的工作环境2.Virtual Machine（程序员）建立在硬件上的虚拟机器,为应用软件提供了许多比计算机硬件功能更强或没有的功能3.Resource Manager(OS开发者1）负责分配，回收，以及控制系统中的各种软硬件资源 Job Organizer（OS开发者2）工作流程的组织者，负责协调各个应用软件的运行次序 JVM是建立在OS之上的虚拟机：可对比理解 软件工程 需求分析→系统设计→编码实现→产品测试 1.2 OS的系统需求软件系统的系统需求指人们从外部对系统提出的诸多期望包括三种类型 提供的服务 OS提供服务需要满足的限制条件 OS具有适应某些变化的能力 第一类系统需求是后两类系统需求赖以存在的基础，称为功能性需求，后两者为非功能性需求 功能性需求计算机用户需要的用户命令，由OS实现的所有用户命令构成的集合，被称为用户接口或者命令接口引用软件需要的系统调用，由OS实现的所有系统调用的集合被称为程序接口或应用编程接口 Interface表示形式：字符，菜单，图形形式使用方式：脱机（off-line处理时不能改变作业步)&#x2F;联机（on-line可随时改变） System Call应用软件在运行过程可以引用的系统服务常用 POSIX.1 WIN32 API 非功能性需求 性能 效率 公平性 可靠性 安全性 可伸缩性 可扩展性 可移植性… … OS对硬件平台的依赖Timer I&#x2F;O Interrupts DMA or Channel Privileged Instructions (特权指令） Memory Protection Mechanism… … 基本概念：Job（作业） 用户一次上机过程中要求计算机为其所做工作的集合；作业中的每项相对独立的工作称为作业步 通常，一组命令来描述作业；其中每个命令定义为一个作业步-…… 基本概念：Thread&amp;Process Thread是指程序的一次相对独立的运行过程，在现代OS中，线程是系统调度的最小单位。 Process是指，系统分配资源的基本对象；在现代OS中，进程仅仅是系统中拥有资源的最小实体；不过在传统OS中进程同时也是系统调度的最小单位 基本概念 Virtual Memory&amp;File 虚拟存储简单的说就是进程的逻辑地址空间；是现代OS对计算机系统中多级物理存储体系进行高度抽象的结果 文件，简单的说就是命名了的字节流；它是现代OS对计算机系统种类繁多的外设设备进行高度抽象的结果 1.3 OS的演变，类型及特点OS进行演变的情况 修改 新服务 硬件升级&#x2F;新的软件类型 效率 串行处理简单批处理系统1.4 OS体系结构需求分析→系统设计→编码实现→产品测试系统设计→软件体系结构设计→软件部件设计 一种常见的OS总体结构风格 大多数现代OS总体结构包含两类子系统 ：用户接口子系统（提供命令接口），和基础平台子系统（提供系统调用） 两者关系单项性，具体来说用户接口子系统在实现各种用户命令时能够引用基础平台子系统提供的各种系统调用，但基础平台子系统在实现各种系统调用不会引用用户命令 OS基础平台子系统结构风格 分层 特征：按层实现一组概念及其相关的基本属性 ，上层只依赖直接下层 分级 类似分层 按级 ~ 只依赖以下各级 级~ 分块 按块实现~ ~ ，所有各块均可任意引用其他各块提供的概念及属性 双模式user model 和kernel model 第二章 进程和调度abstract基础：进程描述及控制实现：互斥与同步避免：死锁与饥饿解决：几个经典问题关于：进程通信策略：进程调度 2.1 进程描述和控制学习目标： 解释什么事进程，交换，线程 掌握分析进程的结构PCB，Process image（进程映像） 描述进程的基本状态及转换规则与原因 区别进程的挂起与阻塞状态 理解OS内核的主要功能 理解原语（process control primitives） 区别Provess Swithcing vs. Mode switching 区别进程线程 OS的主要要求操作系统的主要要求． 交叉执行多个进程，在提供合理响应时间的同时，最大限度地利用处理器 资源支持 进程间通信和用户创建进程 程序执行顺序顺序执行特征：顺序性，封闭性，可再现性并发 间断 非封闭 不可再现 进程也被叫做task是程序在一个数据集合的运行过程，是系统进行资源分配和调度的独立单位特性：动态性 并发 独立 异步结构：代码，数据 PCB（进程控制块进程状态：执行或非执行new（ready suspengd） ready Running exit （blocked suspengd） Bolcked使用两个队列 等待相同资源（例如锁）的在一队 多个不同的队列 Swapping（交换技术）将内存中暂时不能运行的进程，或暂时不用的数据和程序，Swapping-out到外存，以腾出足够的内存空间，把已具备运行条件的进程或进程所需要的数据和程序，Swapping-in内存 程序暂停的原因Swapping用户要求定时任务父进程要求其他系统原因（例如系统怀疑进程会引起问题，会让他暂停） 处理机可处理的一个是新创建的进程或者换入一个以前挂起的进程 进程描述OS如何感知进程，控制进程及其所用的系统资源 OS控制体系 关于进程和资源的当前状态信息 表是为了每个操作系统管理的实体构建的 存储表 I&#x2F;O 表 文件表 程序表存储表分配内存给程序分配二级存储给程序（内存外的所有可访问数据存储器）共享存储区域的访问的保护属性管理虚拟内存所需要的信息 I&#x2F;O表I&#x2F;O设备可选还是已分配I&#x2F;O活动的状态内存中被用作起点或目标的位置 进程表进程在哪为了管理进程而需要的属性 - 进程ID - 进程状态 - 进程所在存储位置进程该过程包括一组要执行的程序 - 数据位置 - 任何定义的常数 - 堆栈进程控制块（PCB） - 属性集合(进程映像) - 程序、数据、堆栈和属性的集合。 2_1_09 PCB OS内核功能资源管理功能进程管理：进程创建，终止，调度，状态转换，同步和通信，管理PCB存储管理：为进程分配空间，对换，段&#x2F;页管理I&#x2F;O管理 ：缓存管理，为进程分配I&#x2F;O通道和设备通过原语 线程 执行状态（正在运行、准备就绪等） 未运行时要保存线程上下文 有一个执行堆栈 每个线程局部变量的静态存储 访问其进程的内存和资源 一个进程的所有线程都共享进程内存和资源线程的好处 创建新线程所需的时间比创建进程所需的时间少 终止线程的时间比终止进程的时间短 在同一进程内的两个线程之间切换的时间更短 由于同一进程中的线程共享内存和文件，因此可以在不调用内核的情况下相互通信。 挂起进程涉及挂起进程的所有线程 -因为所有线程共享相同的地址空间进程的终止，终止进程中的所有线程 多线程操作系统支持在单个进程中执行多个线程。MS-DOS支持单线程。UNIX支持多个用户进程，但每个进程只支持一个线程。Windows 2000、Solaris、Linux、Mach和Os&#x2F;2支持多线程。 线程状态关键状态 Running Ready Blocked改变线程状态的相关操作 派生， 派生其他线程 阻塞 解除阻塞 完成 用户级线程所有线程被应用管理核心不能感知存在描述此类线程的数据结构及控制此类线程的原语在核外子系统中实现 二合一方法Solaris大量线程被创建在用户空间大批调度和同步在用户线程空间 2.2 进程调度abtstract 调度种类 调度准则 调度算法 调度真正时间 目标 解释什么是响应时间，周转时间，截止时间，吞吐量 理解进程调度的目标，类型原则 理解剥夺&amp;非剥夺 研究经典进程调度算法FCFS ，轮转，最短作业优先，最短剩下时间，最高响应比优先，Feedback 理解Real-Time Systems及类型 理解掌握：Real-Time Scheduling,Deadline Scheduling, Rate Monotonic Scheduling(速度单调) 2.2.1调度类型调度目标 响应时间系统吞吐量处理机效率公平性（防止进程饥饿） 进程调度的类型 按OS ：批处理，分时，实时，多处理机按调度层次： 长程 ，中， 短程调度 长期调度 也称高级调度、作业调度，它是为调度作业或用户程序创建进程、分配必要的系统资源，并插入新的进程创建的就绪队列，等待短期调度利用交换技术系统将新创建的进程插入(就绪、挂起)队列，等待中期调度。 批处理系统，第一个作业进入系统后，驻留在磁盘(批处理队列)上,长程调度从该队列选择作业为之创建进程 长期调度确定系统允许哪些程序进行处理 它依赖于调度算法，如FCFS、短分配优先级、基于优先级、响应优先级比大多数高优先级调度算法。 有多少项目被允许进入系统? 控制多编程程度 自己的理解是取决于内存大小,和系统核数共同决定吧 程度调度程序何时被调用? 每次工作终止时 处理器空闲超过阈值 短程调度 ​ ．当事件发生时调用 时钟中断． IO中断 操作系统调用 signals(信号) 2.2.2调度算法简述操作系统进程的相关调度算法先来先服务调度算法短作业有限调度算法优先级调度算法高相应比优先调度算法时间片轮转法 多级反馈队列调度算法：设置多个就绪队列，为各个队列赋予不同的优先级，第一个队列的优先级最高第二个队列次之，其余各队列的优先权逐个降低，该算法赋予各个队列中进程的时间片的大小也各不相同，在优先级越高的队列中，为每个进程所规定的时间片就越小，当一个进程进入内存后，首先将它放入第一队列的末尾，按FCFS原则等待调度，当轮到该进程执行时如能在该时间片完成，便可准备撤离系统，如果未完成，调度程序便将该进程放入第二队列的末尾，同样FCFS 依次往下仅当第一队空闲时，调度程序才调度第二队列中的进程运行如果处理机正处理第i队 为某进程服务时又有新进程进入优先权较高的任何一个队列，此时新进程会抢占正在运行进程的处理机，正在运行的进程被放回第i队的末尾 2.2.3实时系统与实时调度实时系统: ​ 系统的正确性不仅取决于计算的逻辑结果，还取决于产生结果的时间。任务或过程试图控制或响应发生在外部世界的事件。．这些事件是“实时”发生的，流程必须能够跟上它们。 实时操作系统的特点． 用户控件用户指定的优先级 指定分页(存储页面) 哪些进程必须始终驻留在主存中。磁盘算法使用进程的权限 可靠性(Reliability) 性能的降低会带来灾难性的后果(灾难性的) 在继续运行的同时，尝试纠正问题或最小化其影响。．执行最关键的、高优先级的任务。 实时操作系统的特点．快速上下文切换小尺寸．能够快速响应外部中断。多任务处理与进程间通信工具，如信号量，信号和事件。的文件存储数据速率快。 2.3 并发 互斥与同步 死锁与饥饿 目标 解释什么是并发，同步，互斥，死锁，饥饿,临界区 掌握互斥的要求 掌握互斥的方法：软件方法&amp;硬件支持—Semaphores，Monitors，Message Passing 区别掌握信号量的类型和意义 掌握生产者消费者，读写者，哲学家问题 理解死锁的情况，死锁的预防，死锁的避免，死锁侦查，一旦检测出死锁的策略，银行家算法安全情况和不安全情况 2.3.1 并发控制并发设计的问题 进程间交流 共享&#x2F;竞争资源 多进程的同步 处理器时间的分配 并发性的困难 全局资源共享 资源分配的管理 编程错误难以定位 2.3.2 并发进程操作系统问题跟踪活动进程:PCB． 分配和释放资源 处理器时间:调度 记忆:虚拟内存 文件I &#x2F; O设备 保护数据和资源 进程的结果必须独立于其他并发进程的执行 相互作用过程(理解)进程之间不知道彼此——竞争 ​ 互斥、死锁、饥饿 进程可以间接感知彼此——共享合作 ​ 互斥，死锁，饥饿，数据一致性(Data coherence) 进程直接感知彼此 消息传递 死锁,饥饿 进程间的合作通过沟通消息传递 互斥不是控制要求。 ．可能出现死锁每个进程都在等待来自另一个进程的消息。 ．可能会挨饿两个进程互相发送消息，而另一个进程等待消息。 2.3.3 互斥条件与解决方案互斥的要求 空则让进 忙则等待 有限等待 让权等待 互斥的方法(Approaches of Mutual Exclusion) 软件方法 内存级别,保证内存 Access to the same location in main memory are serializedby some sort of memory arbiter (内存仲裁器待学习) 硬件支持 中断禁用(OS中断待学习) 进程运行到调用操作系统服务或中断为止。 禁用中断保证了互斥。 这种方法的代价很高。 多处理 在一个处理器上禁用中断不能保证互斥。 信号量 称为信号量的特殊变量用于发送信号。 如果一个进程正在等待一个信号，那么它将被阻塞，直到该信号被发送。 等待和信号操作不能中断。 Queue用于保存等待信号量的进程。 信号量信号量是一个具有整数值的变量s。可以初始化为非负数。Wait和Signal是原语(是系统调用,原子的，不能被中断，每个例程可以被视为不可分割的步骤)。 可理解为资源数,门口多把钥匙,都能进 管程 队列+互斥区 信号量容易出错(wait和signal操作的顺序很重要) 消息传递 send(地址,消息) 和 recive(地址,消息) 可以是send和recive都可以是非阻塞的 生产者消费者 一个缓冲区同时只能只有一个人访问(互斥) 不能在空的取,不能在满的加(同步) 循环使用缓冲区 Java实现 &#96;&#96;&#96;java&#x2F;&#x2F;同步方式: Object的wait notify, park,unpark(再复习下),Lock.Condition接口的await()和signal()&#x2F;&#x2F;BlockQueue方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187// synchronized 实现互斥,wait,notifyAll()实现同步 public class Cache &#123; private final static int MAX_SIZE = 10; private int cacheSize = 0; public Cache()&#123; cacheSize = 0; &#125; public Cache(int size)&#123; cacheSize = size; &#125; //分析过程 一个生产者进来 不大于直接生产 小于则让出 public void produce()&#123; synchronized (this)&#123; while (cacheSize &gt;= MAX_SIZE )&#123; try &#123; System.out.println(&quot;缓存已满，生产者需要等待&quot;); wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize++; System.out.println(&quot;生产了一个产品。当前产品数量为&quot;+ cacheSize); notify(); &#125; &#125; public void consume()&#123; synchronized (this)&#123; while(cacheSize &lt;= 0)&#123; try &#123; System.out.println(&quot;缓存为空，消费者需要等待&quot;); wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize--; System.out.println(&quot;消费了一个产品。当前产品数量为&quot;+ cacheSize); notify(); &#125; &#125; &#125;​ // Java内部信号量 //public class Cache &#123; // private int cacheSize = 0; // // public Semaphore mutex; // public Semaphore empty; //保证了容器空的时候（empty的信号量&lt;=0), 消费者等待 // public Semaphore full; //保证了容器满的时候（full的信号量 &lt;= 0），生产者等待 // // public Cache(int size) &#123; // mutex = new Semaphore(1); //二进制信号量，表示互斥锁 // empty = new Semaphore(size); // full = new Semaphore(0); // &#125; // // public int getCacheSize()throws InterruptedException&#123; // return cacheSize; // &#125; // // public void produce() throws InterruptedException&#123; // empty.acquire(); // 消耗一个空位 // mutex.acquire(); // cacheSize++; // System.out.println(&quot;生产了一个产品， 当前产品数为&quot; + cacheSize); // mutex.release(); // full.release(); // 增加了一个产品 // // // &#125; // // public void consume() throws InterruptedException&#123; // full.acquire(); // 消耗了一个产品 // mutex.acquire(); // cacheSize--; // System.out.println(&quot;消费了一个产品， 当前产品数为&quot; + cacheSize); // mutex.release(); // empty.release(); // 增加了一个空位 // // &#125; //&#125;​ //管程 public class Cache &#123; private final static int MAX_SIZE = 10; private int cacheSize = 0; private Lock lock; private Condition notFull; private Condition notEmpty; public Cache()&#123; cacheSize = 0; this.lock = new ReentrantLock(); this.notFull = lock.newCondition(); this.notEmpty = lock.newCondition(); &#125; public Cache(int size)&#123; cacheSize = size; this.lock = new ReentrantLock(); this.notFull = lock.newCondition(); this.notEmpty = lock.newCondition(); &#125; //分析过程 一个生产者进来 不大于直接生产 小于则让出 public void produce()&#123; lock.lock(); while (cacheSize == MAX_SIZE )&#123; try &#123; System.out.println(&quot;缓存已满，生产者需要等待&quot;); notFull.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize++; System.out.println(&quot;生产了一个产品。当前产品数量为&quot;+ cacheSize); notEmpty.signal(); lock.unlock(); &#125; public void consume()&#123; lock.lock(); while (cacheSize == 0 )&#123; try &#123; System.out.println(&quot;缓存为空，消费者需要等待&quot;); notEmpty.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; cacheSize--; System.out.println(&quot;消费了一个产品。当前产品数量为&quot;+ cacheSize); notFull.signal(); lock.unlock(); &#125; &#125; //值得一提的信号量自己实现 public class Semaphore&#123; private volatile int permit; public Semaphore(int permit)&#123; this.permit = permit; &#125; // public synchronized void acquire() throws InterruptedException &#123; // 获取许可 while (permit == 0)&#123; // 无法获取许可 //等待 wait(); &#125; //执行permit--操作 permit--; &#125; public synchronized void release()&#123; //释放许可 //执行permit++ 操作 permit++; while (permit &gt; 0)&#123; //唤醒 notifyAll(); &#125; &#125; &#125; //消息传递实现 // // 创建两个队列为邮箱 一个存放&quot;有东西的箱子&quot; 让消费者取,取完将空箱子放到第二个队列, //生产者从空箱子队列取箱子装东西放到 第一个队列 // 生产者阻塞:没有空箱子(队列2为空),消费者阻塞:没有东西取(队列1为空) 2.4 死锁概述 死锁概念 条件 避免 预防 第四章IO设备管理RAIDRAID的基本特性由两部分组成:磁盘阵列(一组磁盘)可以并行工作，磁盘阵列管理软件磁盘阵列一组数据管理软件在逻辑上连续交叉分布，存储在磁盘阵列中到磁盘上。优点:磁盘阵列管理软件可以并行处理一组数据的单个或多个数据访问请求。磁盘阵列管理软件还负责校验存储相关信息。受益:当磁盘阵列出现磁盘故障时，磁盘阵列管理软件可以恢复磁盘上的数据。磁盘阵列管理软件屏蔽了磁盘阵列的物理细节，操作系统的其他成分不知道磁盘阵列的存在;在他们看来，逻辑盘的容量过大。 Linux五种IO模型Java NIO,AIO 依赖于epoll IO两个阶段等待数据 (写入socket直到socket可读) 将数据从内核复制到用户空间复制完成返回成功指示 1. 阻塞IO进程阻塞于两个阶段 直到最后返回成功指示 2. 非阻塞IO 3. IO多路复用select异步阻塞IO 由select帮线程轮询socket 但是进程阻塞于select直到多个socket其中一个可读 select使用了Reactor设计模式 由select帮线程轮询socket 可读后在通知用户线程 4. (信号驱动)第一阶段非阻塞IO5. 真正的异步IO两个阶段都不需要阻塞 linux中的aio_read select、poll、epoll的区别？ select, poll, epoll 都是I&#x2F;O多路复用的具体的实现，之所以有这三个存在，其实是他们出现是有先后顺序的。 https://blog.csdn.net/nanxiaotao/article/details/90612404 https://www.cnblogs.com/aspirant/p/9166944.html https://www.zhihu.com/question/32163005 select 它仅仅知道了，有I&#x2F;O事件发生了，却并不知道是哪那几个流（可能有一个，多个，甚至全部），只能无差别轮询所有流，找出能读出数据，或者写入数据的流，对他们进行操作。所以select具有O(n)的无差别轮询复杂度，同时处理的流越多，无差别轮询时间就越长。 单个进程可监视的fd_set(监听的端口个数)数量被限制：32位机默认是1024个，64位机默认是2048。 pollpoll本质上和select没有区别，采用链表的方式替换原有fd_set数据结构,而使其没有连接数的限制。 epoll epoll可以理解为event poll，不同于忙轮询和无差别轮询，epoll会把哪个流发生了怎样的I&#x2F;O事件通知我们。所以我们说epoll实际上是事件驱动（每个事件关联上fd）的，此时我们对这些流的操作都是有意义的。（复杂度降低到了O(1)） 效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数。即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。 epoll通过内核和用户空间共享一块内存来实现的。select和poll都是内核需要将消息传递到用户空间，都需要内核拷贝动作 epoll有EPOLLLT和EPOLLET两种触发模式。(暂时不去记，有个印象，大致是什么样就可以) linux进程管理进程类型 前台进程 占用终端shell 在Linux系统中执行某些操作时候，有时需要将当前任务暂停调至后台好执行其他命令；有时又得将后台暂停的任务调至前台重新运行。这些操作可使用 jobs、bg 和 fg 三个命令以及 Ctrl + z 快捷键来完成。 按下ctr+z，将当前运行的程序放入后台挂起，此时你就可以执行其他任务了。jobs 命令，显示后台被挂起的所有进程bg N 使第N个序号的任务在后台运行fg N 使第N个序号的任务在前台运行 ctr+c 退出前台进程 注：默认bg,fg不带N时表示对最后一个进程操作!例如：假如我在使用 vim a.js遍历一个文件，此时我可以使用ctrl+z将vim放入后台；再调用less config.cfg查看文件，暂时找到需要的东西后再使用ctrl+z将less查看放入后台； jobs查看所有的后台任务。记下刚才挂起的vim序列号为1，再通过fg 1将其从后台重新放入前台进行编辑。 常用指令netstat从整体上看，netstat的输出结果可以分为两个部分： 一个是Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。 另一个是Active UNIX domain sockets，称为有源Unix域套接口(和网络套接字一样，但是只能用于本机通信，性能可以提高一倍)。 Proto显示连接使用的协议 RefCnt表示连接到本套接口上的进程号 Types显示套接口的类型 State显示套接口当前的状态 Path表示连接到套接口的其它进程使用的路径名 -a 显示所有连接和侦听端口，默认不显示LISTEN相关-b 显示在创建每个连接或侦听端口时涉及的可执行程序。在某些情况下，已知可执行程序承载多个独立的组件，这些情况下，显示创建连接或侦听端口时涉及的组件序列。在此情况下，可执行程序的名称位于底部 [] 中，它调用的组件位于顶部，直至达到 TCP&#x2F;IP。注意，此选项可能很耗时，并且在你没有足够权限时可能失败。-e 显示以太网统计信息。此选项可以与 -s 选项结合使用。 -i 显示网络接口列表-n 以数字形式显示地址和端口号。-o 显示拥有的与每个连接关联的进程 ID。-p proto 显示 proto 指定的协议的连接；proto可以是下列任何一个: TCP、UDP、TCPv6 或 UDPv6。如果与 -s选项一起用来显示每个协议的统计信息，proto 可以是下列任何一个:IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 或 UDPv6。-q 显示所有连接、侦听端口和绑定的非侦听 TCP 端口。绑定的非侦听端口不一定与活动连接相关联。-r 显示路由表。-s 显示每个协议的统计信息。默认情况下，显示 IP、IPv6、ICMP、ICMPv6、TCP、TCPv6、UDP 和 UDPv6 的统计信息;-p 选项可用于指定默认的子网。interval 重新显示选定的统计信息，各个显示间暂停的间隔秒数。按 CTRL+C 停止重新显示统计信息。如果省略，则 netstat 将打印当前的配置信息一次。-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-c 每隔一个固定时间，执行该netstat命令，netstat 将每隔一秒输出网络信息。 psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD","categories":[{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"}],"tags":[]},{"title":"Net","slug":"16net","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:52:20.465Z","comments":true,"path":"2022/09/01/16net/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/16net/","excerpt":"","text":"第一章 计算机网络概述OSI七层模型 TCP&#x2F;IP四层模型 第二章 数据链路层(帧)管理相邻结点的数据通信 封装成帧 转义 差错检测 奇偶校验码 循环冗余校验码CRC r为g(x)最高阶 奇偶校验码就是CRC-1CRC步骤 CRC实例 MTU MTU 路径MTU 帧也不是无限大的 MTU即为最大传输单元 一般为1500 路径MTU由路径中的最小MTU决定 以太网协议(Ethernet应用于数据链路层) 完成相邻设备的数据帧传输 MAC地址 以太网协议数据格式 MAC地址表 以太网协议应用 第三章 网络层决定数据在网络中的路径 解决跨设备的数据传输 主要内容 IP协议的作用 IP头部 版本：占4位，指的是IP协议的 版本，通信双方的版本必须一 致，当前主流版本是4，即IPv4， 也有IPv6 首部位长度：占4位，最大数值 为15，表示的是IP首部长度， 单位是“32位字”（4个字节）， 也即是IP首部最大长度为60字 节 总长度：占16位，最大数值为 65535，表示的是IP数据报总长 度（IP首部+IP数据） 标识 标志 片偏移 TTL：占8位，表明IP数据报文 在网络中的寿命，每经过一个 设备，TTL减1，当TTL&#x3D;0时， 网络设备必须丢弃该报文 协议：占8位，表明IP数据所携 带的具体数据是什么协议的 （如：TCP、UDP等） 首部校验和：占16位，校验IP 首部是否有出错 路由表 IP协议的转发流程 数据帧每一跳的MAC地址都在变化 IP数据报每一跳的IP地址始终不变 ARP协议与RARP协议 由这个协议完成 路由器不知道MAC地址 划分子网 无分类编址CIDR NAT技术 网络地址转换NAT(Network Address Translation) NAT技术用于多个主机通过一个公有IP访问互联网的私有网络中 NAT减缓了IP地址的消耗，但是增加了网络通信的复杂度 ICMP 网际控制报文协议（Internet Control Message Protocol） ICMP协议可以报告错误信息或者异常情况 ICMP头部两种报文 差错报告与询问 第四章 传输层第五章 应用层HTTPhttp缓存机制http研究之旅：expires 将 expires的值设置为了 new Date(Date.now()+60000)，有效期就是一分钟，即60秒看看响应头部（response header）： 在 expires设置的 有效期内，http请求不会真的向服务器发起请求，而是直接从缓存里获取数据expires 是通过 设置资源的有效期来控制http的缓存 浏览器缓存机制详解 一、为什么需要缓存在前端开发中，我们主要追求的是性能和用户体验。对于一个网站查看性能最简单的方式就是打开网站的速度。而一个好的缓存策略可以大大提升网站的性能。使得已经下载后的资源被重复利用。减少客户端和服务器之间的请求次数，减少带宽，减少网络负荷。 二、什么是缓存对于web缓存，主要是针对一些web资源（html、 js、图片、数据等），就是介于web服务器和浏览器之间的文件数据副本。当我们第一次打开某一个网页时，浏览器向服务器发起请求，请求所需要的资源。如果我们使用了web缓存，当我们下一次再次访问该网站页面的时候，我们可以根据一些缓存策略，来决定是否直接使用缓存中的一些资源，还是再次向服务端发起请求，从而避免再次向服务器发起请求，减少客户端和服务器之间通信的时延。 三、缓存的作用 减少网络带宽的消耗 降低服务器压力 减少网络延时，加快页面打开速度。 四、浏览器的缓存机制对于浏览器端的缓存来说，规则是在http协议头和html的mate标签中定义的，他们分别从过期机制和校验值来判断是否直接使用该缓存，还是需要从服务器去获取更新的版本。 1.新鲜度(过期机制)：也就是缓存副本的有效期。一个缓存副本必须满足以下条件之一，浏览器才会认为它是有效的，足够新的，才会直接使用缓存。 http协议头中存在过期时间等信息，并且仍在有效期内。 浏览器已经使用过这个缓存副本，并且在一个会话中已经检查过新鲜度。 2.校验值(验证机制)：服务器相应中，在响应头中存在Etag标签，用来验证资源是否更改的标识，如果缓存的标识和服务器的标识相同则无需重新请求资源，如果不相同，则重新发送资源请求。 五、浏览器缓存控制1.html中的mate标签设置缓存1234设置过期时间＜meta http-equiv=&quot;expires&quot; content=&quot;Wed, 20 Jun 2007 22:33:00 GMT&quot;＞ 设置缓存＜meta http-equiv=&quot;Pragma&quot; content=&quot;no-cache&quot;＞ 2.与缓存有关的字段Cache-control:max-age(单位为s),当某一个资源的响应头设置max-age&#x3D;3600， 则表示在1h时间内，服务器的资源发生变化，浏览器都不会想服务器发送该资源的请求，直接使用缓存。并且max-age会覆盖Expires。 如下图所示 Cache-control:s-maxage,s-maxage表示CDN缓存，也就是代理缓存，如果设置s-maxage&#x3D;60,表示60秒内无论cdn服务器的该资源发生怎么样的改变，都不会重新请求，并且s-maxage会覆盖max-age和Expires. Cache-control:public，指定是否是共享缓存，如果设置Cache-control的值设置为public，则表示多个浏览器之间可以共同使用该资源缓存。如果没有指定Cache-control是为private还是public，则默认是public. Cache-control:private，表示该缓存是私有的，不存在用户共享。 Cache-control:no-cache；Cache-control的值设置为no-cache并不代表不缓存，浏览器是缓存的，但是当每一次访问该资源的时候，都要向服务器请求查看资源是否改变，如果改变，则直接重新下载，如果没有改变，则使用缓存。可以在设置完no-cache之后，在设置private，以及设置过期时间为过去的时间。 Cache-control:no-store,表示严格不缓存，每一次资源必须从服务器上重新获取。 Expires:缓存过期时间，Expires&#x3D;max-age + 最后一次请求的时间。Cache-control和Expires相比，Cache-control的优先级更高。Expires需要和Last-modifyed来一起使用。 Last-Modified和if-modified-since:last-modified是响应头上的属性，if-modifyed-since是请求头的数据。该属性值需要cache-control配合使用。当再次向服务器发送请求该资源时，会携带if-modified-since的请求头字段，到服务器比对和last-modified是否相同。如果相同则直接返回304,直接使用缓存，如果不相同，则再次请求新的数据，返回200。 ETag和if-None-Match:这俩个属性其实和last-modified和if-modified-since类似。不过Etag是服务器更加内容产生的hash字符串，并且Etag是响应头内容。if-None-match是请求头的内容。当再次向服务器发送请求某一个资源时，请求头会携带if-None-match属性，到达服务器后，和Etag进行比对。如果相同，则返回304，如果不相同则返回该资源，并且状态码为200。 六、缓存报文头种类和优先级1.Cache-control和Expires比较Cache-control的优先级比Expires的优先级高。 2.Last-Modified和ETag比较Etag的优先级要高于Last-modified，当在请求头中会先进行ETag比较，然后再进行Last-modified比较，如果两者都相等，则直接返回304,直接使用缓存资源。两者比较一下，你可能会觉得两者的功能差不多，但是为什么要在http1.1中新增Etag呢？ Last-modified精确到秒，如果在一秒钟内修改多次文件，则无法准确拿到最新的文件。 如果缓存文件，打开后但是不修改内容，导致Last-modified发生变化，下一次就没有办法使用缓存文件了。 可能存在服务器没有获取准确的修改时间，或者代理服务器时间不一致的情况。 3.Last-Modified&#x2F;Etag和Cache-control&#x2F;Expires比较Cache-control&#x2F;Expries的优先级要比Last-Modified&#x2F;Etag的优先级高，当第二次发送请求时，会首先查看Cache-control&#x2F;Expries是否过期，如果没有过期，则任然使用该资源，如果过期了，则再次向服务器发送请求来请求最新的资源。到达服务器时通过比对Last-modified&#x2F;Etag是否和原来的值相等，来判断资源是否改变，如果没有改变，则返回304。如果改变了，则返回最新的资源，并且状态码为200。 七、有哪些请求不能进行缓存的无法被浏览器缓存的请求 http信息头部cache-control:no-cache , pragma: nocache或者使用cache-control:max-age&#x3D;0。 根据cookie，认证信息决定输入内容的信息是否可以被缓存的。 经过https加密的请求。 post请求无法被缓存。 在http响应头中不存在last-modified&#x2F;Etag和cache-control&#x2F;expires等。 八、使用缓存流程 上面的过程可以分为三个阶段： 本地缓存阶段：如果本地存在缓存，并且通过检查本地资资源的缓存并没有过期，则直接使用本地缓存。 协商缓存阶段：如果在本地存在该资源，但是本地资源已经过期，此时就需要封装http请求，向服务端发送请求，检查是否存在更改资源。如果资源没有更改，则直接返回304，直接在本地使用资源。 缓存失败阶段：如果资源发生了更改，则重新返回最新的资源，并且返回状态码为200。如果此时不存在该资源，则直接返回404。 九、用户行为与缓存的关系用户在浏览器采用一些操作，例如，返回上一阶段，下一阶段，刷新页面，强制刷新等操作，这些对于一些缓存属性的影响是不一样的。下面将进行详细解读。 刷新（仅仅是F5刷新）：此时对于cache-control&#x2F;Expires是不生效的，但是last-modified&#x2F;Etag都生效的，所以此时会向服务器发起请求，用来判断目标文件是否发生变化。 强制刷新(F5刷新+ctrl)：此时对于cache-control&#x2F;expires和last-modified&#x2F;Etag都不生效，此时必须从服务器拿到新数据。 回车或者转向：此时所有的缓存都生效。 十、从缓存角度改善站点 同一个资源保证只有一个稳定的url地址。 css,js,图片资源增加http缓存头，入口html文件不被缓存。 减少对cookie的依赖。 减少对http协议加密的使用。 HTTPSTLS 握手过程，主要目的是为了协商对称加密的密钥，因为在最终的通信链路上使用对称加解密会更快。 我们知道，生成最终通信的对称密钥需要三个随机数： 客户端随机数 服务端随机数 pre-master 前两个是公开的，那么最重要的就是 pre-master，在协商过程中如何保证它不会被窃取。为了保证协商安全性、对端可靠性，那么采用非对称加密的方式会更加合适，因为私钥只有服务端持有。 在 TLS 中采用的非对称加密方式主要有如下两种： RSA ECDHE 而这两种方式的区别，就在于 pre-master 的生成方式不同。 作者：微微笑的蜗牛链接：https://www.jianshu.com/p/11d6eb418780来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 前向保密:pfs保护过去的会话以免在未来秘钥或密码的（泄露）造成的危害。因为对手或恶意方会活跃地干扰，导致长期秘钥（long-term secret keys）或者密码在未来可能被泄露，但是如果使用了pfs，即使秘钥被泄露，过去的加密的通信和会话记录也不会被恢复和解密。 TLS握手:RSA实现对称秘钥交换防止泄露 三次握手后 C: client hello (发送随机数C,TLS版本号,密码套件列表) S: ACK S: server hello (发送随机数S,确认TLS版本号,使用的密码套件(RSA)) S: 服务器使用的证书 S: 服务器Hello完成 C: ACK (校验证书取得公钥) ​ 之前是TCP三次握手 TLS 第⼀次握⼿客户端⾸先会发⼀个「Client Hello」消息，字⾯意思我们也能理解到，这是跟服务器「打招呼」。 消息⾥⾯有客户端使⽤的 TLS 版本号、⽀持的密码套件列表，以及⽣成的随机数（Client Random），这个随机 数会被服务端保留，它是⽣成对称加密密钥的材料之⼀。 TLS 第⼆次握⼿当服务端收到客户端的「Client Hello」消息后，会确认 TLS 版本号是否⽀持，和从密码套件列表中选择⼀个密码 套件，以及⽣成随机数（Server Random）。 接着，返回「Server Hello」消息，消息⾥⾯有服务器确认的 TLS 版本号，也给出了随机数（Server Random）， 然后从客户端的密码套件列表选择了⼀个合适的密码套件。 可以看到，服务端选择的密码套件是 “Cipher Suite: TLS_RSA_WITH_AES_128_GCM_SHA256”。 这个密码套件看起来真让⼈头晕，好⼀⼤串，但是其实它是有固定格式和规范的。基本的形式是「密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法」， ⼀般 WITH 单词前⾯有两个单词，第⼀个单词是约定密钥交换的算法， 第⼆个单词是约定证书的验证算法。⽐如刚才的密码套件的意思就是： 由于 WITH 单词只有⼀个 RSA，则说明握⼿时密钥交换算法和签名算法都是使⽤ RSA； 握⼿后的通信使⽤ AES 对称算法，密钥⻓度 128 位，分组模式是 GCM； 摘要算法 SHA384 ⽤于消息认证和产⽣随机数； 就前⾯这两个客户端和服务端相互「打招呼」的过程，客户端和服务端就已确认了 TLS 版本和使⽤的密码套件， ⽽且你可能发现客户端和服务端都会各⾃⽣成⼀个随机数，并且还会把随机数传递给对⽅。 那这个随机数有啥⽤呢？其实这两个随机数是后续作为⽣成「会话密钥」的条件，所谓的会话密钥就是数据传输 时，所使⽤的对称加密密钥。 然后，服务端为了证明⾃⼰的身份，会发送「Server Certificate」给客户端，这个消息⾥含有数字证书。 随后，服务端发了「Server Hello Done」消息，⽬的是告诉客户端，我已经把该给你的东⻄都给你了，本次打招 呼完毕。 客户端进行证书验证 TLS 第三次握⼿客户端验证完证书后，认为可信则继续往下⾛。接着，客户端就会⽣成⼀个新的随机数 (pre-master)，⽤服务器 的 RSA 公钥加密该随机数，通过「Change Cipher Key Exchange」消息传给服务端。 服务端收到后，⽤ RSA 私钥解密，得到客户端发来的随机数 (pre-master)。 ⾄此，客户端和服务端双⽅都共享了三个随机数，分别是 Client Random、Server Random、pre-master。 于是，双⽅根据已经得到的三个随机数，⽣成会话密钥（Master Secret），它是对称密钥，⽤于对后续的 HTTP 请求&#x2F;响应的数据加解密。 ⽣成完会话密钥后，然后客户端发⼀个「Change Cipher Spec」，告诉服务端开始使⽤加密⽅式发送消息。 然后，客户端再发⼀个「Encrypted Handshake Message（Finishd）」消息，把之前所有发送的数据做个摘 要，再⽤会话密钥（master secret）加密⼀下，让服务器做个验证，验证加密通信是否可⽤和之前握⼿信息是否有 被中途篡改过。 可以发现，「Change Cipher Spec」之前传输的 TLS 握⼿数据都是明⽂，之后都是对称密钥加密的密⽂。 TLS 第四次握⼿服务器也是同样的操作，发「Change Cipher Spec」和「Encrypted Handshake Message」消息，如果双⽅都 验证加密和解密没问题，那么握⼿正式完成。 最后，就⽤「会话密钥」加解密 HTTP 请求和响应了。 TCP和UDP可以监听同一个端口吗TCP UDP 表述上应该是可以绑定同一个端口吗 答案是 可以 协议的不同也能是不同应用 多个 TCP 服务进程可以绑定同一个端口吗？默认情况下，针对「多个 TCP 服务进程可以绑定同一个端口吗？」这个问题的答案是：如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 注意，如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。 这是因为 0.0.0.0 地址比较特殊，代表任意地址，意味着绑定了 0.0.0.0 地址，相当于把主机上的所有 IP 地址都绑定了。 重启 TCP 服务进程时，为什么会有“Address in use”的报错信息？TCP 服务进程需要绑定一个 IP 地址和一个端口，然后就监听在这个地址和端口上，等待客户端连接的到来。 然后在实践中，我们可能会经常碰到一个问题，当 TCP 服务进程重启之后，总是碰到“Address in use”的报错信息，TCP 服务进程不能很快地重启，而是要过一会才能重启成功。 这是为什么呢？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 而等 TIME_WAIT 状态的连接结束后，重启 TCP 服务进程就能成功。 重启 TCP 服务进程时，如何避免“Address in use”的报错信息？ 我们可以在调用 bind 前，对 socket 设置 SO_REUSEADDR 属性，可以解决这个问题。 12int on = 1;setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;on, sizeof(on)); 因为 SO_REUSEADDR 作用是：如果当前启动进程绑定的 IP+PORT 与处于TIME_WAIT 状态的连接占用的 IP+PORT 存在冲突，但是新启动的进程使用了 SO_REUSEADDR 选项，那么该进程就可以绑定成功。 举个例子，服务端有个监听 0.0.0.0 地址和 8888 端口的 TCP 服务进程。‍ 有个客户端（IP地址：192.168.1.100）已经和服务端（IP 地址：172.19.11.200）建立了 TCP 连接，那么在 TCP 服务进程重启时，服务端会与客户端经历四次挥手，服务端的 TCP 连接会短暂处于 TIME_WAIT 状态： 12客户端地址:端口 服务端地址:端口 TCP 连接状态192.168.1.100:37272 172.19.11.200:8888 TIME_WAIT 如果 TCP 服务进程没有对 socket 设置 SO_REUSEADDR 属性，那么在重启时，由于存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，那么在执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 如果 TCP 服务进程对 socket 设置 SO_REUSEADDR 属性了，那么在重启时，即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 因此，在所有 TCP 服务器程序中，调用 bind 之前最好对 socket 设置 SO_REUSEADDR 属性，这不会产生危害，相反，它会帮助我们在很快时间内重启服务端程序。‍ 前面我提到过这个问题：如果 TCP 服务进程 A 绑定的地址是 0.0.0.0 和端口 8888，而如果 TCP 服务进程 B 绑定的地址是 192.168.1.100 地址（或者其他地址）和端口 8888，那么执行 bind() 时候也会出错。 这个问题也可以由 SO_REUSEADDR 解决，因为它的另外一个作用是：****绑定的 IP地址 + 端口时，只要 IP 地址不是正好(exactly)相同，那么允许绑定。 比如，0.0.0.0:8888 和192.168.1.100:8888，虽然逻辑意义上前者包含了后者，但是 0.0.0.0 泛指所有本地 IP，而 192.168.1.100 特指某一IP，两者并不是完全相同，所以在对 socket 设置 SO_REUSEADDR 属性后，那么执行 bind() 时候就会绑定成功。 客户端的端口可以重复使用吗？客户端在执行 connect 函数的时候，会在内核里随机选择一个端口，然后向服务端发起 SYN 报文，然后与服务端进行三次握手。 所以，客户端的端口选择的发生在 connect 函数，内核在选择端口的时候，会从 net.ipv4.ip_local_port_range 这个内核参数指定的范围来选取一个端口作为客户端端口。 该参数的默认值是 32768 61000，意味着端口总可用的数量是 61000 - 32768 &#x3D; 28232 个。 当客户端与服务端完成 TCP 连接建立后，我们可以通过 netstat 命令查看 TCP 连接。 123$ netstat -napt协议 源ip地址:端口 目的ip地址：端口 状态tcp 192.168.110.182.64992 117.147.199.51.443 ESTABLISHED 那问题来了，上面客户端已经用了 64992 端口，那么还可以继续使用该端口发起连接吗？ 这个问题，很多同学都会说不可以继续使用该端口了，如果按这个理解的话， 默认情况下客户端可以选择的端口是 28232 个，那么意味着客户端只能最多建立 28232 个 TCP 连接，如果真是这样的话，那么这个客户端并发连接也太少了吧，所以这是错误理解。 正确的理解是，TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。所以如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 比如下面这张图，有 2 个 TCP 连接，左边是客户端，右边是服务端，客户端使用了相同的端口 50004 与两个服务端建立了 TCP 连接。 仔细看，上面这两条 TCP 连接的四元组信息中的「目的 IP 地址」是不同的，一个是 180.101.49.12 ，另外一个是 180.101.49.11。 多个客户端可以 bind 同一个端口吗？ bind 函数虽然常用于服务端网络编程中，但是它也是用于客户端的。 前面我们知道，客户端是在调用 connect 函数的时候，由内核随机选取一个端口作为连接的端口。 而如果我们想自己指定连接的端口，就可以用 bind 函数来实现：客户端先通过 bind 函数绑定一个端口，然后调用 connect 函数就会跳过端口选择的过程了，转而使用 bind 时确定的端口。 针对这个问题：多个客户端可以 bind 同一个端口吗？ 要看多个客户端绑定的 IP + PORT 是否都相同，如果都是相同的，那么在执行 bind() 时候就会出错，错误是“Address already in use”。 如果一个绑定在 192.168.1.100:6666，一个绑定在 192.168.1.200:6666，因为 IP 不相同，所以执行 bind() 的时候，能正常绑定。 所以， 如果多个客户端同时绑定的 IP 地址和端口都是相同的，那么执行 bind() 时候就会出错，错误是“Address already in use”。 一般而言，客户端不建议使用 bind 函数，应该交由 connect 函数来选择端口会比较好，因为客户端的端口通常都没什么意义。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 针对这个问题要看，客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。 但是，因为只要客户端连接的服务器不同，端口资源可以重复使用的。 所以，如果客户端都是与不同的服务器建立连接，即使客户端端口资源只有几万个， 客户端发起百万级连接也是没问题的（当然这个过程还会受限于其他资源，比如文件描述符、内存、CPU 等）。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 前面我们提到，如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。 针对这个问题，也是有解决办法的，那就是打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态，如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 举个例子，假设客户端已经与服务器建立了一个 TCP 连接，并且这个状态处于 TIME_WAIT 状态： 12客户端地址:端口 服务端地址:端口 TCP 连接状态192.168.1.100:2222 172.19.11.21:8888 TIME_WAIT 然后客户端又与该服务器（172.19.11.21:8888）发起了连接，在调用 connect 函数时，内核刚好选择了 2222 端口，接着发现已经被相同四元组的连接占用了： 如果没有开启 net.ipv4.tcp_tw_reuse 内核参数，那么内核就会选择下一个端口，然后继续判断，直到找到一个没有被相同四元组的连接使用的端口， 如果端口资源耗尽还是没找到，那么 connect 函数就会返回错误。 如果开启了 net.ipv4.tcp_tw_reuse 内核参数，就会判断该四元组的连接状态是否处于 TIME_WAIT 状态，如果连接处于 TIME_WAIT 状态并且该状态持续的时间超过了 1 秒，那么就会重用该连接，于是就可以使用 2222 端口了，这时 connect 就会返回成功。 再次提醒一次，开启了 net.ipv4.tcp_tw_reuse 内核参数，是客户端（连接发起方） 在调用 connect() 函数时才起作用，所以在服务端开启这个参数是没有效果的。 客户端端口选择的流程总结 至此，我们已经把客户端在执行 connect 函数时，内核选择端口的情况大致说了一遍，为了让大家更明白客户端端口的选择过程，我画了一流程图。 总结 TCP 和 UDP 可以同时绑定相同的端口吗？ 可以的。 TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP&#x2F;UDP，所以可以根据这个信息确定送给哪个模块（TCP&#x2F;UDP）处理，送给 TCP&#x2F;UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此， TCP&#x2F;UDP 各自的端口号也相互独立，互不影响。 多个 TCP 服务进程可以同时绑定同一个端口吗？ 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。 如何解决服务端重启时，报错“Address already in use”的问题？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 要解决这个问题，我们可以对 socket 设置 SO_REUSEADDR 属性。 这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 客户端的端口可以重复使用吗？ 在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。 TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。 所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态。 如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 RSA 握手如果双方使用传统的 RSA 算法进行密钥交换，那么 pre-master 是由客户端生成的一个随机数，然后用服务器公钥加密后发给服务端，服务端用私钥解密得到 pre-master。 双方再根据 client-random + server-random + pre-master 三个参数计算出主密钥 master-key。 流程图如下： RSA.png 但是这种方式会存在安全问题，它不具有前向安全性。那什么是前向安全性？也就是指历史数据的安全性，不会被破解。 假设一个黑客收集了很多历史数据，当他破解服务端私钥后，可以计算出 pre-master，从而根据历史数据中的 client-random + server-random 计算出密钥，然后就可解密所有之前的加密数据。并且，由于私钥是固定的，在后续新的会话中，仍然可以获取 pre-master，继续截获信息。所以 RSA 并不安全。 ECDHE 握手而现在主流的 TLS 握手算法，一般会选择安全性更强的 ECDHE 实现密钥交换，即椭圆曲线算法，相比 RSA 算法来说具有前向安全性。 因为在每次握手过程中，服务端和客户端都重新会生成 ECDHE 算法的参数，也就是一对公私钥，并且是一次一密。即使黑客截获了当前会话，那也只能监听该次通信内容。 流程图如下： ECDHE.png 从图中我们可以看到，在服务端发送 Server Certificate 后，多了一步 Server Key Exchange 的过程。 服务端会生成一个公钥 server-public-key 发给客户端。 客户端在 Client Key Change 时也会生成一个公钥 client-public-key 发给服务端。 最终，pre-master 由服务端的 server-public-key + 客户端的 client-public-key，再根据 ECDHE 算法计算而来。 该算法可以保证两边计算出来的结果是一样的。 ECDHE 原理在了解什么是 ECDHE 之前，首先可先了解下 DH 算法 的原理。 其实很简单，就是利用了模幂运算的特性。 1gᵃᵇ mod p = (gᵃ mod p)ᵇ mod p = (gᵇ mod p)ᵃ mod p 对照上面 ECDHE 握手过程图来说。 客户端在 Client Key Change 这一步的公私钥数据如下： 12私钥：a公钥：A = gᵃ mod p 服务端在 Server Key Change 这一步的公私钥数据如下： 12私钥：b公钥：B = gᵇ mod p 客户端根据服务端传来的公钥 B 和自己的私钥 a，计算出秘钥 k1： 1k1 = Bᵃ mod p = (gᵇ mod p)ᵃ mod p = gᵃᵇ mod p 服务端根据客户端传来的公钥 A 和自己的私钥 b，计算出秘钥 k2： 1k2 = Aᵇ mod p = (gᵃ mod p)ᵇ mod p = gᵃᵇ mod p 那么，由此计算出的 k1 和 k2 是相等的。 那 DHE 又是什么呢？DHE 算法与 DH 原理是相同的，只不过由于 DH 算法一方的公钥是固定的，不具有前向安全性。因此改进成了 DHE，E 代表 ephemeral，短暂的，即每次都生成公私钥。 ECDHE 则是在 DHE 的基础上，将整数域里的离散对数替换成了椭圆曲线的离散对数，使其更难以破解，更加安全。 注意：ECDHE 算法参数 public-key 是不需要加密的。因为即使黑客拿到了公钥参数，也很难计算出 pre-master。 两者区别两种算法的区别主要在于： RSA 私钥是固定的，破解后可以得到所有的算法参数。 ECDHE 是每次重新生成参数，一次一密，更加安全。 因此在 TLS 1.3 中，废除了 RSA 和 DH 算法，使用了更加安全的 ECDHE。 作者：微微笑的蜗牛链接：https://www.jianshu.com/p/11d6eb418780来源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/categories/Net/"}],"tags":[]},{"title":"Linux","slug":"17linux","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:51:49.170Z","comments":true,"path":"2022/09/01/17linux/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/17linux/","excerpt":"","text":"内核参数调整**作为Java开发者，必可避免的需要开发或使用一些中间件，对于Java开发的中间件，除了JVM参数必须调整外， 的一些内核参数也必须要调整，这里几个，仅供参考。 ** 无非都是跟磁盘文件IO、网络通信、内存管理、线程数量有关系的，因为我们的中间件系统在运行的时候无非就是跟这些打交道。 ![介绍几个Java大型中间件系统中须调整的Linux内核参数介绍几个Java大型中间件系统中须调整的Linux内核参数] 这个参数有三个值可以选择，0、1、2。 如果值是0的话，在你的中间件系统申请内存的时候，操作系统内核会检查可用内存是否足够，如果足够的话就分配内存给你，如果感觉剩余内存不是太够了，干脆就拒绝你的申请，导致你申请内存失败，进而导致中间件系统异常出错。因此一般需要将这个参数的值调整为1，意思是把所有可用的物理内存都允许分配给你，只要有内存就给你来用，这样可以避免申请内存失败的问题。 比如我们曾经线上环境部署的Redis就因为这个参数是0，导致在save数据快照到磁盘文件的时候，需要申请大内存的时候被拒绝了，进而导致了异常报错。 可以用如下 修改： 1echo &#x27;vm.overcommit_memory=1&#x27; &gt;&gt; /etc/sysctl.conf vm.max_map_count 限制一个进程可以拥有的VMA(虚拟内存区域)的数量 这个参数的值会影响中间件系统可以开启的线程的数量，同样也是非常重要的。 如果这个参数过小，有的时候可能会导致有些中间件无法开启足够的线程，进而导致报错，甚至中间件系统挂掉。 他的默认值是65536，但是这个值有时候是不够的，比如我们大数据团队的生产环境部署的Kafka集群曾经有一次就报出过这个异常，说无法开启足够多的线程，直接导致Kafka宕机了。 可以用如下 修改： 1echo &#x27;vm.max_map_count=655360&#x27; &gt;&gt; /etc/sysctl.conf vm.swappiness 这个参数是用来控制进程的swap行为的，这个简单来说就是操作系统会把一部分磁盘空间作为swap区域，然后如果有的进程现在可能不是太活跃，就会被操作系统把进程调整为睡眠状态，把进程中的数据放入磁盘上的swap区域，然后让这个进程把原来占用的内存空间腾出来，交给其他活跃运行的进程来使用。 如果这个参数的值设置为0，意思就是尽量别把任何一个进程放到磁盘swap区域去，尽量大家都用物理内存。 如果这个参数的值是100，那么意思就是尽量把一些进程给放到磁盘swap区域去，内存腾出来给活跃的进程使用。 默认这个参数的值是60，有点偏高了，可能会导致我们的中间件运行不活跃的时候被迫腾出内存空间然后放磁盘swap区域去。因此通常在生产环境建议把这个参数调整小一些，比如设置为10，尽量用物理内存，别放磁盘swap区域去。 可以用如下命令修改： 1echo &#x27;vm.swappiness=10&#x27; &gt;&gt; /etc/sysctl.conf ulimit 这个是用来控制linux上的最大文件连接数的，默认值可能是1024，一般肯定是不够的，因为你在大量频繁的读写磁盘文件的时候，或者是进行网络通信的时候，都会跟这个参数有关系 对于一个中间件系统而言肯定是不能使用默认值得，如果你采用默认值，很可能在线上会出现如下错误： 1error: too many open files 因此通常建议用如下命令修改这个值： 1echo &#x27;ulimit -n 1000000&#x27; &gt;&gt; /etc/profile 一点小小的总结 中间件系统肯定要开启大量的线程（跟vm.max_map_count有关）。 而且要进行大量的网络通信和磁盘IO（跟ulimit有关）。 然后大量的使用内存（跟vm.swappiness和vm.overcommit_memory有关）。 所以对OS内核参数的调整，往往也就是围绕跟中间件系统运行最相关的一些东西。 linux如何管理内存采用段页式管理 Linux 操作系统是采用段页式内存管理方式： 页式存储管理能有效地提高内存利用率（解决内存碎片），而分段存储管理能反映程序的逻辑结构并有利于段的共享。将这两种存储管理方法结合起来，就形成了段页式存储管理方式。 段页式存储管理方式即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。在段页式系统中，为了实现从逻辑地址到物理地址的转换，系统中需要同时配置段表和页表，利用段表和页表进行从用户地址空间到物理内存空间的映射。 系统为每一个进程建立一张段表，每个分段有一张页表。段表表项中至少包括段号、页表长度和页表始址，页表表项中至少包括页号和块号。在进行地址转换时，首先通过段表查到页表始址，然后通过页表找到页帧号，最终形成物理地址。","categories":[{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"}],"tags":[]},{"title":"Nginx","slug":"18Nginx","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:51:12.363Z","comments":true,"path":"2022/09/01/18Nginx/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/18Nginx/","excerpt":"","text":"总结 高可用场景及解决方案添加Nginx主备模式,用keepalived 来管理虚拟ip,主备Nginx对vip进行切换, 安装keepalived 进程间检测对两台nginx所在的服务器的keepalived进行配置 主备 虚拟ip 以及分组 最小配置 第一台 123456789101112131415161718192021global_defs &#123; router_id lb110&#125;vrrp_instance atguigu &#123; # state MASTER #网卡 interface ens33 # virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.44.200 &#125;&#125; 第二台 123456789101112131415161718global_defs &#123; router_id lb110&#125;vrrp_instance atguigu &#123; state BACKUP interface ens33 virtual_router_id 51 priority 50 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.44.200 &#125;&#125; 选举方式主机down了 备用机获得vip 接收请求 集群化(上游服务的集群化)nginx对集群实现负载均衡 负载均衡的方式以及实操 负载均衡负载策略 会话维持 upstream 负载均衡流程 限流配置针对同一IP 的qps限制 针对同一IP的线程数限制 漏桶算法 性能测试jmetter 网页输入ip 默认访问80端口,openresty自动返回&#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;html下的index.html 基础使用 默认配置初步讲解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#user root;#工作进程 根据cpu核数 如果分配给一个核启动多个进程反而效率低worker_processes 1;#pid logs/nginx.pid;events &#123; #一个工作进程对应多少连接 worker_connections 1024;&#125;http &#123; #引入其他文件 mime里是其他请求头 文件类型 include mime.types; #mime不包含则 启动该默认类型 字节流 default_type application/octet-stream; #数据零拷贝 sendfile on; #tcp_nopush on; #keepalive_timeout 0; # 反向代理细讲 keepalive_timeout 65; #gzip on; #一个 config可以配置多个主机,一个server就是一个主机 #虚拟主机 vhost server &#123; listen 80; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root html; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125; 配置&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service，通过systemctl启动： 我这里使用的是openresty 123456789101112[Unit]Description=openresty - high performance web serverAfter=network.target remote-fs.target nss-lookup.target[Service]Type=forkingExecStart=/usr/local/openresty/bin/openresty -c /usr/local/openresty/nginx/conf/nginx.confExecReload=/usr/local/openresty/bin/openresty -s reloadExecStop=/usr/local/openresty/bin/openresty -s stop[Install]WantedBy=multi-user.target 2.使配置生效 1systemctl daemon-reload 多vhost测试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950server &#123; listen 88; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root /www/www; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; server &#123; listen 89; server_name localhost; # 域名/可解析的主机名 #charset koi8-r; #access_log logs/host.access.log main; # uri 描述资源 在域名之后的 location / &#123; root /www/www2; index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 5.2 nginx的限流nginx提供两种限流的方式： 一是控制速率 二是控制并发连接数 5.2.1 控制速率控制速率的方式之一就是采用漏桶算法。 (1)漏桶算法实现控制速率限流 漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率.示意图如下: (2)nginx的配置 配置示意图如下： 超出则返回503 我当前无法响应你 binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。zone：定义共享内存区来存储访问信息， contentRateLimit:10m 表示一个大小为10M，名字为contentRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。rate 用于设置最大访问速率，rate&#x3D;10r&#x2F;s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r&#x2F;s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求.我们这里设置成2 方便测试。 修改&#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;conf&#x2F;nginx.conf: 12345678910111213141516171819202122232425262728293031323334353637383940user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; #使用限流配置 limit_req zone=contentRateLimit; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试： 重新加载配置文件 123cd /usr/local/openresty/nginx/sbin./nginx -s reload 访问页面：http://192.168.211.132/read_content?id=1 ,连续刷新会直接报错。 (3)处理突发流量 上面例子限制 2r&#x2F;s，如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 burst 参数使用来解决该问题。 例如，如下配置表示： 上图代码如下： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数,当 rate&#x3D;10r&#x2F;s 时，将1s拆成10份，即每100ms可处理1个请求。 此处，**burst&#x3D;4 **，若同时有4个请求到达，Nginx 会处理第一个请求，剩余3个请求将放入队列，然后每隔500ms从队列中获取一个请求进行处理。若请求数大于4，将拒绝处理多余的请求，直接返回503. 不过，单独使用 burst 参数并不实用。假设 burst&#x3D;50 ，rate依然为10r&#x2F;s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。 因此，burst 往往结合 nodelay 一起使用。 例如：如下配置： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; limit_req zone&#x3D;one burst&#x3D;5 nodelay;第一个参数：zone&#x3D;contentRateLimit 设置使用哪个配置区域来做限制，与上面limit_req_zone 里的name对应。 第二个参数：burst&#x3D;5，重点说明一下这个配置，burst爆发的意思，这个配置的意思是设置一个大小为5的缓冲区当有大量请求(爆发)过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。 第三个参数：nodelay，如果设置，超过访问频次而且缓冲区也满了的时候就会直接返回503，如果没有设置，则所有请求会等待排队。实例二 burst缓存处理 我们看到，我们短时间内发送了大量请求，Nginx按照毫秒级精度统计，超出限制的请求直接拒绝。这在实际场景中未免过于苛刻，真实网络环境中请求到来不是匀速的，很可能有请求“突发”的情况，也就是“一股子一股子”的。Nginx考虑到了这种情况，可以通过burst关键字开启对突发请求的缓存处理，而不是直接拒绝。 来看我们的配置： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4; }}我们加入了burst&#x3D;4，意思是每个key(此处是每个IP)最多允许4个突发请求的到来。如果单个IP在10ms内发送6个请求，结果会怎样呢？ 相比实例一成功数增加了4个，这个我们设置的burst数目是一致的。具体处理流程是：1个请求被立即处理，4个请求被放到burst队列里，另外一个请求被拒绝。通过burst参数，我们使得Nginx限流具备了缓存处理突发流量的能力。 但是请注意：burst的作用是让多余的请求可以先放到队列里，慢慢处理。如果不加nodelay参数，队列里的请求不会立即处理，而是按照rate设置的速度，以毫秒级精确的速度慢慢处理。 实例三 nodelay降低排队时间 实例二中我们看到，通过设置burst参数，我们可以允许Nginx缓存处理一定程度的突发，多余的请求可以先放到队列里，慢慢处理，这起到了平滑流量的作用。但是如果队列设置的比较大，请求排队的时间就会比较长，用户角度看来就是RT变长了，这对用户很不友好。 有什么解决办法呢？nodelay参数允许请求在排队的时候就立即被处理，也就是说只要请求能够进入burst队列，就会立即被后台worker处理，请注意，这意味着burst设置了nodelay时，系统瞬间的QPS可能会超过rate设置的阈值。nodelay参数要跟burst一起使用才有作用。 延续实例二的配置，我们加入nodelay选项： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4 nodelay; }}单个IP 10ms内并发发送6个请求，结果如下： 跟实例二相比，请求成功率没变化，但是总体耗时变短了。这怎么解释呢？实例二中，有4个请求被放到burst队列当中，工作进程每隔500ms(rate&#x3D;2r&#x2F;s)取一个请求进行处理，最后一个请求要排队2s才会被处理；实例三中，请求放入队列跟实例二是一样的，但不同的是，队列中的请求同时具有了被处理的资格，所以实例三中的5个请求可以说是同时开始被处理的，花费时间自然变短了。 但是请注意，虽然设置burst和nodelay能够降低突发请求的处理时间，但是长期来看并不会提高吞吐量的上限，长期吞吐量的上限是由rate决定的，因为nodelay只能保证burst的请求被立即处理，但Nginx会限制队列元素释放的速度，就像是限制了令牌桶中令牌产生的速度。 看到这里你可能会问，加入了nodelay参数之后的限速算法，到底算是哪一个“桶”，是漏桶算法还是令牌桶算法？当然还算是漏桶算法。考虑一种情况，令牌桶算法的token为耗尽时会怎么做呢？由于它有一个请求队列，所以会把接下来的请求缓存下来，缓存多少受限于队列大小。但此时缓存这些请求还有意义吗？如果server已经过载，缓存队列越来越长，RT越来越高，即使过了很久请求被处理了，对用户来说也没什么价值了。所以当token不够用时，最明智的做法就是直接拒绝用户的请求，这就成了漏桶算法 令牌桶算法: 2.5.2 令牌桶算法令牌桶算法是比较常见的限流算法之一，大概描述如下：1）所有的请求在处理之前都需要拿到一个可用的令牌才会被处理；2）根据限流大小，设置按照一定的速率往桶里添加令牌；3）桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝；4）请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除；5）令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流 如下图： 这个算法的实现，有很多技术，Guaua是其中之一，redis客户端也有其实现。 123456789101112routes: - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/brand** filters: - StripPrefix=1 - name: RequestRateLimiter #请求数限流 名字不能随便写 ，使用默认的facatory args: key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 redis-rate-limiter.replenishRate是您希望允许用户每秒执行多少请求，而不会丢弃任何请求。这是令牌桶填充的速率 redis-rate-limiter.burstCapacity是指令牌桶的容量，允许在一秒钟内完成的最大请求数,将此值设置为零将阻止所有请求。 超过也许会返回429错误码 too many request 如上表示： 平均每秒允许不超过2个请求，突发不超过4个请求，并且处理突发4个请求的时候，没有延迟，等到完成之后，按照正常的速率处理。 如上两种配置结合就达到了速率稳定，但突然流量也能正常处理的效果。完整配置代码如下： 123456789101112131415161718192021222324252627282930313233343536373839user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试：如下图 在1秒钟之内可以刷新4次，正常处理。 但是超过之后，连续刷新5次，抛出异常。 5.2.2 控制并发量（连接数）ngx_http_limit_conn_module 提供了限制连接数的能力。主要是利用limit_conn_zone和limit_conn两个指令。 利用连接数限制 某一个用户的ip连接的数量来控制流量。 注意：并非所有连接都被计算在内 只有当服务器正在处理请求并且已经读取了整个请求头时，才会计算有效连接。此处忽略测试。 配置语法： 123Syntax: limit_conn zone number;Default: —;Context: http, server, location; (1)配置限制固定连接数 如下，配置如下： limit_conn_zone $binary_remote_addr zone&#x3D;addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10M limit_conn addr 2; 表示 同一个地址只允许连接2次。 上图配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; #根据IP地址来限制，存储内存大小10M limit_conn_zone $binary_remote_addr zone=addr:1m; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #所有以brand开始的请求，访问本地changgou-service-goods微服务 location /brand &#123; limit_conn addr 2; proxy_pass http://192.168.211.1:18081; &#125; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 表示： 123limit_conn_zone $binary_remote_addr zone=addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10Mlimit_conn addr 2; 表示 同一个地址只允许连接2次。 测试： 此时开3个线程，测试的时候会发生异常，开2个就不会有异常 (2)限制每个客户端IP与服务器的连接数，同时限制与虚拟服务器的连接总数。(了解) 如下配置： 12345678910111213limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m; server &#123; listen 80; server_name localhost; charset utf-8; location / &#123; limit_conn perip 10;#单个客户端ip与服务器的连接数． limit_conn perserver 100; ＃限制与服务器的总连接数 root html; index index.html index.htm; &#125;&#125; 总结反向代理流程: proxy_pass http:&#x2F;&#x2F; 常用配置server location root alias","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"java","slug":"1java","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:47:28.748Z","comments":true,"path":"2022/09/01/1java/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/1java/","excerpt":"","text":"todo this的理解 30道选择评测: 异常都继承自哪个类,Math.arround(-11.5),构造函数不能写void 写了就不是默认构造函数了 &#96;&#96;&#96;javaclass Test {private String name &#x3D; “person”;&#x2F;&#x2F; void Test() {&#x2F;&#x2F; System.out.println(“Test…”);&#x2F;&#x2F; } Test() { System.out.println(“Test…”); }}public class Main extends Test{&#x2F;&#x2F;学一下自己建树Main() { System.out.println(“MAain..”);}public static void main(String[] args) {&#x2F;&#x2F; new Test(); new Main();}}&#x2F;*Test…MAain.. *&#x2F; 123456789101112131415161718## A：静态方法是一个属于类而不属于对象(实例)的方法。（√）B：静态方法只能访问静态数据。无法访问非静态数据(实例变量)。（√）C：静态方法只能调用其他静态方法，不能从中调用非静态方法。（√）![img](http://uploadfiles.nowcoder.com/images/20150921/458054_1442766565525_E93E59ACFE1791E0A5503384BEBDC544)```javabyte b = 1;char c = 1;short s = 1;int i = 1; 123456789101112131415161718192021byte b = 1;char c = 1;short s = 1;int i = 1;// 三目，一边为byte另一边为char，结果为int// 其它情况结果为两边中范围大的。适用包装类型i = true? b : c;// intb = true? b : b; // bytes = true? b : s;// short// 表达式，两边为byte,short,char，结果为int型// 其它情况结果为两边中范围大的。适用包装类型i = b + c; // inti = b + b; // inti = b + s; // int// 当 a 为基本数据类型时，a += b，相当于 a = (a) (a + b)// 当 a 为包装类型时， a += b 就是 a = a + bb += s; // 没问题c += i; // 没问题// 常量任君搞，long以上不能越b = (char) 1+ (short) 1+ (int) 1; // 没问题// i = (long) 1 // 错误 第2章 Java 概述2.1 Java核心机制-JVM JVM 是一个虚拟的计算机，具有指令集并使用不同的存储区域。负责执行指令，管理数据、内存、寄存器，包含在 JDK 中. 对于不同的平台，有不同的虚拟机。 Java 虚拟机机制屏蔽了底层运行平台的差别，实现了“一次编译，到处运行 2.2JDK JRE JDK 的全称(Java Development Kit Java 开发工具包)JDK &#x3D; JRE + java 的开发工具 [java, javac,javadoc,javap 等] JDK 是提供给 Java 开发人员使用的，其中包含了 java 的开发工具，也包括了 JRE。所以安装了 JDK，就不用在单独安装 JRE 了。2.7.2JRE 基本介绍 JRE(Java Runtime Environment Java 运行环境)JRE &#x3D; JVM + Java 的核心类库[类] 包括 Java 虚拟机(JVM Java Virtual Machine)和 Java 程序所需的核心类库等，如果想要运行一个开发好的 Java 程序，计算机中只需要安装 JRE 即可。 JDK &#x3D; JRE + 开发工具集（例如 Javac,java 编译工具等) JRE &#x3D; JVM + Java SE 标准类库（java 核心类库） 如果只想运行开发好的 .class 文件，只需要JRE 2.3 java开发注意事项123456789101112131415161718192021222324252627282930//这是 java 的快速入门， 演示 java 的开发步骤//对代码的相关说明//1. public class Hello 表示 Hello 是一个类,是一个 public 公有的类//2. Hello&#123; &#125; 表示一个类的开始和结束//3. public static void main(String[] args) 表示一个主方法,即我们程序的入口//4. main() &#123;&#125; 表示方法的开始和结束//5. System.out.println(&quot;hello,world~&quot;); 表示输出&quot;hello,world~&quot;到屏幕//6. ;表示语句结束public class Hello &#123;//编写一个 main 方法public static void main(String[] args) &#123;System.out.println(&quot;韩顺平教育 hello&quot;);&#125;&#125;//一个源文件中最多只能有一个 public 类。其它类的个数不限。[演示]//Dog 是一个类//编译后，每一个类，都对于一个.classclass Dog &#123;//一个源文件中最多只能有一个 public 类。其它类的个数不限，也可以将 main 方法写在非 public 类中，//然后指定运行非 public 类，这样入口方法就是非 public 的 main 方法public static void main(String[] args) &#123;System.out.println(&quot;hello, 小狗狗~&quot;);&#125;&#125;class Tiger &#123;public static void main(String[] args) &#123;System.out.println(&quot;hello, 小老虎~&quot;);&#125;&#125; Java的所有都跟class相关 第3章 变量3.1数据类型与API JavaAPI（application program interface 应用程序编程接口）文档 中文 www.matools.com Java 提供了大量的类，API告诉开发者如何使用这些类 3.2Java类的组织形式 3.3 char+字符编码 3.4Unicode 3.5UTF-8 3.6基本数据类型转换精度小的类型自动转化为精度大的 有多种类型混合运算，系统首先自动将所有数据转换成容量最大的那种数据类型再计算 当把容量大的赋值给容量小的就会报错，反之自动类型转换 byte short 和char 不会自动转换 ，在计算式首先转换为int类型 boolen不参与转换 1234567891011121314151617181920212223242526272829303132333435363738int n1 = 10; //ok//float d1 = n1 + 1.1;//错误 n1 + 1.1 =&gt; 结果类型是 double//double d1 = n1 + 1.1;//对 n1 + 1.1 =&gt; 结果类型是 doublefloat d1 = n1 + 1.1F;//对 n1 + 1.1 =&gt; 结果类型是 float//细节 2: 当我们把精度(容量)大 的数据类型赋值给精度(容量)小 的数据类型时，//就会报错，反之就会进行自动类型转换。////int n2 = 1.1;//错误 double -&gt; int//细节 3: (byte, short) 和 char 之间不会相互自动转换//当把具体数赋给 byte 时，(1)先判断该数是否在 byte 范围内，如果是就可以byte b1 = 10; //对 , -128-127// int n2 = 1; //n2 是 int// byte b2 = n2; //错误，原因： 如果是变量赋值，判断类型//// char c1 = b1; //错误， 原因 byte 不能自动转成 char////韩顺平循序渐进学 Java 零基础第 48页//细节 4: byte，short，char 他们三者可以计算，在计算时首先转换为 int 类型byte b2 = 1;byte b3 = 2;short s1 = 1;//short s2 = b2 + s1;//错, b2 + s1 =&gt; intint s2 = b2 + s1;//对, b2 + s1 =&gt; int//byte b4 = b2 + b3; //错误: b2 + b3 =&gt; int////boolean 不参与转换boolean pass = true;//int num100 = pass;// boolean 不参与类型的自动转换//自动提升原则： 表达式结果的类型自动提升为 操作数中最大的类型//看一道题byte b4 = 1;short s3 = 100;int num200 = 1;float num300 = 1.1F;double num500 = b4 + s3 + num200 + num300; //float -&gt; double&#125; 3.7 强制类型转换自动类型转换的逆过程，将容量大的数据类型转换为容量小的数据类型。使用时要加上强制转换符 ( )，但可能造成精度降低或溢出,格外要注意 第4章 运算符4.1命名规范1.包名：多单词组成时所有字母都小写：aaa.bbb.ccc &#x2F;&#x2F;比如 com.hsp.crm 类名、接口名：多单词组成时，所有单词的首字母大写：XxxYyyZzz [大驼峰]比如： TankShotGame 变量名、方法名：多单词组成时，第一个单词首字母小写，第二个单词开始每个单词首字母大写：xxxYyyZzz [小驼峰， 简称 驼峰法]比如： tankShotGame 常量名：所有字母都大写。多单词时每个单词用下划线连接：XXX_YYY_ZZZ比如 ：定义一个所得税率 TAX_RATE 后面我们学习到 类，包，接口，等时，我们的命名规范要这样遵守,更加详细的看文档. 4.2键盘输入语句在编程中，需要接收用户输入的数据，就可以使用键盘输入语句来获取。Input.java , 需要一个 扫描器(对象), 就是 Scanner 步骤 ： 1) 导入该类的所在包, java.util.* 2) 创建该类对象（声明变量） 3) 调用里面的功能 &#96;&#96;&#96;javaimport java.util.Scanner;&#x2F;&#x2F;表示把 java.util 下的 Scanner 类导入public class Input {&#x2F;&#x2F;编写一个 main 方法public static void main(String[] args) {&#x2F;&#x2F;演示接受用户的输入&#x2F;&#x2F;步骤&#x2F;&#x2F;Scanner 类 表示 简单文本扫描器，在 java.util 包&#x2F;&#x2F;1. 引入&#x2F;导入 Scanner 类所在的包&#x2F;&#x2F;2. 创建 Scanner 对象 , new 创建一个对象,体会&#x2F;&#x2F; myScanner 就是 Scanner 类的对象Scanner myScanner &#x3D; new Scanner(System.in);&#x2F;&#x2F;3. 接收用户输入了， 使用 相关的方法System.out.println(“请输入名字”);&#x2F;&#x2F;当程序执行到 next 方法时，会等待用户输入~~~ String name &#x3D; myScanner.next(); &#x2F;&#x2F;接收用户输入字符串System.out.println(“请输入年龄”);int age &#x3D; myScanner.nextInt(); &#x2F;&#x2F;接收用户输入 intSystem.out.println(“请输入薪水”);double sal &#x3D; myScanner.nextDouble(); &#x2F;&#x2F;接收用户输入 doubleSystem.out.println(“人的信息如下:”);韩顺平循序渐进学 Java 零基础第 81页System.out.println(“名字&#x3D;” + name “ 年龄&#x3D;” + age + “ 薪水&#x3D;” + sal);}}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233 # 第6章 数组## 6.1数组的使用### 使用方式1-动态初始化//(1) 第一种动态分配方式//double scores[] = new double[5];//(2) 第 2 种动态分配方式， 先声明数组，再 new 分配空间double scores[] ; //声明数组， 这时 scores 是 nullscores = new double[5]; // 分配内存空间，可以存放数据### 使用方式2-动态初始化### 使用方式3-静态初始化int[] a =&#123;2,3,5,6,9,41,2,1&#125;;## 6.2数组使用细节1) 数组是多个相同类型数据的组合，实现对这些数据的统一管理2) 数组中的元素可以是任何数据类型，包括基本类型和引用类型，但是不能混用。3) 数组创建后，如果没有赋值，有默认值int 0，short 0, byte 0, long 0, float 0.0,double 0.0，char \\u0000，boolean false，String null4) 使用数组的步骤 1. 声明数组并开辟空间 2 给数组各个元素赋值 3 使用数组5) 数组的下标是从 0 开始的。6) 数组下标必须在指定范围内使用，否则报：下标越界异常，比如韩顺平循序渐进学 Java 零基础第 148页int [] arr=new int[5]; 则有效下标为 0-47) 数组属引用类型，数组型数据是对象(object)## 6.3数组赋值机制[(141条消息) java堆，栈，常量池最通俗易懂的图文解释_我的博客-CSDN博客_堆栈常量池](https://blog.csdn.net/weixin_41804049/article/details/80393892)1）基本类型赋值，这个值就是具体的数据，而且互不影响2)数组在默认情况下是引用传递，赋的值是地址 栈存放基本数据类型的变量值 和对象的引用堆存放对象的具体实例 new出来的字符串常量对象存放在常量池中## 6.4数组拷贝//将 int[] arr1 = &#123;10,20,30&#125;; 拷贝到 arr2 数组, //要求数据空间是独立的. int[] arr1 = &#123;10,20,30&#125;;//创建一个新的数组 arr2,开辟新的数据空间//大小 arr1.length;int[] arr2 = new int[arr1.length];//遍历 arr1 ，把每个元素拷贝到 arr2 对应的元素位置for(int i = 0; i &lt; arr1.length; i++) &#123;arr2[i] = arr1[i];&#125;//老师修改 arr2， 不会对 arr1 有影响. arr2[0] = 100;//输出 arr1System.out.println(&quot;====arr1 的元素====&quot;);for(int i = 0; i &lt; arr1.length; i++) &#123;System.out.println(arr1[i]);//10,20,30&#125;## 6.5数组翻转方式 1：通过找规律反转 【思路分析】public class ArrayReverse &#123;//编写一个 main 方法public static void main(String[] args) &#123;//定义数组int[] arr = &#123;11, 22, 33, 44, 55, 66&#125;;//老韩思路//规律//1. 把 arr[0] 和 arr[5] 进行交换 &#123;66,22,33,44,55,11&#125;韩顺平循序渐进学 Java 零基础第 155页//2. 把 arr[1] 和 arr[4] 进行交换 &#123;66,55,33,44,22,11&#125;//3. 把 arr[2] 和 arr[3] 进行交换 &#123;66,55,44,33,22,11&#125;//4. 一共要交换 3 次 = arr.length / 2//5. 每次交换时，对应的下标 是 arr[i] 和 arr[arr.length - 1 -i]//代码//优化int temp = 0;int len = arr.length; //计算数组的长度for( int i = 0; i &lt; len / 2; i++) &#123;temp = arr[len - 1 - i];//保存arr[len - 1 - i] = arr[i];arr[i] = temp;&#125;System.out.println(&quot;===翻转后数组===&quot;);for(int i = 0; i &lt; arr.length; i++) &#123;System.out.print(arr[i] + &quot;\\t&quot;);//66,55,44,33,22,11&#125;&#125;&#125;方式 2：使用逆序赋值方式 【思路分析, 学员自己完成】 ArrayReverse02.javapublic class ArrayReverse02 &#123;//编写一个 main 方法public static void main(String[] args) &#123;//定义数组int[] arr = &#123;11, 22, 33, 44, 55, 66&#125;;//使用逆序赋值方式//老韩思路//1. 先创建一个新的数组 arr2 ,大小 arr.length//2. 逆序遍历 arr ,将 每个元素拷贝到 arr2 的元素中(顺序拷贝)//3. 建议增加一个循环变量 j -&gt; 0 -&gt; 5int[] arr2 = new int[arr.length];//逆序遍历 arrfor(int i = arr.length - 1, j = 0; i &gt;= 0; i--, j++) &#123;arr2[j] = arr[i];&#125;//4. 当 for 循环结束，arr2 就是一个逆序的数组 &#123;66, 55, 44,33, 22, 11&#125;//5. 让 arr 指向 arr2 数据空间, 此时 arr 原来的数据空间就没有变量引用// 会被当做垃圾，销毁arr = arr2;System.out.println(&quot;====arr 的元素情况=====&quot;);//6. 输出 arr 看看for(int i = 0; i &lt; arr.length; i++) &#123;韩顺平循序渐进学 Java 零基础第 157页System.out.print(arr[i] + &quot;\\t&quot;);&#125;## 6.6数组添加/扩容思路分析1. 定义初始数组 int[] arr = &#123;1,2,3&#125;//下标 0-22. 定义一个新的数组 int[] arrNew = new int[arr.length+1];3. 遍历 arr 数组，依次将 arr 的元素拷贝到 arrNew 数组4. 将 4 赋给 arrNew[arrNew.length - 1] = 4;把 4 赋给 arrNew 最后一个元素5. 让 arr 指向 arrNew ; arr = arrNew; 那么 原来 arr 数组就被销毁6. 创建一个 Scanner 可以接受用户输入## 补充1.&gt; 基本数据类型： 四类八种：1. 整数型：byte（1字节）、short（2字节） 、int（4字节）、long（8字节）long 类型要在后面加L，（可以省略，超出int 的范围时需要加L）2. 浮点型：float（4字节）、double（8字节）float 类型 要在数字后面加f3. 字符型：char（2字节）4. 布尔型：boolean（1位）是非对错&gt; 引用数据类型：除了基本数据类型之外的、都叫引用类型。1. 类2. 接口3. 数组4. 枚举## 4.什么是字节1. 位（bit）： 是计算机 内部数据 存储的最小单位，11001100是一个八位二进制数。2. 字节（byte） 是计算机中 数据处理 的基本单位，习惯上用大写B来表示；1B（byte字节）1B（byte字节） = 8bit （位）3. 字符： 是指计算机中使用的字母、数字、和符号&gt; 常见单位换算 1bit 表示 1位 1byte 表示一个字节 1B=8b 1024b = 1kb 1024kb = 1M 1024M = 1G 1024G = 1TB## 5.进制问题&gt; 进制说明二进制：0b十进制：默认八进制：0 逢八进一十六进制：0x 逢十六进一银行业务用什么表示？用BigDecimal类 数学工具类不能使用浮点数。- float： 浮点数是有限的 舍入误差，大约，接近但不等于- double： 最好完全使用浮点数进行比较 最好完全使用浮点数进行比较 最好完全使用浮点数进行比较```javapublic static void main(String[] args) &#123; int i1 = 10; int i2 = 010; int i3 = 0x10; System.out.println(i1);//10 System.out.println(i2);//8 System.out.println(i3);//16 float f = 0.1f;//0.1 double d = 1.0/10;//0.1 System.out.println(f == d);//false&#125;1234567891011121314 字符扩展？ 1234567char c1 = &#x27;a&#x27;;char c2 = &#x27;中&#x27;;System.out.println(c1);System.out.println((int)c1);//97 强制转换System.out.println(c2);System.out.println((int)c2);//20013123456 所有的字符本质还是数字编码问题 Unicode 表：（97 &#x3D; a，65 &#x3D; A） 编码 占了两个字节 转义字符\\t 制表符\\n 换行… 6.类型转换 通过查看byte包装类型得到：byte 最大值信息 1234public static final byte MIN_VALUE = -128;public static final byte MAX_VALUE = 127;123 由于java是强类型语言，所以要进行有些运算的时候，需要用到类型转换。 低 ———————————————-&gt;高 byte，short，char—&gt;，int—&gt;，float—&gt;，long—&gt;，double 运算中，不同类型的数据先转换为同一类型，然后进行运算。（小数优先级高于整数） 强制类型转换 高到低 12345int i = 128;byte b = (byte) i;//内存溢出System.out.println(i);//128System.out.println(b);//-1281234 自动类型转换 低到高 12345int i = 128;double d = i;System.out.println(i);//128System.out.println(d);//128.01234 总结： 不能对布尔类型进行转换。（不能把人转成猪，可以把男人转女人） 不能把对象类型转换为不相干的类型。 在把高容量转换为低容量的时候，需要强制转换。 转换的时候可能存在内存溢出，或者精度问题。 Java流程控制1.Scanner 类 通过scanner 类的next() 与nextLine() 方法获取输入的字符串，在读取我们一般需要使用hasNext() 与 hasNextLine() 判断是否还有输入的数据。 1234567891011121314151617Scanner sc = new Scanner(System.in)1public static void main(String[] args) &#123; //从键盘接收数据 Scanner sc = new Scanner(System.in); System.out.println(&quot;使用next方式接收：&quot;); //判断是否还有输入 if (sc.hasNext())&#123; //next() 只读取第一个字符串 //String str = sc.next(); //nextLine() 获取一行数据 String str = sc.nextLine(); System.out.println(&quot;用户输入是：&quot;+str); &#125; sc.close();&#125;1234567891011121314 next()： 一定要读取到有效字符后才可以结束输入。 对输入有效字符之前遇到的空白，next（）方法会自动将其去掉。 只有输入有效字符后才将其后面输入的空白作为分隔符或者结束符。 next（）方法不能得到有空格的不能得到有空格的字符串 nextLine(): 以Enter为结束符，也就是说 nextLine() 方法返回的是输入回车之前的所有字符 可以获得空白。 4.switch 多选择语句 switch case 语句判断一个变量和与一系列值中某个值相等，每个值称为一个分支。 switch 语句中的变量类型可以是： byte、short、int、或者是char 从 Java SE 7开始 switch 开始支持字符串 String 类型了 同时case 标签必须为字符串常量或字面量 第7章面向对象基础部分7.1创建对象 先声明再创建 Cat cat ; &#x2F;&#x2F;声明对象 cat cat &#x3D; new Cat(); &#x2F;&#x2F;创建 直接创建 Cat cat &#x3D; new Cat(); 创建对象 在方法区加载 7.2Java 内存的结构分析 栈： 一般存放基本数据类型(局部变量) 堆： 存放对象(Cat cat , 数组等) 方法区：常量池(常量，比如字符串)， 类加载信息 示意图 [Cat (name, age, price)] Java 创建对象的流程简单分析 Person p &#x3D; new Person(); p.name &#x3D; “jack”; p.age &#x3D; 10 先加载 Person 类信息(属性和方法信息, 只会加载一次) 在堆中分配空间, 进行默认初始化(看规则) 把地址赋给 p , p 就指向对象 进行指定初始化， 比如 p.name &#x3D;”jack” 方法的调用机制原理 1.当程序执行到方法时，就会开辟一个独立的空间（栈空间） 2.当方法执行完毕，或者执行到return语句时就会返回 3.返回到调用方法的地方 4.返回后继续执行方法后面的代码 5.当main方法（栈）执行完毕，整个程序退出 方法的访问修饰符 方法的返回数据类型 void可无return，或者return; 7.4成员方法传参机制7.4.1基本数据类型的传参机制 基本数据类型，传递的是值 ，形参的任何改变不影响实参 7.4.2引用数据类型的传参机制 B 类中编写一个方法 test100，可以接收一个数组，在方法中修改该数组，看看原来的数组是否变化？会变化 B 类中编写一个方法 test200，可以接收一个 Person(age,sal)对象，在方法中修改该对象属性，看看原来的对象是否变化？会变化 引用类型传递的是地址（传递也是值，但是值是地址） 方法栈 如果对方法传入的引入对象 然后对引入对象进行更改，并没有对原指针更改，而是将指针传给另一个值，可以根据这个地址更改原来指向的对象的属性，但不能更改原来的对象的地址 &#x2F;&#x2F;编写一个方法 copyPerson，可以复制一个 Person 对象，返回复制的对象。克隆对象， 123456789101112131415//注意要求得到新对象和原来的对象是两个独立的对象，只是他们的属性相同////编写方法的思路//1. 方法的返回类型 Person//2. 方法的名字 copyPerson//3. 方法的形参 (Person p)//4. 方法体, 创建一个新对象，并复制属性，返回即可public Person copyPerson(Person p) &#123;//创建一个新的对象Person p2 = new Person();p2.name = p.name; //把原来对象的名字赋给 p2.namep2.age = p.age; //把原来对象的年龄赋给 p2.agereturn p2;&#125;&#125; 7.4.3方法的递归调用 每次调用方法 都会形成方法栈 占用主栈的空间 7.5方法重载 java 中允许同一个类中，多个同名方法的存在，但要求 形参列表不一致！ 比如：System.out.println(); out 是 PrintStream 类 方法名相同 形参列表必须不同 形参类型 或个数 或顺序 7.6可变参数7.6.1基本概念 java 允许将同一个类中多个同名同功能但参数个数不同的方法，封装成一个方法。 就可以通过可变参数实现 7.6.2基本语法 访问修饰符 返回类型 方法名(数据类型… 形参名) { } 7.6.3快速入门案例 12345678910111213141516171819202122232425262728293031323334看一个案例 类 HspMethod，方法 sum 【可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。】public class VarParameter01 &#123;//编写一个 main 方法public static void main(String[] args) &#123;HspMethod m = new HspMethod();System.out.println(m.sum(1, 5, 100)); //106System.out.println(m.sum(1,19)); //20&#125;&#125;class HspMethod &#123;//可以计算 2 个数的和，3 个数的和 ， 4. 5， 。。//可以使用方法重载// public int sum(int n1, int n2) &#123;//2 个数的和// return n1 + n2;// &#125;// public int sum(int n1, int n2, int n3) &#123;//3 个数的和// return n1 + n2 + n3;// &#125;// public int sum(int n1, int n2, int n3, int n4) &#123;//4 个数的和// return n1 + n2 + n3 + n4;// &#125;//..... //上面的三个方法名称相同，功能相同, 参数个数不同-&gt; 使用可变参数优化//1. int... 表示接受的是可变参数，类型是 int ,即可以接收多个 int(0-多)//2. 使用可变参数时，可以当做数组来使用 即 nums 可以当做数组//3. 遍历 nums 求和即可public int sum(int... nums) &#123;//System.out.println(&quot;接收的参数个数=&quot; + nums.length);int res = 0;for(int i = 0; i &lt; nums.length; i++) &#123;res += nums[i];&#125;return res;&#125;&#125; 7.6.4注意事项和使用细节 可变参数的实参可以为0或任意多个 可变参数的实参可以维数组 可变参数的本质就是数组 可变参数可以和普通参数一起放在形参列表，但必须保证可变参数在最后 一个形参列表中只能出现一个可变参数 &#x2F;&#x2F;细节: 可变参数的实参可以为数组 int[] arr &#x3D; {1, 2, 3}; T t1 &#x3D; new T(); t1.f1(arr); } } class T { public void f1(int… nums) { System.out.println(“长度&#x3D;” + nums.length); } 7.7作用域（对类来说） 1.在Java编程中，主要的变量就是属性（成员变量）和局部变量 2.我们说的局部变量一般是指在成员方法中定义的变量 3.Java中作用域的分类 全局变量：也就是属性，作用域为整个类体 局部变量：也就是除了属性之外的其他变量，作用域为定义它的代码块中 4.属性可以不赋值，直接使用，因为有默认值，局部变量必须赋值后，才能使用因为没有默认值 5.注意事项和细节使用 属性和局部变量可以重名，访问时采用就近原则 同作用域不能重名 属性生命周期较长 伴随着对象的创建而创建，伴随着对象的销毁而销毁 局部变量伴随着代码快的执行而创建，伴随着代码的结束而销毁。即在一次方法调用过程中 作用域范围不同 ：属性可以被本类使用，或其他类使用（通过对象调用） 局部变量只能在本类中对应的方法中调用 修饰符不同 属性可以加修饰符，局部变量不可以加修饰符 7.8构造方法&#x2F;构造器第10章面向对象高级部分10.5 final 关键字 可以修饰类，属性，方法和局部变量 第13 章 常用类 13.1包装类 有了类的特点，就可以调用类的方法 13.1.1装箱拆箱 JDK5前 装箱：基本数据类型-&gt;包装类 拆箱相反 手动 12345678910111213141516171819public class Integer01 &#123;public static void main(String[] args) &#123;//演示 int &lt;--&gt; Integer 的装箱和拆箱//jdk5 前是手动装箱和拆箱//手动装箱 int-&gt;Integerint n1 = 100;Integer integer = new Integer(n1);Integer integer1 = Integer.valueOf(n1);//两种方式都可以 都是手动装箱//手动拆箱//Integer -&gt; intint i = integer.intValue();//jdk5 后，就可以自动装箱和自动拆箱int n2 = 200;//自动装箱 int-&gt;IntegerInteger integer2 = n2; //底层使用的是 Integer.valueOf(n2)//自动拆箱 Integer-&gt;intint n3 = integer2; //底层仍然使用的是 intValue()方法&#125;&#125; 13.1.2包装类之间的转化 案例演示, 以 Integer 和 String 转换为例，其它类似: 1234567891011121314151617181920212223package com.hspedu.wrapper;/*** @author 韩顺平* @version 1.0 */ public class WrapperVSString &#123; public static void main(String[] args) &#123; //包装类(Integer)-&gt;String Integer i = 100;//自动装箱 //方式 1 String str1 = i + &quot;&quot;; //方式 2 String str2 = i.toString(); //方式 3 String str3 = String.valueOf(i); //String -&gt; 包装类(Integer) String str4 = &quot;12345&quot;; Integer i2 = Integer.parseInt(str4);//使用到自动装箱 Integer i3 = new Integer(str4);//构造器 System.out.println(&quot;ok~~&quot;); &#125; &#125; 13.1.3Integer 类和 Character类的常用方法 public class WrapperMethod { public static void main(String[] args) { System.out.println(Character.isDigit(‘a’));&#x2F;&#x2F;判断是不是数字 System.out.println(Character.isLetter(‘a’));&#x2F;&#x2F;判断是不是字母 System.out.println(Character.isUpperCase(‘a’));&#x2F;&#x2F;判断是不是大写 System.out.println(Character.isLowerCase(‘a’));&#x2F;&#x2F;判断是不是小写 System.out.println(Character.isWhitespace(‘a’));&#x2F;&#x2F;判断是不是空格 System.out.println(Character.toUpperCase(‘a’));&#x2F;&#x2F;转成大写 System.out.println(Character.toLowerCase(‘A’));&#x2F;&#x2F;转成小写 13.1.4面试题 1234567891011121314151617181920212223242526public class WrapperExercise02 &#123;public static void main(String[] args) &#123;Integer i = new Integer(1);Integer j = new Integer(1);System.out.println(i == j); //False//所以，这里主要是看范围 -128 ~ 127 就是直接返回/*//1. 如果 i 在 IntegerCache.low(-128)~IntegerCache.high(127),就直接从数组返回//2. 如果不在 -128~127,就直接 new Integer(i)public static Integer valueOf(int i) &#123;if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high)return IntegerCache.cache[i + (-IntegerCache.low)];return new Integer(i);&#125;*/Integer m = 1; //底层 Integer.valueOf(1); -&gt; 阅读源码Integer n = 1;//底层 Integer.valueOf(1);System.out.println(m == n); //T//所以，这里主要是看范围 -128 ~ 127 就是直接返回//，否则，就 new Integer(xx);Integer x = 128;//底层 Integer.valueOf(1);Integer y = 128;//底层 Integer.valueOf(1);System.out.println(x == y);//False&#125;&#125; 13.2 String 类（双引号括起来）13.2.1 String 类的理解和创建对象 字符串常量用双引号扩起的字符序列 ： “你好”，”12.97”等 使用Unicode编码 一个字符（不区分字母还是汉子)占两个字节 String 类常用构造器（看手册） &#x2F;&#x2F;1.String 对象用于保存字符串，也就是一组字符序列 &#x2F;&#x2F;2. “jack” 字符串常量, 双引号括起的字符序列 &#x2F;&#x2F;3. 字符串的字符使用 Unicode 字符编码，一个字符(不区分字母还是汉字)占两个字节 &#x2F;&#x2F;4. String 类有很多构造器，构造器的重载 &#x2F;&#x2F; 常用的有 String s1 &#x3D; new String(); &#x2F;&#x2F; &#x2F;&#x2F;String s2 &#x3D; new String(String original); &#x2F;&#x2F;String s3 &#x3D; new String(char[] a); &#x2F;&#x2F;String s4 &#x3D; new String(char[] a,int startIndex,int count) &#x2F;&#x2F;String s5 &#x3D; new String(byte[] b) &#x2F;&#x2F;5. String 类实现了接口 Serializable【String 可以串行化:可以在网络传输】 &#x2F;&#x2F; 接口 Comparable [String 对象可以比较大小] &#x2F;&#x2F;6. String 是 final 类，不能被其他的类继承 &#x2F;&#x2F;7. String 有属性 private final char value[]; 用于存放字符串内容 &#x2F;&#x2F;8. 一定要注意：value 是一个 final 类型， 不可以修改(需要功力)：即 value 不能指向 &#x2F;&#x2F; 新的地址，但是单个字符内容是可以变化 13.2.2创建String对象的两种方式 方式一：直接赋值 String s &#x3D; “ashndkjand”; 方式二: 调用构造器 String s &#x3D; new String( “ashndkjand”); 13.2.3两种方式的区别 方式一：先从常量池找是否有 “ashndkjand”数据空间，若有则直接指向，若没有则在常量池中创建，然后指向 方式二：先在堆中创建空间，里面维护了value属性，指向常量池的”ashndkjand”空间。如果常量池中没有 “ashndkjand”空间，重新创建，如果有value直接指向常量池， s最终指向的对空间 13.3字符串的特性 ​ 13.4 String类的常见方法 ​ String类是保存字符串常量的，每次更新都需要重新开辟空间，效率较低，因此java设计者还提供了StringBuilder和StringBuffer来增强String的功能，并提高效率 1234567891011121314151617181920212223242526272829303132333435public class StringMethod01 &#123;public static void main(String[] args) &#123;**//1. equals 前面已经讲过了. 比较内容是否相同，区分大小写**String str1 = &quot;hello&quot;;String str2 = &quot;Hello&quot;;System.out.println(str1.equals(str2));//**// 2.equalsIgnoreCase 忽略大小写的判断内容是否相等**String username = &quot;johN&quot;;if (&quot;john&quot;.equalsIgnoreCase(username)) &#123;System.out.println(&quot;Success!&quot;);&#125; else &#123;System.out.println(&quot;Failure!&quot;);&#125;**// 3.length 获取字符的个数，字符串的长度**System.out.println(&quot;韩顺平&quot;.length());**// 4.indexOf 获取字符在字符串对象中第一次出现的索引，索引从 0 开始，如果找不到，返回-1**String s1 = &quot;wer@terwe@g&quot;;int index = s1.indexOf(&#x27;@&#x27;);System.out.println(index);// 3System.out.println(&quot;weIndex=&quot; + s1.indexOf(&quot;we&quot;));//0**// 5.lastIndexOf 获取字符在字符串中最后一次出现的索引，索引从 0 开始，如果找不到，返回-1**s1 = &quot;wer@terwe@g@&quot;;index = s1.lastIndexOf(&#x27;@&#x27;);System.out.println(index);//11System.out.println(&quot;ter 的位置=&quot; + s1.lastIndexOf(&quot;ter&quot;));//4**// 6.substring 截取指定范围的子串**String name = &quot;hello,张三&quot;;//下面 name.substring(6) 从索引 6 开始截取后面所有的内容System.out.println(name.substring(6));//截取后面的字符//name.substring(0,5)表示从索引 0 开始截取，截取到索引 5-1=4 位置System.out.println(name.substring(2,5));//llo&#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class StringMethod02 &#123;public static void main(String[] args) &#123;**// 1.toUpperCase 转换成大写**String s = &quot;heLLo&quot;;System.out.println(s.toUpperCase());//HELLO**// 2.toLowerCase**System.out.println(s.toLowerCase());//hello**// 3.concat 拼接字符串**String s1 = &quot;宝玉&quot;;s1 = s1.concat(&quot;林黛玉&quot;).concat(&quot;薛宝钗&quot;).concat(&quot;together&quot;);System.out.println(s1);//宝玉林黛玉薛宝钗 together**// 4.replace 替换字符串中的字符**s1 = &quot;宝玉 and 林黛玉 林黛玉 林黛玉&quot;;//在 s1 中，将 所有的 林黛玉 替换成薛宝钗// 老韩解读: s1.replace() 方法执行后，返回的结果才是替换过的. // 注意对 s1 没有任何影响String s11 = s1.replace(&quot;宝玉&quot;, &quot;jack&quot;);System.out.println(s1);//宝玉 and 林黛玉 林黛玉 林黛玉System.out.println(s11);//jack and 林黛玉 林黛玉 林黛玉**// 5.split 分割字符串, 对于某些分割字符，我们需要 转义比如 | \\\\等**String poem = &quot;锄禾日当午,汗滴禾下土,谁知盘中餐,粒粒皆辛苦&quot;;//老韩解读：// 1. 以 , 为标准对 poem 进行分割 , 返回一个数组// 2. 在对字符串进行分割时，如果有特殊字符，需要加入 转义符 \\String[] split = poem.split(&quot;,&quot;);poem = &quot;E:\\\\aaa\\\\bbb&quot;;split = poem.split(&quot;\\\\\\\\&quot;);System.out.println(&quot;==分割后内容===&quot;);for (int i = 0; i &lt; split.length; i++) &#123;System.out.println(split[i]);&#125;**// 6.toCharArray 转换成字符数组**s = &quot;happy&quot;;char[] chs = s.toCharArray();for (int i = 0; i &lt; chs.length; i++) &#123;System.out.println(chs[i]);&#125;**// 7.compareTo 比较两个字符串的大小，如果前者大，****// 则返回正数，后者大，则返回负数，如果相等，返回 0**// 老韩解读// (1) 如果长度相同，并且每个字符也相同，就返回 0// (2) 如果长度相同或者不相同，但是在进行比较时，可以区分大小// 就返回 if (c1 != c2) &#123;// return c1 - c2;// &#125;// (3) 如果前面的部分都相同，就返回 str1.len - str2.lenString a = &quot;jcck&quot;;// len = 3String b = &quot;jack&quot;;// len = 4System.out.println(a.compareTo(b)); // 返回值是 &#x27;c&#x27; - &#x27;a&#x27; = 2 的值**// 8.format 格式字符串**/* 占位符有:%s 字符串 %c 字符 %d 整型 %.2f 浮点型**/String name = &quot;john&quot;;int age = 10;double score = 56.857;char gender = &#x27;男&#x27;;//将所有的信息都拼接在一个字符串. String info =&quot;我的姓名是&quot; + name + &quot;年龄是&quot; + age + &quot;,成绩是&quot; + score + &quot;性别是&quot; + gender + &quot;。希望大家喜欢我！&quot;;System.out.println(info);//老韩解读//1. %s , %d , %.2f %c 称为占位符//2. 这些占位符由后面变量来替换//3. %s 表示后面由 字符串来替换//4. %d 是整数来替换//5. %.2f 表示使用小数来替换，替换后，只会保留小数点两位, 并且进行四舍五入的处理//6. %c 使用 char 类型来替换String formatStr = &quot;我的姓名是%s 年龄是%d，成绩是%.2f 性别是%c.希望大家喜欢我！&quot;;String info2 = String.format(formatStr, name, age, score, gender);System.out.println(&quot;info2=&quot; + info2); //7 split()（分裂）根据匹配给定的正则表达式来拆分字符串 //注意、.、$、|、等转义字符，必须加\\\\ 注意：多个分隔符可以用|作为连字符 public String[] split(String regex, int limit) //regex 正则表达式分隔符 limit 分割的份数 //return 字符串&#125;&#125; 13.5 StringBuffer类13.5.1基本介绍 java.lang.StringBuffer代表可变的字符序列，可以对字符串内容进行增删。 很多方法与String相同 ，但StringBuilder是可变长度的。 1234567891011121314public class StringBuffer01 &#123;public static void main(String[] args) &#123;//老韩解读//1. StringBuffer 的直接父类 是 AbstractStringBuilder//2. StringBuffer 实现了 Serializable, 即 StringBuffer 的对象可以串行化//3. 在父类中 AbstractStringBuilder 有属性 char[] value,不是 final// 该 value 数组存放 字符串内容，引出存放在堆中的//4. StringBuffer 是一个 final 类，不能被继承//5. 因为 StringBuffer 字符内容是存在 char[] value, 所有在变化(增加/删除)// 不用每次都更换地址(即不是每次创建新对象)， 所以效率高于 StringStringBuffer stringBuffer = new StringBuffer(&quot;hello&quot;);&#125;&#125; 13.5.2 String vs StringBuffer String保存的是字符串常量，里面的值不能更改，每次String类的更新实际上就是更改地址，效率较低 StringBuffer保存的是字符串变量，里面的值可以更改，每次StringBuffer的更新实际上可以更改内容，不用每次都更新地址效率较高 13.5.3 两者的互相转换 123456789101112131415161718public class StringAndStringBuffer &#123;public static void main(String[] args) &#123;//看 String——&gt;StringBufferString str = &quot;hello tom&quot;;//方式 1 使用构造器//注意： 返回的才是 StringBuffer 对象，对 str 本身没有影响StringBuffer stringBuffer = new StringBuffer(str);//方式 2 使用的是 append 方法StringBuffer stringBuffer1 = new StringBuffer();stringBuffer1 = stringBuffer1.append(str);//看看 StringBuffer -&gt;StringStringBuffer stringBuffer3 = new StringBuffer(&quot;韩顺平教育&quot;);//方式 1 使用 StringBuffer 提供的 toString 方法String s = stringBuffer3.toString();//方式 2: 使用构造器来搞定String s1 = new String(stringBuffer3);&#125;&#125; 13.5.4 StringBuffer 类常见方法 12345678910111213141516171819202122232425262728293031323334public class StringBufferMethod &#123;public static void main(String[] args) &#123;StringBuffer s = new StringBuffer(&quot;hello&quot;);//增s.append(&#x27;,&#x27;);// &quot;hello,&quot;s.append(&quot;张三丰&quot;);//&quot;hello,张三丰&quot;s.append(&quot;赵敏&quot;).append(100).append(true).append(10.5);//&quot;hello,张三丰赵敏 100true10.5&quot; System.out.println(s);//&quot;hello,张三丰赵敏 100true10.5&quot;//删/** 删除索引为&gt;=start &amp;&amp; &lt;end 处的字符* 解读: 删除 11~14 的字符 [11, 14)*/s.delete(11, 14);System.out.println(s);//&quot;hello,张三丰赵敏 true10.5&quot;//改//老韩解读，使用 周芷若 替换 索引 9-11 的字符 [9,11)s.replace(9, 11, &quot;周芷若&quot;);System.out.println(s);//&quot;hello,张三丰周芷若 true10.5&quot;//查找指定的子串在字符串第一次出现的索引，如果找不到返回-1int indexOf = s.indexOf(&quot;张三丰&quot;);System.out.println(indexOf);//6//插//老韩解读，在索引为 9 的位置插入 &quot;赵敏&quot;,原来索引为 9 的内容自动后移s.insert(9, &quot;赵敏&quot;);System.out.println(s);//&quot;hello,张三丰赵敏周芷若 true10.5&quot;//长度System.out.println(s.length());//22System.out.println(s);&#125;&#125; 13.6 StringBuilder 线程不安全的StringBuffer13.7 Math 类 Math类包含用于执行基本数学运算的方法，如初等函数、对数、平方根和三角函数。 均为静态方法 12345678910111213141516171819202122232425262728293031323334353637383940414243public class MathMethod &#123;public static void main(String[] args) &#123;//看看 Math 常用的方法(静态方法)//1.abs 绝对值int abs = Math.abs(-9);System.out.println(abs);//9//2.pow 求幂double pow = Math.pow(2, 4);//2 的 4 次方System.out.println(pow);//16//3.ceil 向上取整,返回&gt;=该参数的最小整数(转成 double);double ceil = Math.ceil(3.9);System.out.println(ceil);//4.0//4.floor 向下取整，返回&lt;=该参数的最大整数(转成 double)double floor = Math.floor(4.001);System.out.println(floor);//4.0//5.round 四舍五入 Math.floor(该参数+0.5)long round = Math.round(5.51);System.out.println(round);//6//6.sqrt 求开方double sqrt = Math.sqrt(9.0);System.out.println(sqrt);//3.0//7.random 求随机数// random 返回的是 0 &lt;= x &lt; 1 之间的一个随机小数// 思考：请写出获取 a-b 之间的一个随机整数,a,b 均为整数 ，比如 a = 2, b=7// 即返回一个数 x 2 &lt;= x &lt;= 7// 解读 Math.random() * (b-a) 返回的就是 0 &lt;= 数 &lt;= b-a// (1) (int)(a) &lt;= x &lt;= (int)(a + Math.random() * (b-a +1) )// (2) 使用具体的数给小伙伴介绍 a = 2 b = 7// (int)(a + Math.random() * (b-a +1) ) = (int)( 2 + Math.random()*6)// Math.random()*6 返回的是 0 &lt;= x &lt; 6 小数// 2 + Math.random()*6 返回的就是 2&lt;= x &lt; 8 小数// (int)(2 + Math.random()*6) = 2 &lt;= x &lt;= 7// (3) 公式就是 (int)(a + Math.random() * (b-a +1) )for(int i = 0; i &lt; 100; i++) &#123;System.out.println((int)(2 + Math.random() * (7 - 2 + 1)));&#125;//max , min 返回最大值和最小值int min = Math.min(1, 9);int max = Math.max(45, 90);System.out.println(&quot;min=&quot; + min);System.out.println(&quot;max=&quot; + max);&#125;&#125; 13.8 Arrays类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166public class ArraysMethod01 &#123;public static void main(String[] args) &#123;Integer[] integers = &#123;1, 20, 90&#125;;//遍历数组// for(int i = 0; i &lt; integers.length; i++) &#123;// System.out.println(integers[i]);// &#125;//直接使用 Arrays.toString 方法，显示数组// System.out.println(Arrays.toString(integers));////演示 sort 方法的使用Integer arr[] = &#123;1, -1, 7, 0, 89&#125;;//进行排序//老韩解读//1. 可以直接使用冒泡排序 , 也可以直接使用 Arrays 提供的 sort 方法排序//2. 因为数组是引用类型，所以通过 sort 排序后，会直接影响到 实参 arr//3. sort 重载的，也可以通过传入一个接口 Comparator 实现定制排序//4. 调用 定制排序 时，传入两个参数 (1) 排序的数组 arr// (2) 实现了 Comparator 接口的匿名内部类 , 要求实现 compare 方法//5. 先演示效果，再解释//6. 这里体现了接口编程的方式 , 看看源码，就明白// 源码分析//(1) Arrays.sort(arr, new Comparator()//(2) 最终到 TimSort 类的 private static &lt;T&gt; void binarySort(T[] a, int lo, int hi, int start, // Comparator&lt;? super T&gt; c)()//(3) 执行到 binarySort 方法的代码, 会根据动态绑定机制 c.compare()执行我们传入的// 匿名内部类的 compare ()// while (left &lt; right) &#123;// int mid = (left + right) &gt;&gt;&gt; 1;// if (c.compare(pivot, a[mid]) &lt; 0)// right = mid;// else// left = mid + 1;// &#125;//(4) new Comparator() &#123;// @Override// public int compare(Object o1, Object o2) &#123;// Integer i1 = (Integer) o1;// Integer i2 = (Integer) o2;// return i2 - i1;// &#125;// &#125;//(5) public int compare(Object o1, Object o2) 返回的值&gt;0 还是 &lt;0// 会影响整个排序结果, 这就充分体现了 接口编程+动态绑定+匿名内部类的综合使用// 将来的底层框架和源码的使用方式，会非常常见//Arrays.sort(arr); // 默认排序方法//定制排序Arrays.sort(arr, new Comparator() &#123;@Overridepublic int compare(Object o1, Object o2) &#123;Integer i1 = (Integer) o1;Integer i2 = (Integer) o2;return i2 - i1;&#125;&#125;);System.out.println(&quot;===排序后===&quot;);System.out.println(Arrays.toString(arr));//&#125;&#125;package com.hspedu.arrays_;import java.util.Arrays;import java.util.Comparator;/*** @author 韩顺平* @version 1.0*/public class ArraysSortCustom &#123;韩顺平循序渐进学 Java 零基础第 575页public static void main(String[] args) &#123;int[] arr = &#123;1, -1, 8, 0, 20&#125;;//bubble01(arr);bubble02(arr, new Comparator() &#123;@Overridepublic int compare(Object o1, Object o2) &#123;int i1 = (Integer) o1;int i2 = (Integer) o2;return i2 - i1;// return i2 - i1;&#125;&#125;);System.out.println(&quot;==定制排序后的情况==&quot;);System.out.println(Arrays.toString(arr));&#125;//使用冒泡完成排序public static void bubble01(int[] arr) &#123;int temp = 0;for (int i = 0; i &lt; arr.length - 1; i++) &#123;for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123;//从小到大if (arr[j] &gt; arr[j + 1]) &#123;韩顺平循序渐进学 Java 零基础第 576页temp = arr[j];arr[j] = arr[j + 1];arr[j + 1] = temp;&#125;&#125;&#125;&#125;//结合冒泡 + 定制public static void bubble02(int[] arr, Comparator c) &#123;int temp = 0;for (int i = 0; i &lt; arr.length - 1; i++) &#123;for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123;//数组排序由 c.compare(arr[j], arr[j + 1])返回的值决定if (c.compare(arr[j], arr[j + 1]) &gt; 0) &#123;temp = arr[j];arr[j] = arr[j + 1];arr[j + 1] = temp;&#125;&#125;&#125;&#125;&#125;package com.hspedu.arrays_;import java.util.Arrays;import java.util.List;/*** @author 韩顺平* @version 1.0*/public class ArraysMethod02 &#123;public static void main(String[] args) &#123;Integer[] arr = &#123;1, 2, 90, 123, 567&#125;;// binarySearch 通过二分搜索法进行查找，要求必须排好// 老韩解读//1. 使用 binarySearch 二叉查找//2. 要求该数组是有序的. 如果该数组是无序的，不能使用 binarySearch//3. 如果数组中不存在该元素，就返回 return -(low + 1); // key not found. int index = Arrays.binarySearch(arr, 567);System.out.println(&quot;index=&quot; + index);//copyOf 数组元素的复制// 老韩解读//1. 从 arr 数组中，拷贝 arr.length 个元素到 newArr 数组中//2. 如果拷贝的长度 &gt; arr.length 就在新数组的后面 增加 null//3. 如果拷贝长度 &lt; 0 就抛出异常 NegativeArraySizeException//4. 该方法的底层使用的是 System.arraycopy()Integer[] newArr = Arrays.copyOf(arr, arr.length);System.out.println(&quot;==拷贝执行完毕后==&quot;);韩顺平循序渐进学 Java 零基础第 578页System.out.println(Arrays.toString(newArr));//ill 数组元素的填充Integer[] num = new Integer[]&#123;9,3,2&#125;;//老韩解读//1. 使用 99 去填充 num 数组，可以理解成是替换原理的元素Arrays.fill(num, 99);System.out.println(&quot;==num 数组填充后==&quot;);System.out.println(Arrays.toString(num));//equals 比较两个数组元素内容是否完全一致Integer[] arr2 = &#123;1, 2, 90, 123&#125;;//老韩解读//1. 如果 arr 和 arr2 数组的元素一样，则方法 true;//2. 如果不是完全一样，就返回 falseboolean equals = Arrays.equals(arr, arr2);System.out.println(&quot;equals=&quot; + equals);//asList 将一组值，转换成 list//老韩解读//1. asList 方法，会将 (2,3,4,5,6,1)数据转成一个 List 集合//2. 返回的 asList 编译类型 List(接口)//3. asList 运行类型 java.util.Arrays#ArrayList, 是 Arrays 类的// 静态内部类 private static class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt;// implements RandomAccess, java.io.SerializableList asList = Arrays.asList(2,3,4,5,6,1);System.out.println(&quot;asList=&quot; + asList);System.out.println(&quot;asList 的运行类型&quot; + asList.getClass());&#125;&#125; 13.9 System类 123456789101112131415161718192021222324252627282930313233343536package com.hspedu.system_;import java.util.Arrays;/*** @author 韩顺平* @version 1.0*/public class System_ &#123;public static void main(String[] args) &#123;//exit 退出当前程序// System.out.println(&quot;ok1&quot;);// //老韩解读// //1. exit(0) 表示程序退出// //2. 0 表示一个状态 , 正常的状态// System.exit(0);//// System.out.println(&quot;ok2&quot;);//arraycopy ：复制数组元素，比较适合底层调用，// 一般使用 Arrays.copyOf 完成复制数组int[] src=&#123;1,2,3&#125;;int[] dest = new int[3];// dest 当前是 &#123;0,0,0&#125;//老韩解读//1. 主要是搞清楚这五个参数的含义//2. // 源数组韩顺平循序渐进学 Java 零基础第 586页// * @param src the source array. // srcPos： 从源数组的哪个索引位置开始拷贝// * @param srcPos starting position in the source array. // dest : 目标数组，即把源数组的数据拷贝到哪个数组// * @param dest the destination array. // destPos: 把源数组的数据拷贝到 目标数组的哪个索引// * @param destPos starting position in the destination data. // length: 从源数组拷贝多少个数据到目标数组// * @param length the number of array elements to be copied. System.arraycopy(src, 0, dest, 0, src.length);// int[] src=&#123;1,2,3&#125;;System.out.println(&quot;dest=&quot; + Arrays.toString(dest));//[1, 2, 3]//currentTimeMillens:返回当前时间距离 1970-1-1 的毫秒数// 老韩解读:System.out.println(System.currentTimeMillis());&#125;&#125; 13.10 BigInteger和BigDecimal类 应用场景： BigInteger适合保存较大的整形 BigDecimal适合保存精度更高的浮点型 两者的常见方法 1234567891011121314151617181920212223public class BigInteger_ &#123;public static void main(String[] args) &#123;//当我们编程中，需要处理很大的整数，long 不够用//可以使用 BigInteger 的类来搞定// long l = 23788888899999999999999999999l;// System.out.println(&quot;l=&quot; + l);BigInteger bigInteger = new BigInteger(&quot;23788888899999999999999999999&quot;);BigInteger bigInteger2 = newBigInteger(&quot;10099999999999999999999999999999999999999999999999999999999999999999999999999999999&quot;);System.out.println(bigInteger);//解读//1. 在对 BigInteger 进行加减乘除的时候，需要使用对应的方法，不能直接进行 + - * ///2. 可以创建一个 要操作的 BigInteger 然后进行相应操作BigInteger add = bigInteger.add(bigInteger2);System.out.println(add);//BigInteger subtract = bigInteger.subtract(bigInteger2);System.out.println(subtract);//减BigInteger multiply = bigInteger.multiply(bigInteger2);System.out.println(multiply);//乘BigInteger divide = bigInteger.divide(bigInteger2);System.out.println(divide);//除&#125;&#125; 123456789101112131415161718192021public class BigDecimal_ &#123;public static void main(String[] args) &#123;//当我们需要保存一个精度很高的数时，double 不够用//可以是 BigDecimal// double d = 1999.11111111111999999999999977788d;// System.out.println(d);BigDecimal bigDecimal = new BigDecimal(&quot;1999.11&quot;);BigDecimal bigDecimal2 = new BigDecimal(&quot;3&quot;);System.out.println(bigDecimal);//老韩解读//1. 如果对 BigDecimal 进行运算，比如加减乘除，需要使用对应的方法//2. 创建一个需要操作的 BigDecimal 然后调用相应的方法即可System.out.println(bigDecimal.add(bigDecimal2));System.out.println(bigDecimal.subtract(bigDecimal2));System.out.println(bigDecimal.multiply(bigDecimal2));//System.out.println(bigDecimal.divide(bigDecimal2));//可能抛出异常 ArithmeticException//在调用 divide 方法时，指定精度即可. BigDecimal.ROUND_CEILING//如果有无限循环小数，就会保留 分子 的精度System.out.println(bigDecimal.divide(bigDecimal2, BigDecimal.ROUND_CEILING));&#125;&#125; 第14章 集合 Deque是一个双端队列接口，继承自Queue接口，Deque的实现类是LinkedList、ArrayDeque、LinkedBlockingDeque，其中LinkedList是最常用的。 关于Queue的介绍可以看上一篇文章：Java队列Queue使用详解 Deque有三种用途： 普通队列(一端进另一端出): Queue queue &#x3D; new LinkedList()或Deque deque &#x3D; new LinkedList() 双端队列(两端都可进出) Deque deque &#x3D; new LinkedList() 堆栈 Deque deque &#x3D; new LinkedList() 注意：Java堆栈Stack类已经过时，Java官方推荐使用Deque替代Stack使用。Deque堆栈操作方法：push()、pop()、peek()。 Deque是一个线性collection，支持在两端插入和移除元素。名称 deque 是“double ended queue（双端队列）”的缩写，通常读为“deck”。大多数 Deque 实现对于它们能够包含的元素数没有固定限制，但此接口既支持有容量限制的双端队列，也支持没有固定大小限制的双端队列。 此接口定义在双端队列两端访问元素的方法。提供插入、移除和检查元素的方法。每种方法都存在两种形式：一种形式在操作失败时抛出异常，另一种形式返回一个特殊值（null 或 false，具体取决于操作）。插入操作的后一种形式是专为使用有容量限制的 Deque 实现设计的；在大多数实现中，插入操作不能失败。 下表总结了上述 12 种方法： 第一个元素 (头部) 最后一个元素 (尾部) 抛出异常 特殊值 抛出异常 特殊值 插入 addFirst(e) offerFirst(e) addLast(e) offerLast(e) 删除 removeFirst() pollFirst() removeLast() pollLast() 检查 getFirst() peekFirst() getLast() peekLast() Deque接口扩展(继承)了 Queue 接口。在将双端队列用作队列时，将得到 FIFO（先进先出）行为。将元素添加到双端队列的末尾，从双端队列的开头移除元素。从 Queue 接口继承的方法完全等效于 Deque 方法，如下表所示： Queue方法 等效Deque方法 add(e) addLast(e) offer(e) offerLast(e) remove() removeFirst() poll() pollFirst() element() getFirst() peek() peekFirst() 增加一个方法，可以返回当前栈顶的值, 但是不是真正的 pop 双端队列也可用作 LIFO（后进先出）堆栈。应优先使用此接口而不是遗留 Stack 类。在将双端队列用作堆栈时，元素被推入双端队列的开头并从双端队列开头弹出。堆栈方法完全等效于 Deque 方法，如下表所示： 堆栈方法 等效Deque方法 push(e) addFirst(e) pop() removeFirst() peek() peekFirst( ———————————————— 版权声明：本文为CSDN博主「devnn」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/devnn/article/details/82716447 14.1集合的理解与好处 数组长度固定不能更改 CRUD（增加 (Create)、读取(Retrieve) (重新得到数据)、更新 (Update)和删除 (Delete)增删改查）不方便 集合可以动态保存任意多个对象（底层源码运用扩容机制），提供了一系列方便的方法，add、remove、set、get等 14.2集合的框架体系 单列集合 add（“tom”） 接口定义方法类会自己具体实现各种方法 双列集合 put（“NO1”,“北京”） 14.3 Collection接口和常用方法 Collection的实现类 有可以存放重复的元素 有些不可以 ​ 有些是有序的（list）有些是无序的（set） Collection接口没有直接的实现子类，是通过子接口set和list来实现的 123456789101112131415161718192021222324252627282930313233343536373839404142Collection 接口常用方法,以实现子类 ArrayList 来演示. CollectionMethod.javapackage com.hspedu.collection_;import java.util.ArrayList;import java.util.List;public class CollectionMethod &#123;@SuppressWarnings(&#123;&quot;all&quot;&#125;)public static void main(String[] args) &#123;List list = new ArrayList();// add:添加单个元素list.add(&quot;jack&quot;);list.add(10);//list.add(new Integer(10))list.add(true);System.out.println(&quot;list=&quot; + list);// remove:删除指定元素//list.remove(0);//删除第一个元素list.remove(true);//指定删除某个元素System.out.println(&quot;list=&quot; + list);// contains:查找元素是否存在System.out.println(list.contains(&quot;jack&quot;));//T// size:获取元素个数System.out.println(list.size());//2// isEmpty:判断是否为空System.out.println(list.isEmpty());//F// clear:清空list.clear();System.out.println(&quot;list=&quot; + list);// addAll:添加多个元素ArrayList list2 = new ArrayList();list2.add(&quot;红楼梦&quot;);list2.add(&quot;三国演义&quot;);list.addAll(list2);System.out.println(&quot;list=&quot; + list);// containsAll:查找多个元素是否都存在System.out.println(list.containsAll(list2));//T// removeAll：删除多个元素list.add(&quot;聊斋&quot;);list.removeAll(list2);System.out.println(&quot;list=&quot; + list);//[聊斋]// 说明：以 ArrayList 实现类来演示. &#125;&#125; 第21章 网络编程 查询 API 的一般流程是：找包→找类或接口→查看类或接口→找方法或变量 21.1网络的相关概念 计算机网络 21.2InetAddress类21.2.1相关方法 1.获取本机InetAddress对象 InetAddress.getLocalHost 静态方法 ，return LAPTOP-PH64GORS&#x2F;192.168.137.1 2.根据指定主机名&#x2F;域名获取ip地址对象InetAddress.getByName（主机名） 3.获取InetAddress对象的主机名 getHostName 4.获取InetAddress对象的地址 getHostAddress 12345678910111213141516171819//1. 获取本机的InetAddress 对象InetAddress localHost = InetAddress.getLocalHost();System.out.println(localHost);//DESKTOP-S4MP84S/192.168.12.1//2. 根据指定主机名 获取 InetAddress对象InetAddress host1 = InetAddress.getByName(&quot;LAPTOP-PH64GORS&quot;);System.out.println(&quot;host1=&quot; + host1);//DESKTOP-S4MP84S/192.168.12.1//3. 根据域名返回 InetAddress对象, 比如 www.baidu.com 对应InetAddress host2 = InetAddress.getByName(&quot;www.baidu.com&quot;);System.out.println(&quot;host2=&quot; + host2);//www.baidu.com / 110.242.68.4//4. 通过 InetAddress 对象，获取对应的地址String hostAddress = host2.getHostAddress();//IP 110.242.68.4System.out.println(&quot;host2 对应的ip = &quot; + hostAddress);//110.242.68.4//5. 通过 InetAddress 对象，获取对应的主机名/或者的域名String hostName = host2.getHostName();System.out.println(&quot;host2对应的主机名/域名=&quot; + hostName); // www.baidu.com 21.3 Socket21.3.1基本介绍 1.套接字（Socket）开发网络应用程序被广泛采用，以至成为事实上的标准 2.通信两端都要是Socket，是两台机器间通信的端点 3.网络通信其实就是Socket的通信 4.Socket允许程序把网络连接当成一个流，数据在两个Socket间通过IO传输。 5.一般主动发起通信的应用程序属于客户端，等待的为服务端 21.4 TCP网络通信编程 21.4.1使用字节流 123456789101112131415161718public class SocketTCP01Client &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 连接服务端 (ip , 端口） //解读: 连接本机的 9999端口, 如果连接成功，返回Socket对象 Socket socket = new Socket(InetAddress.getLocalHost(), 9999); System.out.println(&quot;客户端 socket返回=&quot; + socket.getClass()); //2. 连接上后，生成Socket, 通过socket.getOutputStream() // 得到 和 socket对象关联的输出流对象 OutputStream outputStream = socket.getOutputStream(); //3. 通过输出流，写入数据到 数据通道 outputStream.write(&quot;hello, server&quot;.getBytes()); //4. 关闭流对象和socket, 必须关闭 outputStream.close(); socket.close(); System.out.println(&quot;客户端退出.....&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930public class SocketTCP01Server &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 在本机 的9999端口监听, 等待连接 // 细节: 要求在本机没有其它服务在监听9999 // 细节：这个 ServerSocket 可以通过 accept() 返回多个Socket[多个客户端连接服务器的并发] ServerSocket serverSocket = new ServerSocket(9999); System.out.println(&quot;服务端，在9999端口监听，等待连接..&quot;); //2. 当没有客户端连接9999端口时，程序会 阻塞, 等待连接 // 如果有客户端连接，则会返回Socket对象，程序继续 Socket socket = serverSocket.accept(); System.out.println(&quot;服务端 socket =&quot; + socket.getClass()); // //3. 通过socket.getInputStream() 读取客户端写入到数据通道的数据, 显示 InputStream inputStream = socket.getInputStream(); //4. IO读取 byte[] buf = new byte[1024]; int readLen = 0; while ((readLen = inputStream.read(buf)) != -1) &#123; System.out.println(new String(buf, 0, readLen));//根据读取到的实际长度，显示内容. &#125; //5.关闭流和socket inputStream.close(); socket.close(); serverSocket.close();//关闭 &#125;&#125; 21.4.2使用字符流 1234567891011121314151617181920212223242526272829public static void main(String[] args) throws IOException &#123; //思路 //1. 连接服务端 (ip , 端口） //解读: 连接本机的 9999端口, 如果连接成功，返回Socket对象 Socket socket = new Socket(InetAddress.getLocalHost(), 9999); System.out.println(&quot;客户端 socket返回=&quot; + socket.getClass()); //2. 连接上后，生成Socket, 通过socket.getOutputStream() // 得到 和 socket对象关联的输出流对象 OutputStream outputStream = socket.getOutputStream(); //3. 通过输出流，写入数据到 数据通道, 使用字符流 BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(outputStream)); bufferedWriter.write(&quot;hello, server 字符流&quot;); bufferedWriter.newLine();//插入一个换行符，表示写入的内容结束, 注意，要求对方使用readLine()!!!! bufferedWriter.flush();// 如果使用的字符流，需要手动刷新，否则数据不会写入数据通道 //4. 获取和socket关联的输入流. 读取数据(字符)，并显示 InputStream inputStream = socket.getInputStream(); BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); String s = bufferedReader.readLine(); System.out.println(s); //5. 关闭流对象和socket, 必须关闭 bufferedReader.close();//关闭外层流 bufferedWriter.close(); socket.close(); System.out.println(&quot;客户端退出.....&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839public class SocketTCP03Server &#123; public static void main(String[] args) throws IOException &#123; //思路 //1. 在本机 的9999端口监听, 等待连接 // 细节: 要求在本机没有其它服务在监听9999 // 细节：这个 ServerSocket 可以通过 accept() 返回多个Socket[多个客户端连接服务器的并发] ServerSocket serverSocket = new ServerSocket(9999); System.out.println(&quot;服务端，在9999端口监听，等待连接..&quot;); //2. 当没有客户端连接9999端口时，程序会 阻塞, 等待连接 // 如果有客户端连接，则会返回Socket对象，程序继续 Socket socket = serverSocket.accept(); System.out.println(&quot;服务端 socket =&quot; + socket.getClass()); // //3. 通过socket.getInputStream() 读取客户端写入到数据通道的数据, 显示 InputStream inputStream = socket.getInputStream(); //4. IO读取, 使用字符流, 老师使用 InputStreamReader 将 inputStream 转成字符流 BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); String s = bufferedReader.readLine(); System.out.println(s);//输出 //5. 获取socket相关联的输出流 OutputStream outputStream = socket.getOutputStream(); // 使用字符输出流的方式回复信息 BufferedWriter bufferedWriter = new BufferedWriter(new OutputStreamWriter(outputStream)); bufferedWriter.write(&quot;hello client 字符流&quot;); bufferedWriter.newLine();// 插入一个换行符，表示回复内容的结束 bufferedWriter.flush();//注意需要手动的flush //6.关闭流和socket bufferedWriter.close(); bufferedReader.close(); socket.close(); serverSocket.close();//关闭 &#125;&#125; 21.4.3文件发送21.4.4 netstat 指令 telnet ip 端口号 连接服务器 crtl+】 显示字符 用Java连接服务器 12345678try (var s = new Socket(&quot;time-a.nist.gov&quot;, 13); //打开一个关键字，负责启动该程序内部和外部的通信 若连接失败 它将抛出 // 一个UnknownHostException, // 如果存在其他问题将抛出一个IOException 因为UnknownException是IOException的一个子类 这仅是示例程序仅捕获超类的异常 var in = new Scanner(s.getInputStream(), StandardCharsets.UTF_8))//一旦套接字打开 Socket 类中的getInputStream 类就会返回一个InputSteam对象，该对象可以像任何其他流对象使用，该程序一旦获得了 //这个流 该程序会将直接把每一行打印到标准输出 该程序只适用非常简单的服务器 //在比较复杂的网络程序中 客户端发送请求而服务器可能在响应结束是并不立刻断开连接 //java库隐藏了建立网络连接和通过连接发送数据的复杂过程 12345678910111213141516171819202122232425262728import java.io.*;import java.net.*;/** * This program demonstrates the InetAddress class. Supply a host name as command-line * argument, or run without command-line arguments to see the address of the local host. * @version 1.02 2012-06-05 * @author Cay Horstmann */public class InetAddressTest&#123; public static void main(String[] args) throws IOException &#123; if (args.length &gt; 0) &#123; String host = args[0]; InetAddress[] addresses = InetAddress.getAllByName(host);//为给定的主机名创建一个InetAddress对象 // for (InetAddress a : addresses) System.out.println(a); &#125; else &#123; InetAddress localHostAddress = InetAddress.getLocalHost(); //为本机主机创建一个InetAddress对象 System.out.println(localHostAddress); &#125; &#125;&#125; InetAddress Socket 基础知识基础类型 byte是字节类型 char 是unicode 是万国码 16位 兼容assic 类型转换 自动向上类型转换 强制向下转换(对表达式强转) 比较器 &amp;gt 则返回 1 &amp;lt 则返回 -1 sort 默认从小到大,比较器不关注sort,只关注传递大小信息 lamda随便写写 基本类型不支持比较器 Date File类 创建文件对象,对对象进行操作 文件夹或文件皆可抽象为file,但文件夹不能被输入输出 IO流 字节与字符的区别 字节(Byte)是计量单位，表示数据量多少，是计算机信息技术用于计量存储容量的一种计量单位，通常情况下一字节等于八位。 字符(Character)计算机中使用的字母、数字、字和符号，比如’A’、’B’、’$’、’&amp;’等。 一般在英文状态下一个字母或字符占用一个字节，一个汉字用两个字节表示。 字节与字符： ASCII 码中，一个英文字母（不分大小写）为一个字节，一个中文汉字为两个字节。 UTF-8 编码中，一个英文字为一个字节，一个中文为三个字节。 Unicode 编码中，一个英文为一个字节，一个中文为两个字节。 符号：英文标点为一个字节，中文标点为两个字节。例如：英文句号 . 占1个字节的大小，中文句号 。占2个字节的大小。 UTF-16 编码中，一个英文字母字符或一个汉字字符存储都需要 2 个字节（Unicode 扩展区的一些汉字存储需要 4 个字节）。 UTF-32 编码中，世界上任何字符的存储都需要 4 个字节。 输出流默认覆盖写,文件不存在则创建文件 权限修饰符类 1、外部类前可以修饰：public、default、abstract、final 对于顶级类(外部类)来说，只有两种修饰符：public和默认(default)。因为外部类的上一单元是包，所以外部类只有两个作用域：同包，任何位置。因此，只需要两种控制权限：包控制权限和公开访问权限，也就对应两种控制修饰符：public和默认(default)。可以满足所有的情况了。 如果类使用了private修饰符，说明是个内部类。内部类的上一级是外部类，那么对应的有四种访问控制修饰符：本类(private)，同包(default)，父子类(protected)，任何位置(public)。当一个内部类使用了private修饰后，只能在该类的外部类内部使用。 上面这些都是平时使用司空见惯的，但是为什么是这种情况呢？ 可以想一下，一个java项目是不可能在一个class里面完成的。mvc模式中，是把类分为三层，一层层调用类。如果定义为私有的和受保护的就无法调用。换句话说，对于一个java文件，要么就是自己单独运行，要么就是被其他程序作为库调用，如果一个java文件的类被private修饰，那么是不是其他的程序或是类是无法使用它的，那么他作为一个单独的文件就没啥用了。如果它作为单个文件运行，类加载怎么找到它呢，因为它对外不可见。同时，也失去了类的存在意义。 2、内部类前可以修饰：public、protected、default、private、abstract、final、static 3、局部(指方法 代码块等)内部类前可以修饰：abstract、final 成员变量 1、 public ：对 所有用户 开放，所有用户都可直接调用 2、 private ：私有。 除了class自己之外，任何人都不可直接使用 ，私有财产神圣不可侵犯嘛，即便是子女，朋友，都不可使用。 default 类内部 与 同包 ,即对于外包的类和子类朋友类相当于为私有不能访问 3、 protected ：对于子女、朋友来说，就是public的，可自由使用，无任何限制；而对于其他的外部class，protected就变成private。（ 同一个包中的类，若不在同一个包中，必须为其子孙类才可使用 ） 接口抽象类 接口里的静态方法，即static修饰的有方法体的方法不会被继承或者实现，但是静态变量会被继承 接口中的static方法不能被继承，也不能被实现类调用，只能被自身调用 Guava入门 guava就是类库，是java api的增强与扩展，里面有大量的方法供我们使用，使用之前需要引入包 &lt;!--guava依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1&lt;/version&gt; &lt;/dependency&gt; guava有哪些方法呢？我们先从以下几方面开始学习： 字符串处理：分割，连接，填充 新增的集合类型 原生类型 1.原生类型 定义list，map public void test() { &#x2F;&#x2F;JDK List list &#x3D; new ArrayList(); list.add(“a”); list.add(“b”); list.add(“c”); list.add(“d”); &#x2F;&#x2F;guava List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); List lists1 &#x3D; Lists.newArrayList(); Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); } guava就是类库，是java api的增强与扩展，里面有大量的方法供我们使用，使用之前需要引入包 &lt;!--guava依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1&lt;/version&gt; &lt;/dependency&gt; guava有哪些方法呢？我们先从以下几方面开始学习： 字符串处理：分割，连接，填充 新增的集合类型 原生类型 1.原生类型 定义list，map public void test() { &#x2F;&#x2F;JDK List list &#x3D; new ArrayList(); list.add(“a”); list.add(“b”); list.add(“c”); list.add(“d”); &#x2F;&#x2F;guava List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); List lists1 &#x3D; Lists.newArrayList(); Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); } 2.新增集合（这里我只讲一下Mulitmap,平时用这个会使代码很方便，这里我就多讲一下） a Multimap的使用 Multimap就是将相同key的value值放在一个list里面，这样子取相同key下面的所有value值就非常简单了，不然还得for循环去匹配，把相同key值的value值找出来，在进行处理。map&lt;key,value&gt;键值key不能重复，所以当遇到这样子场景的时候map就非常不适合了，guava提供了Multimap适用于该场景。 当我们需要一个map中包含key为String类型，value为List类型的时候，以前我们是这样写的 &#x2F;&#x2F; jdk方式 Map&lt;String,List&gt; map &#x3D; new HashMap&lt;String,List&gt;(); List list &#x3D; new ArrayList(); list.add(1); list.add(2); map.put(“aa”, list); System.out.println(map.get(“aa”));&#x2F;&#x2F;[1, 2] &#x2F;&#x2F; guava方式 Multimap&lt;String,Integer&gt; map &#x3D; ArrayListMultimap.create(); map.put(“aa”, 1); map.put(“aa”, 2); System.out.println(map.get(“aa”)); &#x2F;&#x2F;[1, 2] Multimap.get(key)即使没有key值，会返回空的list。 Multimap.keySet()返回的用set表示的不重复key; Multimap.keys()返回的是用Multiset表示的key,key数量跟value值数量一致； Multimap.containKeys()是表示是否包含这个key; Multimap.size()返回所有值的个数，而非不同键的个数。要得到不同键的个数，要用Multimap.keySet().size() 想要更多了解Multimap可以参考 https://www.jianshu.com/p/e0537d878b6c 3.字符串的处理：分割，连接，填充 a. joiner 连接器 joiner on就是将list用，连接转成字符串 @Test public void joinerListTest() { List lists &#x3D; Lists.newArrayList(“a”,”b”,”g”,”8”,”9”); String result &#x3D; Joiner.on(“,”).join(lists); System.out.println(result); } 结果：a,b,g,8,9 joiner skipNulls()连接跳过null元素(第一个test为了跟第二个进行比对一下) @Test public void joinerListTest1() { List lists &#x3D; Lists.newArrayList(“a”,”b”,”g”,null,”8”,”9”); String result &#x3D; Joiner.on(“,”).join(lists); System.out.println(result); } 结果：a,b,g,null,8,9 @Test public void joinerListTest2() { List lists &#x3D; Lists.newArrayList(“a”,”b,”g”,null,”8”,”9”); String result &#x3D; Joiner.on(“,”).skipNulls().join(lists); System.out.println(result); } 结果：a,b,g,8,9 如果连接的时候list里面含有null值，会报空指针，因为join实现如下： public final String join(Iterable&lt;?&gt; parts) { return this.join(parts.iterator()); } public final String join(Iterator&lt;?&gt; parts) { return this.appendTo(new StringBuilder(), parts).toString(); } @CanIgnoreReturnValue public final StringBuilder appendTo(StringBuilder builder, Iterator&lt;?&gt; parts) { try { this.appendTo((Appendable)builder, (Iterator)parts); return builder; } catch (IOException var4) { throw new AssertionError(var4); } } @CanIgnoreReturnValue public A appendTo(A appendable, Iterator&lt;?&gt; parts) throws IOException { Preconditions.checkNotNull(appendable); if (parts.hasNext()) { appendable.append(this.toString(parts.next())); while(parts.hasNext()) { appendable.append(this.separator); appendable.append(this.toString(parts.next())); } } return appendable; } @CanIgnoreReturnValue public static T checkNotNull(T reference) { if (reference &#x3D;&#x3D; null) { throw new NullPointerException(); } else { return reference; } } joiner useForNull(final String value)用value替换null元素值 @Test public void useNullListTest() { List lists &#x3D; Lists.newArrayList(“a”, “b”, “g”, null, “8”, “9”); String result &#x3D; Joiner.on(“,”).useForNull(“哈哈”).join(lists); System.out.println(result); } 结果：a,b,g,哈哈,8,9 joiner withKeyValueSeparator(String value) map连接器，keyValueSeparator为key和value之间的分隔符 @Test public void withMapTest() { Map&lt;Integer, String&gt; maps &#x3D; Maps.newHashMap(); maps.put(1, “哈哈”); maps.put(2, “压压”); String result &#x3D; Joiner.on(“,”).withKeyValueSeparator(“:”).join(maps); System.out.println(result); System.out.println(maps); } 结果： 1:哈哈,2:压压 {1&#x3D;哈哈, 2&#x3D;压压} b. splitter 拆分器 splitter on 拆分 @Test public void splitterListTest() { String test &#x3D; “34344,34,34,哈哈”; List lists &#x3D; Splitter.on(“,”).splitToList(test); System.out.println(lists); } 结果：[34344, 34, 34, 哈哈] splitter trimResults 拆分去除前后空格 @Test public void trimResultListTest() { String test &#x3D; “ 34344,34,34,哈哈 “; List lists &#x3D; Splitter.on(“,”).trimResults().splitToList(test); System.out.println(lists); } 结果：[34344, 34, 34, 哈哈] splitter omitEmptyStrings 去除拆分出来空的字符串 @Test public void omitEmptyStringsTest() { String test &#x3D; “ 3434,434,34,,哈哈 “; List lists &#x3D; Splitter.on(“,”).omitEmptyStrings().splitToList(test); System.out.println(lists); } 结果：[ 3434, 434, 34, 哈哈 ] splitter fixedLength(int lenght) 把字符串按固定长度分割 @Test public void fixedLengthTest() { String test &#x3D; “343443434哈哈”; List lists &#x3D; Splitter.fixedLength(3).splitToList(test); System.out.println(lists); } 结果：[343, 443, 434, 哈哈] b. charMatcher 匹配器 charMatcher is(Char char) 给单一字符匹配 @Test public void isTest() { String str &#x3D; “12312,agg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.retainFrom(str)); } 结果：gg charMatcher retainFrom(String s) 在字符序列中保留匹配字符，移除其他字符 @Test public void charMatcherTest() { String str &#x3D; “12312,agg “; &#x2F;&#x2F;两个匹配符,先匹配再操作 CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘1’); CharMatcher charMatcher2 &#x3D; CharMatcher.is(‘2’); &#x2F;&#x2F;两个CharMatcher或操作 CharMatcher charMatcher3 &#x3D; charMatcher1.or(charMatcher2); System.out.println(charMatcher3.retainFrom(str)); } 结果：1212 charMatcher matchersAllOf(Char char) 测试是否字符序列所有字符都匹配 @Test public void matchesAllOfTest() { String str &#x3D; “12312,agg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.matchesAllOf(str)); } 结果：false @Test public void matchesAllOfTest() { String str &#x3D; “ggggg”; CharMatcher charMatcher1 &#x3D; CharMatcher.is(‘g’); System.out.println(charMatcher1.matchesAllOf(str)); } 结果：true arr与list转换前言 123456int[] ints = &#123;2, 34, 55, 22, 11&#125;; long[] longs = &#123;1, 2, 3&#125;; double[] doubles = &#123;1, 2, 3&#125;; Arrays.stream(ints).boxed().collect(Collectors.toList()); Arrays.stream(longs).boxed().collect(Collectors.toList()); Arrays.stream(doubles).boxed().collect(Collectors.toList()); 直接for,add就完事了反之同理 java数组转list误区 一、不能把基本数据类型转化为列表 仔细观察可以发现asList接受的参数是一个泛型的变长参数，而基本数据类型是无法泛型化的，如下所示： 123456789101112public class App &#123; public static void main(String[] args) &#123; int [] intarray = &#123; 1 , 2 , 3 , 4 , 5 &#125;; //List&lt;Integer&gt; list = Arrays.asList(intarray); 编译通不过 List&lt; int []&gt; list = Arrays.asList(intarray); System.out.println(list); &#125;&#125;output：[[I @66d3c617 ]1234567891011 这是因为把int类型的数组当参数了，所以转换后的列表就只包含一个int[]元素。 解决方案： 要想把基本数据类型的数组转化为其包装类型的list，可以使用guava类库的工具方法，示例如下： 12int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;;List&lt;Integer&gt; list = Ints.asList(intArray); 第一种方式(未必最佳):使用ArrayList.asList(strArray) 使用Arrays工具类Arrays.asList(strArray)方式,转换完成后,只能对List数组进行查改,不能增删,增删就会抛出UnsupportedOperationException 异常 12345678910import java.util.Arrays;import java.util.List; public static void Demo1() &#123; String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; //注意这个List不是Collections包内的List,而是util包里面的List接口 List&lt;String&gt; ints = Arrays.asList(str); //这里会报错 ints.add(&quot;laopo&quot;); &#125;123456789 添加数据报错: 1234567891011Exception in thread &quot;main&quot; java.lang.UnsupportedOperationExceptionat java.util.AbstractList.add(AbstractList.java:148)at java.util.AbstractList.add(AbstractList.java:108)at JAVA基础.JDK8新特性.Java数组转List.Demo1(Java数组转List.java:20)at JAVA基础.JDK8新特性.Java数组转List.main(Java数组转List.java:13)报错原因:Arrays.asList(str)返回值是java.util.Arrays类中一个私有静态内部类 java.utiil.Arrays.Arraylist,并不是我们平时用的java.util.ArrayList();使用场景:Arrays.asList(strArray)方式仅能用在将数组转换为List后，不需要增删其中的值，仅作为数据源读取使用。12345678910 第二种方法(支持增删查改): 通过ArrayList的构造器,将Arrays.asList(strArray)的返回值由java.utilArrays.ArrayList转为java.util.ArrayList. 关键代码：ArrayList list &#x3D; new ArrayList(Arrays.asList(strArray)) ; 1234567 String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; //注意这个List不是Collections包内的List,而是util包里面的List接口 java.util.ArrayList&lt;String&gt; strings = new ArrayList&lt;&gt;(Arrays.asList(str)); strings.add(&quot;aop&quot;); strings.stream().forEach(System.out::println);123456 使用场景:需要在将数组转换为List后，对List进行增删改查操作，在List的数据量不大的情况下，可以使用。 第三种方式(通过集合工具类Collections.addAll()方法(最高效)) 通过Collections.addAll(arrayList, strArray)方式转换，根据数组的长度创建一个长度相同的List，然后通过Collections.addAll()方法，将数组中的元素转为二进制，然后添加到List中，这是最高效的方法。 123456public static void Demo3() &#123; //注意这个List不是Collections包内的List,而是util包里面的List接口 String[] str = &#123;&quot;fgx&quot;, &quot;lzy&quot;&#125;; java.util.ArrayList&lt;String&gt; stringList = new ArrayList&lt;&gt;(str.length); Collections.addAll(stringList,str); &#125; 第四种方式通过JDK8的Stream流将3总基本类型数组转为List 如果JDK版本在1.8以上,使用流stream来将下列3种数组快速转为List,分别是int[],long[],double[],不支持short[ ],byte[ ],char[]在JDK1.8中暂不支持. 1234567 int[] ints = &#123;2, 34, 55, 22, 11&#125;; long[] longs = &#123;1, 2, 3&#125;; double[] doubles = &#123;1, 2, 3&#125;; Arrays.stream(ints).boxed().collect(Collectors.toList()); Arrays.stream(longs).boxed().collect(Collectors.toList()); Arrays.stream(doubles).boxed().collect(Collectors.toList());123456 TIPs:为什么int[]不能直接转为List,而Integer[]可以转为List,而Integer[]就可以转为List了,因为List中的泛型必须是引用类型。 java数组转list误区 一、不能把基本数据类型转化为列表 仔细观察可以发现asList接受的参数是一个泛型的变长参数，而基本数据类型是无法泛型化的，如下所示： 123456789101112public class App &#123; public static void main(String[] args) &#123; int [] intarray = &#123; 1 , 2 , 3 , 4 , 5 &#125;; //List&lt;Integer&gt; list = Arrays.asList(intarray); 编译通不过 List&lt; int []&gt; list = Arrays.asList(intarray); System.out.println(list); &#125;&#125;output：[[I @66d3c617 ]1234567891011 这是因为把int类型的数组当参数了，所以转换后的列表就只包含一个int[]元素。 解决方案： 要想把基本数据类型的数组转化为其包装类型的list，可以使用guava类库的工具方法，示例如下： 123int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;;List&lt;Integer&gt; list = Ints.asList(intArray);12 二、asList方法返回的是数组的一个视图 视图意味着，对这个list的操作都会反映在原数组上，而且这个list是定长的，不支持add、remove等改变长度的方法。 12345678910111213141516public class App &#123; public static void main(String[] args) &#123; int [] intArray = &#123; 1 , 2 , 3 , 4 &#125;; List&lt;Integer&gt; list = Ints.asList(intArray); list.set( 0 , 100 ); System.out.println(Arrays.toString(intArray)); list.add( 5 ); list.remove( 0 ); &#125;&#125;output：[ 100 , 2 , 3 , 4 ]UnsupportedOperationExceptionUnsupportedOperationException 类与对象 依赖 use-a 聚合 has-a 继承 is-a 1.域变量与局部变量 域变量会初始化为自动默认值,如果不明确初始化会影响代码的可读性 局部变量不会自动初始化为null 123456789101112131415class Employee &#123; public String name; ...&#125;Emplpyee[] staff = new Employee[3];// 省略不同的名字初始化// 如果需要返回一个可变数据域的拷贝,应该使用cloneEmployee boss = new Employee();System.out.println(boss.name); // nullpublic boolean equals(Employee other) &#123; //访问 int a; System.out.println(a);//报错 在使用时a未被初始化 当然如果不使用a就不会报错 &#125; 首先方法可以访问所调用对象的私有数据(平常使用显而易见). 然后让人奇怪的是一个方法可以访问所属类的所有对象的私有数据 1234567891011121314151617181920212223class Employee &#123; private String name;// ... Employee (String name) &#123; this.name = name; &#125; public boolean equals(Employee other) &#123; //访问 return name.equals(other.name); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; Employee boss = new Employee(&quot;boss&quot;); Employee harry = new Employee(&quot;harry&quot;);// System.out.println(boss.name); // 编译器报错 if (boss.equals(harry)) &#123; System.out.println(&quot;Employee的方法可以访问Employee类任何一个对象的私有域&quot;); &#125; &#125;&#125; private 方法内部使用,想删就删(不会被外部使用) final实例域 必须确保在每个构造器执行后这个域的值被设置 123456789101112class Employee &#123; public final String name;//编译器报错 //如果去掉无参构造器则不报错 Employee () &#123; &#125; Employee (String name) &#123; this.name = name; this.name = &quot;不能改&quot;;// 报错: name&#x27; might already have been assigned(分配) to,因为涉及更改引用 &#125;&#125; final关键字只是表示不会在指向别的地方,对象本身可以更改 静态方法不需要构建对象就可以调用 静态代码(只有在装载类的时候被执行) 12345678910111213141516171819202122232425class A &#123; static &#123; System.out.println(&quot;1&quot;); &#125; A() &#123; System.out.println(&quot;a&quot;); &#125;&#125;class B extends A &#123; static &#123; System.out.println(&quot;2&quot;); &#125; B() &#123; System.out.println(&quot;b&quot;); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; B b = new B(); A a = new A(); // 12aba &#125;&#125; 每一个类都可以有一个main方法 方法可以改变一个对象参数的状态,但不能整体改变(用x指向别的)因为是局部变量只是改变局部变量的值 真正的引用未被改变 12345678910//伪代码A a(a),b(b)swap(a,b);swap(x,y) &#123; A temp = x; x = y; y = x; &#125;// 如果是传应用sout(a,b) 输出 b,a但是并没有奏效输出还是a,b 重载 方法同名不同参数 继承覆盖方法 子类覆盖父类:方法签名覆盖 方法名与参数列表完全一致 区分重载: super不是一个对象的引用,不能将super赋给另一个对象变量,它只是一个指示编译器调用超类方法的特殊关键字 方法调用 方法名+参数列表称为方法签名 动态绑定 C x &#x3D; new B(); x.f(args) 获取父类public且对应名称的方法和声明类的对应名称方法 找一个参数类型完全匹配的方法(这个过程称为重载解析) 有可能类型转换 没找到就会报错 实际调用会调用x的真正类型的方法,虚拟机预先为每个类有一个方法表 作用: 无需对现存代码修改,就可以对程序扩展 静态绑定: private,static,final或者构造器等编译器可以准确的知道应该调用哪个方法,所以称为静态绑定 修饰类 final类 为不允许扩展的的类,所有方法自动成为final,不包括域,确保它们不会在子类改变语义 如果一个方法没有被覆盖并且很短就可被优化为内联 例如: e.getNname 将被替换为访问e.name域 详见:Java核心卷10 强制类型转换 允许子类引用赋值给父类反之必须类型转换才能通过运行时检查 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Employee &#123; public String name; // ... Employee () &#123; &#125; Employee (String name) &#123; this.name = name; &#125; public boolean equals(Employee other) &#123; //访问 return name.equals(other.name); &#125; public void work() &#123; System.out.println(&quot;员工共有之work&quot;+name); &#125;&#125;class Manager extends Employee&#123; Manager (String name) &#123; super(name); &#125; public void manage() &#123; System.out.println(&quot;经理独有之manage&quot;+name); &#125; //每个经理是一个员工&#125;public class Main &#123; public static void main(String[] args) &#123; double x = 3.405; int nx = (int) x; Manager a = new Manager(&quot;a&quot;); Employee[] staff = new Employee[3];// Employee b = new Employee(&quot;b&quot;);// Manager c = (Manager)b;// c.manage(); 这三行会报 ClassCastException //改进: Employee b = new Employee(&quot;b&quot;); Manager c; // 局部变量未使用不会报错 if (b instanceof Manager) &#123; c = (Manager) b; &#125; staff[0] = a;// 转化为E 实际为M// staff[0].manage();//报错 a = (Manager)staff[0];// 将父类强转为子类 唯一原因是:暂时忽视对象的?后,使用对象的全部功能 a.manage(); // 正常调用 &#125;&#125; 总结:父类转换为子类之前要使用instanceof,并且只能在继承层次内进行类型转换 抽象类 含有抽象方法(不需要实现的方法)的必须是抽象类 不含有抽象方法也可以声明为抽象类,只是不能被实例化 可以包含具体数据与方法 Object **hashCode()**每个对象都有,其值为对象的存储地址 1234567891011121314public class Main &#123; public static void main(String[] args) &#123; String s = &quot;ok&quot;; StringBuilder sb = new StringBuilder(s); System.out.println(s.hashCode() + &quot; &quot;+sb.hashCode()); String t = new String(&quot;ok&quot;); StringBuilder tb = new StringBuilder(t); System.out.println(t.hashCode() + &quot; &quot;+tb.hashCode()); &#125;&#125; 3548 4601419583548 1163157884 String 的hashcode()是内容导向的 如果equals为true那么hashcode也要一致 为什么重写equals方法就必须重写hashCode方法？ 在散列表中，1、如果两个对象相等，那么它们的hashCode()值一定要相同； 这里的相等是指，通过equals()比较两个对象 时返回true 2、如果两个对象hashCode()相等，它们并不一定相等。(不相等时就是哈希冲突) 注意：这是在散列表中的情况。在非散列表中一定如此！ 考虑只重写equals而不重写 hashcode 时，虽然两个属性值完全相同的对象通过equals方法判断为true，但是当把这两个对象加入到 HashSet 时。会发现HashSet中有重复元素，这就是因为HashSet 使用 hashcode 判断对象是否已存在时造成了歧义，结果会导 致HashSet 的不正常运行。所以重写 equals 方法必须重写 hashcode 方法。 map的put 拿到hashcode()进行散列定槽 如果槽为空直接加入 如果槽不为空(判断key是否相等(key不能重复),)如果key相同直接覆盖 value ,否则解决哈希冲突 还是从set角度说重写hashCode enum关键字接口、lambda表达式与内部类 接口,lambda 内部类机制,内部类中的方法可以访问外部的域 代理 1234567891011121314151617181920public class Main &#123; A a; public static void main(String[] args) &#123; Main main = new Main(); &#125; public interface Iinterface &#123; // 内部接口默认为static的 void print(); &#125; protected class A implements Iinterface &#123; @Override public void print() &#123; System.out.println(&quot;内部接口可被private,protectd static(默认static) public, 修饰&quot;); System.out.println(&quot;外部类(接口)权限修饰符只能是public 或者不写&quot;); System.out.println(&quot;外部类可被final修饰(接口不行),不能被继承,不能static修饰&quot;); &#125; &#125;&#125; 接口 1234public interface Comparable&lt;T&gt; &#123; int compareTo(T other);&#125;// 接口方法自动的是public,实现类必须写好 类泛型 写在类名之后&lt;&gt; 可以用在类内部域,方法参数,方法返回值,方法参数,局部变量等所有能声明的: 从结果来说可以理解为限定了一个限定符 等指定之后在进行替换不指定则为null,当第一次被确定时就被确定了比如set,在使用时都进行泛型指定在实现Comparable接口的类中必须提供下列方法int compareTo(Employee other )可用instanceof 检查对象是否实现了某个特定的接口,instancof限定上限 默认方法 可以为接口提供默认实现 123456789101112131415161718192021222324interface Comparable&lt;T&gt; &#123; default int compareTo(Tother) &#123;return 0&#125;;// 可以不用实现但既然 //用处并不大,每一个实际实现都要覆盖这个方法(逻辑覆盖并不是一定要实现),不过有些情况默认方法可能很有用 // 重要用法: 接口演化&#125;interface DeInterface &#123; int size(); default boolean isEmpty() &#123; return size() == 0; &#125;&#125;public class Main implements DeInterface&#123; //不用实现默认方法 public static void main(String[] args) &#123; &#125; @Override public int size() &#123; return 0; &#125;&#125; 异常、断言和日志 简述: 程序运行过程发生错误就会”抛出异常”,抛出异常比终止程序灵活,因为可以提供一个”捕获”异常的处理器(handler)对异常情况进行处理,如果没有提供处理器程序就会终止,并在控制台打印信息 异常有两种类型,未检查异常和已检查异常 对于已检查异常,编译器会检查是否提供处理器,(提示要添加异常处理的都是已检查异常) 常见的如空指针都属于未检查异常,编译器不会查看,因为应该精心编写代码来避免这些错误 1234try&#123;&#125; catch &#123; handler action&#125; 异常层次 Error（错误） Error 类及其子类：程序中无法处理的错误，表示运行应用程序中出现了严重的错误。 此类错误一般表示代码运行时 JVM 出现问题。通常有 Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如 OutOfMemoryError：内存不足错误；StackOverflowError：栈溢出错误。此类错误发生时，JVM 将终止线程。 这些错误是不受检异常，非代码性错误。因此，当此类错误发生时，应用程序不应该去处理此类错误。按照Java惯例，我们是不应该实现任何新的Error子类的！ Exception（异常） 程序本身可以捕获并且可以处理的异常。Exception 这种异常又分为两类：运行时异常和编译时异常。 运行时异常 未检查异常 都是RuntimeException类及其子类异常，如NullPointerException(空指针异常)、IndexOutOfBoundsException(下标越界异常)等，这些异常是不检查异常，程序中可以选择捕获处理，也可以不处理。这些异常一般是由程序逻辑错误引起的，程序应该从逻辑角度尽可能避免这类异常的发生。 运行时异常的特点是Java编译器不会检查它，也就是说，当程序中可能出现这类异常，即使没有用try-catch语句捕获它，也没有用throws子句声明抛出它，也会编译通过。 非运行时异常 （编译异常必须从语法角度进行处理） 已检查异常 是RuntimeException以外的异常，类型上都属于Exception类及其子类。从程序语法角度讲是必须进行处理的异常，如果不处理，程序就不能编译通过。如IOException、SQLException等以及用户自定义的Exception异常，一般情况下不自定义检查异常。 应用 应该寻找更加适当的子类或创建自己的异常类","categories":[],"tags":[]},{"title":"Mysql","slug":"11Mysql","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:54:00.159Z","comments":true,"path":"2022/09/01/11Mysql/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/11Mysql/","excerpt":"","text":"创建数据库语法12345678910CREATE DATABASE [IF NOT EXISTS] &lt;数据库名&gt;[[DEFAULT] CHARACTER SET &lt;字符集名&gt;] [[DEFAULT] COLLATE &lt;校对规则名&gt;];说明：（1）[]中的内容是可选的。（2）&lt;数据库名&gt;：要创建数据库的名称。（3）IF NOT EXISTS：只有该数据库不存在时才能执行创建操作，可以避免因为数据库已经存在而引起错误。（4）[DEFAULT] CHARACTER SET：指定数据库的默认字符集。字符集是用来定义 MySQL 存储字符串的方式。该选项可以省略，如果省略就采用配置文件中指定的字符集。MySQL 不允许在同一个系统中创建两个相同名称的数据库。（5）[DEFAULT] COLLATE：指定字符集的默认校对规则。校对规则用来定义比较字符串的方式，以解决排序和字符分组的问题。该选项可以省略，如果省略就采用配置文件中指定的校对规则。 1、查看MySQL默认的字符集和校对规则12345678910111213141516171819202122232425mysql&gt; show variables like &#x27;%char%&#x27;; --查看系统默认的字符集+--------------------------+--------------------------------------+| Variable_name | Value |+--------------------------+--------------------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8mb4 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8mb4 || character_set_system | utf8 || character_sets_dir | /usr/local/mysql-5.7/share/charsets/ |+--------------------------+--------------------------------------+8 rows in set (0.26 sec)mysql&gt; show variables like &#x27;%COLL%&#x27;; --查看系统默认的校对规则+----------------------+--------------------+| Variable_name | Value |+----------------------+--------------------+| collation_connection | utf8_general_ci || collation_database | utf8mb4_general_ci || collation_server | utf8mb4_general_ci |+----------------------+--------------------+3 rows in set (0.03 sec) 修改数据库主要是修改字符集和校对规则 1alter database mydb character set gbk collate gbk_chinese_ci; 删除数据库1drop database 数据库名; 创建数据表(table)语法12345678910111213141516171819202122232425262728293031create [temporary] table [if not exists] &lt;表名&gt; ([列定义选项])[表选项][分区选项];[列定义选项]的格式为： &lt;列名1&gt; &lt;类型&gt; &lt;数据完整性约束&gt; [, &lt;列名2&gt; &lt;类型&gt; &lt;数据完整性约束&gt; [, ... [&lt;列名n&gt; &lt;类型n&gt;]]]&lt;数据完整性约束&gt;的格式： [NOT NULL | NULL] --非空约束 [DEFAULT default_value] --默认值 [AUTO_INCREMENT] --定义为自增列（类型必须是整型） [PRIMARY KEY] --主键 [[unique] index] --索引，唯一索引 [foreign KEY(column_name) REFERENCES table_name(column_name))] --定义外键 [COMMENT &#x27;string&#x27;] --为字段添加注释 [表选项]的格式为： ENGINE = engine_name（存储引擎的名称） --指定存储引擎 --常用的存储引擎有 InnoDB 和 MyISAM AUTO_INCREMENT = value --设置自增字段的起始值 [DEFAULT] CHARACTER SET charset_name（字符集名称） --设置字符集 [COLLATE collation_name（校对集名称）] --设置校对集 COMMENT = &#x27;string&#x27; --表注释--参数说明：（1）temporary：表示创建连时表。只有在当前连接情况下，TEMPORARY表才是可见的。当连接关闭时，TEMPORARY表被自动取消。这意味着两个不同的连接可以使用相同的临时表名称，同时两个临时表不会互相冲突，也不与原有的同名的非临时表冲突。（2）IF NOT EXISTS：如果加了该选项，只有在要创建的表不存在的情况下，才创建该表。如果要创建的表已经存在，则不执行 create table 命令。（3）表名：如果只指定表名，则把表创建到当前数据库中，可以使用【db_name.table_name】格式指定表名，则把表创建到指定的数据库中。 举例(1) 创建t01 1create table hist.t01(id int,name char(20)); (2) 包含主键 1234567create table if not exists hist.dept ( dept_id int primary key comment &#x27;部门编号&#x27;, dept_name char(20) not null comment &#x27;部门名称&#x27; ) comment &#x27;部门表&#x27;; （3）创建 emp 表，包含自增字段（从1001开始），通过外键和 dept 表关联 1234567891011121314151617181920212223242526272829mysql&gt; create table if not exists emp ( emp_id int auto_increment primary key comment &#x27;员工编号&#x27;, emp_name char(20) not null comment &#x27;员工姓名&#x27;, birth datetime comment &#x27;出生日期&#x27;, phone char(20) comment &#x27;员工表&#x27;, dept_id int, foreign key(dept_id) references dept(dept_id) ) comment &#x27;员工表&#x27; auto_increment=1001 default character set utf8 collate utf8_general_ci engine=InnoDB;Query OK, 0 rows affected (0.04 sec)mysql&gt; show create table emp\\G*************************** 1. row *************************** Table: empCreate Table: CREATE TABLE `emp` ( `emp_id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;员工编号&#x27;, `emp_name` char(20) NOT NULL COMMENT &#x27;员工姓名&#x27;, `birth` datetime DEFAULT NULL COMMENT &#x27;出生日期&#x27;, `phone` char(20) DEFAULT NULL COMMENT &#x27;员工表&#x27;, `dept_id` int(11) DEFAULT NULL, PRIMARY KEY (`emp_id`), KEY `dept_id` (`dept_id`), CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`dept_id`)) ENGINE=InnoDB AUTO_INCREMENT=1001 DEFAULT CHARSET=utf8 COMMENT=&#x27;员工表&#x27;1 row in set (0.00 sec) （4）创建 stu 表，包含默认值，索引 12345678910111213141516171819202122232425262728293031323334353637383940mysql&gt; create table if not exists stu( s_no int auto_increment primary key, s_name char(20) not null, birth datetime, phone char(11), addr char(100), index(s_name), unique index(phone), index(birth,s_name) );Query OK, 0 rows affected (0.04 sec)mysql&gt; desc stu;+--------+-----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+--------+-----------+------+-----+---------+----------------+| s_no | int(11) | NO | PRI | NULL | auto_increment || s_name | char(20) | NO | MUL | NULL | || birth | datetime | YES | MUL | NULL | || phone | char(11) | YES | UNI | NULL | || addr | char(100) | YES | | NULL | |+--------+-----------+------+-----+---------+----------------+5 rows in set (0.00 sec)mysql&gt; show create table stu\\G*************************** 1. row *************************** Table: stuCreate Table: CREATE TABLE `stu` ( `s_no` int(11) NOT NULL AUTO_INCREMENT, `s_name` char(20) NOT NULL, `birth` datetime DEFAULT NULL, `phone` char(11) DEFAULT NULL, `addr` char(100) DEFAULT NULL, PRIMARY KEY (`s_no`), UNIQUE KEY `phone` (`phone`), KEY `s_name` (`s_name`), KEY `birth` (`birth`,`s_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 通过已有的表建新表12345create [temporary] table [if not exists] table_namelike old_tbale_name;--说明：使用该命令可以创建一个和原数据表结构完全相同的新表，但不会复制原数据表中的数据。 举例 12345678910111213141516171819202122232425262728293031mysql&gt; create table if not exists student like stu;Query OK, 0 rows affected (0.04 sec)mysql&gt; desc student;+--------+-----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+--------+-----------+------+-----+---------+----------------+| s_no | int(11) | NO | PRI | NULL | auto_increment || s_name | char(20) | NO | MUL | NULL | || birth | datetime | YES | MUL | NULL | || phone | char(11) | YES | UNI | NULL | || addr | char(100) | YES | | NULL | |+--------+-----------+------+-----+---------+----------------+5 rows in set (0.01 sec)mysql&gt; show create table student\\G*************************** 1. row *************************** Table: studentCreate Table: CREATE TABLE `student` ( `s_no` int(11) NOT NULL AUTO_INCREMENT, `s_name` char(20) NOT NULL, `birth` datetime DEFAULT NULL, `phone` char(11) DEFAULT NULL, `addr` char(100) DEFAULT NULL, PRIMARY KEY (`s_no`), UNIQUE KEY `phone` (`phone`), KEY `s_name` (`s_name`), KEY `birth` (`birth`,`s_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 使用 select 查询的结果集来创建表123456789create [temporary] table [if not exists] table_name [(create_definition,...)] [table_options] [partition_options] [ignore | replace] [as] query_expression --说明：使用此命令可以根据查询结果创建一张新表，并且把查询到的数据插入到新建的表中。 举例： （1）创建新表 stu1，表结构来源于对 stu 表的查询结果 123456789101112131415161718192021222324252627282930mysql&gt; create table if not exists stu1 as select * from stu;Query OK, 3 rows affected (0.04 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from stu;+------+--------+---------------------+-------------+----------+| s_no | s_name | birth | phone | addr |+------+--------+---------------------+-------------+----------+| 1 | Jack | 1999-01-23 00:00:00 | 13703735566 | Beijing || 2 | Mark | 1999-10-03 00:00:00 | 13783735566 | Beijing || 3 | Rose | 2000-11-21 00:00:00 | 13783735522 | Shanghai |+------+--------+---------------------+-------------+----------+3 rows in set (0.01 sec)mysql&gt; show create table stu1\\G*************************** 1. row *************************** Table: stu1Create Table: CREATE TABLE `stu1` ( `s_no` int(11) NOT NULL DEFAULT &#x27;0&#x27;, `s_name` char(20) NOT NULL, `birth` datetime DEFAULT NULL, `phone` char(11) DEFAULT NULL, `addr` char(100) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)--新建的表包含了查询结果中的数据，但新建表的结构和原数据表的结构并不完全相同。 （2）创建新表 stu2，并且重新定义表结构 12345678910111213141516171819202122232425mysql&gt; create table if not exists stu2( s_no int auto_increment primary key, s_name char(30), phone char(11) ) engine=InnoDB character set utf8 collate utf8_general_ci as select s_no,s_name,phone from stu;Query OK, 3 rows affected (0.02 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; show create table stu2\\G*************************** 1. row *************************** Table: stu2Create Table: CREATE TABLE `stu2` ( `s_no` int(11) NOT NULL AUTO_INCREMENT, `s_name` char(30) DEFAULT NULL, `phone` char(11) DEFAULT NULL, PRIMARY KEY (`s_no`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf81 row in set (0.00 sec) 删除数据表123drop table 表名; 修改表结构12345678910mysql&gt; desc employee;+-------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(10) | YES | | NULL | || addr | varchar(100) | YES | | NULL | |+-------+--------------+------+-----+---------+-------+3 rows in set (0.00 sec)-- 以这个表为例 在 MySQL 中可以使用 ALTER TABLE 语句来改变原有数据表的结构。修改表结构涉及到的操作主要有：（1）增加和删除字段；（2）修改字段的数据类型；（3）修改字段名；（4）修改字段的排列位置；（5）添加和删除完整性约束；（6）修改表名；（7）更改表的存储引擎等。 12345678910ALTER TABLE &lt;表名&gt; [修改选项];[修改选项]的格式为：ADD COLUMN &lt;列名&gt; &lt;类型&gt;| CHANGE COLUMN &lt;旧列名&gt; &lt;新列名&gt; &lt;新列类型&gt;| ALTER COLUMN &lt;列名&gt; [ SET DEFAULT &lt;默认值&gt; | DROP DEFAULT ]| MODIFY COLUMN &lt;列名&gt; &lt;类型&gt;| DROP COLUMN &lt;列名&gt;| RENAME TO &lt;新表名&gt; 一、增加和删除字段1、增加字段1234ALTER TABLE &lt;表名&gt; ADD COLUMN &lt;列名&gt; &lt;类型&gt; &lt;完整性约束&gt; [FIRST | AFTER 列名];说明：使用 [FIRST | AFTER 列名] 选项可以设置新增字段的位置。 举例： （1）添加一个列 phone123456789101112131415mysql&gt; alter table employee add phone char(20);Query OK, 0 rows affected (0.17 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+-------+--------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(10) | YES | | NULL | || addr | varchar(100) | YES | | NULL | || phone | char(20) | YES | | NULL | |+-------+--------------+------+-----+---------+-------+4 rows in set (0.00 sec) （2）添加一个列 salary，默认值为0，不能取空值12345678910111213141516 mysql&gt; alter table employee add salary decimal(10,2) default 0 not null;Query OK, 0 rows affected (0.11 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+--------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(10) | YES | | NULL | || addr | varchar(100) | YES | | NULL | || phone | char(20) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+--------+---------------+------+-----+---------+-------+5 rows in set (0.00 sec) （3）新增一个字段 birth，并放在 name 字段之后1234567891011121314151617mysql&gt; ALTER TABLE employee ADD COLUMN birth datetime after name;Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+--------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(10) | YES | | NULL | || birth | datetime | YES | | NULL | || addr | varchar(100) | YES | | NULL | || phone | char(20) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+--------+---------------+------+-----+---------+-------+6 rows in set (0.00 sec) 2、删除字段1ALTER TABLE &lt;表名&gt; DROP COLUMN &lt;列名&gt;; 12345678910111213141516mysql&gt; alter table employee drop column salary;Query OK, 0 rows affected (0.17 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+--------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(10) | YES | | NULL | || birth | datetime | YES | | NULL | || addr | varchar(100) | YES | | NULL | || phone | char(20) | YES | | NULL | |+--------+---------------+------+-----+---------+-------+6 rows in set (0.00 sec) 二、修改字段的数据类型、名称和字段的位置1、修改字段的数据类型修改字段的数据类型使用 MODIFY 选项。语法格式如下： 1234ALTER TABLE &lt;表名&gt; MODIFY COLUMN &lt;列名&gt; &lt;类型&gt;;说明：使用 MODIFY 选项不能更改字段的名称。 举例：把字段 name 的类型更为 varchar，长度为20 12345678910111213141516mysql&gt; alter table employee modify name varchar(20);Query OK, 2 rows affected (0.05 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+--------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | || birth | datetime | YES | | NULL | || addr | varchar(100) | YES | | NULL | || phone | char(20) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+--------+---------------+------+-----+---------+-------+6 rows in set (0.01 sec) 2、修改字段的名称修改字段的名称使用 CHANGE 选项。语法格式如下： 1234ALTER TABLE &lt;表名&gt; CHANGE COLUMN &lt;旧列名&gt; &lt;新列名&gt; &lt;新列类型&gt;;说明：使用 CHANGE 选项时，如果新列名和旧列名相同，则作用和 MODIFY 相同。 举例：把 addr 列的名称修改为 address，类型修改为 varchar(200)，放到字段 phone 之后 1234567891011121314151617mysql&gt; alter table employee change addr address varchar(200) after phone;Query OK, 0 rows affected (0.13 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+---------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || address | varchar(200) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+---------+---------------+------+-----+---------+-------+6 rows in set (0.00 sec) 3、更改字段的位置在使用 ADD 选项添加字段、使用 MODIFY 选项修改字段的类型和长度、使用 CHANGE 选项修改字段名称和类型时，可以使用 [FIRST | AFTER 列名] 选项 指定字段的位置。语法格式如下： 1234567--添加字段同时指定新增字段的位置ALTER TABLE &lt;表名&gt; ADD COLUMN &lt;列名&gt; &lt;类型&gt; &lt;完整性约束&gt; [FIRST | AFTER 列名]; --修改字段的类型和长度同时指定字段的位置ALTER TABLE &lt;表名&gt; MODIFY COLUMN &lt;列名&gt; &lt;类型&gt; [FIRST | AFTER 列名];--修改字段的名称、类型和长度同时指定字段的位置ALTER TABLE &lt;表名&gt; CHANGE COLUMN &lt;旧列名&gt; &lt;新列名&gt; &lt;新列类型&gt; [FIRST | AFTER 列名]; 举例： （1）添加一个字段 comm，类型为 varchar(500)，位置放在第一位 123456789101112131415161718mysql&gt; alter table employee add comm varchar(500) first;Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+---------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+---------------+------+-----+---------+-------+| comm | varchar(500) | YES | | NULL | || id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || address | varchar(200) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+---------+---------------+------+-----+---------+-------+7 rows in set (0.01 sec) 2）把字段 comm 的长度修改为 1000，并放在字段 salary 之前 123456789101112131415161718mysql&gt; alter table employee modify comm varchar(1000) after address;Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+---------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || address | varchar(200) | YES | | NULL | || comm | varchar(1000) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | |+---------+---------------+------+-----+---------+-------+7 rows in set (0.01 sec) （3）把字段 comm 的名称修改为 comme，并放在表的结尾 123456789101112131415161718mysql&gt; alter table employee change comm comme varchar(1000) after salary;Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc employee;+---------+---------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+---------+---------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || address | varchar(200) | YES | | NULL | || salary | decimal(10,2) | NO | | 0.00 | || comme | varchar(1000) | YES | | NULL | |+---------+---------------+------+-----+---------+-------+7 rows in set (0.00 sec) 三、修改表名语法格式如下： 12ALTER TABLE &lt;表名&gt; RENAME [TO] &lt;新表名&gt;; 举例：把数据表 employee 名称修改为 emp 123456789101112131415mysql&gt; alter table employee rename emp;Query OK, 0 rows affected (0.10 sec)mysql&gt; show tables;+-----------------+| Tables_in_hist |+-----------------+| dept || emp || stu || student || user_permission |+-----------------+5 rows in set (0.00 sec) 数据更新之INSERTMySQL 数据库和其它的关系型数据库一样，支持数据的增（插入：insert）、删（删除：delete）、改（更新：update）、查（查询：select）操作。 一、数据准备创建两张表：部门（dept）和员工（emp），表结构如下： 12345678910111213141516171819202122mysql&gt; desc dept;+-----------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------+----------+------+-----+---------+-------+| dept_id | int(11) | NO | PRI | NULL | || dept_name | char(20) | NO | | NULL | |+-----------+----------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; desc emp;+----------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------+------+-----+---------+----------------+| emp_id | int(11) | NO | PRI | NULL | auto_increment || emp_name | char(20) | NO | | NULL | || gender | char(2) | YES | | 男 | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || dept_id | int(11) | YES | MUL | NULL | |+----------+----------+------+-----+---------+----------------+6 rows in set (0.01 sec) 二、数据的插入操作插入数据使用 Insert 命令，Insert 命令有三种用法： 1、插入一条或多条数据记录语法如下： 123456789-- 下面的命令一次可以插入一条或多条数据记录INSERT [INTO] 表名 [(列名,...)] VALUES (&#123;表达式 | DEFAULT&#125;, ...), (...),...说明：（1）每条数据记录包含在一对括号中。（2）如果省略表名后面的列名，则 values 后面的数据必须和表中字段的数量与顺序对应。（3）使用 DEFAULT 可以把字段的默认值插入表中。（4）使用一条 INSERT 命令插入多条数据记录时，多条记录之间用逗号分开。 （1）用 insert 命令一次插入一条记录1234567891011121314151617181920212223242526272829303132333435363738394041mysql&gt; insert into dept values(11,&#x27;人事部&#x27;);Query OK, 1 row affected (0.01 sec)mysql&gt; insert into dept(dept_id,dept_name) values(12,&#x27;财务部&#x27;);Query OK, 1 row affected (0.02 sec)mysql&gt; select * from dept;+---------+-----------+| dept_id | dept_name |+---------+-----------+| 11 | 人事部 || 12 | 财务部 |+---------+-----------+2 rows in set (0.00 sec)mysql&gt; insert into emp(emp_name,gender,birth,phone,dept_id) values(&#x27;Jack&#x27;,default,&#x27;1995-2-3&#x27;,&#x27;15937321555&#x27;,11);Query OK, 1 row affected (0.01 sec)mysql&gt; insert into emp values(1101,&#x27;Mark&#x27;,&#x27;男&#x27;,&#x27;1997-12-15&#x27;,&#x27;15903732155&#x27;,11);Query OK, 1 row affected (0.02 sec)mysql&gt; insert into emp(emp_name,gender,birth,phone,dept_id) values(&#x27;Jack&#x27;,&#x27;女&#x27;,null,&#x27;15937321666&#x27;,12);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into emp(emp_name,gender,dept_id) values(&#x27;Jack&#x27;,default,12);Query OK, 1 row affected (0.00 sec)mysql&gt; mysql&gt; select * from emp;+--------+----------+--------+---------------------+-------------+---------+| emp_id | emp_name | gender | birth | phone | dept_id |+--------+----------+--------+---------------------+-------------+---------+| 1 | Jack | 男 | 1995-02-03 00:00:00 | 15937321555 | 11 || 1101 | Mark | 男 | 1997-12-15 00:00:00 | 15903732155 | 11 || 1102 | Jack | 女 | NULL | 15937321666 | 12 || 1103 | Jack | 男 | NULL | NULL | 12 |+--------+----------+--------+---------------------+-------------+---------+4 rows in set (0.00 sec) （2）用 insert 命令一次插入多条记录12345678910111213141516mysql&gt; insert into dept values(13,&#x27;生产制造部&#x27;),(14,&#x27;销售部&#x27;),(15,&#x27;公关部&#x27;);Query OK, 3 rows affected (0.01 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from dept;+---------+-----------------+| dept_id | dept_name |+---------+-----------------+| 11 | 人事部 || 12 | 财务部 || 13 | 生产制造部 || 14 | 销售部 || 15 | 公关部 |+---------+-----------------+5 rows in set (0.00 sec) 2、使用 set 参数插入数据12345INSERT [INTO] 表名 SET col_name=&#123;表达式 | DEFAULT&#125;, ... 说明：使用这种形式的 INSERT 语句不能插入多行。 举例 : 12345678910111213141516mysql&gt; insert into dept set dept_id=16,dept_name=&#x27;信息部&#x27;;Query OK, 1 row affected (0.01 sec)mysql&gt; select * from dept;+---------+-----------------+| dept_id | dept_name |+---------+-----------------+| 11 | 人事部 || 12 | 财务部 || 13 | 生产制造部 || 14 | 销售部 || 15 | 公关部 || 16 | 信息部 |+---------+-----------------+6 rows in set (0.00 sec) 3、把一个查询的结果插入到数据表中格式如下： 12345INSERT [INTO] 表名 [(列名,...)] SELECT ... 说明：查询命令不能包含 ORDER BY子句，而且INSERT语句的目的表不能出现在查询命令的 FROM 子句中。 举例： （1）创建一张数据表表结构如下： 1234567891011121314151617mysql&gt; create table emp001 select emp_id,emp_name,phone from emp where 2=3;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc emp001;+----------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+----------+----------+------+-----+---------+-------+| emp_id | int(11) | NO | | 0 | || emp_name | char(20) | NO | | NULL | || phone | char(20) | YES | | NULL | |+----------+----------+------+-----+---------+-------+3 rows in set (0.00 sec)mysql&gt; select * from emp001;Empty set (0.00 sec) （2）从 emp 表中查询中男性员工的信息插入 emp001 表中1234567891011121314mysql&gt; insert into emp001 select emp_id,emp_name,phone from emp where gender=&#x27;男&#x27;;Query OK, 3 rows affected (0.02 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from emp001;+--------+----------+-------------+| emp_id | emp_name | phone |+--------+----------+-------------+| 1 | Jack | 15937321555 || 1101 | Mark | 15903732155 || 1103 | Jack | NULL |+--------+----------+-------------+3 rows in set (0.00 sec) 4、replace into 命令replace into 命令的格式与 insert into 命令基本相同。使用 insert into 命令插入数据时，如果主键重复，则插入失败，使用replace into 命令插入数据时，如果主键重复，则替换该行的所有数据，相当于将主键这条记录彻底删除，再插入新的记录。也就是说，将所有的字段都更新了。 语法如下： 123456REPLACE [INTO] 表名 [(列名, ...)] VALUES (expr, ...),(...),...REPLACE [INTO] 表名 [(列名, ...)] SELECT ... 举例：在 dept 表中插入一个新部门：技术部 12345678910111213141516171819202122232425262728293031mysql&gt; select * from dept;+---------+-----------------+| dept_id | dept_name |+---------+-----------------+| 11 | 人事部 || 12 | 财务部 || 13 | 生产制造部 || 14 | 销售部 || 15 | 公关部 || 16 | 信息部 |+---------+-----------------+6 rows in set (0.00 sec)mysql&gt; replace into dept values(16,&#x27;技术部&#x27;);Query OK, 2 rows affected (0.01 sec)mysql&gt; select * from dept;+---------+-----------------+| dept_id | dept_name |+---------+-----------------+| 11 | 人事部 || 12 | 财务部 || 13 | 生产制造部 || 14 | 销售部 || 15 | 公关部 || 16 | 技术部 |+---------+-----------------+6 rows in set (0.00 sec)-- 可以看到，当命令执行成功后，“信息部”被替换为了“技术部”。 数据更新之UPDATEMySQL 数据库和其它的关系型数据库一样，支持数据的增（插入：insert）、删（删除：delete）、改（更新：update）、查（查询：select）操作。 一、数据准备创建两张表：部门（dept）和员工（emp），表结构如下： 12345678910111213141516171819202122mysql&gt; desc dept;+-----------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------+----------+------+-----+---------+-------+| dept_id | int(11) | NO | PRI | NULL | || dept_name | char(20) | NO | | NULL | |+-----------+----------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; desc emp;+----------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+----------+----------+------+-----+---------+----------------+| emp_id | int(11) | NO | PRI | NULL | auto_increment || emp_name | char(20) | NO | | NULL | || gender | char(2) | YES | | 男 | || birth | datetime | YES | | NULL | || phone | char(20) | YES | | NULL | || dept_id | int(11) | YES | MUL | NULL | |+----------+----------+------+-----+---------+----------------+6 rows in set (0.01 sec) 二、数据更新操作使用 UPDATE 命令可以对表中的数据进行更新，该命令有两种用法： 1、为表中的字段指定一个常量或表达式格式如下： 123456789UPDATE 表名 SET 列名 = 表达式 [, 列名 = 表达式 ...] [WHERE 条件] [ORDER BY ...] [LIMIT row_count]说明：（1） ORDER BY：按照指定的顺序对行进行更新。（2） LIMIT：限制可更新的行数。 举例： 12345678910111213141516171819202122232425262728293031mysql&gt; select * from emp;+--------+----------+--------+---------+-------------+---------+| emp_id | emp_name | gender | salary | phone | dept_id |+--------+----------+--------+---------+-------------+---------+| 1 | Jack | 男 | 4500.00 | 15937321555 | 11 || 1101 | Mark | 男 | 7100.00 | 15903732155 | 11 || 1102 | Rose | 女 | 6200.00 | 15937321666 | 12 || 1103 | Jerry | 男 | 3700.00 | NULL | 12 |+--------+----------+--------+---------+-------------+---------+4 rows in set (0.00 sec)mysql&gt; update emp set phone=&#x27;15537312999&#x27; where emp_id=1103;--更新1103员工的电话Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update emp set salary=salary*1.05; --所有人工资增加 5%Query OK, 4 rows affected (0.04 sec)Rows matched: 4 Changed: 4 Warnings: 0mysql&gt; select * from emp;+--------+----------+--------+---------+-------------+---------+| emp_id | emp_name | gender | salary | phone | dept_id |+--------+----------+--------+---------+-------------+---------+| 1 | Jack | 男 | 4725.00 | 15937321555 | 11 || 1101 | Mark | 男 | 7455.00 | 15903732155 | 11 || 1102 | Rose | 女 | 6510.00 | 15937321666 | 12 || 1103 | Jerry | 男 | 3885.00 | 15537312999 | 12 |+--------+----------+--------+---------+-------------+---------+4 rows in set (0.00 sec) 2、利用另一个表中的数据更新当前表格式如下： 1234567UPDATE 表1 join 表2 on 表1.列名 = 表2.列名 SET 列名 = 表达式 [, 列名 = 表达式 ...] [WHERE 条件]说明：（1）利用 表2 中的数据更新 表1，要求两个表必须存在关联字段。（2）表1 和 表2 关联的条件为 表1.列名 = 表2.列名。 举例： （1）为 emp 表添加一个列 dept_name，并使用 dept 表的相关数据进行填充 1234567891011121314151617181920212223242526272829303132mysql&gt; alter table emp add dept_name char(20);Query OK, 0 rows affected (0.07 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; select * from emp;+--------+----------+--------+---------+-------------+---------+-----------+| emp_id | emp_name | gender | salary | phone | dept_id | dept_name |+--------+----------+--------+---------+-------------+---------+-----------+| 1 | Jack | 男 | 4725.00 | 15937321555 | 11 | NULL || 1101 | Mark | 男 | 7455.00 | 15903732155 | 11 | NULL || 1102 | Rose | 女 | 6510.00 | 15937321666 | 12 | NULL || 1103 | Jerry | 男 | 3885.00 | 15537312999 | 12 | NULL |+--------+----------+--------+---------+-------------+---------+-----------+4 rows in set (0.00 sec)mysql&gt; update emp join dept on emp.dept_id = dept.dept_id -&gt; set emp.dept_name = dept.dept_name -&gt; where emp.dept_id = 11;Query OK, 2 rows affected (0.01 sec)Rows matched: 2 Changed: 2 Warnings: 0mysql&gt; mysql&gt; select * from emp;+--------+----------+--------+---------+-------------+---------+-----------+| emp_id | emp_name | gender | salary | phone | dept_id | dept_name |+--------+----------+--------+---------+-------------+---------+-----------+| 1 | Jack | 男 | 4725.00 | 15937321555 | 11 | 人事部 || 1101 | Mark | 男 | 7455.00 | 15903732155 | 11 | 人事部 || 1102 | Rose | 女 | 6510.00 | 15937321666 | 12 | NULL || 1103 | Jerry | 男 | 3885.00 | 15537312999 | 12 | NULL |+--------+----------+--------+---------+-------------+---------+-----------+ （2）有两个表 stu 和 certificate，使用 stu 表的 s_name 和 phone 列的数据更新 certificate 表。 1234567891011121314151617181920212223242526272829303132333435363738394041mysql&gt; select * from stu;+------+--------+---------------------+-------------+----------+| s_no | s_name | birth | phone | addr |+------+--------+---------------------+-------------+----------+| 1 | Jack | 1999-01-23 00:00:00 | 13703735566 | Beijing || 2 | Mark | 1999-10-03 00:00:00 | 13783735566 | Beijing || 3 | Rose | 2000-11-21 00:00:00 | 13783735522 | Shanghai || 4 | John | 2000-03-04 00:00:00 | 18503735214 | Xinxiang || 5 | Jerry | 2001-04-25 00:00:00 | 13303735266 | Xinxiang |+------+--------+---------------------+-------------+----------+5 rows in set (0.00 sec)mysql&gt; select * from certificate;+------+--------+-------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------+-----------------+| 1 | NULL | NULL | 英语四级 || 2 | NULL | NULL | 英语四级 || 3 | NULL | NULL | 英语六级 || 4 | NULL | NULL | 计算机二级 || 5 | NULL | NULL | 英语四级 |+------+--------+-------+-----------------+5 rows in set (0.00 sec)mysql&gt; update certificate c join stu s on c.s_no = s.s_no -&gt; set c.s_name = s.s_name, c.phone = s.phone;Query OK, 5 rows affected (0.01 sec)Rows matched: 5 Changed: 5 Warnings: 0mysql&gt; select * from certificate;+------+--------+-------------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------------+-----------------+| 1 | Jack | 13703735566 | 英语四级 || 2 | Mark | 13783735566 | 英语四级 || 3 | Rose | 13783735522 | 英语六级 || 4 | John | 18503735214 | 计算机二级 || 5 | Jerry | 13303735266 | 英语四级 |+------+--------+-------------+-----------------+5 rows in set (0.00 sec) 数据更新之DELETE一、使用 delete 命令删除表中的数据格式如下： 123456789DELETE FROM 表名 [WHERE 条件] [ORDER BY ...] [LIMIT row_count] 说明：（1） ORDER BY：按照指定的顺序对行删除操作。（2） LIMIT：限制可删除的行数。 举例： （1）从 certificate 表删除 phone 为 133 开头的记录 1234567891011121314151617181920212223242526mysql&gt; select * from certificate;+------+--------+-------------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------------+-----------------+| 1 | Jack | 13703735566 | 英语四级 || 2 | Mark | 13783735566 | 英语四级 || 3 | Rose | 13783735522 | 英语六级 || 4 | John | 18503735214 | 计算机二级 || 5 | Jerry | 13303735266 | 英语四级 |+------+--------+-------------+-----------------+5 rows in set (0.00 sec)mysql&gt; delete from certificate where phone like &#x27;133%&#x27;;Query OK, 1 row affected (0.02 sec)mysql&gt; select * from certificate;+------+--------+-------------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------------+-----------------+| 1 | Jack | 13703735566 | 英语四级 || 2 | Mark | 13783735566 | 英语四级 || 3 | Rose | 13783735522 | 英语六级 || 4 | John | 18503735214 | 计算机二级 |+------+--------+-------------+-----------------+4 rows in set (0.00 sec) （2）删除时限制删除的行数：从 certificate 表删除 phone 为 137 开头的记录，但一次最多删除两行。并且按学号降序的顺序删除。 12345678910111213141516171819202122232425mysql&gt; select * from certificate;+------+--------+-------------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------------+-----------------+| 1 | Jack | 13703735566 | 英语四级 || 2 | Mark | 13783735566 | 英语四级 || 3 | Rose | 13783735522 | 英语六级 || 4 | John | 18503735214 | 计算机二级 |+------+--------+-------------+-----------------+4 rows in set (0.00 sec)mysql&gt; delete from certificate where phone like &#x27;137%&#x27; -&gt; order by s_no desc -&gt; limit 2;Query OK, 2 rows affected (0.07 sec)mysql&gt; select * from certificate;+------+--------+-------------+-----------------+| s_no | s_name | phone | certificate |+------+--------+-------------+-----------------+| 1 | Jack | 13703735566 | 英语四级 || 4 | John | 18503735214 | 计算机二级 |+------+--------+-------------+-----------------+2 rows in set (0.01 sec) 二、使用 truncate 命令删除表中的数据使用 truncate 命令可以删除表中的所有数据，和不带条件的 delete 命令结果相同。两个命令的区别有两点：（1）delete 命令在删除时逐行判断和删除，效率较低；truncate 命令是直接删除表，然后重建表，因此删除的效率很高。（2）当表中有自增字段时，如果使用 delete 命令删除全部记录，当重新插入记录时，自增字段的值从删除之前的编号的最大值开始增加。如果使用 truncate 命令删除全部记录，重新插入记录时，自增字段的值将重新开始编号。 truncate 命令的语法如下： truncate [TABLE] 表名;1举例： （1）创建表 t1，id 为自增字段，初始值为 1，然后插入数据。根据 t1 表生成 t2，t2 表的结构和数据与 t1 表完全相同。 结果如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mysql&gt; desc t1;+-------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | |+-------+----------+------+-----+---------+----------------+2 rows in set (0.00 sec)mysql&gt; select * from t1;+----+-------+| id | name |+----+-------+| 1 | zhang || 2 | wang || 3 | li || 4 | zhao || 5 | liu |+----+-------+5 rows in set (0.00 sec)mysql&gt; create table t2 like t1;Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into t2 select * from t1;Query OK, 5 rows affected (0.08 sec)Records: 5 Duplicates: 0 Warnings: 0mysql&gt; desc t2;+-------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | |+-------+----------+------+-----+---------+----------------+2 rows in set (0.00 sec)mysql&gt; select * from t2;+----+-------+| id | name |+----+-------+| 1 | zhang || 2 | wang || 3 | li || 4 | zhao || 5 | liu |+----+-------+5 rows in set (0.00 sec) （2）使用 delete 命令删除 t1 表中的所有数据，然后插入新记录 可以看到，新插入的记录 id 从 6 开始编号。 1234567891011121314151617mysql&gt; delete from t1;Query OK, 5 rows affected (0.02 sec)mysql&gt; insert into t1(name) values(&#x27;Tom&#x27;),(&#x27;Jack&#x27;),(&#x27;Jerry&#x27;);Query OK, 3 rows affected (0.02 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from t1;+----+-------+| id | name |+----+-------+| 6 | Tom || 7 | Jack || 8 | Jerry |+----+-------+3 rows in set (0.00 sec) （3）使用 truncate 命令删除 t2 表中的所有数据，然后插入新记录 可以看到，新插入的记录 id 从 1 重新开始编号。 1234567891011121314151617mysql&gt; truncate table t2;Query OK, 0 rows affected (0.03 sec)mysql&gt; insert into t2(name) values(&#x27;Tom&#x27;),(&#x27;Jack&#x27;),(&#x27;Jerry&#x27;);Query OK, 3 rows affected (0.03 sec)Records: 3 Duplicates: 0 Warnings: 0mysql&gt; select * from t2;+----+-------+| id | name |+----+-------+| 1 | Tom || 2 | Jack || 3 | Jerry |+----+-------+3 rows in set (0.00 sec) 完整性约束概述数据完整性是指数据的正确性和相容性，是为了防止数据库中存在不符合语义的数据，即防止数据库中存在不正确的数据。在MySQL中提供了多种完整性约束。完整性约束根据约束的规则不同可分为三类： （1）实体完整性：可以保证数据的唯一性。可以使用主键约束与唯一性约束来定义。（2）参照完整性：一个表中某个字段的取值要参照另一个表的主键。使用外键约束来定义。（3）域完整性：又称为用户自定义完整性。可以针对某个列的取值由用户定义约束的规则。 MySQL中的完整性约束如下表所示： 完整性约束 说明PRIMARY KEY 主键约束（实体完整性）UNIQUE 唯一性约束（实体完整性）FOREIGN KEY 外键约束（参照完整性）NOT NULL 非空约束（域完整性）AUTO_INCREMENT 自增字段DEFAULT 默认值 一、主键约束主键是表中的某一个或多个列，由多个列组合而成的主键也称为复合主键。主键的取值能够唯一标识表中的记录。主键的定义必须遵守以下规则： （1）一个表只能定义一个主键。（2）主键的值必须能够唯一标识表中的每一条记录，且不能为空（NULL）。 创建表时定义1、定义列时同时定义主键12345create table 表名 ( &lt;字段名&gt; &lt;数据类型&gt; PRIMARY KEY [AUTO_INCREMENT], ....); 12345678910111213141516171819202122232425262728mysql&gt; create table emp001( -&gt; id int primary key auto_increment, -&gt; name char(20), -&gt; salary decimal(8,1) -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; desc emp001;+--------+--------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+--------+--------------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | || salary | decimal(8,1) | YES | | NULL | |+--------+--------------+------+-----+---------+----------------+3 rows in set (0.00 sec)mysql&gt; show create table emp001\\G*************************** 1. row *************************** Table: emp001Create Table: CREATE TABLE `emp001` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, `salary` decimal(8,1) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 2、在定义完所有列之后定义主键12345create table 表名 ( &lt;字段定义&gt;... , PRIMARY KEY (字段名)); 1234567891011121314151617181920212223242526272829mysql&gt; create table stu001( -&gt; s_no char(11), -&gt; s_name char(20), -&gt; birth datetime, -&gt; primary key(s_no) -&gt; );Query OK, 0 rows affected (0.04 sec)mysql&gt; desc stu001;+--------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+--------+----------+------+-----+---------+-------+| s_no | char(11) | NO | PRI | NULL | || s_name | char(20) | YES | | NULL | || birth | datetime | YES | | NULL | |+--------+----------+------+-----+---------+-------+3 rows in set (0.01 sec)mysql&gt; show create table stu001\\G*************************** 1. row *************************** Table: stu001Create Table: CREATE TABLE `stu001` ( `s_no` char(11) NOT NULL, `s_name` char(20) DEFAULT NULL, `birth` datetime DEFAULT NULL, PRIMARY KEY (`s_no`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 定义多列主键1234567891011121314151617181920212223242526272829mysql&gt; create table score( -&gt; stu_no char(11), -&gt; course_no char(10), -&gt; socre int, -&gt; primary key(stu_no,course_no) -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; desc score;+-----------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-----------+----------+------+-----+---------+-------+| stu_no | char(11) | NO | PRI | NULL | || course_no | char(10) | NO | PRI | NULL | || socre | int(11) | YES | | NULL | |+-----------+----------+------+-----+---------+-------+3 rows in set (0.00 sec)mysql&gt; show create table score\\G*************************** 1. row *************************** Table: scoreCreate Table: CREATE TABLE `score` ( `stu_no` char(11) NOT NULL, `course_no` char(10) NOT NULL, `socre` int(11) DEFAULT NULL, PRIMARY KEY (`stu_no`,`course_no`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 创建表之后添加主键1alter table 表名 add 列名 类型 primary key [auto_increment]; 1234567891011121314151617181920212223242526mysql&gt; create table t11(name char(11));Query OK, 0 rows affected (0.02 sec)mysql&gt; alter table t11 add id int primary key auto_increment first;Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc t11;+-------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(11) | YES | | NULL | |+-------+----------+------+-----+---------+----------------+2 rows in set (0.00 sec)mysql&gt; show create table t11\\G*************************** 1. row *************************** Table: t11Create Table: CREATE TABLE `t11` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(11) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 把一个已经存在的列定义为主键1alter table 表名 add primary key(列名); 1234567891011121314151617181920212223242526272829mysql&gt; create table t22( -&gt; id int not null, -&gt; name char(20) -&gt; );Query OK, 0 rows affected (0.01 sec)mysql&gt; alter table t22 add primary key(id);Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc t22;+-------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | char(20) | YES | | NULL | |+-------+----------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; show create table t22\\G*************************** 1. row *************************** Table: t22Create Table: CREATE TABLE `t22` ( `id` int(11) NOT NULL, `name` char(20) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 删除主键12345678910111213141516171819202122mysql&gt; alter table t22 drop primary key;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc t22;+-------+----------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+-------+| id | int(11) | NO | | NULL | || name | char(20) | YES | | NULL | |+-------+----------+------+-----+---------+-------+2 rows in set (0.00 sec)mysql&gt; show create table t22\\G*************************** 1. row *************************** Table: t22Create Table: CREATE TABLE `t22` ( `id` int(11) NOT NULL, `name` char(20) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 二、唯一约束唯一约束使用 UNIQUE 关键字来定义。唯一约束的要求某一列的取值必须是唯一的，不能取重复值。主键约束同时也是唯一约束，唯一约束与主键之间存在以下区别：（1）一个表只能创建一个主键，但可以定义多个唯一约束。（2）定义主键约束时，系统会自动创建 PRIMARY KEY 索引，定义 UNIQUE 约束时，系统会自动创建 UNIQUE 索引。 一、UNIQUE 约束与 PRIMARY KEY 约束的区别和联系（1）唯一性约束所在的列允许空值，但是主键约束所在的列不允许空值。（2）可以把唯一性约束放在一个或者多个列上，这些列或列的组合必须有唯一的。但是，唯一性约束所在的列并不是表的主键列。（3）唯一性约束强制在指定的列上创建一个唯一性索引。（4）一个表最多只有一个主键，但可以有多个唯一键。（5）UNIQUE 约束 &#x3D; PRIMARY KEY 约束 + NOT NULL 。 二、创建表时，同时创建 UNIQUE 约束1、定义列的同时定义 UNIQUE 约束语法如下： 1234567create table 表名 ( 列名 类型 ... , 列名 类型 unique, ....);说明：该方法不能指定约束名。可以使用【show index from 表名; 】命令查看索引名。 12345678910111213141516171819202122232425262728293031mysql&gt; create table t12 ( -&gt; id int primary key auto_increment, -&gt; name char(20), -&gt; phone char(11) unique, -&gt; ID_number char(18) unique -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; desc t12;+-----------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-----------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | || phone | char(11) | YES | UNI | NULL | || ID_number | char(18) | YES | UNI | NULL | |+-----------+----------+------+-----+---------+----------------+4 rows in set (0.00 sec)mysql&gt; show index from t12;+-------+------------+-----------+--------------+-------------+-----------+-------------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality +-------+------------+-----------+--------------+-------------+-----------+-------------| t12 | 0 | PRIMARY | 1 | id | A | 0 | t12 | 0 | phone | 1 | phone | A | 0 | t12 | 0 | ID_number | 1 | ID_number | A | 0 +-------+------------+-----------+--------------+-------------+-----------+-------------3 rows in set (0.00 sec)-- unique 约束的名称默认和字段名相同。 2、在所有的列定义之后定义 UNIQUE 约束123456create table 表名 ( 列名 类型 ... , [constraint 约束名] unique(列名));说明：该方法可以指定约束名。可以使用【show index from 表名; 】命令查看索引名。 1234567891011121314151617181920212223242526272829303132mysql&gt; create table t13 ( -&gt; id int primary key auto_increment, -&gt; name char(20), -&gt; phone char(11), -&gt; ID_number char(18), -&gt; constraint uq_phone unique(phone), -&gt; constraint uq_ID_number unique(ID_number) -&gt;);Query OK, 0 rows affected (0.04 sec)mysql&gt; desc t13;+-----------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-----------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | || phone | char(11) | YES | UNI | NULL | || ID_number | char(18) | YES | UNI | NULL | |+-----------+----------+------+-----+---------+----------------+4 rows in set (0.00 sec)mysql&gt; show index from t13;+-------+------------+--------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+--------------+--------------+-------------+-----------| t13 | 0 | PRIMARY | 1 | id | A | t13 | 0 | uq_phone | 1 | phone | A | t13 | 0 | uq_ID_number | 1 | ID_number | A +-------+------------+--------------+--------------+-------------+-----------3 rows in set (0.00 sec) 三、创建表之后添加 UNIQUE 约束创建表之后可以添加 UNIQUE 约束，语法格式如下： 1alter table 表名 add [constraint 约束名] unique(列名); 123456789101112131415161718192021222324252627282930mysql&gt; create table t14( -&gt; id int primary key auto_increment, -&gt; name char(20), -&gt; phone char(11) -&gt; );Query OK, 0 rows affected (0.04 sec)mysql&gt; alter table t14 add constraint uq_phone unique(phone);Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc t14;+-------+----------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------+------+-----+---------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | char(20) | YES | | NULL | || phone | char(11) | YES | UNI | NULL | |+-------+----------+------+-----+---------+----------------+3 rows in set (0.00 sec)mysql&gt; show index from t14;+-------+------------+----------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+----------+--------------+-------------+-----------| t14 | 0 | PRIMARY | 1 | id | A | t14 | 0 | uq_phone | 1 | phone | A +-------+------------+----------+--------------+-------------+-----------2 rows in set (0.00 sec) 四、删除 UNIQUE 约束删除 UNIQUE 约束之前可以使用 【show index from 表名; 】查看 UNIQUE 约束的名称。删除 UNIQUE 约束的命令格式如下： 1alter table 表名 drop index 约束名; 1234567891011121314151617181920212223mysql&gt; show index from t13;+-------+------------+--------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+--------------+--------------+-------------+-----------| t13 | 0 | PRIMARY | 1 | id | A | t13 | 0 | uq_phone | 1 | phone | A | t13 | 0 | uq_ID_number | 1 | ID_number | A +-------+------------+--------------+--------------+-------------+-----------3 rows in set (0.00 sec)mysql&gt; alter table t13 drop index uq_ID_number;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show index from t13;+-------+------------+----------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+----------+--------------+-------------+-----------| t13 | 0 | PRIMARY | 1 | id | A | t13 | 0 | uq_phone | 1 | phone | A +-------+------------+----------+--------------+-------------+-----------2 rows in set (0.00 sec) 三、外键约束两个表必须是 InnoDB 存储引擎 一个表外键所包含的列的类型和与之发生关联的另一个表的主键列的数据类型必须相似，也就是可以相互转换类型的列，数据类型最好相同。 外键用于定义多个表之间的参照完整性。参照完整性指多个表之间的对应关系，在一张表中执行数据的插入、更新、删除等操作时，会和另一张表进行对照，以确保数据存储的完整性。 外键约束必须遵守以下规则： （1）某一个表中的某个字段的取值依赖于另一张表中某个字段的值。（2）主键所在的表为主表，外键所在的表为从表，每一个外键值必须与另一个表中的主键值相对应。 一、创建表的同时定义外键在创建表时可以同时创建外键，语法如下： 123456789101112131415create table 表名 ( 列名 类型 ... , [CONSTRAINT 约束名] FOREIGN KEY (列名) REFERENCES 表名 (列名) [ON DELETE &#123;RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT&#125;] [ON UPDATE &#123;RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT&#125;]);说明：定义了外键约束之后，在删除父表记录和更新父表的主键时可以设置以下操作方式。（1）cascade 方式：在父表上更新和删除记录时，子表匹配的记录也同步进行更新（级联更新）和删除（级联删除）；（2）set null 方式：在父表上更新和删除记录时，子表匹配的记录的外键设为null；（3）No action 方式：如果子表中有匹配的记录，则不允许对父表对应的主键进行更新和删除操作；（4）Restrict 方式：同 no action, 都是立即检查外键约束；（5）Set default 方式：父表上更新和删除记录时，子表将外键列设置成一个默认的值；（6）系统默认为No action 方式。 例子： 1、创建部门（dept）和员工（emp）表，并创建外键。（1）创建表 12345678910111213create table dept ( id int primary key auto_increment, name char(20));create table emp ( id int primary key auto_increment, name char(20), salary decimal(8,2), dept_id int, foreign key(dept_id) references dept(id)); (2) 插入数据 123456789101112131415161718192021222324mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 1 | 人事部 || 2 | 销售部 || 3 | 技术部 || 4 | 财务部 |+----+-----------+4 rows in set (0.00 sec)mysql&gt; select * from emp;+----+-----------+---------+---------+| id | name | salary | dept_id |+----+-----------+---------+---------+| 1 | 张鹏 | 4500.00 | 1 || 2 | 王晶 | 5700.00 | 1 || 3 | 刘云 | 4900.00 | 2 || 4 | 王晓刚 | 5200.00 | 2 || 5 | 刘大鹏 | 4200.00 | 2 || 6 | 王军军 | 5600.00 | 3 |+----+-----------+---------+---------+6 rows in set (0.00 sec) （3）验证外键约束 在删除父表记录和更新父表的主键时，子表的操作方式默认为No action 方式。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152-- 1、删除 dept 表（父表）：删除失败mysql&gt; drop table dept;ERROR 1217 (23000): Cannot delete or update a parent row: a foreign key constraint fails-- 2、删除 dept 表（父表）中的 1号部门（人事部）：子表中有相关记录，删除失败mysql&gt; delete from dept where id=1;ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`wgx`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`id`))-- 3、删除 dept 表（父表）中的 4号部门（财务部）：由于子表中没有相关记录，成功删除mysql&gt; delete from dept where id=4;Query OK, 1 row affected (0.01 sec)mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 1 | 人事部 || 2 | 销售部 || 3 | 技术部 |+----+-----------+3 rows in set (0.00 sec)-- 4、重新插入财务部的信息mysql&gt; insert into dept(id,name) values(4,&#x27;财务部&#x27;);Query OK, 1 row affected (0.02 sec)mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 1 | 人事部 || 2 | 销售部 || 3 | 技术部 || 4 | 财务部 |+----+-----------+4 rows in set (0.00 sec)-- 5、更新父表的字段 id，更新人事部的 id 失败，因为子表中存在相关记录mysql&gt; update dept set id=11 where id=1;ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`wgx`.`emp`, CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`id`))-- 6、更新父表的字段 id，更新财务部的 id 成功，因为子表中不存在相关记录mysql&gt; update dept set id=44 where id=4;Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 1 | 人事部 || 2 | 销售部 || 3 | 技术部 || 44 | 财务部 |+----+-----------+4 rows in set (0.00 sec) 2、把 emp 表的外键设置为级联更新和级联删除不提供对外键的直接修改，可以先删除外键，然后再重新创建。 （1）查看外键约束的名称 12345678910111213141516mysql&gt; show create table emp\\G*************************** 1. row *************************** Table: empCreate Table: CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, `salary` decimal(8,2) DEFAULT NULL, `dept_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `dept_id` (`dept_id`), CONSTRAINT `emp_ibfk_1` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)-- 外键约束的名称为：emp_ibfk_1 （2）删除外键约束 1234mysql&gt; alter table emp drop foreign key emp_ibfk_1;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 （3）重新添加外键约束 12345678910111213141516171819202122mysql&gt; alter table emp -&gt; add constraint fk_emp_dept_id foreign key(dept_id) -&gt; references dept(id) -&gt; on update cascade -&gt; on delete cascade;Query OK, 6 rows affected (0.06 sec)Records: 6 Duplicates: 0 Warnings: 0mysql&gt; show create table emp\\G*************************** 1. row *************************** Table: empCreate Table: CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, `salary` decimal(8,2) DEFAULT NULL, `dept_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `fk_emp_dept_id` (`dept_id`), CONSTRAINT `fk_emp_dept_id` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`id`) ON DELETE CASCADE ON UPDATE CASCADE) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) （4）测试外键约束 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566-- 1、在dept表中把人事部的编号修改为11，销售部的编号修改22，技术部的编号修改为33-- 可以看到，子表中对应的部门编号自动被修改mysql&gt; update dept set id=11 where id=1;Query OK, 1 row affected (0.03 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update dept set id=22 where id=2;Query OK, 1 row affected (0.02 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; update dept set id=33 where id=3;Query OK, 1 row affected (0.01 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 11 | 人事部 || 22 | 销售部 || 33 | 技术部 || 44 | 财务部 |+----+-----------+4 rows in set (0.00 sec)mysql&gt; select * from emp;+----+-----------+---------+---------+| id | name | salary | dept_id |+----+-----------+---------+---------+| 1 | 张鹏 | 4500.00 | 11 || 2 | 王晶 | 5700.00 | 11 || 3 | 刘云 | 4900.00 | 22 || 4 | 王晓刚 | 5200.00 | 22 || 5 | 刘大鹏 | 4200.00 | 22 || 6 | 王军军 | 5600.00 | 33 |+----+-----------+---------+---------+6 rows in set (0.01 sec)-- 2、在dept表中删除技术部的信息-- 可以看到，技术部的员工自动被删除mysql&gt; delete from dept where id=33;Query OK, 1 row affected (0.01 sec)mysql&gt; select * from dept;+----+-----------+| id | name |+----+-----------+| 11 | 人事部 || 22 | 销售部 || 44 | 财务部 |+----+-----------+3 rows in set (0.00 sec)mysql&gt; select * from emp;+----+-----------+---------+---------+| id | name | salary | dept_id |+----+-----------+---------+---------+| 1 | 张鹏 | 4500.00 | 11 || 2 | 王晶 | 5700.00 | 11 || 3 | 刘云 | 4900.00 | 22 || 4 | 王晓刚 | 5200.00 | 22 || 5 | 刘大鹏 | 4200.00 | 22 |+----+-----------+---------+---------+5 rows in set (0.00 sec) 二、删除外键约束删除外键约束的语法格式如下： 1alter table 表名 drop foreign key 约束名; 删除外键约束之前需要先查询外键约束名，可以使用 show create table 表名： 1234567891011121314151617181920212223242526-- 查看外键约束名mysql&gt; show create table emp\\G*************************** 1. row *************************** Table: empCreate Table: CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, `salary` decimal(8,2) DEFAULT NULL, `dept_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `fk_emp_dept_id` (`dept_id`), CONSTRAINT `fk_emp_dept_id` FOREIGN KEY (`dept_id`) REFERENCES `dept` (`id`) ON DELETE CASCADE ON UPDATE CASCADE) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)-- 创建外键约束会自动创建一个索引-- 查看外键约束对应的索引mysql&gt; show index from emp;+-------+------------+----------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+----------------+--------------+-------------+-----------| emp | 0 | PRIMARY | 1 | id | A | emp | 1 | fk_emp_dept_id | 1 | dept_id | A +-------+------------+----------------+--------------+-------------+-----------2 rows in set (0.00 sec) 举例：删除 emp 表的外键约束 123456789101112131415161718192021222324252627282930mysql&gt; alter table emp drop foreign key fk_emp_dept_id;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table emp\\G*************************** 1. row *************************** Table: empCreate Table: CREATE TABLE `emp` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, `salary` decimal(8,2) DEFAULT NULL, `dept_id` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `fk_emp_dept_id` (`dept_id`)) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec)-- 删除约束不会自动删除约束对应的索引mysql&gt; show index from emp;+-------+------------+----------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+----------------+--------------+-------------+-----------| emp | 0 | PRIMARY | 1 | id | A | emp | 1 | fk_emp_dept_id | 1 | dept_id | A +-------+------------+----------------+--------------+-------------+-----------2 rows in set (0.00 sec)-- 手工删除外键约束对应的索引mysql&gt; alter table emp drop index fk_emp_dept_id;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 三、为已有的表添加外键约束命令格式如下： 1234567alter table 表名add constraint 约束名foreign key(列名) references 父表名(列名)[ON DELETE &#123;RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT&#125;][ON UPDATE &#123;RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT&#125;]; 举例：为 emp 表添加外键约束，名称为 fk_emp_dept_id，on delete 设置为 set null，on update 设置为 cascade。 12345678mysql&gt; alter table emp add constraint fk_emp_dept_id foreign key(dept_id) references dept(id) on delete set null on update cascade;Query OK, 5 rows affected (0.03 sec)Records: 5 Duplicates: 0 Warnings: 0 四、非空约束非空约束就是限制必须为某个列提供值。空值（NULL）既不是数字0，也不是空字符串。空值用 NULL 表示，使用空值参与的运算结果仍然为空值。 12345create table 表名( 列名 类型 not null, ....); 五、默认值可通过关键字 DEFAULT 为某个字段设置默认值。如果为某个字段设置了默认值，当在数据表中插入一条新记录时，如果没有为某个字段赋值，则自动为这个字段插入默认值。 12345create table 表名 ( &lt;字段名&gt; &lt;数据类型&gt; DEFAULT &lt;默认值&gt;, ....); 六、自增列可以使用关键字 AUTO_INCREMENT 把某个字段设置为自增列，当向数据表中插入新记录时，该字段上的值会自动生成唯一的ID。一个表只能把一个字段设置为自增字段，并且字段的数据类型必须是整型。由于设置 AUTO_INCREMENT 约束后的字段会生成唯一的 ID，因此该字段也经常会同时设置成主键 插入数据：插入数据时如果手工指定自增列的数据，则自增列的起始值变为新插入的数据，下次插入数据时从当前插入的数据值递增。 修改表结构重新定义字段类型，并且去掉 auto_increment 关键词即可 alter table t2 modify id int auto_increment; 3、修改自增列的起始值1234567891011121314mysql&gt; alter table t2 auto_increment=1001;Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table t2\\G*************************** 1. row *************************** Table: t2Create Table: CREATE TABLE `t2` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` char(20) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1001 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 索引索引包含了对表中所有记录的引用指针，通俗地说，索引好比是一本书前面的目录，能加快数据库的查询速度，索引用于快速找出在某个列中有一特定值的行。当执行查询操作时，如果不使用索引，MySQL 必须从第一条记录开始读完整个表，直到找出相关的行，表越大，查询数据所花费的时间就越多。如果表中查询的列有一个索引，MySQL 能够快速到达一个位置去搜索数据文件，而不必查看所有数据。MySQL 索引的存储类型有两种：BTREE 和 HASH，也就是用树或者Hash值来存储索引字段。 一、使用索引的优点使用索引可以加快数据的查询速度。 例如：有一张 person 表，其中有 2W 条记录，记录着 2W 个人的信息。有一个 Phone 字段记录每个人的电话号码，现在想要查询出电话号码为 xxxx 的人的信息。如果没有索引，将从表中第一条记录一条条往下遍历，直到找到该条信息为止。如果根据 Phone 字段创建了索引，会将该字段通过一定的方法进行存储，当查询该字段上的信息时，能够快速找到对应的数据，而不必再去遍历 2W 条数据了。 二、使用索引的缺点虽然通过创建索引可以提供查询数据的速度，但过多的创建索引也存在一些问题。 1、创建索引和维护索引需要耗费时间，并且随着数据量的增加所耗费的时间也会增加。因此，不要创建非必要的索引。2、索引需要占用存储空间。3、当对表中的数据进行增删改操作时，索引也需要动态的维护，降低了数据维护的速度。 三、索引的使用原则通过对索引优缺点的分析，应该知道：索引并不是越多越好，而是根据自己的需要创建索引，创建索引的原则如下：（1）经常进行数据更新的表要避免创建过多的索引，而经常用于查询的字段应该创建索引。（2）数据量小的表最好不要使用索引。如果数据较少，可能查询全部数据花费的时间比遍历索引的时间还要短。（3）如果一个字段的值大量重复，则不要创建索引，比如学生表的性别字段上只有男，女两个不同值。 四、索引的分类1、根据索引的存储结构划分按索引的存储结构分为 BTREE 索引和 HASH 索引。MyISAM 和 InnoDB 存储引擎，只支持 BTREE 索引；MEMORY 和 HEAP 存储引擎，支持 BTREE 索引和 HASH 索引。 2、根据索引所包含的列数划分根据索引所包含的列数分为单列索引和组合索引。（1）单列索引：一个索引只包含一个列。（2）组合索引：根据表中多个字段的组合创建索引。只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时遵循最左前缀原则。 3、根据索引对数据的要求划分根据索引对数据的要求分为普通索引、唯一索引和主键索引。（1）普通索引：MySQL 中基本的索引类型，对数据没有什么限制，允许在定义索引的列中插入重复值和空值。（2）唯一索引：要求索引列中的值必须唯一，但是允许为空值。（3）主键索引：是一种特殊的唯一索引，除了要求索引列中的值必须唯一之外，还不允许有空值。 创建索引分两种情况：（1）在创建表时同时创建索引。（2）创建表之后有需要时再创建索引。 在创建表时同时创建索引举例：创建一张表 employee，并且为相应的字段创建索引。 123456789101112131415161718create table dept ( dept_id int primary key, dept_name char(20));create table emp ( e_id int primary key auto_increment, e_name char(20), birth datetime, salary decimal(10,2), phone char(20), addr varchar(100), dept_id int, foreign key(dept_id) references dept(dept_id), index idx_ename(e_name), unique key uq_idx_phone(phone), index idx_addr(addr(10))); 查看索引 1234567891011121314151617mysql&gt; show index from dept\\G*************************** 1. row *************************** Table: dept Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: dept_id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: 1 row in set (0.00 sec) 创建表之后有需要时再创建索引Alter 添加索引123ALTER TABLE 表名 ADD [UNIQUE|FULLTEXT|SPATIAL] INDEX|KEY [索引名](字段名[(长度)] [ASC | DESC]); 12345678910111213141516171819202122create table stu ( s_id int primary key auto_increment, s_name char(20), gender char(2), birth datetime, phone char(20), weight decimal(4,1), class char(10));mysql&gt; alter table stu add index idx_sname (s_name);Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table stu add unique index uq_idx_phone (phone);Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table stu add index idx_class_sname (class,s_name);Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0 使用 create index 命令添加索引12CREATE [UNIQUE | FULLTEXT | SPATIAL] INDEX 索引名 ON 表名(字段名); 123456789101112131415161718192021create table book ( id int primary key auto_increment, name char(20), isbn char(20), author char(20), publisher char(100), price decimal(10,2));mysql&gt; create index idx_name on book(name);Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; create unique index uq_idx_isbn on book(isbn);Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; create index idx_publisher_price on book(publisher,price);Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273mysql&gt; show index from book\\G*************************** 1. row *************************** Table: book Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: *************************** 2. row *************************** Table: book Non_unique: 0 Key_name: uq_idx_isbn Seq_in_index: 1 Column_name: isbn Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: *************************** 3. row *************************** Table: book Non_unique: 1 Key_name: idx_name Seq_in_index: 1 Column_name: name Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: *************************** 4. row *************************** Table: book Non_unique: 1 Key_name: idx_publisher_price Seq_in_index: 1 Column_name: publisher Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: *************************** 5. row *************************** Table: book Non_unique: 1 Key_name: idx_publisher_price Seq_in_index: 2 Column_name: price Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: 5 rows in set (0.00 sec) 使用 alter table 命令删除索引1ALTER TABLE 表名 DROP INDEX 索引名; 123456789101112mysql&gt; show index from stu;+-------+------------+-----------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+-----------------+--------------+-------------+-----------| stu | 0 | PRIMARY | 1 | s_id | A | stu | 0 | uq_idx_phone | 1 | phone | A | stu | 1 | idx_sname | 1 | s_name | A | stu | 1 | idx_class_sname | 1 | class | A | stu | 1 | idx_class_sname | 2 | s_name | A +-------+------------+-----------------+--------------+-------------+-----------5 rows in set (0.00 sec) 12345678mysql&gt; alter table stu drop index idx_sname;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table stu drop index idx_class_sname;Query OK, 0 rows affected (0.04 sec)Records: 0 Duplicates: 0 Warnings: 0 使用 drop index 命令删除索引123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354DROP INDEX 索引名 ON 表名;mysql&gt; show index from book;+-------+------------+---------------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+---------------------+--------------+-------------+-----------| book | 0 | PRIMARY | 1 | id | A | book | 0 | uq_idx_isbn | 1 | isbn | A | book | 1 | idx_name | 1 | name | A | book | 1 | idx_publisher_price | 1 | publisher | A | book | 1 | idx_publisher_price | 2 | price | A +-------+------------+---------------------+--------------+-------------+-----------5 rows in set (0.00 sec)mysql&gt; drop index idx_name on book;Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; drop index idx_publisher_price on book;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show index from book\\G*************************** 1. row *************************** Table: book Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: *************************** 2. row *************************** Table: book Non_unique: 0 Key_name: uq_idx_isbn Seq_in_index: 1 Column_name: isbn Collation: A Cardinality: 0 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: 2 rows in set (0.00 sec) explain分析索引使用情况1explain select 语句; 一、数据准备有一个 emp 表，表中的索引信息如下： 123456789101112mysql&gt; show index from emp;+-------+------------+--------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+--------------+--------------+-------------+-----------| emp | 0 | PRIMARY | 1 | e_id | A | emp | 0 | uq_idx_phone | 1 | phone | A | emp | 1 | dept_id | 1 | dept_id | A | emp | 1 | idx_ename | 1 | e_name | A | emp | 1 | idx_addr | 1 | addr | A +-------+------------+--------------+--------------+-------------+-----------5 rows in set (0.00 sec) 二、使用 explain 分析查询执行以下命令： 12345678910111213141516mysql&gt; explain select * from emp where e_name=&#x27;Mark&#x27;\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: refpossible_keys: idx_ename key: idx_ename key_len: 81 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec) 查询结果中的各个选项的含义如下： 123456789101112（1） id：SELECT 识别符（2） select_type：SELECT 查询的类型（3） table：数据表的名字（4） partitions：匹配的分区（5） type：访问表的方式（6） possible_keys：查询时可能使用的索引（7） key：实际使用的索引（8） key_len：索引字段的长度（9） ref：连接查询时，用于显示关联的字段（10） rows：需要扫描的行数(估算的行数)（11） filtered：按条件过滤后查询到的记录的百分比（12） Extra：执行情况的描述和说明 三、explain 各个选项的详细说明及举例1、idSELECT 识别符，是SELECT 查询的序列号。表示查询中执行 select 子句或操作表的顺序，id 相同，执行顺序从上到下，id 不同，id 值越大则执行的优先级越高。例如： 12345678910111213141516171819mysql&gt; explain select * from emp where dept_id=(select dept_id from dept where dept_name=&#x27;财务部&#x27;)\\G--如果包含子查询，id 的序号会递增，id 值越大执行优先级越高*************************** 1. row *************************** id: 1 ---外部查询 select_type: PRIMARY table: emp*************************** 2. row *************************** id: 2 ---子查询 select_type: SUBQUERY--id相同，执行顺序从上到下mysql&gt; explain select * from emp,dept where emp.dept_id=dept.dept_id\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: dept*************************** 2. row *************************** id: 1 select_type: SIMPLE table: emp 2、select_type12345678910111213141516171819202122232425262728293031323334353637383940414243表示查询中每个 select 子句的类型。有以下几种类型：(1) SIMPLE（简单的 select 查询，查询中不包含子查询或 union 查询）例如：mysql&gt; explain select * from dept\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE(2) PRIMARY（子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的 select 被标记为 PRIMARY）例如：mysql&gt; explain select * from emp where dept_id=(select dept_id from dept where dept_name=&#x27;财务部&#x27;)\\G*************************** 1. row *************************** id: 1 ---外部查询 select_type: PRIMARY table: emp*************************** 2. row *************************** id: 2 ---子查询 select_type: SUBQUERY(3) SUBQUERY（子查询中的第一个SELECT，结果不依赖于外部查询）例如：见上一个例子。(4) DEPENDENT SUBQUERY(子查询中的第一个SELECT，依赖于外部查询)例如：mysql&gt; explain select * from dept where exists (select * from emp where emp.dept_id=dept.dept_id)\\G*************************** 1. row *************************** id: 1 select_type: PRIMARY*************************** 2. row *************************** id: 2 select_type: DEPENDENT SUBQUERY (5) UNION（UNION中的第二个或后面的SELECT语句）例如：mysql&gt; explain select * from emp where dept_id=11 union select * from emp where dept_id=22\\G*************************** 1. row *************************** id: 1 select_type: PRIMARY*************************** 2. row *************************** id: 2 select_type: UNION*************************** 3. row *************************** id: NULL select_type: UNION RESULT(6) UNION RESULT（UNION的结果，union语句中第二个select开始后面所有select）例如：见上一个例子 3、table查询所用的表名称，可能是别名。例如： 12345678910111213141516mysql&gt; explain select * from emp e,dept d where e.dept_id=d.dept_id\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: d*************************** 2. row *************************** id: 1 select_type: SIMPLE table: emysql&gt; explain select * from emp\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp 4、partitions查询匹配的分区：例子略。 5、type访问表的方式，表示 MySQL 在表中找到所需行的方式。常用的类型有： ALL、index、range、 ref、eq_ref、const、system、NULL（从左到右，性能从差到好）。例如： 查询用到的表及索引情况如下： 123456789101112131415161718192021mysql&gt; show index from dept;+-------+------------+----------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+----------+--------------+-------------+-----------| dept | 0 | PRIMARY | 1 | dept_id | A +-------+------------+----------+--------------+-------------+-----------1 row in set (0.00 sec)mysql&gt; show index from emp;+-------+------------+--------------+--------------+-------------+-----------| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation +-------+------------+--------------+--------------+-------------+-----------| emp | 0 | PRIMARY | 1 | e_id | A | emp | 0 | uq_idx_phone | 1 | phone | A | emp | 1 | dept_id | 1 | dept_id | A | emp | 1 | idx_ename | 1 | e_name | A | emp | 1 | idx_addr | 1 | addr | A +-------+------------+--------------+--------------+-------------+-----------5 rows in set (0.00 sec) 常用的类型的说明及举例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172（1）ALL：Full Table Scan，如果查询没有使用索引，MySQL将遍历全表以找到匹配的行。例如：mysql&gt; explain select * from emp where birth=&#x27;1998-1-1&#x27;\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALL--该查询的条件没有创建索引，因此是全表扫描。（2）index: 全索引扫描，和 ALL 相比，index 只遍历索引树，通常比 ALL 快，因为索引文件通常比数据文件小。例如：mysql&gt; explain select e_id from emp\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: index--该查询是把 e_id 列中所有数据全部取出，并且对 e_id 列创建了索引，因此需要遍历整个索引树（3）range：检索给定范围的行，可以在 key 列中查看使用的索引，一般出现在 where 语句的条件中，如使用between、&gt;、&lt;、in等查询。例如：mysql&gt; explain select * from emp where e_id&gt;1000\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: range（4）ref: 非唯一性索引扫描，返回匹配某个单独值的所有行。本质上也是一种索引访问，返回匹配某条件的多行值。例如：mysql&gt; explain select * from emp where dept_id=11\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ref--该查询针对 dept_id 字段进行查询，查询到多条记录，并且为 dept_id 字段创建了索引。（5）eq_ref: 唯一索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见主键或唯一索引扫描。mysql&gt; explain select * from emp join dept on emp.dept_id=dept.dept_id where e_name=&#x27;Jack&#x27;\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ref*************************** 2. row *************************** id: 1 select_type: SIMPLE table: dept partitions: NULL type: eq_ref（6）const: 表示通过一次索引就找到了结果，常出现于 primary key 或 unique 索引。因为只匹配一行数据，所以查询非常快。如将主键置于 where 条件中，MySQL 就能将查询转换为一个常量。例如：mysql&gt; explain select * from emp where e_id=1002\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: const（7）NULL: 执行时不用访问表或索引。例如：mysql&gt; explain select 1 from dual\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: NULL partitions: NULL type: NULL 6、possible_keys显示可能应用在表中的索引，可能一个或多个。查询涉及到的字段若存在索引，则该索引将被列出，但不一定被查询实际使用。例如： 7、key实际使用的索引，如为NULL，则表示未使用索引。若查询中使用了覆盖索引，则该索引和查询的 select 字段重叠。 见上一个例子。 8、key_len表示索引所使用的字节数，可通过该列计算查询中使用的索引长度。在不损失精确性的情况下，长度越短越好。 例如： 9、ref显示关联的字段，如果是非连接查询，则显示 const，如果是连接查询，则会显示关联的字段。例如： 12345678910111213141516171819202122mysql&gt; explain select * from emp join dept on emp.dept_id=dept.dept_id where e_name=&#x27;Jack&#x27;\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: refpossible_keys: dept_id,idx_ename key: idx_ename key_len: 81 ref: const*************************** 2. row *************************** id: 1 select_type: SIMPLE table: dept partitions: NULL type: eq_refpossible_keys: PRIMARY key: PRIMARY key_len: 4 ref: wanggx.emp.dept_id 11、filtered表示选取的行和读取的行的百分比，100表示选取了100%，80表示读取了80%。例如： 12345678910111213141516171819202122232425262728293031323334mysql&gt; explain select * from emp where phone = &#x27;13703735488&#x27;\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: constpossible_keys: uq_idx_phone key: uq_idx_phone key_len: 81 ref: const rows: 1 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.01 sec)--根据phone创建了唯一索引，并且条件是等号（=），因此filtered为100%mysql&gt; explain select * from emp where salary = 5200\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 2 filtered: 50.00 Extra: Using where1 row in set, 1 warning (0.00 sec)--由于salary字段没有创建索引，因此执行全表扫描，filtered为50% 10、rows根据表统计信息及索引选用情况大致估算出找到所需记录所要读取的行数。当然该值越小越好。 123456789101112131415161718192021222324252627282930313233mysql&gt; explain select * from emp where salary=5000\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 3 filtered: 33.33 Extra: Using where1 row in set, 1 warning (0.00 sec)--没有使用索引，需要进行全表扫描，一共读取3行mysql&gt; explain select * from emp where dept_id=11\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: refpossible_keys: dept_id key: dept_id key_len: 5 ref: const rows: 2 filtered: 100.00 Extra: NULL1 row in set, 1 warning (0.00 sec)--使用了索引，扫描2行 12、extra显示一些重要的额外信息。一般有以下几项： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798（1）Using filesort：对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序”。例如：mysql&gt; explain select * from emp order by salary\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra: Using filesort1 row in set, 1 warning (0.01 sec)（2）Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by 与 order by。例如：mysql&gt; explain select count(*) from emp group by salary\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra: Using temporary; Using filesort1 row in set, 1 warning (0.00 sec)（3）Using index：表明相应的select操作中使用了覆盖索引（select的数据列只从索引中就能取得数据，不必读取数据行）。mysql&gt; explain select e_name,salary from emp order by e_name\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra: Using filesort1 row in set, 1 warning (0.00 sec)mysql&gt; explain select e_name from emp order by e_name\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: indexpossible_keys: NULL key: idx_ename key_len: 81 ref: NULL rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec)（4）Using join buffer：表明在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，应注意根据查询的具体情况可能需要添加索引来改进能。例如：mysql&gt; explain select * from emp join dept on emp.dept_id=dept.dept_id\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: dept partitions: NULL type: ALLpossible_keys: PRIMARY key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra: NULL*************************** 2. row *************************** id: 1 select_type: SIMPLE table: emp partitions: NULL type: ALLpossible_keys: dept_id key: NULL key_len: NULL ref: NULL rows: 2 filtered: 100.00 Extra: Using where; Using join buffer (Block Nested Loop)2 rows in set, 1 warning (0.00 sec) select 查询之 select 语法一、SELECT 语句的语法123456789101112131415SELECT DISTINCT &lt;字段或表达式列表&gt;FROM &lt;表名&gt; &lt;连接类型&gt;JOIN &lt;表名&gt; ON &lt;连接条件&gt;WHERE &lt;筛选条件&gt;GROUP BY &lt;分组字段列表&gt;HAVING &lt;分组筛选条件&gt;ORDER BY &lt;排序字段&gt;LIMIT &lt;m,n&gt; 二、SELECT 语句各部分的说明1234567891、DISTINCT：消除重复行。2、&lt;字段或表达式列表&gt;：表示所要查询字段的名称，可以使用（*）表示所有字段。3、&lt;连接类型&gt;：可以使用 inner，left，right 分别表示内连接，左连接，右连接。4、&lt;连接条件&gt;：一般使用两个表的共同字段进行构造连接条件。5、&lt;筛选条件&gt;：限定查询数据必须满足该查询条件。6、&lt;分组字段列表&gt;：按照指定的字段分组。7、&lt;分组筛选条件&gt;：对分组进行筛选。8、&lt;排序字段&gt;：对查询结果进行排序，可以进行升序（ASC）和降序（DESC）排列，默认是升序。9、LIMIT &lt;m,n&gt;：分页显示数据。 利用字段名、运算符和函数构造表达式12345678910111213141516171819202122232425mysql&gt; SELECT -&gt; stu_name, -&gt; birth, -&gt; year(now())-year(birth) -&gt; FROM -&gt; stu;+-----------+---------------------+-------------------------+| stu_name | birth | year(now())-year(birth) |+-----------+---------------------+-------------------------+| 王占峰 | 1999-12-30 00:00:00 | 21 || 刘国量 | 2000-08-14 00:00:00 | 20 || 巩莉 | 2000-06-18 00:00:00 | 20 || 宋丹风 | 1999-11-20 00:00:00 | 21 || 王艳艳 | 1999-09-30 00:00:00 | 21 || 赵牡丹 | 2001-08-10 00:00:00 | 19 || 王大强 | 2000-10-19 00:00:00 | 20 || 王宏伟 | 2001-02-15 00:00:00 | 19 || 张静静 | 2001-08-17 00:00:00 | 19 || 李刚 | 2000-12-25 00:00:00 | 20 || 刘鹏 | 2001-12-18 00:00:00 | 19 |+-----------+---------------------+-------------------------+11 rows in set (0.00 sec)--说明：本查询使用了两个函数 now()和year() 以及算数运算符（-）对字段 birth 进行运算，得到学生的年龄。 为字段和表达式指定别名12345678910111213141516171819202122232425262728293031323334353637383940414243444546mysql&gt; SELECT -&gt; stu_name, -&gt; birth, -&gt; year(now())-year(birth) as age -&gt; FROM -&gt; stu;+-----------+---------------------+------+| stu_name | birth | age |+-----------+---------------------+------+| 王占峰 | 1999-12-30 00:00:00 | 21 || 刘国量 | 2000-08-14 00:00:00 | 20 || 巩莉 | 2000-06-18 00:00:00 | 20 || 宋丹风 | 1999-11-20 00:00:00 | 21 || 王艳艳 | 1999-09-30 00:00:00 | 21 || 赵牡丹 | 2001-08-10 00:00:00 | 19 || 王大强 | 2000-10-19 00:00:00 | 20 || 王宏伟 | 2001-02-15 00:00:00 | 19 || 张静静 | 2001-08-17 00:00:00 | 19 || 李刚 | 2000-12-25 00:00:00 | 20 || 刘鹏 | 2001-12-18 00:00:00 | 19 |+-----------+---------------------+------+11 rows in set (0.00 sec)mysql&gt; SELECT -&gt; stu_name, -&gt; birth, -&gt; year(now())-year(birth) age -&gt; FROM -&gt; stu;+-----------+---------------------+------+| stu_name | birth | age |+-----------+---------------------+------+| 王占峰 | 1999-12-30 00:00:00 | 21 || 刘国量 | 2000-08-14 00:00:00 | 20 || 巩莉 | 2000-06-18 00:00:00 | 20 || 宋丹风 | 1999-11-20 00:00:00 | 21 || 王艳艳 | 1999-09-30 00:00:00 | 21 || 赵牡丹 | 2001-08-10 00:00:00 | 19 || 王大强 | 2000-10-19 00:00:00 | 20 || 王宏伟 | 2001-02-15 00:00:00 | 19 || 张静静 | 2001-08-17 00:00:00 | 19 || 李刚 | 2000-12-25 00:00:00 | 20 || 刘鹏 | 2001-12-18 00:00:00 | 19 |+-----------+---------------------+------+11 rows in set (0.01 sec) 使用 distinct 参数对查询结果去重1234567891011121314-- 查询有学生的系的系名mysql&gt; SELECT DISTINCT -&gt; dept_name -&gt; FROM -&gt; dept d inner join stu s on d.dept_id = s.dept_id;+--------------+| dept_name |+--------------+| 管理系 || 计算机系 || 数学系 |+--------------+3 rows in set (0.00 sec) select 查询之查询条件convert Order by123ORDER BY字段名或表达式 [DESC] [,...] 1234567891011121314151617181920212223mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; ORDER BY -&gt; convert(stu_name using gbk);+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201801103 | 巩莉 | 女 | 2000-06-18 00:00:00 | 170.0 | 15937320456 | D02 || 201901004 | 李刚 | 男 | 2000-12-25 00:00:00 | 178.0 | 15937320321 | D01 || 201801102 | 刘国强 | 男 | 2000-08-14 00:00:00 | 174.0 | 15937320789 | D02 || 201901005 | 刘鹏 | 男 | 2001-12-18 00:00:00 | 176.0 | NULL | D01 || 201801104 | 宋丹风 | 女 | 1999-11-20 00:00:00 | 165.0 | 15937320444 | D02 || 201901002 | 王宏伟 | 男 | 2001-02-15 00:00:00 | 180.0 | 15937320255 | D01 || 201801203 | 王鹏飞 | 男 | 2000-10-19 00:00:00 | 174.0 | 15937320555 | D03 || 201801201 | 王艳艳 | 女 | 1999-09-30 00:00:00 | 162.0 | NULL | NULL || 201801101 | 王占峰 | 男 | 1999-12-30 00:00:00 | 177.0 | 15937320987 | D02 || 201901003 | 张静静 | 女 | 2001-08-17 00:00:00 | 167.0 | 15937320123 | D01 || 201801202 | 赵牡丹 | 女 | 2001-08-10 00:00:00 | 160.0 | 15937320666 | D03 |+-----------+-----------+--------+---------------------+--------+-------------+---------+11 rows in set (0.05 sec) 123456789101112131415161718mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; WHERE -&gt; dept_id = &#x27;D01&#x27; -&gt; ORDER BY -&gt; height DESC;+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201901002 | 王宏伟 | 男 | 2001-02-15 00:00:00 | 180.0 | 15937320255 | D01 || 201901004 | 李刚 | 男 | 2000-12-25 00:00:00 | 178.0 | 15937320321 | D01 || 201901005 | 刘鹏 | 男 | 2001-12-18 00:00:00 | 176.0 | NULL | D01 || 201901003 | 张静静 | 女 | 2001-08-17 00:00:00 | 167.0 | 15937320123 | D01 |+-----------+-----------+--------+---------------------+--------+-------------+---------+4 rows in set (0.02 sec) 查询所有学生的信息，并且按学生的年龄排序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mysql&gt; SELECT -&gt; stu_id, -&gt; stu_name, -&gt; birth, -&gt; year(now())-year(birth) as age -&gt; FROM -&gt; stu -&gt; ORDER BY 4;+-----------+-----------+---------------------+------+| stu_id | stu_name | birth | age |+-----------+-----------+---------------------+------+| 201801202 | 赵牡丹 | 2001-08-10 00:00:00 | 19 || 201901002 | 王宏伟 | 2001-02-15 00:00:00 | 19 || 201901003 | 张静静 | 2001-08-17 00:00:00 | 19 || 201901005 | 刘鹏 | 2001-12-18 00:00:00 | 19 || 201801102 | 刘国强 | 2000-08-14 00:00:00 | 20 || 201801103 | 巩莉 | 2000-06-18 00:00:00 | 20 || 201801203 | 王鹏飞 | 2000-10-19 00:00:00 | 20 || 201901004 | 李刚 | 2000-12-25 00:00:00 | 20 || 201801101 | 王占峰 | 1999-12-30 00:00:00 | 21 || 201801104 | 宋丹风 | 1999-11-20 00:00:00 | 21 || 201801201 | 王艳艳 | 1999-09-30 00:00:00 | 21 |+-----------+-----------+---------------------+------+11 rows in set (0.00 sec)mysql&gt; SELECT -&gt; stu_id, -&gt; stu_name, -&gt; birth -&gt; FROM -&gt; stu -&gt; ORDER BY year(now())-year(birth);+-----------+-----------+---------------------+| stu_id | stu_name | birth |+-----------+-----------+---------------------+| 201801202 | 赵牡丹 | 2001-08-10 00:00:00 || 201901002 | 王宏伟 | 2001-02-15 00:00:00 || 201901003 | 张静静 | 2001-08-17 00:00:00 || 201901005 | 刘鹏 | 2001-12-18 00:00:00 || 201801102 | 刘国强 | 2000-08-14 00:00:00 || 201801103 | 巩莉 | 2000-06-18 00:00:00 || 201801203 | 王鹏飞 | 2000-10-19 00:00:00 || 201901004 | 李刚 | 2000-12-25 00:00:00 || 201801101 | 王占峰 | 1999-12-30 00:00:00 || 201801104 | 宋丹风 | 1999-11-20 00:00:00 || 201801201 | 王艳艳 | 1999-09-30 00:00:00 |+-----------+-----------+---------------------+11 rows in set (0.00 sec) 如果按多个字段排序，则先按第一个字段排序，然后针对第一个字段的重复记录再按第二个字段排序，以此类推。 1234567891011121314151617181920212223mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; ORDER BY -&gt; dept_id,height;+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201801201 | 王艳艳 | 女 | 1999-09-30 00:00:00 | 162.0 | NULL | NULL || 201901003 | 张静静 | 女 | 2001-08-17 00:00:00 | 167.0 | 15937320123 | D01 || 201901005 | 刘鹏 | 男 | 2001-12-18 00:00:00 | 176.0 | NULL | D01 || 201901004 | 李刚 | 男 | 2000-12-25 00:00:00 | 178.0 | 15937320321 | D01 || 201901002 | 王宏伟 | 男 | 2001-02-15 00:00:00 | 180.0 | 15937320255 | D01 || 201801104 | 宋丹风 | 女 | 1999-11-20 00:00:00 | 165.0 | 15937320444 | D02 || 201801103 | 巩莉 | 女 | 2000-06-18 00:00:00 | 170.0 | 15937320456 | D02 || 201801102 | 刘国强 | 男 | 2000-08-14 00:00:00 | 174.0 | 15937320789 | D02 || 201801101 | 王占峰 | 男 | 1999-12-30 00:00:00 | 177.0 | 15937320987 | D02 || 201801202 | 赵牡丹 | 女 | 2001-08-10 00:00:00 | 160.0 | 15937320666 | D03 || 201801203 | 王鹏飞 | 男 | 2000-10-19 00:00:00 | 174.0 | 15937320555 | D03 |+-----------+-----------+--------+---------------------+--------+-------------+---------+11 rows in set (0.00 sec) limit12345678910LIMIT [offset,] rows或者LIMIT rows OFFSET offset 说明：（1）LIMIT 接受一个或两个整型数字参数。（2）第一条记录的偏移量为 0。（3）如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。（4）如果只给定一个参数，则从第一条记录开始返回若干条记录。 1234567891011121314151617mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; ORDER BY -&gt; height DESC -&gt; LIMIT 3,2;+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201901005 | 刘鹏 | 男 | 2001-12-18 00:00:00 | 176.0 | NULL | D01 || 201801102 | 刘国强 | 男 | 2000-08-14 00:00:00 | 174.0 | 15937320789 | D02 |+-----------+-----------+--------+---------------------+--------+-------------+---------+2 rows in set (0.00 sec)--说明：从第 4 条记录开始，返回 2 条记录。 123456789101112131415161718mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; ORDER BY -&gt; height DESC -&gt; LIMIT 3;+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201901002 | 王宏伟 | 男 | 2001-02-15 00:00:00 | 180.0 | 15937320255 | D01 || 201901004 | 李刚 | 男 | 2000-12-25 00:00:00 | 178.0 | 15937320321 | D01 || 201801101 | 王占峰 | 男 | 1999-12-30 00:00:00 | 177.0 | 15937320987 | D02 |+-----------+-----------+--------+---------------------+--------+-------------+---------+3 rows in set (0.00 sec)--说明： LIMIT 3 等价于 LIMIT 0,3。取出前 3 条记录。 1234567891011121314151617mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu -&gt; ORDER BY -&gt; height DESC -&gt; LIMIT 4 OFFSET 2;+-----------+-----------+--------+---------------------+--------+-------------+---------+| stu_id | stu_name | gender | birth | height | phone | dept_id |+-----------+-----------+--------+---------------------+--------+-------------+---------+| 201801101 | 王占峰 | 男 | 1999-12-30 00:00:00 | 177.0 | 15937320987 | D02 || 201901005 | 刘鹏 | 男 | 2001-12-18 00:00:00 | 176.0 | NULL | D01 || 201801102 | 刘国强 | 男 | 2000-08-14 00:00:00 | 174.0 | 15937320789 | D02 || 201801203 | 王鹏飞 | 男 | 2000-10-19 00:00:00 | 174.0 | 15937320555 | D03 |+-----------+-----------+--------+---------------------+--------+-------------+---------+4 rows in set (0.00 sec)-- 输出从第二行(下标)开始的4行记录 连接查询一、交叉连接（CROSS JOIN）交叉连接（CROSS JOIN）又称为关系的笛卡儿积。是用左表中的每一行与右表中的每一行进行连接，所得到的结果是这两个表中各行数据的所有组合。例如： 12345678910mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu cross join dept; -- 也可以把 cross join 换成逗号（,）mysql&gt; SELECT -&gt; * -&gt; FROM -&gt; stu,dept; 二、内连接（INNER JOIN）查询结果中只包含两表的公共字段（子表的外键和父表的主键）值相等的行。语法如下： 1234567FROM 子表 INNER JOIN 父表 ON 子表.外键 = 父表.主键或FROM 子表 JOIN 父表 ON 子表.外键 = 父表.主键或FROM 子表, 父表 WHERE 子表.外键 = 父表.主键 说明：连接查询时如果需要使用的列在多个表中被使用，需要在列名前面加上表名加以限定，即：表名.列名。 多表联查1234567891011121314151617181920212223242526272829303132333435....FROM(表1 [inner] join 表2 on 表1.外键 = 表2.主键)[inner] join 表3 on 表2.外键 = 表3.主键...mysql&gt; SELECT -&gt; s.stu_id, -&gt; s.stu_name, -&gt; d.dept_id, -&gt; d.dept_name, -&gt; c.c_id, -&gt; c.c_name, -&gt; sc.score -&gt; FROM -&gt; ((stu s JOIN dept d ON s.dept_id = d.dept_id) -&gt; JOIN score sc ON s.stu_id = sc.stu_id) -&gt; JOIN course c ON sc.c_id = c.c_id;+-----------+-----------+---------+--------------+-------+-----------------------+-------+| stu_id | stu_name | dept_id | dept_name | c_id | c_name | score |+-----------+-----------+---------+--------------+-------+-----------------------+-------+| 201901002 | 王宏伟 | D01 | 管理系 | C0101 | 管理学原理 | 78 || 201901002 | 王宏伟 | D01 | 管理系 | C0102 | 政治经济学 | 92 || 201901002 | 王宏伟 | D01 | 管理系 | C0103 | 数据库系统原理 | 87 || 201901002 | 王宏伟 | D01 | 管理系 | C0104 | 企业管理概论 | 74 || 201801101 | 王占峰 | D02 | 计算机系 | C0201 | 数据结构 | 80 || 201801102 | 刘国强 | D02 | 计算机系 | C0201 | 数据结构 | 98 || 201801202 | 赵牡丹 | D03 | 数学系 | C0301 | 高等数学 | 75 || 201801202 | 赵牡丹 | D03 | 数学系 | C0302 | 运筹学 | 69 || 201801202 | 赵牡丹 | D03 | 数学系 | C0303 | 概率论 | 62 || 201801203 | 王鹏飞 | D03 | 数学系 | C0301 | 高等数学 | 89 || 201801203 | 王鹏飞 | D03 | 数学系 | C0302 | 运筹学 | 82 || 201801203 | 王鹏飞 | D03 | 数学系 | C0303 | 概率论 | 64 |+-----------+-----------+---------+--------------+-------+-----------------------+-------+12 rows in set (0.00 sec) 聚合函数聚合函数又称为统计函数，可以对查询结果进行统计和汇总，对一组值进行计算并返回一个单一的值。聚合函数经常与 GROUP BY 子句一同使用。常用的聚合函数包括SUM、COUNT、AVG、MAX 和 MIN，实现对表中数据的统计（求和、计数、平均值、最大值和最小值等）。 举例： count1、统计男生人数12345678910111213141516171819202122232425mysql&gt; SELECT -&gt; count(*) as 男生人数 -&gt; FROM -&gt; stu -&gt; WHERE -&gt; gender=&#x27;男&#x27;;+--------------+| 男生人数 |+--------------+| 6 |+--------------+1 row in set (0.17 sec)--也可以不使用 WHERE 子句，在 count()函数中使用表达式mysql&gt; SELECT -&gt; count(if(gender = &#x27;男&#x27;,1,null)) as 男生人数 -&gt; FROM -&gt; stu;+--------------+| 男生人数 |+--------------+| 6 |+--------------+1 row in set (0.00 sec) 2、统计身高超过 175 的学生人数12345678910111213mysql&gt; SELECT -&gt; count(*) as 人数 -&gt; FROM -&gt; stu -&gt; WHERE -&gt; height &gt;= 175;+--------+| 人数 |+--------+| 4 |+--------+1 row in set (0.02 sec) 3、统计有学生的系的个数1234567891011mysql&gt; SELECT -&gt; count(distinct dept_id) as 系的个数 -&gt; FROM -&gt; stu;+--------------+| 系的个数 |+--------------+| 3 |+--------------+1 row in set (0.03 sec) sumSUM( ) 函数返回一组值的和，忽略 NULL 值。如果找不到匹配行，返回 NULL 值。常与 GROUP BY 子句连用。格式如下： 1234567891011mysql&gt; SELECT -&gt; sum(salary) as 工资总额 -&gt; FROM -&gt; emp;+--------------+| 工资总额 |+--------------+| 84500.00 |+--------------+1 row in set (0.00 sec) 1234567891011mysql&gt; SELECT -&gt; sum(if(gender = &#x27;女&#x27;,1,0)) as 女生人数 -&gt; FROM -&gt; stu;+--------------+| 女生人数 |+--------------+| 5 |+--------------+1 row in set (0.00 sec) 12345678910111213mysql&gt; SELECT -&gt; sum(salary) as 工资总额 -&gt; FROM -&gt; emp -&gt; WHERE -&gt; salary &gt; 10000;+--------------+| 工资总额 |+--------------+| NULL |+--------------+1 row in set (0.01 sec) avg()1234567891011mysql&gt; SELECT -&gt; avg(salary) as 平均工资 -&gt; FROM -&gt; emp;+--------------+| 平均工资 |+--------------+| 6500.000000 |+--------------+1 row in set (0.00 sec) max,minmax( ) 函数返回一组值中的最大值。计算时忽略 NULL 值。常与 GROUP BY 子句连用。 group by使用 GROUP BY 关键字可以将查询结果按照某个字段或多个字段进行分组。分组的依据为 GROUP BY 后面的字段中取值相等的分为一组。GROUP BY 通常与聚合函数一起使用。语法格式如下： 123456789GROUP BY 字段名|表达式 [HAVING 条件表达式] [WITH ROLLUP]说明：（1）使用分组查询时，select后面的字段列表只能包含 GROUP BY 后面的字段名或表达式以及聚合函数，不能包含其他字段或表达式，否则会报错。（2）字段名|表达式：分组依据，按字段或表达式进行分组。（3）HAVING 条件表达式：对分组进行选择，符合条件表达式的结果才会显示。（4）WITH ROLLUP：在所有记录的最后加上一条记录，该记录为对所有行的统计结果。（5）可以使用 GROUP_CONCAT() 函数把某个字段中的所有值连接成一个字符串。 1234567891011121314mysql&gt; SELECT -&gt; gender -&gt; FROM -&gt; stu -&gt; GROUP BY -&gt; gender;+--------+| gender |+--------+| 女 || 男 |+--------+2 rows in set (0.00 sec) select 选择的字段列表包含了其他的字段名或表达式12345678910mysql&gt; SELECT -&gt; stu_name,gender -&gt; FROM -&gt; stu -&gt; GROUP BY -&gt; gender; ERROR 1055 (42000): Expression #1 of SELECT list is not in GROUP BY clause and contains nonaggregated column &#x27;wgx.stu.stu_name&#x27; which is not functionally dependent on columns in GROUP BY clause; this is incompatible with sql_mode=only_full_group_by 分别统计男生和女生的平均身高1234567891011121314mysql&gt; SELECT -&gt; gender,avg(height) as 平均身高 -&gt; FROM -&gt; stu -&gt; GROUP BY -&gt; gender;+--------+--------------+| gender | 平均身高 |+--------+--------------+| 女 | 164.80000 || 男 | 176.50000 |+--------+--------------+2 rows in set (0.00 sec) 统计每个系的学生人数1234567891011121314151617mysql&gt; SELECT -&gt; d.dept_id, -&gt; d.dept_name, -&gt; count(*) as 学生人数 -&gt; FROM -&gt; stu s JOIN dept d ON s.dept_id = d.dept_id -&gt; GROUP BY -&gt; d.dept_id;+---------+--------------+--------------+| dept_id | dept_name | 学生人数 |+---------+--------------+--------------+| D01 | 管理系 | 4 || D02 | 计算机系 | 4 || D03 | 数学系 | 2 |+---------+--------------+--------------+3 rows in set (0.01 sec) 统计各个年级的学生人数12345678910111213141516mysql&gt; SELECT -&gt; left(stu_id,4), -&gt; count(*) as 学生人数 -&gt; FROM -&gt; stu -&gt; GROUP BY -&gt; left(stu_id,4);+----------------+--------------+| left(stu_id,4) | 学生人数 |+----------------+--------------+| 2018 | 7 || 2019 | 4 |+----------------+--------------+2 rows in set (0.00 sec)-- LEFT(stu_Id,4)取出stu_Id字段从左侧数前四个字符 SELECT 语句的语法顺序12345678select [distinct] 字段或表达式列表from 表名 join(left join, right join) on 连接条件where 筛选条件group by 字段列表having 字段列表order by 字段列表limit &lt;m,n&gt;-- 说服我干活哦 1234567891011121314151617181920212223241. FROM子句2. WHERE子句3. GROUP BY子句 4. HAVING子句5. SELECT子句6. ORDER BY子句7. LIMIT子句/* ########################################################################################执行顺序的说明：(1) from 子句组装来自不同数据源的数据； (2) where 子句基于指定的条件对记录行进行筛选； (3) group by 子句将数据划分为多个分组； (4) 使用聚集函数进行计算； (5) 使用 having 子句筛选分组； (6) 计算所有的表达式； (7) select 字段；(8) 使用 order by 对结果集进行排序。关于字段别名的使用：(1) WHERE子句不能使用字段别名。(2) 从 GROUP BY 子句开始，后面的所有子句可以使用字段别名。######################################################################################## */ DL(Data Definition Language)数据定义语言一、操作库– 创建库create database db1;– 创建库是否存在，不存在则创建create database if not exists db1;– 查看所有数据库show databases;– 查看某个数据库的定义信息show create database db1;– 修改数据库字符信息alter database db1 character set utf8;– 删除数据库drop database db1;二、操作表–创建表create table student( id int, name varchar(32), age int , score double(4,1), birthday date, insert_time timestamp); 一、基础关键字 BETWEEN…AND （在什么之间）和 IN( 集合) – 查询年龄大于等于20 小于等于30 SELECT * FROM student WHERE age &gt;&#x3D; 20 &amp;&amp; age &lt;&#x3D;30;SELECT * FROM student WHERE age &gt;&#x3D; 20 AND age &lt;&#x3D;30;SELECT * FROM student WHERE age BETWEEN 20 AND 30; – 查询年龄22岁，18岁，25岁的信息SELECT * FROM student WHERE age &#x3D; 22 OR age &#x3D; 18 OR age &#x3D; 25SELECT * FROM student WHERE age IN (22,18,25); is null(不为null值) 与 like（模糊查询）、distinct（去除重复值） – 查询英语成绩不为nullSELECT * FROM student WHERE english IS NOT NULL; :单个任意字符 %：多个任意字符– 查询姓马的有哪些？ likeSELECT * FROM student WHERE NAME LIKE ‘马%’;– 查询姓名第二个字是化的人 SELECT * FROM student WHERE NAME LIKE “化%”; – 查询姓名是3个字的人SELECT * FROM student WHERE NAME LIKE ‘_’; – 查询姓名中包含德的人SELECT * FROM student WHERE NAME LIKE ‘%德%’; – 关键词 DISTINCT 用于返回唯一不同的值。– 语法：SELECT DISTINCT 列名称 FROM 表名称SELECT DISTINCT NAME FROM student ;二、排序查询 order by语法：order by 子句 order by 排序字段1 排序方式1 ， 排序字段2 排序方式2... 注意： 如果有多个排序条件，则当前边的条件值一样时，才会判断第二条件。 – 例子SELECT * FROM person ORDER BY math; –默认升序SELECT * FROM person ORDER BY math desc; –降序三、 聚合函数：将一列数据作为一个整体，进行纵向的计算。1.count：计算个数 2.max：计算最大值 3.min：计算最小值 4.sum：计算和 5.avg：计算平均数 四、 分组查询 grout by 语法：group by 分组字段; 注意：分组之后查询的字段：分组字段、聚合函数 – 按照性别分组。分别查询男、女同学的平均分SELECT sex , AVG(math) FROM student GROUP BY sex; – 按照性别分组。分别查询男、女同学的平均分,人数 SELECT sex , AVG(math),COUNT(id) FROM student GROUP BY sex; – 按照性别分组。分别查询男、女同学的平均分,人数 要求：分数低于70分的人，不参与分组SELECT sex , AVG(math),COUNT(id) FROM student WHERE math &gt; 70 GROUP BY sex; – 按照性别分组。分别查询男、女同学的平均分,人数 要求：分数低于70分的人，不参与分组,分组之后。人数要大于2个人SELECT sex , AVG(math),COUNT(id) FROM student WHERE math &gt; 70 GROUP BY sex HAVING COUNT(id) &gt; 2;SELECT sex , AVG(math),COUNT(id) 人数 FROM student WHERE math &gt; 70 GROUP BY sex HAVING 人数 &gt; 2;五、 分页查询 1. 语法：limit 开始的索引,每页查询的条数; 2. 公式：开始的索引 &#x3D; （当前的页码 - 1） * 每页显示的条数 3. limit 是一个MySQL”方言” – 每页显示3条记录 SELECT * FROM student LIMIT 0,3; – 第1页 SELECT * FROM student LIMIT 3,3; – 第2页 SELECT * FROM student LIMIT 6,3; – 第3页六、内连接查询： 1. 从哪些表中查询数据 2.条件是什么 3. 查询哪些字段 1.隐式内连接：使用where条件消除无用数据– 查询员工表的名称，性别。部门表的名称SELECT emp.name,emp.gender,dept.name FROM emp,dept WHERE emp.dept_id &#x3D; dept.id; SELECT t1.name, – 员工表的姓名 t1.gender,– 员工表的性别 t2.name – 部门表的名称FROM emp t1, dept t2WHERE t1.dept_id &#x3D; t2.id; 2.显式内连接– 语法：select 字段列表 from 表名1 [inner] join 表名2 on 条件– 例如：SELECT * FROM emp INNER JOIN dept ON emp.dept_id &#x3D; dept.id;SELECT * FROM emp JOIN dept ON emp.dept_id &#x3D; dept.id; 七、外连接查询1.左外连接 – 查询的是左表所有数据以及其交集部分。– 语法：select 字段列表 from 表1 left [outer] join 表2 on 条件；– 例子：– 查询所有员工信息，如果员工有部门，则查询部门名称，没有部门，则不显示部门名称SELECT t1.*,t2.name FROM emp t1 LEFT JOIN dept t2 ON t1.dept_id &#x3D; t2.id; 2.右外连接 – 查询的是右表所有数据以及其交集部分。– 语法：select 字段列表 from 表1 right [outer] join 表2 on 条件；– 例子：SELECT * FROM dept t2 RIGHT JOIN emp t1 ON t1.dept_id &#x3D; t2.id; 八、子查询：查询中嵌套查询– 查询工资最高的员工信息– 1 查询最高的工资是多少 9000SELECT MAX(salary) FROM emp; – 2 查询员工信息，并且工资等于9000的SELECT * FROM emp WHERE emp.salary &#x3D; 9000; – 一条sql就完成这个操作。这就是子查询SELECT * FROM emp WHERE emp.salary &#x3D; (SELECT MAX(salary) FROM emp); 1.子查询的结果是单行单列的 子查询可以作为条件，使用运算符去判断。 运算符： &gt; &gt;&#x3D; &lt; &lt;&#x3D; &#x3D; – 查询员工工资小于平均工资的人SELECT * FROM emp WHERE emp.salary &lt; (SELECT AVG(salary) FROM emp); 2. 子查询的结果是多行单列的： 子查询可以作为条件，使用运算符in来判断 – 查询’财务部’和’市场部’所有的员工信息SELECT id FROM dept WHERE NAME &#x3D; ‘财务部’ OR NAME &#x3D; ‘市场部’;SELECT * FROM emp WHERE dept_id &#x3D; 3 OR dept_id &#x3D; 2; – 子查询SELECT * FROM emp WHERE dept_id IN (SELECT id FROM dept WHERE NAME &#x3D; ‘财务部’ OR NAME &#x3D; ‘市场部’); 3. 子查询的结果是多行多列的： 子查询可以作为一张虚拟表参与查询 – 查询员工入职日期是2011-11-11日之后的员工信息和部门信息– 子查询SELECT * FROM dept t1 ,(SELECT * FROM emp WHERE emp.join_date &gt; ‘2011-11-11’) t2 WHERE t1.id &#x3D; t2.dept_id; – 普通内连接SELECT * FROM emp t1,dept t2 WHERE t1.dept_id &#x3D; t2.id AND t1.join_date &gt; ‘2011-11-11’ DCL(Data Control Language)数据控制语言管理用户添加用户语法：CREATE USER ‘用户名‘@’主机名’ IDENTIFIED BY ‘密码’;删除用户语法：DROP USER ‘用户名‘@’主机名’;权限管理查询权限– 查询权限SHOW GRANTS FOR ‘用户名‘@’主机名’;SHOW GRANTS FOR ‘lisi‘@’%’;授予权限– 授予权限grant 权限列表 on 数据库名.表名 to ‘用户名‘@’主机名’; – 给张三用户授予所有权限，在任意数据库任意表上GRANT ALL ON . TO ‘zhangsan‘@’localhost’;撤销权限– 撤销权限：revoke 权限列表 on 数据库名.表名 from ‘用户名‘@’主机名’;REVOKE UPDATE ON db3.account FROM ‘lisi‘@’%’; 作者：刘金玉数据库中对数据进行查询必须使用Select关键词。本期教程跟老刘一起对数据库查询的几种情况进行学习。 第一种:单表查询语法结构: select 字段名称 from 表名称或者如果我们要查询表的所以字段，就直接使用select * from 表名 这个语法即可，这里的星号*表示所有字段名称。案例：查询用户表user的所有信息Select * from user 第二种:带有条件筛选的单表查询 where这个语法只是在select查询语句的最好加上一条where语句进行数据的进一步过滤。语法结构：where 字段1 表达式符号 相应条件值举例：查询姓名为刘金玉的用户信息Select * from user where trueName&#x3D;’刘金玉’这里要注意的是“刘金玉”为一个字符串，因此要加上单引号，在数据库查询语句中，我们之前强调过，如果字段类型为字符串类型(例如char、varchar、nchar、nvarchar、text等)就要在查询和录入的时候加上相应的单引号‘’ 第三种:多表查询 join我们很多时候往往要多个表的数据举行查询，因为根据关系型数据库设计的特点，我们需要的各个字段的数据往往分布于各个不同的数据表内。虽然在数据库中我们也可以采用where语句进行关键表的字段，但是这样做有很多弊端：一是条件语句不清晰，二是查询效率降低。因此，我们引出了join这个关键词。Join有三种类型：left join 左连接 （默认的join就是left join）right join 右连接inner join 内连接语法结构：Select * from 表1 left&#x2F;right&#x2F;inner join 表2 on 表1.字段&#x3D;表2.字段举例：关联用户表和新闻表，关联字段为useridSelect * from user left join news on user.userid&#x3D; news. userid根据这样说表关联，就可以显示文章的作者信息啦！当然，我们也可以采用给表取别名的方式关联。Select * from user a left join news b on a.userid&#x3D; b. userid在使用join关键词进行关联的时候，一定要注意的是主表是哪个，这个跟现实结果记录数有关系。最好结合老刘的《零基础数据库教程》视频学习，注意观察一下不同的使用，得到的不同表关联结果。以下简单说明一下：A left join B 就是A为主表A right join B 就是B为主表A inner join B 就是取两张表的公共部分副表在这里只是根据关键词对主表进行匹配，可能会被多次匹配，这要看数据表设计时候的表关系。 第四种:过滤相同列数据 distinct如果我们得到的查询结果中有相同的数据行，我们可以通过distinct关键词进行过滤。语法结构：select distinct 字段 from 表没错，只需要在查询select关键词后加上distinct关键词即可。举例：查询用户表一共有哪些用户昵称。Select distinct nickname from user 第五种:数据排序order by我们很多时候都是要将查询后的数据进行排序的，按照我们查询的指定字段为主关键词和次要关键词进行排序，这个时候，我们需要使用order by这个重要关键词。这个关键词往往用在查询语句的最后。Order by 往往结合asc和desc这两个关键词，其中asc表示升序，desc表示降序。语法结构：Select 字段 from 表 『where语句』 order by 字段1 asc&#x2F;desc, 字段2 asc&#x2F;desc…使用案例：查询用户表所有信息，并按照用户编号进行升序排序。Select * from user order by userid asc其实在这个语句中，我们也可以省略asc关键词，因为order by 默认是以升序作为排序规则的。所以这个语句，我们也可以写成：Select * from user order by userid 第六种:数据记录显示limit我们很多使用数据库的人员中，很多人都是做软件来发的，因此limit这个关键词就非常实用了，因为我们可以结合这个关键词，为我们的软件查询出来的数据记录结果做一个分页功能。limit这个关键词往往用在查询语句的最后。语法结构：Select 字段 from 表 [where语句] [order by语句] [limit语句]举例：获取用户表的前十条记录Select * from user limit 10获取用户表的第11～20条记录Select * from user limit 10，20 第七种:聚合函数 sum count等sum函数用来求和、count函数用来统计数据记录数。但要注意，聚合函数会自动忽略类型值为null的记录。下面分别对两个函数进行讲解:1.求和函数sum。使用注意，该函数用于统计数值类字段。使用时配合select语句。函数参数传入字段名，格式sum（字段名称）。举例:统计某学生各科总成绩。select stuname，sum（score） from student_score这里的stuname是学生姓名，score是指各科目对应的成绩字段，student_score是学生各科成绩表 第八种:数据分组group by group by的意思就是根据哪些字段进行分组，这里注意，后面接的第一个字段是主要关键词，其它的依次都是次要关键词。 分组的数据一般都是where语句筛选后的最终数据，再进行依次筛选，这样的好处是可以减少分组的数据，以进一步提高数据库性能。 语法结构： group by 字段1，字段2，字段3 语句所在sql中的位置： select 字段 from 表 [where语句][group by语句] 一般来说，group by后面接几个字段，在select中就会列出几个字段。分组最终的目的是为了统计数据，比如对每一个学生的各科成绩求和。 案例：统计每个城市有多少人，我们可以从人口信息表中查询出要统计的数据结果。 select city,count(*) from persons group by city 这里的persons是人口信息表，city是城市名称，count（*）表示统计记录数 第九种:分组后筛选数据 having我们有时候常常还会将分组后的数据进行进一步过滤，那么，此时就要使用到having了。 语句所在sql中的位置： select 字段 from 表 [where语句][group by语句][having语句] 至于having之后的字段表达式的用法类似于where语句，唯一不同的就是having之后的筛选条件的字段是group by之后的字段。 案例：筛选出总成绩大于300分的学生 思路：先用group by分组求出每个学生的总成绩，然后将分组后的总成绩中筛选出成绩大于300的结果记录。 select stuname,sum(score) from student_score group by stuname having sum(score)&gt;300 因为，这里筛选的是总成绩这个字段，而这个字段的名称就是sum(score)，因此在having中使用sum(score)作为字段名称。 其中：DISTINCT|UNIQUE|ALL：指定查询结果集中的重复记录 处理方式，默认值为ALL。select_list：指定从数据库中返回的目标列或表达式。query_table_expression：指定数据来源的表、视图或实体化视图等。·join_clause：进行连接查询。where_clause：限制从数据源中返回的记录需要满足的条件。hierarchical_query_clause：层次查询。group_by_clause：分组查询。order_by_clause：查询结果排序。row_limiting_clause：返回查询结果中指定的若干记录。 SELECT语句执行的基本步骤为： 1、当执行一条SELECT语句时，系统会根据WHERE子句 的条件表达式condition，从FROM子句指定的数据源（基本 表、视图、实体化视图、连接查询等）中找出满足条件的记 录，再按SELECT子句中指定的目标列或表达式形成结果集。2、如果数据源是多表连接，则先进行多表连接操作，形 成一个大的结果集，作为外部查询的数据源。3、如果包含层次查询，则返回的记录在满足WHERE过滤 条件的同时，还要符合层次查询条件。4、如果包含分组查询，则将返回的结果集按特定的分组 列进行分组。如果需要对分组进行过滤，最后返回的结果集还 要满足分组的过滤条件。5、如果需要对查询结果进行排序，则返回的结果集需要 进行二次处理，返回有序数据。6、如果只返回排序后的部分记录，则从排序后的结果集 中返回指定的记录 索引什么是索引？索引是一种数据结构，是数据库管理系统中一个排序的数据结构，以协助快速查询数据库表中数据。索引的实现通常使用B+树或hash表。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。 索引有哪些优缺点？索引的优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化器，提高系统的性能。 索引的缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增&#x2F;删&#x2F;改的执行效率； 空间方面：索引需要占物理空间。 @$索引有哪几种类型？主键索引：数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引：数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引：基本的索引类型，没有唯一性的限制，允许为NULL值，一个表允许多个列创建普通索引。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 全文索引：是目前搜索引擎使用的一种关键技术，MyISAM存储引擎才有全文索引。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 @$索引的数据结构（B+树，Hash）在MySQL中使用较多的索引有Hash索引，B+树索引等，索引的数据结构和具体存储引擎的实现有关，而我们经常使用的InnoDB存储引擎，默认索引实现为：B+树索引。对于哈希索引来说，底层的数据结构就是哈希表，因此在绝大多数需求为单条记录等值查询的时候，可以选择哈希索引，查询性能最快；其余大部分场景，建议选择B+树索引。 1）B+树索引 MySQL通过存储引擎存取数据，基本上90%的人用的就是InnoDB了，按照实现方式分，InnoDB的索引类型目前只有两种：BTREE（B树）索引和HASH索引。B树索引是MySQL数据库中使用最频繁的索引类型，基本所有存储引擎都支持BTree索引。通常我们说的索引不出意外指的就是（B树）索引（实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引） B+树数据结构 众所周知，一颗传统的M阶B+树需要满足以下几个要求： 从根节点到叶节点的所有路径都具有相同的长度 所有数据信息都存储在叶子节点，非叶子节点仅作为叶节点的索引存在 叶子节点通过指针连在一起 根节点至少拥有两个子树 每个树节点最多拥有M个子树 每个树节点(除了根节点)拥有至少M&#x2F;2个子树 B+树是为了磁盘及其他存储辅助设备而设计的一种平衡查找树(不是二叉树)，在B+树中，所有记录的节点按大小顺序存放在同一层的叶节点中，各叶子节点用指针进行连接，而B+树索引本质上就是B+树在数据库中的实现，与纯粹的B+树数据结构还是有点区别。 B+树与B+树索引的区别如下： B+树 B+树索引 存储位置 内存 磁盘 扇出率 低 高 并发控制 可以不考虑 需考虑 分裂方向 不需要考虑 向左、向右 B+树的一些特性： 1、B+树中的B不是代表的二叉（Binary） ，而是代表平衡（Balance），因为B+树是从最早的平衡二叉树演化而来，但是B+树不是一个二叉树。 2、B+树是为磁盘或其他直接存取辅助设备设计的一种平衡查找树，在B+树中，所有的记录节点都是按照键值大小顺序存在同一层的叶子节点，由叶子节点指针进行相连。 3、B+树在数据库中的特点就是高扇出，因此在数据库中B+树的高度一般都在24层，这也就是说查找一个键值记录时，最多只需要2到4次IO，当前的机械硬盘每秒至少可以有100次IO，24次IO意味着查询时间只需要0.02~0.04秒。 4、B+树索引并不能找到一个给定键值的具体行，B+树索引能找到的只是被查找的键值所在行的页，然后数据库把页读到内存，再内存中进行查找，最后找到要查找的数据。 5、数据库中B+树索引可以分为，聚簇索引和非聚簇索引，但是不管是聚簇索引还是非聚簇索引，其内部都是B+树实现的，即高度是平衡的，叶子节点存放着所有的数据，聚簇索引和非聚簇索引不同的是，叶子节点是否存储的是一整行信息。每张表只能有一个聚簇索引。 6、B+树的每个数据页（叶子节点）是通过一个双向链表进行链接，数据页上的数据的顺序是按照主键顺序存储的。 2）哈希索引 简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在MySQL中用哈希索引时，主要就是通过Hash算法（常见的Hash算法有直接定址法、平方取中法、折叠法、除数取余法、随机数法），将数据库字段数据转换成定长的Hash值，与这条数据的行指针一并存入Hash表的对应位置；如果发生Hash碰撞（两个不同关键字的Hash值相同），则在对应Hash键下以链表形式存储。当然这只是简略模拟图。 数据库为什么使用B+树而不是B树 B+树的叶子节点存储了所有的数据，非叶子节点中存储的是比较关键字。而B树所有的节点都会存储数据。B+树的叶子节点之间存在一个指针连接，B树不存在指针连接。B+树这种设计结构能带来什么好处呢？B+树所有的数据都存储在叶子节点，那么顺着叶子节点从左往右即可完成对数据的遍历，极大了简化了排序操作。这也是mysql设计索引是采用B+树的原因，不仅仅能方便查找，而且有助于排序，在mysql的索引中叶子节点之间数双向链表可正反遍历，更加灵活； B树只适合随机检索，而B+树同时支持随机检索和顺序检索； B+树空间利用率更高，可减少I&#x2F;O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I&#x2F;O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素； B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 索引算法有哪些？索引算法有 BTree算法和Hash算法 BTree算法 BTree是最常用的mysql数据库索引算法，也是mysql默认的算法。因为它不仅可以被用在&#x3D;,&gt;,&gt;&#x3D;,&lt;,&lt;&#x3D;和between这些比较操作符上，而且还可以用于like操作符，只要它的查询条件是一个不以通配符开头的常量， 例如： 1234-- 只要它的查询条件是一个不以通配符开头的常量select * from user where name like &#x27;jack%&#x27;;-- 如果一通配符开头，或者没有使用常量，则不会使用索引，例如：select * from user where name like &#x27;%jack&#x27;; Hash算法 Hash算法只能用于对等比较，例如&#x3D;,&lt;&#x3D;&gt;（相当于&#x3D;）操作符。由于是一次定位数据，不像BTree索引需要从根节点到枝节点，最后才能访问到叶子节点这样多次IO访问，所以检索效率远高于BTree索引。 @$创建索引的原则？索引设计的原则？索引虽好，但也不是无限制的使用，最好符合以下几个原则 为常作为查询条件的字段建立索引，where子句中的列，或者连接子句中指定的列 为经常需要排序、分组操作的字段建立索引 更新频繁字段不适合创建索引 不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 对于定义为text、image和bit的数据类型的列不要建立索引 最左前缀原则，就是最左边的优先。指的是联合索引中，优先走最左边列的索引。对于多个字段的联合索引，如 index(a,b,c) 联合索引，则相当于创建了 a 单列索引，(a,b)联合索引，和(a,b,c)联合索引（但并不是建立了多个索引树）。mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &#x3D; 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 非空字段：应该指定列为NOT NULL，除非你想存储NULL。在mysql中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长 什么情况使用了索引，查询还是慢 索引全表扫描 索引过滤性不好 频繁回表的开销 MySQL使用自增主键的好处 自增主键按顺序存放，增删数据速度快，对于检索非常有利； 数字型，占用空间小，易排序； 使用整形才可以使用AUTO_INCREAMENT，不用担心主键重复问题。 @$什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据与索引放到了一块，索引结构的叶子节点存储了行数据，找到索引也就找到了数据 非聚簇索引：将数据与索引分开存储，索引结构的叶子节点存储的是行数据的地址 聚簇索引的优点 数据访问更快。聚族索引将索引和数据保存在同一个B+树中，因此从聚族索引中获取数据通常比非聚族索引中查找更快。 当你需要取出一定范围内的数据时，用聚簇索引也比用非聚簇索引好。 使用覆盖索引扫描的查询可以直接使用节点中的主键值。 聚簇索引的缺点 插入速度严重依赖于插入顺序，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列作为主键。 更新主键的代价很高，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新。 通过辅助索引访问需要两次索引查找，第一次找到主键值，第二次根据主键值找到行数据。 几个概念 对于普通索引，如 name 字段，则需要根据 name 字段的索引树（非聚簇索引）找到叶子节点对应的主键，然后再通过主键去主键索引树查询一遍，才可以得到要找的记录，这就叫回表查询。先定位主键值，再定位行记录，它的性能较扫描一遍索引树的效率更低 InnoDB的行锁是建立在索引的基础之上的，行锁锁的是索引，不是数据，所以提高并发写的能力要在查询字段添加索引 主索引和辅助索引：主索引就是主键索引，辅助索引就是根据业务需要，自己设置的普通的非主键的索引。这个在Myisam里面区别不大，但是在Innodb的时候差别很大 聚簇索引：Innodb的主索引采用的是聚簇索引，一个表只能有1个聚簇索引，因为表数据存储的物理位置是唯一的。聚簇索引的value存的就是真实的数据，不是数据的地址。主索引树里面包含了真实的数据。key是主键值，value值就是data，key值按照B+树的规则分散排布的叶子节点。 非聚簇索引：Myisam的主索引和辅助索引都采用的是非聚簇索引，索引和表数据是分离的，索引的value值存储的是行数据的地址。 Innodb的索引：主索引采用聚簇索引，叶子节点的value值，直接存储的真实的数据。辅助索引是非聚簇索引，value值指向主索引的位置。所以在Innodb中，根据辅助索引查询值需要遍历2次B+树，同时主键的长度越短越好，越短辅助索引的value值就越小。Innodb中根据主键进行范围查询，会特别快。 Myisam的索引：主索引和辅助索引都是非聚簇索引 B+树：不管是什么索引，在mysql中的数据结构都是B+树的结构，可以充分利用数据块，来减少IO查询的次数，提升查询的效率。一个数据块data里面，存储了很多个相邻key的value值，所有的非叶子节点都不存储数据，都是指针。 mysql采用B+树的优点：IO读取次数少（每次都是页读取），范围查找更快捷（相邻页之间有指针） 联合索引是什么？组合索引是什么？MySQL可以使用多个字段组合建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 @$联合索引数据结构和实现原理，使用联合索引是怎么进行查询的假设，我们对(a,b)字段建立索引，那么入下图所示 如上图所示他们是按照a来进行排序，在a相等的情况下，才按b来排序。 因此，我们可以看到a是有序的1，1，2，2，3，3。而b是一种全局无序，局部相对有序状态！什么意思呢？ 从全局来看，b的值为1，2，1，4，1，2，是无序的，因此直接执行b = 2这种查询条件没有办法利用索引。 从局部来看，当a的值确定的时候，b是有序的。例如a &#x3D; 1时，b值为1，2是有序的状态。当a&#x3D;2时候，b的值为1,4也是有序状态。因此，你执行a = 1 and b = 2是a,b字段能用到索引的。而你执行a &gt; 1 and b = 2时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段用不上索引。 综上所示，最左匹配原则，在遇到范围查询的时候，就会停止匹配。 @$什么是最左前缀原则？什么是最左匹配原则？为什么需要注意联合索引中的顺序？ 最左前缀原则，就是最左边的优先。指的是联合索引中，优先走最左边列的索引。对于多个字段的联合索引，如 index(a,b,c) 联合索引，则相当于创建了 a 单列索引，(a,b)联合索引，和(a,b,c)联合索引（但并不是建立了多个索引树）。mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &#x3D; 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 &#x3D;和in可以乱序，比如a &#x3D; 1 and b &#x3D; 2 and c &#x3D; 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。 如果建立的索引顺序是 (a,b) 那么直接采用 where b &#x3D; 5 这种查询条件是无法利用到索引的，这一条最能体现最左匹配的特性。 总结什么是索引mysql的一种数据结构,innodb用B+树实现,通俗的说为了提高查询效率的一种目录 索引优缺点B+树索引存在磁盘上 优点:极大提高速度 MySQL数据类型精讲1.MySQL中的数据类型 常见数据类型的属性，如下： 2.整数类型2.1 类型介绍整数类型一共有 5 种，包括 TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和 BIGINT 2.2可选属性整数类型的可选属性有三个： 2.2.1 MM : 表示显示宽度，M的取值范围是(0, 255)。例如，int(5)：当数据宽度小于5位的时候在数字前面需要用字符填满宽度。该项功能需要配合“ ZEROFILL ”使用，表示用“0”填满宽度，否则指定显示宽度无效。 如果设置了显示宽度，那么插入的数据宽度超过显示宽度限制，会不会截断或插入失败？ 答案：不会对插入的数据有任何影响，还是按照类型的实际宽度进行保存，即 显示宽度与类型可以存储的 值范围无关 。从MySQL 8.0.17开始，整数数据类型不推荐使用显示宽度属性。整型数据类型可以在定义表结构时指定所需要的显示宽度，如果不指定，则系统为每一种类型指定默认的宽度值。 举例： CREATE TABLE test_int1 ( x TINYINT, y SMALLINT, z MEDIUMINT, m INT, n BIGINT ); 查看表结构 （MySQL5.7中显式如下，MySQL8中不再显式范围） TINYINT有符号数和无符号数的取值范围分别为-128127和0255，由于负号占了一个数字位，因此TINYINT默认的显示宽度为4。同理，其他整数类型的默认显示宽度与其有符号数的最小值的宽度相同。 举例： CREATE TABLE test_int2( f1 INT, f2 INT(5), f3 INT(5) ZEROFILL )DESC test_int2;INSERT INTO test_int2(f1,f2,f3)VALUES(1,123,123);INSERT INTO test_int2(f1,f2)VALUES(123456,123456);INSERT INTO test_int2(f1,f2,f3)VALUES(123456,123456,123456)1234567891011 2.2.2 UNSIGNEDUNSIGNED : 无符号类型（非负），所有的整数类型都有一个可选的属性UNSIGNED（无符号属性），无符号整数类型的最小取值为0。所以，如果需要在MySQL数据库中保存非负整数值时，可以将整数类型设置为无符号类型。 int类型默认显示宽度为int(11)，无符号int类型默认显示宽度为int(10)。 2.2.3 ZEROFILLZEROFILL : 0填充,（如果某列是ZEROFILL，那么MySQL会自动为当前列添加UNSIGNED属性），如果指定了ZEROFILL只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可。原来，在 int(M) 中，M 的值跟 int(M) 所占多少存储空间并无任何关系。 int(3)、int(4)、int(8) 在磁盘上都是占用 4 bytes 的存储空间。也就是说，int(M) ，必须和 UNSIGNED ZEROFILL 一起使用才有意义。如果整数值超过M位，就按照实际位数存储。只是无须再用字符 0 进行填充。 2.3使用场景TINYINT ：一般用于枚举数据，比如系统设定取值范围很小且固定的场景。SMALLINT ：可以用于较小范围的统计数据，比如统计工厂的固定资产库存数量等。MEDIUMINT ：用于较大整数的计算，比如车站每日的客流量等INT、INTEGER ：取值范围足够大，一般情况下不用考虑超限问题，用得最多。比如商品编号。BIGINT ：只有当你处理特别巨大的整数时才会用到。比如双十一的交易量、大型门户网站点击量、证券公司衍生产品持仓等2.4如何选择在评估用哪种整数类型的时候，你需要考虑 存储空间 和 可靠性 的平衡问题：一方 面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间， 使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起 系统错误 ，影响可靠性。举个例子，商品编号采用的数据类型是 INT。原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。如果使用 SMALLINT 类型，虽然占用字节数比 INT 类型的整数少，但是却不能保证数据不会超出范围65535。相反，使用 INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。你要注意的是，在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，我建议你首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。 3.浮点类型3.1类型介绍浮点数和定点数类型的特点是可以 处理小数 ，你可以把整数看成小数的一个特例。因此，浮点数和定点数的使用场景，比整数大多了。 MySQL支持的浮点数类型，分别是 FLOAT、DOUBLE、REAL。 FLOAT 表示单精度浮点数； DOUBLE 表示双精度浮点数； REAL默认就是 DOUBLE。如果你把 SQL 模式设定为启用“ REAL_AS_FLOAT ”，那 么，MySQL 就认为REAL 是 FLOAT。如果要启用“REAL_AS_FLOAT”，可以通过以下 SQL 语句实现： SET sql_mode &#x3D; “REAL_AS_FLOAT”; 问题1： FLOAT 和 DOUBLE 这两种数据类型的区别是啥呢？ FLOAT 占用字节数少，取值范围小；DOUBLE 占用字节数多，取值范围也大。 问题2：为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？ MySQL 存储浮点数的格式为： 符号(S) 、 尾数(M) 和 阶码(E) 。因此，无论有没有符号，MySQL 的浮点数都会存储表示符号的部分。因此， 所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。 3.2 数据精度说明对于浮点类型，在MySQL中单精度值使用 4 个字节，双精度值使用 8 个字节。 MySQL允许使用 非标准语法 （其他数据库未必支持，因此如果涉及到数据迁移，则最好不要这么 用）： FLOAT(M,D) 或 DOUBLE(M,D) 。这里，M称为 精度 ，D称为 标度 。(M,D)中 M&#x3D;整数位+小数位，D&#x3D;小数位。 D&lt;&#x3D;M&lt;&#x3D;255，0&lt;&#x3D;D&lt;&#x3D;30。 例如，定义为FLOAT(5,2)的一个列可以显示为-999.99-999.99。如果超过这个范围会报错。 FLOAT和DOUBLE类型在不指定(M,D)时，默认会按照实际的精度（由实际的硬件和操作系统决定）来显示。 说明：浮点类型，也可以加 UNSIGNED ，但是不会改变数据范围，例如：FLOAT(3,2) UNSIGNED仍然只能表示0-9.99的范围。 不管是否显式设置了精度(M,D)，这里MySQL的处理方案如下： 如果存储时，整数部分超出了范围，MySQL就会报错，不允许存这样的值如果存储时，小数点部分若超出范围，就分以下情况：若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。例如在FLOAT(5,2)列内插入999.009，近似结果是999.01。若四舍五入后，整数部分超出范围，则MySQL报错，并拒绝处理。如FLOAT(5,2)列内插入999.995和-999.995都会报错。从MySQL 8.0.17开始，FLOAT(M,D) 和DOUBLE(M,D)用法在官方文档中已经明确不推荐使用，将来可能被移除。另外，关于浮点型FLOAT和DOUBLE的UNSIGNED也不推荐使用了，将来也可能被移除 举例 CREATE TABLE test_double1( f1 FLOAT, f2 FLOAT(5,2), f3 DOUBLE, f4 DOUBLE(5,2));DESC test_double1;INSERT INTO test_double1 VALUES(123.456,123.456,123.4567,123.45);#Out of range value for column ‘f2’ at row 1INSERT INTO test_double1 VALUES(123.456,1234.456,123.4567,123.45);SELECT * FROM test_double1;12345678910113.3精度误差说明浮点数类型有个缺陷，就是不精准。下面我来重点解释一下为什么 MySQL 的浮点数不够精准。比如，我们设计一个表，有f1这个字段，插入值分别为0.47,0.44,0.19，我们期待的运行结果是：0.47 + 0.44 + 0.19 &#x3D; 1.1。而使用sum之后查询： CREATE TABLE test_double2(f1 DOUBLE);INSERT INTO test_double2 VALUES(0.47),(0.44),(0.19);1234 查询结果是 1.0999999999999999。看到了吗？虽然误差很小，但确实有误差。 你也可以尝试把数据类型改成 FLOAT，然后运行求和查询，得到的是， 1.0999999940395355。显然，误差更大了。 那么，为什么会存在这样的误差呢？问题还是出在 MySQL 对浮点类型数据的存储方式上。 MySQL 用 4 个字节存储 FLOAT 类型数据，用 8 个字节来存储 DOUBLE 类型数据。无论哪个，都是采用二进制的方式来进行存储的。比如 9.625，用二进制来表达，就是 1001.101，或者表达成 1.001101×2^3。如果尾数不是 0 或 5（比如 9.624），你就无法用一个二进制数来精确表达。进而，就只好在取值允许的范围内进行四舍五入。 在编程中，如果用到浮点数，要特别注意误差问题，因为浮点数是不准确的，所以我们要避免使用“&#x3D;”来判断两个数是否相等。同时，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。那么，MySQL 有没有精准的数据类型呢？当然有，这就是定点数类型： DECIMAL 。 4.定点数类型4.1 类型介绍MySQL中的定点数类型只有 DECIMAL 一种类型。 使用 DECIMAL(M,D) 的方式表示高精度小数。其中，M被称为精度，D被称为标度。0&lt;&#x3D;M&lt;&#x3D;65， 0&lt;&#x3D;D&lt;&#x3D;30，D&lt;M。例如，定义DECIMAL（5,2）的类型，表示该列取值范围是-999.99~999.99。 DECIMAL(M,D) 的最大取值范围与 DOUBLE 类型一样，但是有效的数据范围是由M和D决定的。DECIMAL 的存储空间并不是固定的，由精度值M决定，总共占用的存储空间为M+2个字节。也就是说，在一些对精度要求不高的场景下，比起占用同样字节长度的定点数，浮点数表达的数值范围可以更大一些。 定点数在MySQL内部是以 字符串 的形式进行存储，这就决定了它一定是精准的。 当DECIMAL类型不指定精度和标度时，其默认为DECIMAL(10,0)。当数据的精度超出了定点数类型的精度范围时，则MySQL同样会进行四舍五入处理。 浮点数 vs 定点数 浮点数相对于定点数的优点是在长度一定的情况下，浮点类型取值范围大，但是不精准，适用于需要取值范围大，又可以容忍微小误差的科学计算场景（比如计算化学、分子建模、流体动力学等）定点数类型取值范围相对小，但是精准，没有误差，适合于对精度要求极高的场景 （比如涉及金额计算的场景）举例 CREATE TABLE test_decimal1( f1 DECIMAL, f2 DECIMAL(5,2));DESC test_decimal1;INSERT INTO test_decimal1(f1,f2) VALUES(123.123,123.456);#Out of range value for column ‘f2’ at row 1INSERT INTO test_decimal1(f2) VALUES(1234.34)12345678 举例 我们运行下面的语句，把test_double2表中字段“f1”的数据类型修改为 DECIMAL(5,2)： ALTER TABLE test_double2MODIFY f1 DECIMAL(5,2);12然后，我们再一次运行求和语句： 4.2开发中经验“由于 DECIMAL 数据类型的精准性，在我们的项目中，除了极少数（比如商品编号）用到整数类型外，其他的数值都用的是 DECIMAL，原因就是这个项目所处的零售行业，要求精准，一分钱也不能差。 ” ——来自某项目经理 5.位类型：BITBIT类型中存储的是二进制值，类似010110。 BIT类型，如果没有指定(M)，默认是1位。这个1位，表示只能存1位的二进制值。这里(M)是表示二进制的位数，位数最小值为1，最大值为64。 CREATE TABLE test_bit1( f1 BIT, f2 BIT(5), f3 BIT(64));INSERT INTO test_bit1(f1) VALUES(1);#Data too long for column ‘f1’ at row 1INSERT INTO test_bit1(f1) VALUES(2);INSERT INTO test_bit1(f2) VALUES(23);123456789注意：在向BIT类型的字段中插入数据时，一定要确保插入的数据在BIT类型支持的范围内。 使用SELECT命令查询位字段时，可以用 BIN() 或 HEX() 函数进行读取。 可以看到，使用b+0查询数据时，可以直接查询出存储的十进制数据的值。 6.日期与时间类型日期与时间是重要的信息，在我们的系统中，几乎所有的数据表都用得到。原因是客户需要知道数据的时间标签，从而进行数据查询、统计和处理。MySQL有多种表示日期和时间的数据类型，不同的版本可能有所差异，MySQL8.0版本支持的日期和时间类型主要有：YEAR类型、TIME类型、DATE类型、DATETIME类型和TIMESTAMP类型。 YEAR 类型通常用来表示年DATE 类型通常用来表示年、月、日TIME 类型通常用来表示时、分、秒DATETIME 类型通常用来表示年、月、日、时、分、秒TIMESTAMP 类型通常用来表示带时区的年、月、日、时、分、秒 可以看到，不同数据类型表示的时间内容不同、取值范围不同，而且占用的字节数也不一样，你要根据实际需要灵活选取。为什么时间类型 TIME 的取值范围不是 -23:59:59～23:59:59 呢？原因是 MySQL 设计的 TIME 类型，不光表示一天之内的时间，而且可以用来表示一个时间间隔，这个时间间隔可以超过 24 小时。 6.1 YEAR类型YEAR类型用来表示年份，在所有的日期时间类型中所占用的存储空间最小，只需要 1个字节 的存储空间。在MySQL中，YEAR有以下几种存储格式： 以4位字符串或数字格式表示YEAR类型，其格式为YYYY，最小值为1901，最大值为2155。以2位字符串格式表示YEAR类型，最小值为00，最大值为99。当取值为01到69时，表示2001到2069；当取值为70到99时，表示1970到1999；当取值整数的0或00添加的话，那么是0000年；当取值是日期&#x2F;字符串的’0’添加的话，是2000年从MySQL5.5.27开始，2位格式的YEAR已经不推荐使用。YEAR默认格式就是“YYYY”，没必要写成YEAR(4)， 从MySQL 8.0.19开始，不推荐使用指定显示宽度的YEAR(4)数据类型。 CREATE TABLE test_year( f1 YEAR, f2 YEAR(4));1234 6.2 DATE类型DATE类型表示日期，没有时间部分，格式为 YYYY-MM-DD ，其中，YYYY表示年份，MM表示月份，DD表示日期。需要 3个字节 的存储空间。在向DATE类型的字段插入数据时，同样需要满足一定的格式条件。 以 YYYY-MM-DD 格式或者 YYYYMMDD 格式表示的字符串日期，其最小取值为1000-01-01，最大取值为9999-12-03。YYYYMMDD格式会被转化为YYYY-MM-DD格式。以 YY-MM-DD 格式或者 YYMMDD 格式表示的字符串日期，此格式中，年份为两位数值或字符串满足YEAR类型的格式条件为：当年份取值为00到69时，会被转化为2000到2069；当年份取值为70到99时，会被转化为1970到1999。使用 CURRENT_DATE() 或者 NOW() 函数，会插入当前系统的日期。举例： 创建数据表，表中只包含一个DATE类型的字段f1。 CREATE TABLE test_date1( f1 DATE);Query OK, 0 rows affected (0.13 sec)#插入数据：INSERT INTO test_date1VALUES (‘2020-10-01’), (‘20201001’),(20201001); INSERT INTO test_date1VALUES (‘00-01-01’), (‘000101’), (‘69-10-01’), (‘691001’), (‘70-01-01’), (‘700101’), (‘99-01-01’), (‘990101’); INSERT INTO test_date1VALUES (000301), (690301), (700301), (990301); INSERT INTO test_date1VALUES (CURRENT_DATE()), (NOW()); SELECT * FROM test_date1; 1234567891011121314151617186.3 TIME类型TIME类型用来表示时间，不包含日期部分。在MySQL中，需要 3个字节 的存储空间来存储TIME类型的数据，可以使用“HH:MM:SS”格式来表示TIME类型，其中，HH表示小时，MM表示分钟，SS表示秒。 在MySQL中，向TIME类型的字段插入数据时，也可以使用几种不同的格式。 （1）可以使用带有冒号的字符串，比如’ D HH:MM:SS’ 、’ HH:MM:SS ‘、’ HH:MM ‘、’ D HH:MM ‘、’ D HH ‘或’ SS ‘格式，都能被正确地插入TIME类型的字段中。其中D表示天，其最小值为0，最大值为34。如果使用带有D格式的字符串插入TIME类型的字段时，D会被转化为小时，计算格式为D*24+HH。当使用带有冒号并且不带D的字符串表示时间时，表示当天的时间，比如12:10表示12:10:00，而不是00:12:10。 （2）可以使用不带有冒号的字符串或者数字，格式为’ HHMMSS ‘或者 HHMMSS 。如果插入一个不合法的字符串或者数字，MySQL在存储数据时，会将其自动转化为00:00:00进行存储。比如1210，MySQL会将最右边的两位解析成秒，表示00:12:10，而不是12:10:00。 （3）使用 CURRENT_TIME() 或者 NOW() ，会插入当前系统的时间。 举例： 创建数据表，表中包含一个TIME类型的字段f1。 CREATE TABLE test_time1( f1 TIME);Query OK, 0 rows affected (0.02 sec)INSERT INTO test_time1VALUES(‘2 12:30:29’), (‘12:35:29’), (‘12:40’), (‘2 12:40’),(‘1 05’), (‘45’);INSERT INTO test_time1VALUES (‘123520’), (124011),(1210);INSERT INTO test_time1 VALUES (NOW()), (CURRENT_TIME());SELECT * FROM test_time1;123456789106.4 DATETIMEDATETIME类型在所有的日期时间类型中占用的存储空间最大，总共需要 8 个字节的存储空间。在格式上为DATE类型和TIME类型的组合，可以表示为 YYYY-MM-DD HH:MM:SS ，其中YYYY表示年份，MM表示月份，DD表示日期，HH表示小时，MM表示分钟，SS表示秒。在向DATETIME类型的字段插入数据时，同样需要满足一定的格式条件。 以 YYYY-MM-DD HH:MM:SS 格式或者 YYYYMMDDHHMMSS 格式的字符串插入DATETIME类型的字段时，最小值为1000-01-01 00:00:00，最大值为9999-12-03 23:59:59以YYYYMMDDHHMMSS格式的数字插入DATETIME类型的字段时，会被转化为YYYY-MM-DD HH:MM:SS格式。以 YY-MM-DD HH:MM:SS 格式或者 YYMMDDHHMMSS 格式的字符串插入DATETIME类型的字段时，两位数的年份规则符合YEAR类型的规则，00到69表示2000到2069；70到99表示1970到1999。使用函数 CURRENT_TIMESTAMP() 和 NOW() ，可以向DATETIME类型的字段插入系统的当前日期和时间。举例： #创建数据表，表中包含一个DATETIME类型的字段dt。CREATE TABLE test_datetime1( dt DATETIME);Query OK, 0 rows affected (0.02 sec)#插入数据：INSERT INTO test_datetime1VALUES (‘2021-01-01 06:50:30’), (‘20210101065030’);INSERT INTO test_datetime1VALUES (‘99-01-01 00:00:00’), (‘990101000000’), (‘20-01-01 00:00:00’), (‘200101000000’);INSERT INTO test_datetime1VALUES (20200101000000), (200101000000), (19990101000000), (990101000000);INSERT INTO test_datetime1 VALUES (CURRENT_TIMESTAMP()), (NOW());123456789101112136.5 TIMESTAMP类型TIMESTAMP类型也可以表示日期时间，其显示格式与DATETIME类型相同，都是 YYYY-MM-DD HH:MM:SS ，需要4个字节的存储空间。但是TIMESTAMP存储的时间范围比DATETIME要小很多，只能存储“1970-01-01 00:00:01 UTC”到“2038-01-19 03:14:07 UTC”之间的时间。其中，UTC表示世界统一时间，也叫作世界标准时间。 存储数据的时候需要对当前时间所在的时区进行转换，查询数据的时候再将时间转换回当前的时区。因此，使用TIMESTAMP存储的同一个时间值，在不同的时区查询时会显示不同的时间。向TIMESTAMP类型的字段插入数据时，当插入的数据格式满足YY-MM-DD HH:MM:SS和YYMMDDHHMMSS时，两位数值的年份同样符合YEAR类型的规则条件，只不过表示的时间范围要小很多。如果向TIMESTAMP类型的字段插入的时间超出了TIMESTAMP类型的范围，则MySQL会抛出错误信息。 举例： #创建数据表，表中包含一个TIMESTAMP类型的字段ts。CREATE TABLE test_timestamp1( ts TIMESTAMP);#插入数据：INSERT INTO test_timestamp1VALUES (‘1999-01-01 03:04:50’), (‘19990101030405’), (‘99-01-01 03:04:05’), (‘990101030405’);INSERT INTO test_timestamp1VALUES (‘2020@01@01@00@00@00’), (‘20@01@01@00@00@00’);INSERT INTO test_timestamp1VALUES (CURRENT_TIMESTAMP()), (NOW());#Incorrect datetime valueINSERT INTO test_timestamp1 VALUES (‘2038-01-20 03:14:07’);12345678910111213TIMESTAMP和DATETIME的区别： TIMESTAMP存储空间比较小，表示的日期时间范围也比较小底层存储方式不同，TIMESTAMP底层存储的是毫秒值，距离1970-1-1 0:0:0 0毫秒的毫秒值。两个日期比较大小或日期计算时，TIMESTAMP更方便、更快。TIMESTAMP和时区有关。TIMESTAMP会根据用户的时区不同，显示不同的结果。而DATETIME则只能反映出插入时当地的时区，其他时区的人查看数据必然会有误差的。 6.6 开发中经验用得最多的日期时间类型，就是 DATETIME 。虽然 MySQL 也支持 YEAR（年）、 TIME（时间）、DATE（日期），以及 TIMESTAMP 类型，但是在实际项目中，尽量用 DATETIME 类型。因为这个数据类型包括了完整的日期和时间信息，取值范围也最大，使用起来比较方便。毕竟，如果日期时间信息分散在好几个字段，很不容易记，而且查询的时候，SQL 语句也会更加复杂。此外，一般存注册时间、商品发布时间等，不建议使用DATETIME存储，而是使用 时间戳 ，因为DATETIME虽然直观，但不便于计算。 7.文本字符串类型在实际的项目中，我们还经常遇到一种数据，就是字符串数据。 MySQL中，文本字符串总体上分为 CHAR 、 VARCHAR 、 TINYTEXT 、 TEXT 、MEDIUMTEXT 、 LONGTEXT 、 ENUM 、 SET 等类型 7.1CHAR与VARCHAR类型CHAR和VARCHAR类型都可以存储比较短的字符串。 CHAR类型： CHAR(M) 类型一般需要预先定义字符串长度。如果不指定(M)，则表示长度默认是1个字符。如果保存时，数据的实际长度比CHAR类型声明的长度小，则会在 右侧填充 空格以达到指定的长度。当MySQL检索CHAR类型的数据时，CHAR类型的字段会去除尾部的空格。定义CHAR类型字段时，声明的字段长度即为CHAR类型字段所占的存储空间的字节数。CREATE TABLE test_char1( c1 CHAR, c2 CHAR(5));DESC test_char1; INSERT INTO test_char1VALUES(‘a’,’Tom’); SELECT c1,CONCAT(c2,’***’)FROM test_char1; INSERT INTO test_char1(c2)VALUES(‘a ‘); SELECT CHAR_LENGTH(c2)FROM test_char1;1234567891011121314151617VARCHAR类型： VARCHAR(M) 定义时， 必须指定 长度M，否则报错。MySQL4.0版本以下，varchar(20)：指的是20字节，如果存放UTF8汉字时，只能存6个（每个汉字3字节） ；MySQL5.0版本以上，varchar(20)：指的是20字符。检索VARCHAR类型的字段数据时，会保留数据尾部的空格。VARCHAR类型的字段所占用的存储空间为字符串实际长度加1个字节。CREATE TABLE test_varchar1( NAME VARCHAR #错误);#Column length too big for column ‘NAME’ (max &#x3D; 21845);CREATE TABLE test_varchar2( NAME VARCHAR(65535) #错误)CREATE TABLE test_varchar3( NAME VARCHAR(5));INSERT INTO test_varchar3VALUES(‘尚硅谷’),(‘尚硅谷教育’);#Data too long for column ‘NAME’ at row 1INSERT INTO test_varchar3 VALUES(‘尚硅谷IT教育’)1234567891011121314哪些情况使用 CHAR 或 VARCHAR 更好 情况1：存储很短的信息。比如门牌号码101，201……这样很短的信息应该用char，因为varchar还要占个byte用于存储信息长度，本来打算节约存储的，结果得不偿失。 情况2：固定长度的。比如使用uuid作为主键，那用char应该更合适。因为他固定长度，varchar动态根据长度的特性就消失了，而且还要占个长度信息。 情况3：十分频繁改变的column。因为varchar每次存储都要有额外的计算，得到长度等工作，如果一个非常频繁改变的，那就要有很多的精力用于计算，而这些对于char来说是不需要的。 情况4：具体存储引擎中的情况： MyISAM 数据存储引擎和数据列：MyISAM数据表，最好使用固定长度(CHAR)的数据列代替可变长度(VARCHAR)的数据列。这样使得整个表静态化，从而使 数据检索更快 ，用空间换时间。MEMORY 存储引擎和数据列：MEMORY数据表目前都使用固定长度的数据行存储，因此无论使用CHAR或VARCHAR列都没有关系，两者都是作为CHAR类型处理的。InnoDB 存储引擎，建议使用VARCHAR类型。因为对于InnoDB数据表，内部的行存储格式并没有区分固定长度和可变长度列（所有数据行都使用指向数据列值的头指针），而且主要影响性能的因素 是数据行使用的存储总量，由于char平均占用的空间多于varchar，所以除了简短并且固定长度的，其他考虑varchar。这样节省空间，对磁盘I&#x2F;O和数据存储总量比较好。7.2 TEXT类型在MySQL中，TEXT用来保存文本类型的字符串，总共包含4种类型，分别为TINYTEXT、TEXT、 MEDIUMTEXT 和 LONGTEXT 类型。 在向TEXT类型的字段保存和查询数据时，系统自动按照实际长度存储，不需要预先定义长度。这一点和VARCHAR类型相同。 每种TEXT类型保存的数据长度和所占用的存储空间不同，如下： 由于实际存储的长度不确定， MySQL 不允许 TEXT 类型的字段做主键。遇到这种情况，你只能采用CHAR(M)，或者 VARCHAR(M)。 举例： #创建数据表：CREATE TABLE test_text( tx TEXT );INSERT INTO test_text VALUES(‘atguigu ‘);SELECT CHAR_LENGTH(tx) FROM test_text; #101234说明在保存和查询数据时，并没有删除TEXT类型的数据尾部的空格。 开发中经验： TEXT文本类型，可以存比较大的文本段，搜索速度稍慢，因此如果不是特别大的内容，建议使用CHAR， VARCHAR来代替。还有TEXT类型不用加默认值，加了也没用。而且text和blob类型的数据删除后容易导致“空洞”，使得文件碎片比较多，所以频繁使用的表不建议包含TEXT类型字段，建议单独分出去，单独用一个表。 8.ENUM类型ENUM类型也叫作枚举类型，ENUM类型的取值范围需要在定义字段时进行指定。设置字段值时，ENUM类型只允许从成员中选取单个值，不能一次选取多个值。其所需要的存储空间由定义ENUM类型时指定的成员个数决定 当ENUM类型包含1～255个成员时，需要1个字节的存储空间；当ENUM类型包含256～65535个成员时，需要2个字节的存储空间。ENUM类型的成员个数的上限为65535个。举例： #创建表如下：CREATE TABLE test_enum( season ENUM(‘春’,’夏’,’秋’,’冬’,’unknow’))#添加数据：INSERT INTO test_enum VALUES(‘春’),(‘秋’); 忽略大小写INSERT INTO test_enum VALUES(‘UNKNOW’); 允许按照角标的方式获取指定索引位置的枚举值INSERT INTO test_enum VALUES(‘1’),(3); Data truncated for column ‘season’ at row 1INSERT INTO test_enum VALUES(‘ab’); 当ENUM类型的字段没有声明为NOT NULL时，插入NULL也是有效的INSERT INTO test_enum VALUES(NULL);12345678910111213149.SET类型SET表示一个字符串对象，可以包含0个或多个成员，但成员个数的上限为 64 。设置字段值时，可以取取值范围内的 0 个或多个值。当SET类型包含的成员个数不同时，其所占用的存储空间也是不同的，具体如下： SET类型在存储数据时成员个数越多，其占用的存储空间越大。注意：SET类型在选取成员时，可以一次选择多个成员，这一点与ENUM类型不同。 #举例CREATE TABLE test_set( s SET (‘A’, ‘B’, ‘C’) );#向表中插入数据：INSERT INTO test_set (s) VALUES (‘A’), (‘A,B’);#插入重复的SET类型成员时，MySQL会自动删除重复的成员INSERT INTO test_set (s) VALUES (‘A,B,C,A’);#向SET类型的字段插入SET成员中不存在的值时，MySQL会抛出错误。INSERT INTO test_set (s) VALUES (‘A,B,C,D’);SELECT * FROM test_set;#举例2CREATE TABLE temp_mul( gender ENUM(‘男’,’女’), hobby SET(‘吃饭’,’睡觉’,’打豆豆’,’写代码’));INSERT INTO temp_mul VALUES(‘男’,’睡觉,打豆豆’); #成功 Data truncated for column ‘gender’ at row 1INSERT INTO temp_mul VALUES(‘男,女’,’睡觉,写代码’); #失败 Data truncated for column ‘gender’ at row 1INSERT INTO temp_mul VALUES(‘妖’,’睡觉,写代码’);#失败INSERT INTO temp_mul VALUES(‘男’,’睡觉,写代码,吃饭’); #成功123456789101112131415161718192010.二进制字符串类型MySQL中的二进制字符串类型主要存储一些二进制数据，比如可以存储图片、音频和视频等二进制数据。MySQL中支持的二进制字符串类型主要包括BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB类型。 BINARY与VARBINARY类型 BINARY和VARBINARY类似于CHAR和VARCHAR，只是它们存储的是二进制字符串。 BINARY (M)为固定长度的二进制字符串，M表示最多能存储的字节数，取值范围是0~255个字符。如果未指定(M)，表示只能存储 1个字节 。例如BINARY (8)，表示最多能存储8个字节，如果字段值不足(M)个字节，将在右边填充’\\0’以补齐指定长度。 VARBINARY (M)为可变长度的二进制字符串，M表示最多能存储的字节数，总字节数不能超过行的字节长度限制65535，另外还要考虑额外字节开销，VARBINARY类型的数据除了存储数据本身外，还需要1或2个字节来存储数据的字节数。VARBINARY类型 必须指定(M) ，否则报错。 举例： #创建表CREATE TABLE test_binary1( f1 BINARY, f2 BINARY(3), # f3 VARBINARY, f4 VARBINARY(10))INSERT INTO test_binary1(f1,f2)VALUES(‘a’,’a’);INSERT INTO test_binary1(f1,f2)VALUES(‘尚’,’尚’);#失败INSERT INTO test_binary1(f2,f4)VALUES(‘ab’,’ab’)123456789101112 BLOB类型 BLOB是一个 二进制大对象 ，可以容纳可变数量的数据。 MySQL中的BLOB类型包括TINYBLOB、BLOB、MEDIUMBLOB和LONGBLOB 4种类型，它们可容纳值的最大长度不同。可以存储一个二进制的大对象，比如 图片 、 音频 和 视频 等。需要注意的是，在实际工作中，往往不会在MySQL数据库中使用BLOB类型存储大对象数据，通常会将图片、音频和视频文件存储到 服务器的磁盘上 ，并将图片、音频和视频的访问路径存储到MySQL中。 举例： CREATE TABLE test_blob1( id INT, img MEDIUMBLOB)1234TEXT和BLOB的使用注意事项 在使用text和blob字段类型时要注意以下几点，以便更好的发挥数据库的性能。 ① BLOB和TEXT值也会引起自己的一些问题，特别是执行了大量的删除或更新操作的时候。删除这种值会在数据表中留下很大的” 空洞 “，以后填入这些”空洞”的记录可能长度不同。为了提高性能，建议定期使用 OPTIMIZE TABLE 功能对这类表进行 碎片整理 。 ② 如果需要对大文本字段进行模糊查询，MySQL 提供了 前缀索引 。但是仍然要在不必要的时候避免检索大型的BLOB或TEXT值。例如，SELECT * 查询就不是很好的想法，除非你能够确定作为约束条件的WHERE子句只会找到所需要的数据行。否则，你可能毫无目的地在网络上传输大量的值。 ③ 把BLOB或TEXT列 分离到单独的表 中。在某些环境中，如果把这些数据列移动到第二张数据表中，可以让你把原数据表中的数据列转换为固定长度的数据行格式，那么它就是有意义的。这会 减少主表中的 碎片 ，使你得到固定长度数据行的性能优势。它还使你在主数据表上运行 SELECT * 查询的时候不会通过网络传输大量的BLOB或TEXT值。 11.JSON类型JSON（JavaScript Object Notation）是一种轻量级的 数据交换格式 。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。它易于人阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。JSON 可以将 JavaScript 对象中表示的一组数据转换为字符串，然后就可以在网络或者程序之间轻 松地传递这个字符串，并在需要的时候将它还原为各编程语言所支持的数据格式。 在MySQL 5.7中，就已经支持JSON数据类型。在MySQL 8.x版本中，JSON类型提供了可以进行自动验证的JSON文档和优化的存储结构，使得在MySQL中存储和读取JSON类型的数据更加方便和高效。 创建数据表，表中包含一个JSON类型的字段 js 。 CREATE TABLE test_json( js json);#向表中插入JSON数据。INSERT INTO test_json (js)VALUES (‘{“name”:”songhk”, “age”:18, “address”:{“province”:”beijing”, “city”:”beijing”}}’);123456查询t19表中的数据。 当需要检索JSON类型的字段中数据的某个具体值时，可以使用“-&gt;”和“-&gt;&gt;”符号 通过“-&gt;”和“-&gt;&gt;”符号，从JSON字段中正确查询出了指定的JSON数据的值。 12.空间类型MySQL 空间类型扩展支持地理特征的生成、存储和分析。这里的地理特征表示世界上具有位置的任何东西，可以是一个实体，例如一座山；可以是空间，例如一座办公楼；也可以是一个可定义的位置，例如一个十字路口等等。MySQL中使用 Geometry（几何） 来表示所有地理特征。Geometry指一个点或点的集合，代表世界上任何具有位置的事物。 MySQL的空间数据类型（Spatial Data Type）对应于OpenGIS类，包括单值类型：GEOMETRY、POINT、 LINESTRING、POLYGON以及集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、 GEOMETRYCOLLECTION 。 Geometry是所有空间集合类型的基类，其他类型如POINT、LINESTRING、POLYGON都是Geometry的子类。Point，顾名思义就是点，有一个坐标值。例如POINT(121.213342 31.234532)POINT(30 10)，坐标值支持DECIMAL类型，经度（longitude）在前，维度（latitude）在后，用空格分隔。LineString，线，由一系列点连接而成。如果线从头至尾没有交叉，那就是简单的（simple）；如果起点和终点重叠，那就是封闭的（closed）。例如LINESTRING(30 10,10 30,40 40)，点与点之间用逗号分隔，一个点中的经纬度用空格分隔，与POINT格式一致。Polygon，多边形。可以是一个实心平面形，即没有内部边界，也可以有空洞，类似纽扣。最简单的就是只有一个外边界的情况，例如POLYGON((0 0,10 0,10 10, 0 10))。下面展示几种常见的几何图形元素： MultiPoint、MultiLineString、MultiPolygon、GeometryCollection 这4种类型都是集合类，是多个Point、LineString或Polygon组合而成。下面展示的是多个同类或异类几何图形元素的组合： 13.小结及选择建议在定义数据类型时，如果确定是 整数 ，就用 INT ； 如果是 小数 ，一定用定点数类型DECIMAL(M,D) ； 如果是日期与时间，就用 DATETIME 。 这样做的好处是，首先确保你的系统不会因为数据类型定义出错。不过，凡事都是有两面的，可靠性好，并不意味着高效。比如，TEXT 虽然使用方便，但是效率不如 CHAR(M) 和VARCHAR(M)。 关于字符串的选择，建议参考如下阿里巴巴的《Java开发手册》规范： 阿里巴巴《Java开发手册》之MySQL数据库： 任何字段如果为非负数，必须是 UNSIGNED 【 强制 】小数类型为 DECIMAL，禁止使用 FLOAT 和 DOUBLE。 说明：在存储的时候，FLOAT 和 DOUBLE 都存在精度损失的问题，很可能在比较值的候，得到不正确的结果。如果存储的数据范围超过 DECIMAL 的范围，建议将数据拆成整数和小数并分开存储。【 强制 】如果存储的字符串长度几乎相等，使用 CHAR 定长字符串类型。 【 强制 】VARCHAR 是可变长字符串，不预先分配存储空间，长度不要超过 5000。如果存储长度大于此值，定义字段类型为 TEXT，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 数据库优化数据库结构优化一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意： 冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 @$大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？水平分表： 保持数据表结构不变，通过某种策略进行存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。 水平拆分可以支持非常大的数据量。需要注意的一点是：水平分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。 适用场景：支持非常大的数据量存储 水平拆分优点：支持非常大的数据量存储 水平拆分缺点：给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作；分片事务难以解决 ，跨库join性能较差，逻辑复杂。 《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I&#x2F;O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据库中间加了一个代理层。分片逻辑统一维护在中间件服务中。 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 MySQL的主从复制原理以及流程主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行，从而使得从数据库的数据与主数据库保持一致。 主从复制的作用 高可用和故障切换：主数据库出现问题，可以切换到从数据库。 负载均衡：可以进行数据库层面的读写分离。 数据备份：可以在从数据库上进行日常备份。 复制过程 Binary log：主数据库的二进制日志 Relay log：从数据库的中继日志 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I&#x2F;O Thread，该线程在master打开一个普通连接，将这些事件写入到中继日志中。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 读写分离有哪些解决方案？未完成读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。主从复制要求slave不能写只能读 方案一 利用中间件来做代理，使用mysql-proxy代理，负责对数据库的请求识别出读还是写，并分发到不同的数据库中。 优点：直接实现读写分离和负载均衡，不用修改代码，数据库和应用程序弱耦合，master和slave用一样的帐号，mysql官方不建议实际生产中使用 缺点：降低性能， 不支持事务，代理存在性能瓶颈和可靠性风险增加。 方案二 使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。 如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert&#x2F;update&#x2F;delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。 不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。 方案三 使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务 缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需要进行特殊处理 SQL查询的基本原理六、SQL查询的基本原理 单表查询：根据WHERE条件过滤表中的记录，形成中间表（这个中间表对用户是不可见的）；然后根据SELECT的选择列选择相应的列进行返回最终结果。 两表连接查询：对两表求积（笛卡尔积）并用ON条件和连接连接类型进行过滤形成中间表；然后根据WHERE条件过滤中间表的记录，并根据SELECT指定的列返回查询结果。 多表连接查询：先对第一个和第二个表按照两表连接做查询，然后用查询结果和第三个表做连接查询，以此类推，直到所有的表都连接上为止，最终形成一个中间的结果表，然后根据WHERE条件过滤中间表的记录，并根据SELECT指定的列返回查询结果。理解SQL查询的过程是进行SQL优化的理论依据。 @$SQL的生命周期？一条SQL查询语句是如何执行的？MySQL总体架构—&gt;SQL执行流程—&gt;语句执行顺序 连接：应用服务器与数据库服务器建立一个连接 获得请求SQL：数据库进程拿到请求sql 查询缓存：如果查询命中缓存则直接返回结果 语法解析和预处理： 首先通过mysql关键字将语句解析，会生成一个内部解析树，mysql解析器将对其解析，查看是否是有错误的关键字，关键字顺序是否正确；预处理器则是根据mysql的规则进行进一步的检查，检查mysql语句是否合法，如库表是否存在，字段是否存在，字段之间是否模棱两可等等，预处理器也会验证权限。 查询优化器：sql语句在优化器中转换成执行计划，一条sql语句可以有多种方式查询，最后返回的结果肯定是相同，但是不同的查询方式效果不同，优化器的作用就是：选择一种合适的执行计划。mysql是基于成本的优化器，他将预测执行此计划的成本，并选择成本最小的那条 执行计划，执行SQL：在解析和优化后，MySQL将生成查询对应的执行计划，由执行计划调用存储引擎的API来执行查询 将结果返回给客户端 关掉连接，释放资源 一条更新语句的执行流程又是怎样的呢？持久化机制12-- 如果要将 ID=2 这一行的值加 1mysql&gt; update T set c=c+1 where ID=2; 你执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。 在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 补充: redo是存储在磁盘上的 是顺序写 提前分配好一块区域追加写速度很快(Kafka,mq等就是顺序写) 直接往磁盘(账本)上写是随机写,修改这条数据和修改那条数据,所以很慢 ​ 重要的日志模块：binlog 前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 事务与锁事务什么是数据库事务？事务是逻辑上的一组操作，要么都执行，要么都不执行 @$事务的四大特性(ACID)介绍一下?关系性数据库需要遵循ACID规则，具体内容如下： 特性 说明 原子性 Atomic 事务是最小的执行单位，不允许分割。事务包含的所有操作要么全部成功，要么全部失败回滚。 一致性 Consistency 事务执行之前和执行之后都必须处于一致性状态。举例：拿转账来说，假设用户A和用户B两者的钱加起来一共是5000，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是5000，这就是事务的一致性。 隔离性 Isolation 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间是相互隔离的。数据库规定了多种事务隔离级别，不同的隔离级别对应不同的干扰程度。隔离级别越高，数据一致性越好，但并发性越差。 持久性 Durability 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下，也不会丢失提交事务的操作。 @$什么是脏读？不可重复读？幻读？ 脏读(Dirty Read)：一个事务读取到另外一个事务未提交的数据。举例：一个事务1读取了被另一个事务2修改但还未提交的数据。由于某种异常事务2回滚，则事务1读取的是无效数据。 不可重复读(Non-repeatable read)：一个事务读取同一条记录2次，得到的结果不一致。这可能是两次查询过程中间，另一个事务更新了这条记录。 幻读(Phantom Read)：幻读发生在两个完全相同的查询，得到的结果不一致。这可能是两次查询过程中间，另一个事务增加或者减少了行记录。 不可重复度和幻读区别 不可重复读的重点是修改，幻读的重点在于新增或者删除。 @$什么是事务的隔离级别？MySQL的默认的隔离级别是什么？为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，后三个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × SQL 标准定义了四个隔离级别 READ-UNCOMMITTED(读未提交)：最低的隔离级别，一个事务可以读取另一个事务更新但未提交的数据。可能会导致脏读、不可重复读或幻读。 READ-COMMITTED(读已提交)：一个事务提交后才能被其他事务读取到，可以阻止脏读，但是不可重复读或幻读仍有可能发生。 REPEATABLE-READ(可重复读)：对同一记录的多次读取结果都是一致的，除非数据是被本身事务所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)：最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 这里需要注意的是：MySQL 默认采用的 REPEATABLE_READ隔离级别，Oracle 默认采用的 READ_COMMITTED隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是**READ-COMMITTED(读取已提交)**，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重复读）并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。 @$MySQL数据库可重复读隔离级别是怎么实现的，MVCC并发版本控制原理实际上就是一个copyOrwrite 写的是副本数据,写完在提交,读的是旧数据,所以写的可能是脏写,所以加CAS 读操作会有一个版本号,写的时候必须要和持有的版本号一致,负责重查 事务一查询 version1 500 事务二 修改为 1000 version2 事务一 在查询的数据上+300 提交时发现 version不是1 则重新查询 ​ 查询version 2 1000 ​ +300 version 是2 则提交 变为1300 MySQL可重复读是通过MVCC实现的 MVCC(Multi Version Concurrency Control的简称)，代表多版本并发控制。与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control)。MVCC最大的优势：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能 MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（system version number)。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。 InnoDB MVCC 实现原理 InnoDB 中 MVCC 的实现方式为：每一行记录都有两个隐藏列：DATA_TRX_ID、DATA_ROLL_PTR（如果没有主键，则还会多一个隐藏的主键列）。 DATA_TRX_ID 记录最近更新这条行记录的事务 ID，大小为 6 个字节 DATA_ROLL_PTR 表示指向该行回滚段（rollback segment）的指针，大小为 7 个字节，InnoDB 便是通过这个指针找到之前版本的数据。该行记录上所有旧版本，在 undo 中都通过链表的形式组织。 DB_ROW_ID 行标识（隐藏单调自增 ID），大小为 6 字节，如果表没有主键，InnoDB 会自动生成一个隐藏主键，因此会出现这个列。另外，每条记录的头信息（record header）里都有一个专门的 bit（deleted_flag）来表示当前记录是否已经被删除。 增删改查 假设初始版本号为1： INSERT 1insert into user (id,name) values (1,&#x27;Tom&#x27;); id name create_version delete_version 1 Tom 1 下面模拟一下文章开头的场景： SELECT (事务A) 1select * from user where id = 1; 此时读到的版本号为1 UPDATE(事务B) 1update user set name = &#x27;Jerry&#x27; where id = 1; 在更新操作的时候，该事务的版本号在原来的基础上加1，所以版本号为2。先将要更新的这条数据标记为已删除，并且删除的版本号是当前事务的版本号，然后插入一行新的记录 id name create_version delete_version 1 Tom 1 2 1 Jerry 2 SELECT (事务A) 此时事务A再重新读数据： 1select * from user where id = 1; 由于事务A一直没提交，所以此时读到的版本号还是为1，所以读到的还是Tom这条数据，也就是可重复读 DELETE 1delete from user where id = 1; 在删除操作的时候，该事务的版本号在原来的基础上加1，所以版本号为3删除时，将当前版本号作为删除版本号 id name create_version delete_version 1 Jerry 2 3 长事务的优化 锁对MySQL的锁了解吗当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。 隔离级别与锁的关系在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，事务执行完毕后才释放共享锁。 在SERIALIZABLE级别下，是限制性最强的隔离级别，该级别下锁定整个范围的键，并一直持有锁，直到事务完成。 按照锁的粒度分数据库锁有哪些？在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认采用行级锁 行级锁，表级锁和页级锁对比 行级锁 行级锁是MySQL中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：锁定粒度最小，对当前操作的行记录加锁，发生锁冲突的概率最低，并发度也最高；加锁开销大，加锁慢；会出现死锁； 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：锁定粒度大，对当前操作的整张表加锁，发出锁冲突的概率最高，并发度最低；加锁开销小，加锁快；不会出现死锁； 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 共享锁和排他锁的区别共享锁 共享锁 share lock 又称读锁 read lock，简称S锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。 如果事务T对数据A加上共享锁后，则其他事务只能对A再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据 读取为什么要加读锁呢：防止数据在被读取的时候被别的线程加上写锁 使用方式：在需要执行的语句后面加上 for update就可以了 排他锁 排他锁 exclusive lock 又称写锁 writer lock，简称X锁。排他锁是悲观锁的一种实现。 若事务T对数据A加上排他锁，则只允许事务T读取和修改数据A，其他任何事务都不能再对A加任何类型的锁，直到事务T释放X锁。排他锁会阻塞所有的排他锁和共享锁 数据库的乐观锁和悲观锁是什么？怎么实现的？数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取同一数据时不破坏事务的隔离性和一致性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，每次去查询数据的时候都认为别人会修改，每次查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，每次去查询数据的时候都认为别人不会修改，所以不会上锁，在修改数据的时候才把事务锁起来。实现方式：乐观锁一般会使用版本号机制或CAS算法实现 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 SQL优化@$如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到？或者说怎么才可以知道这条语句运行很慢的原因？对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。而执行计划，就是显示数据库引擎对于SQL语句执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。 执行计划包含的信息 id 由一组数字组成。表示一个查询中各个子查询的执行顺序; id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为null时，表示一个合并结果集的操作的执行id为null，常出现在包含union等查询语句中。 select_type 每个子查询的查询类型，一些常见的查询类型。 id select_type description 1 SIMPLE 不包含任何子查询或union查询 2 PRIMARY 包含子查询时最外层查询就显示为 PRIMARY 3 SUBQUERY 在select或where子句中出现的子查询 4 DERIVED from字句中出现的子查询 5 UNION union连接的两个select查询，第一个查询是dervied派生表，除了第一个表外，第二个以后的表select_type都是union。 6 UNION RESULT 包含union的结果集，在union和union all语句中，因为它不需要参与查询，所以id字段为null 7 dependent subquery 与dependent union类似，表示这个subquery的查询要受到外部表查询的影响。 8 dependent union 与union一样，出现在union 或union all语句中，但是这个查询要受到外部查询的影响 table 显示的查询表名，如果查询使用了别名，那么这里显示的是别名。 type访问类型(非常重要，可以看到有没有走索引) 依次从好到差：system，const，eq_ref，ref，fulltext，ref_or_null，unique_subquery，index_subquery，range，index_merge，index，ALL。 除了all之外，其他的type都可以使用到索引，除了index_merge之外，其他的type只可以用到一个索引。 类型 描述 system 表中只有一行数据或者是空表，且只能用于myisam和memory表。如果是Innodb引擎表，type列在这个情况通常都是all或者index。 const 使用唯一索引或者主键，返回记录是1行记录的等值where条件时，通常type是const。其他数据库也叫做唯一索引扫描。 eq_ref 出现在要连接多个表的查询计划中，驱动表只返回一行数据，且这行数据是第二个表的主键或者唯一索引，且必须为not null，唯一索引和主键是多列时，只有所有的列都用作比较时才会出现eq_ref。 ref 像eq_ref那样要求连接顺序，也没有主键和唯一索引的要求，只要使用相等条件检索时就可能出现，常见于普通索引的等值查找。或者多列主键、唯一索引中，使用第一个列之外的列作为等值查找也会出现，总之，返回数据不唯一的等值查找就可能出现。 fulltext 全文索引检索，要注意，全文索引的优先级很高，若全文索引和普通索引同时存在时，mysql不管代价，优先选择使用全文索引。 ref_or_null 与ref方法类似，只是增加了null值的比较。实际用的不多。 unique_subquery 用于where中的in形式子查询，子查询返回不重复值唯一值。 index_subquery 用于in形式子查询使用到了辅助索引或者in常数列表，子查询可能返回重复值，可以使用索引将子查询去重。 range 索引范围扫描，常见于使用&gt;,&lt;,is null,between ,in ,like等运算符的查询中。 index_merge 表示查询使用了两个以上的索引，最后取交集或者并集。常见and ，or的条件使用了不同的索引，官方排序这个在ref_or_null之后，但是实际上由于要读取多个索引，性能可能都不如range。 index 索引全表扫描。把索引从头到尾扫一遍，常见于使用索引列就可以处理，不需要读取数据文件的查询、可以使用索引排序或者分组的查询。 ALL 全表扫描数据文件 possible_keys 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。 key_length 索引长度 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 这里是执行计划中估算的扫描行数，不是精确值。 extra 的信息非常丰富，常见的有： Using index 使用覆盖索引 Using where 使用了where子句来过滤结果集 Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。 Using temporary 使用了临时表 SQL优化的目标可以参考阿里开发手册 123456【推荐】SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。说明：1） consts 单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。2） ref 指的是使用普通的索引（normal index）。3） range 对索引进行范围检索。反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描是小巫见大巫。 @$SQL的生命周期？一条SQL查询语句是如何执行的？MySQL总体架构—&gt;SQL执行流程—&gt;语句执行顺序MySQL 的逻辑架构图 12-- 比如，你有个最简单的表，表里只有一个 ID 字段，在执行下面这个查询语句时：mysql&gt; select * from T where ID=10; MySQL的框架有几个组件, 各是什么作用? Server层和存储引擎层各是什么作用？ 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 SQL执行流程，SQL的生命周期？ 连接器 第一步，客户端与数据库server层的连接器进行连接。连接器负责跟客户端建立连接、获取权限、维持和管理连接。 查询缓存 连接建立完成后，会判断查询缓存是否开启，如果已经开启，会判断sql是select还是update&#x2F;insert&#x2F;delete，对于select，尝试去查询缓存，如果命中缓存直接返回数据给客户端， 如果缓存没有命中，或者没有开启缓存， 会进入到下一步分析器。 分析器 分析器进行词法分析和语法分析，分析器先会做“词法分析”，分析SQL中的字符串分别是什么，校验数据库表和字段是否存在，然后进行语法分析，判断SQL是否满足MySQL语法。 优化器 优化器对sql执行计划分析，得到最终执行计划，得到优化后的执行计划交给执行器。 优化器是在表里面有多个索引的时候，决定使用哪个索引，或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误，如果有权限，执行器调用存储引擎api执行sql，得到响应结果， 将结果返回给客户端，如果缓存是开启状态， 会更新缓存。 详细逻辑架构图 连接：应用服务器与数据库服务器建立一个连接 获得请求SQL：数据库进程拿到请求sql 查询缓存：如果查询命中缓存则直接返回结果 语法解析和预处理： 首先通过mysql关键字将语句解析，会生成一个内部解析树，mysql解析器将对其解析，查看是否是有错误的关键字，关键字顺序是否正确；预处理器则是根据mysql的规则进行进一步的检查，检查mysql语句是否合法，如库表是否存在，字段是否存在，字段之间是否模棱两可等等，预处理器也会验证权限。 查询优化器：sql语句在优化器中转换成执行计划，一条sql语句可以有多种方式查询，最后返回的结果肯定是相同，但是不同的查询方式效果不同，优化器的作用就是：选择一种合适的执行计划。mysql是基于成本的优化器，他将预测执行此计划的成本，并选择成本最小的那条 执行计划，执行SQL：在解析和优化后，MySQL将生成查询对应的执行计划，由执行计划调用存储引擎的API来执行查询 将结果返回给客户端 关掉连接，释放资源 一条更新语句的执行流程又是怎样的呢？12-- 如果要将 ID=2 这一行的值加 1mysql&gt; update T set c=c+1 where ID=2; 你执行语句前要先连接数据库，这是连接器的工作。 前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表 T 上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。 接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用 ID 这个索引。然后，执行器负责具体执行，找到这一行，然后更新。 与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）。 在 MySQL 里也有这个问题，如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程 IO 成本、查找成本都很高。为了解决这个问题，MySQL 的设计者就用了类似酒店掌柜粉板的思路来提升更新效率。 而粉板和账本配合的整个过程，其实就是 MySQL 里经常说到的 WAL 技术，WAL 的全称是 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘，也就是先写粉板，等不忙的时候再写账本。 具体来说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，这就像打烊以后掌柜做的事。 如果今天赊账的不多，掌柜可以等打烊后再整理。但如果某天赊账的特别多，粉板写满了，又怎么办呢？这个时候掌柜只好放下手中的活儿，把粉板中的一部分赊账记录更新到账本中，然后把这些记录从粉板上擦掉，为记新账腾出空间。 write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。 write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。 有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为crash-safe。 要理解 crash-safe 这个概念，可以想想我们前面赊账记录的例子。只要赊账记录记在了粉板上或写在了账本上，之后即使掌柜忘记了，比如突然停业几天，恢复生意后依然可以通过账本和粉板上的数据明确赊账账目。 重要的日志模块：binlog 前面我们讲过，MySQL 整体来看，其实就有两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的粉板 redo log 是 InnoDB 引擎特有的日志，而 Server 层也有自己的日志，称为 binlog（归档日志）。 我想你肯定会问，为什么会有两份日志呢？ 因为最开始 MySQL 里并没有 InnoDB 引擎。MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用另外一套日志系统——也就是 redo log 来实现 crash-safe 能力。 这两种日志有以下三点不同。 redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID&#x3D;2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 常用SQL查询语句优化方法 不要使用select * from t，用具体的字段列表代替“*”，使用星号会降低查询效率，如果数据库字段改变，可能出现不可预知隐患。 应尽量避免在where子句中使用!&#x3D;或&lt;&gt;操作符，避免在where子句中字段进行null值判断，存储引擎将放弃使用索引而进行全表扫描。 避免使用左模糊，左模糊查询将导致全表扫描。 IN语句查询时包含的值不应过多，否则将导致全表扫描。 为经常作为查询条件的字段，经常需要排序、分组操作的字段建立索引。 在使用联合索引字段作为条件时，应遵循最左前缀原则。 OR前后两个条件都要有索引，整个SQL才会使用索引，只要有一个条件没索引整个SQL就不会使用索引。 尽量用union all代替union，union需要将结果集合并后再进行唯一性过滤操作，这就会涉及到排序，增加大量的CPU运算，加大资源消耗及延迟。 总结ACID隔离级别Explain工具介绍使用EXPLAIN关键字可以模拟优化器执行SQL语句，分析你的查询语句或是结构的性能瓶颈 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询会返回执行计划的信息，而不是 执行这条SQL 注意：如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中 Explain分析示例 示例表 1234567891011121314151617181920212223242526272829303132333435363738394041424344CREATE TABLE ggq_tulin;USE ggq_tulin;DROP TABLE IF EXISTS `actor`;CREATE TABLE `actor` (`id` INT(11) NOT NULL,`name` VARCHAR(45) DEFAULT NULL,`update_time` DATETIME DEFAULT NULL,PRIMARY KEY (`id`)) ENGINE=INNODB DEFAULT CHARSET=utf8;SHOW CREATE DATABASE ggq_tulin;INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,&#x27;a&#x27;,&#x27;2017-12-22 15:27:18&#x27;), (2,&#x27;b&#x27;,&#x27;2017-12-22 15:27:18&#x27;), (3,&#x27;c&#x27;,&#x27;2017-12-22 15:27:18&#x27;);SELECT * FROM actor;DROP TABLE IF EXISTS `film`;CREATE TABLE `film` (`id` INT(11) NOT NULL AUTO_INCREMENT,`name` VARCHAR(10) DEFAULT NULL,PRIMARY KEY (`id`),KEY `idx_name` (`name`)) ENGINE=INNODB DEFAULT CHARSET=utf8;SHOW DATABASES;USE ggq_tuling;INSERT INTO `film` (`id`, `name`) VALUES (3,&#x27;film0&#x27;),(1,&#x27;film1&#x27;),(2,&#x27;film2&#x27;);SELECT * FROM film;DROP TABLE IF EXISTS `film_actor`;CREATE TABLE `film_actor` (`id` INT(11) NOT NULL,`film_id` INT(11) NOT NULL,`actor_id` INT(11) NOT NULL,`remark` VARCHAR(255) DEFAULT NULL,PRIMARY KEY (`id`),KEY `idx_film_actor_id` (`film_id`,`actor_id`)) ENGINE=INNODB DEFAULT CHARSET=utf8;INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); 1explain select * from actor; 在查询中的每个表会输出一行，如果有两个表通过 join 连接查询，那么会输出两行 explain 两个变种 1紧随其后通过 show warnings 命令可 以得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows * filtered&#x2F;100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的 表）。 123explain select * from film where id = 1; show warnings; explain中的列接下来我们将展示 explain 中每个列的信息。 \\1. id列不一定唯一 id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。 id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 \\2. select_type列select_type 表示对应行是简单还是复杂的查询。 1）simple：简单查询。查询不包含子查询和union 11 mysql&gt; explain select * from film where id = 2; 2）primary：复杂查询中最外层的 select 3）subquery：包含在 select 中的子查询（不在 from 子句中） 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表(原来没有的表)中，也称为派生(衍生)表（derived的英文含 义） 用这个例子来了解 primary、subquery 和 derived 类型 11 mysql&gt; set session optimizer_switch=&#x27;derived_merge=off&#x27;; #关闭mysql5.7新特性对衍生表的合并优化 1232 mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der;1 mysql&gt; set session optimizer_switch=&#x27;derived_merge=on&#x27;; #还原默认配置 5）union：在 union 中的第二个和随后的 select 11 mysql&gt; explain select 1 union all select 1; \\3. table列这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id&#x3D;N 的查询，于是先执行 id&#x3D;N 的查 询。 当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 \\4. type列这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。 效率依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL 一般来说，得保证查询达到range级别，最好达到ref 很少: NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可 以单独查找索引来完成，不需要在执行时访问表 11 mysql&gt; explain select min(id) from film; const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是 const的特例，表里只有一条元组匹配时为system 1231 mysql&gt; explain extended select * from (select * from film where id = 1) tmp;1 mysql&gt; show warnings; eq_ref：primary key 或 unique key 索引的所有部分被连接使用&#x2F;唯一索引 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 11 mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id; ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会 找到多个符合条件的行。 \\1. 简单 select 查询，name是普通索引（非唯一索引） 11 mysql&gt; explain select * from film where name = &#x27;film1&#x27;; 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 11 mysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.fi lm_id; range：范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;&#x3D; 等操作中。使用一个索引来检索给定范围的行。 11 mysql&gt; explain select * from actor where id &gt; 1; index：扫描全索引就能拿到结果，一般是扫描某个二级索引，这种扫描不会从索引树根节点开始快速查找，而是直接 对二级索引的叶子节点遍历和扫描，速度还是比较慢的，这种查询一般为使用覆盖索引，二级索引一般比较小，所以这 种通常比ALL快一些。 1 mysql&gt; explain select * from film; ALL：即全表扫描，扫描你的聚簇索引的所有叶子节点。通常情况下这需要增加索引来进行优化了。 11 mysql&gt; explain select * from actor; \\5. possible_keys列这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引 对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提 高查询性能，然后用 explain 查看效果。 \\6. key列这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 \\7. key_len列这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通 过结果中的key_len&#x3D;4可推断出查询使用了第一个列：film_id列来执行索引查找。 11 mysql&gt; explain select * from film_actor where film_id = 2; key_len计算规则如下： 字符串，char(n)和varchar(n)，5.0.3以后版本中，n均代表字符数，而不是字节数，如果是utf-8，一个数字 或字母占1个字节，一个汉字占3个字节 char(n)：如果存汉字长度就是 3n 字节 varchar(n)：如果存汉字则长度是 3n + 2 字节，加的2字节用来存储字符串长度，因为 varchar是变长字符串 数值类型 tinyint：1字节 smallint：2字节 int：4字节 bigint：8字节 时间类型 date：3字节 timestamp：4字节 datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索 引。 \\8. ref列这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） \\9. rows列这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 \\10. Extra列这一列展示的是额外信息。常见的重要值如下： 1）Using index：使用覆盖索引 覆盖索引定义：mysql执行计划explain结果里的key有使用索引，如果select后面查询的字段都可以从这个索引的树中 获取，这种情况一般可以说是用到了覆盖索引，extra里一般都有using index；覆盖索引一般针对的是辅助索引，整个 查询结果只通过辅助索引就能拿到结果，不需要通过辅助索引树找到主键，再通过主键去主键索引树里获取其它字段值 11 mysql&gt; explain select film_id from film_actor where film_id = 1; 2）Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖 11 mysql&gt; explain select * from actor where name = &#x27;a&#x27;; 3）Using index condition：查询的列不完全被索引覆盖，where条件中是一个前导列的范围； 11 mysql&gt; explain select * from film_actor where film_id &gt; 1; 4）Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索 引来优化。 \\1. actor.name没有索引，此时创建了张临时表来distinct 11 mysql&gt; explain select distinct name from actor; \\2. film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表 11 mysql&gt; explain select distinct name from film; 5）Using filesort：将用外部排序而不是索引排序，数据较小时从内存排序，否则需要在磁盘完成排序。这种情况下一 般也是要考虑使用索引来优化的。 \\1. actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 11 mysql&gt; explain select * from actor order by name; \\2. film.name建立了idx_name索引,此时查询时extra是using index 11 mysql&gt; explain select * from film order by name; 6）Select tables optimized away：使用某些聚合函数（比如 max、min）来访问存在索引的某个字段是 11 mysql&gt; explain select min(id) from film; 索引最佳实践12345678910111213141516171819202122232425262728291 示例表：2 CREATE TABLE `employees` (3 `id` int(11) NOT NULL AUTO_INCREMENT,4 `name` varchar(24) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;姓名&#x27;,5 `age` int(11) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;年龄&#x27;,6 `position` varchar(20) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;职位&#x27;,7 `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;入职时间&#x27;,8 PRIMARY KEY (`id`),9 KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE10 ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COMMENT=&#x27;员工记录表&#x27;;1112 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;LiLei&#x27;,22,&#x27;manager&#x27;,NOW());13 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;HanMeimei&#x27;,23,&#x27;dev&#x27;,NOW());14 INSERT INTO employees(name,age,position,hire_time) VALUES(&#x27;Lucy&#x27;,23,&#x27;dev&#x27;,NOW()); 1.全值匹配 123451 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27;;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22 AND position =&#x27;manager&#x27;; 2.最左前缀法则 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 12345678910111 EXPLAIN SELECT * FROM employees WHERE name = &#x27;Bill&#x27; and age = 31;2 EXPLAIN SELECT * FROM employees WHERE age = 30 AND position = &#x27;dev&#x27;;3 EXPLAIN SELECT * FROM employees WHERE position = &#x27;manager&#x27;;3.不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描1 EXPLAIN SELECT * FROM employees WHERE name = &#x27;LiLei&#x27;;2 EXPLAIN SELECT * FROM employees WHERE left(name,3) = &#x27;LiLei&#x27;; 给hire_time增加一个普通索引： 1231 ALTER TABLE `employees` ADD INDEX `idx_hire_time` (`hire_time`) USING BTREE ;1 EXPLAIN select * from employees where date(hire_time) =&#x27;2018‐09‐30&#x27;; 转化为日期范围查询，有可能会走索引： 1231 EXPLAIN select * from employees where hire_time &gt;=&#x27;2018‐09‐30 00:00:00&#x27; and hire_time &lt;=&#x27;2018‐09‐30 23:59:59&#x27;; 还原最初索引状态 11 ALTER TABLE `employees` DROP INDEX `idx_hire_time`; 4.存储引擎不能使用索引中范围条件右边的列 1231 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 22 AND position =&#x27;manager&#x27;;2 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age &gt; 22 AND position =&#x27;manager&#x27;; 5.尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少 select * 语句 12345671 EXPLAIN SELECT name,age FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 23 AND position=&#x27;manager&#x27;;1 EXPLAIN SELECT * FROM employees WHERE name= &#x27;LiLei&#x27; AND age = 23 AND position =&#x27;manager&#x27;; 6.mysql在使用不等于（！&#x3D;或者&lt;&gt;），not in ，not exists 的时候无法使用索引会导致全表扫描 &lt; 小于、 &gt; 大于、 &lt;&#x3D;、&gt;&#x3D; 这些，mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引 11 EXPLAIN SELECT * FROM employees WHERE name != &#x27;LiLei&#x27;; 7.is null,is not null 一般情况下也无法使用索引 11 EXPLAIN SELECT * FROM employees WHERE name is null 8.like以通配符开头（’$abc…’）mysql索引失效会变成全表扫描操作 1231 EXPLAIN SELECT * FROM employees WHERE name like &#x27;%Lei&#x27;1 EXPLAIN SELECT * FROM employees WHERE name like &#x27;Lei%&#x27; 问题：解决like’%字符串%’索引不被使用的方法？ a）使用覆盖索引，查询字段必须是建立覆盖索引字段 11 EXPLAIN SELECT name,age,position FROM employees WHERE name like &#x27;%Lei%&#x27;; b）如果不能使用覆盖索引则可能需要借助搜索引擎 9.字符串不加单引号索引失效 1231 EXPLAIN SELECT * FROM employees WHERE name = &#x27;1000&#x27;;2 EXPLAIN SELECT * FROM employees WHERE name = 1000; 10.少用or或in，用它查询时，mysql不一定使用索引，mysql内部优化器会根据检索比例、表大小等多个因素整体评 估是否使用索引，详见范围查询优化 1 EXPLAIN SELECT * FROM employees WHERE name &#x3D; ‘LiLei’ or name &#x3D; ‘HanMeimei’; 11.范围查询优化 给年龄添加单值索引 1231 ALTER TABLE `employees` ADD INDEX `idx_age` (`age`) USING BTREE ;1 explain select * from employees where age &gt;=1 and age &lt;=2000; 没走索引原因：mysql内部优化器会根据检索比例、表大小等多个因素整体评估是否使用索引。比如这个例子，可能是 由于单次数据量查询过大导致优化器最终选择不走索引 优化方法：可以将大的范围拆分成多个小范围 1231 explain select * from employees where age &gt;=1 and age &lt;=1000;2 explain select * from employees where age &gt;=1001 and age &lt;=2000; 还原最初索引状态 11 ALTER TABLE `employees` DROP INDEX `idx_age`; 索引使用总结： like KK%相当于&#x3D;常量，%KK和%KK% 相当于范围 1231 ‐‐ mysql5.7关闭ONLY_FULL_GROUP_BY报错2 select version(), @@sql_mode;SET sql_mode=(SELECT REPLACE(@@sql_mode,&#x27;ONLY_FULL_GROUP_BY&#x27;,&#x27;&#x27;)); 文档：02-VIP-Explain详解与索引最佳实践 1 http://note.youdao.com/noteshare?id=59d7a574ef9a905e3bb0982bbe33e74d&amp;sub=83A39BAAADD14B8F99E1DCEFFB 7642CA 随笔什么是候选键：能够完全决定所有属性的那个属性或者属性组是候选键什么是非主属性：不包含在任何候选键中的属性是非主属性什么是完全函数依赖：如果x决定y那么x的任何一个真子集x&#96;都不能决定y 就叫做 24.3 SQL语句分类DDL：数据定义语句【create 表，库…】 DML：数据操作语句【增加 insert，修改update，删除delete】 DQL：数据查询语句【select】 DCL：数据控制语句【管理数据库：比如用户权限 grant revoke】 第24章 零基础 MYSQL管理员身份开启cmd ,net start mysql 登陆mysql root 默认密码 什么都没有 直接回车就行 24.1数据库三层结构1.所谓安装Mysql数据库，就是在主机安装一个数据库管理系统（DBMS，data base manage system），这个管理程序可以管理多个数据库。 2.一个数据库可以创建多个表，以保存数据（信息） 3，数据库管理系统，数据库和表的关系如图所示 客户端，java等通过过3306端口与DBMS通信对数据库操作 普通表的本质仍然是文件 24.2 数据在数据库中的存储方式 24.3 SQL语句分类DDL：数据定义语句【create 表，库…】 DML：数据操作语句【增加 insert，修改update，删除delete】 DQL：数据查询语句【select】 DCL：数据控制语句【管理数据库：比如用户权限 grant revoke】 24.4 创建数据库 反引号 将关键字括在一起可创建带关键字的数据库 备份和 CHARACTER SET 指定数据库采用的字符集，如果不指定字符集，默认utf8 COLLATE：指定数据库字符集的校对规则（常用的utf8_bin[区分大小写]、默认是utf8_general_ci[不区分大小写]） 24.5查看、删除数据库显示数据库语句：show databases 显示数据库创建语句：show create database db_name 数据库删除语句: drop database [if exists] db_name 为了规避关键字，可以使用反引号解决 &#96; 反引号在TAB上面 24.6 备份恢复数据库备份数据库（注意：在DOS执行） mysqldump -u 用户名 -p -B 数据库1 数据库n &gt;文件名.sql 恢复数据库（注意：进入Mysql命令在执行） source 文件名.sql 24.7创建表 123456CREATE TABLE `emp2` (id INT, `name` VARCHAR(32),sex CHAR(1), birthday DATE,entry_date DATETIME,job VARCHAR(32),salary DOUBLE, `resume` TEXT)CHARSET utf8 COLLATE utf8_bin ENGINE INNODB; CREATE TABLE user ( id INT, name VARCHAR(255), password VARCHAR(255), birthday DATE) CHARACTER SET utf8 COLLATE utf8_bin ENGINE INNODB 24.8Mysql常用数据类型（列类型） 24.8.1 整形使用#1. 如果没有指定 unsinged , 则 TINYINT 就是有符号 #2. 如果指定 unsinged , 则 TINYINT 就是无符号 0-2 24.8.2 bit的使用#演示 bit 类型使用 #说明 #1. bit(m) m 在 1-64 #2. 添加数据 范围 按照你给的位数来确定，比如 m &#x3D; 8 表示一个字节 0~255 #3. 显示按照 bit #4. 查询时，仍然可以按照数来查询 24.8.3 小数的基本使用FLOAT DOUBLE DECIMAL[M,D] 24.8.4字符串的基本使用#演示字符串类型使用 char varchar #注释的快捷键 shift+ctrl+c , 注销注释 shift+ctrl+r – CHAR(size) – 固定长度字符串 最大 255 字符 – VARCHAR(size) 0~65535 字节 – 可变长度字符串 最大 65532 字节 【utf8 编码最大 21844 字符 1-3 个字节用于记录大小】 – 如果表的编码是 utf8 varchar(size) size &#x3D; (65535-3) &#x2F; 3 &#x3D; 21844 – 如果表的编码是 gbk varchar(size) size &#x3D; (65535-3) &#x2F; 2&#x3D;32766 24.8.5日期类型的基本使用\\ 12345678910111213141516#演示时间相关的类型 #创建一张表, date , datetime , timestamp CREATE TABLE t14 ( birthday DATE , -- 生日 job_time DATETIME, -- 记录年月日 时分秒 login_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP); -- 登录时间, 如果希望 login_time 列自动更新, 需要配置 SELECT * FROM t14; INSERT INTO t14(birthday, job_time) VALUES(&#x27;2022-11-11&#x27;,&#x27;2022-11-11 10:10:10&#x27;); -- 如果我们更新 t14 表的某条记录，login_time 列会自动的以当前时间进行更新 24.9 修改表使用alter table 语句追加，修改，或删除列的语法 添加列：alter table tablename add （column datatype [default expr ] [,column datatype]...); 修改列：alter table tablename modify （column datatype [default expr ] [,column datatype]...); 删除列：alter table tablename drop （column); 修改表名 rename table 表名 to 新表名 修改表字符集：alter table 表名 character set 字符集 1234567891011121314151617#修改表的操作练习-- 员工表 emp 的上增加一个 image 列，varchar 类型(要求在 resume 后面)。ALTER TABLE empADD image VARCHAR(32) NOT NULL DEFAULT &#x27;&#x27; AFTER RESUMEDESC employee -- 显示表结构，可以查看表的所有列-- 修改 job 列，使其长度为 60。ALTER TABLE empMODIFY job VARCHAR(60) NOT NULL DEFAULT &#x27;&#x27; -- 删除 sex 列。ALTER TABLE empDROP sex-- 表名改为 employee。RENAME TABLE emp TO employee-- 修改表的字符集为 utf8ALTER TABLE employee CHARACTER SET utf8-- 列名 name 修改为 user_nameALTER TABLE employeeCHANGE `name` `user_name` VARCHAR(64) NOT NULL DEFAULT &#x27;&#x27; DESC employee 24.10 数据库CRUD语句insert 添加数据 update 更新数据 delete 删除数据 select 查找数据 24.11 insert 语句12345Insert into tablename [(column[,column...])]values (value [,value...]); 12345678910111213#练习 insert 语句-- 创建一张商品表 goods (id int , goods_name varchar(10), price double ); -- 添加 2 条记录CREATE TABLE `goods` (id INT , goods_name VARCHAR(10), -- 长度 10price DOUBLE NOT NULL DEFAULT 100 ); -- 添加数据INSERT INTO `goods` (id, goods_name, price)VALUES(10, &#x27;华为手机&#x27;, 2000);INSERT INTO `goods` (id, goods_name, price)VALUES(20, &#x27;苹果手机&#x27;, 3000);SELECT * FROM goods;CREATE TABLE `goods2` (id INT , goods_name VARCHAR(10), -- 长度 10price DOUBLE NOT NULL DEFAULT 使用细节1234567891011121314151617181920212223#说明 insert 语句的细节-- 1.插入的数据应与字段的数据类型相同。-- 比如 把 &#x27;abc&#x27; 添加到 int 类型会错误INSERT INTO `goods` (id, goods_name, price)VALUES(&#x27;韩顺平&#x27;, &#x27;小米手机&#x27;, 2000); -- 2. 数据的长度应在列的规定范围内，例如：不能将一个长度为 80 的字符串加入到长度为 40 的列中。INSERT INTO `goods` (id, goods_name, price)VALUES(40, &#x27;vovo 手机 vovo 手机 vovo 手机 vovo 手机 vovo 手机&#x27;, 3000); -- 3. 在 values 中列出的数据位置必须与被加入的列的排列位置相对应。INSERT INTO `goods` (id, goods_name, price) -- 不对VALUES(&#x27;vovo 手机&#x27;,40, 2000); -- 4. 字符和日期型数据应包含在单引号中。INSERT INTO `goods` (id, goods_name, price)VALUES(40, vovo 手机, 3000); -- 错误的 vovo 手机 应该 &#x27;vovo 手机&#x27; -- 5. 列可以插入空值[前提是该字段允许为空]，insert into table value(null)INSERT INTO `goods` (id, goods_name, price)VALUES(40, &#x27;vovo 手机&#x27;, NULL); -- 6. insert into tab_name (列名..) values (),(),() 形式添加多条记录INSERT INTO `goods` (id, goods_name, price)VALUES(50, &#x27;三星手机&#x27;, 2300),(60, &#x27;海尔手机&#x27;, 1800); -- 7. 如果是给表中的所有字段添加数据，可以不写前面的字段名称INSERT INTO `goods` VALUES(70, &#x27;IBM 手机&#x27;, 5000); -- 8. 默认值的使用，当不给某个字段值时，如果有默认值就会添加默认值，否则报错-- 如果某个列 没有指定 not null ,那么当添加数据时，没有给定值，则会默认给 null -- 如果我们希望指定某个列的默认值，可以在创建表时指定INSERT INTO `goods` (id, goods_name)VALUES(80, &#x27;格力手机&#x27;);SELECT * FROM goods;INSERT INTO `goods2` (id, goods_name)VALUES(10, &#x27;顺平手机&#x27;);SELECT * FROM goods2; 24.12 update 语句update tablename set col_name1 =expr1[,] 123456789101112131415-- 演示 update 语句-- 要求: 在上面创建的 employee 表中修改表中的纪录-- 1. 将所有员工薪水修改为 5000 元。[如果没有带 where 条件，会修改所有的记录，因此要小心]UPDATE employee SET salary = 5000-- 2. 将姓名为 小妖怪 的员工薪水修改为 3000 元。UPDATE employeeSET salary = 3000WHERE user_name = &#x27;小妖怪&#x27; -- 3. 将 老妖怪 的薪水在原有基础上增加 1000 元INSERT INTO employeeVALUES(200, &#x27;老妖怪&#x27;, &#x27;1990-11-11&#x27;, &#x27;2000-11-11 10:10:10&#x27;, &#x27;捶背的&#x27;, 5000, &#x27;给大王捶背&#x27;, &#x27;d:\\\\a.jpg&#x27;);UPDATE employeeSET salary = salary + 1000WHERE user_name = &#x27;老妖怪&#x27; -- 可以修改多个列的值UPDATE employeeSET salary = salary + 1000 , job = &#x27;出主意的&#x27; WHERE user_name = &#x27;老妖怪&#x27; SELECT * 使用细节 24.13 delete语句12345678-- 删除表中名称为’老妖怪’的记录。DELETE FROM employeeWHERE user_name = &#x27;老妖怪&#x27;; -- 删除表中所有记录, 老师提醒，一定要小心DELETE FROM employee; -- Delete 语句不能删除某一列的值（可使用 update 设为 null 或者 &#x27;&#x27;）UPDATE employee SET job = &#x27;&#x27; WHERE user_name = &#x27;老妖怪&#x27;;SELECT * FROM employee-- 要删除这个表DROP TABLE employee; 如果不使用where子句，将删除表中所有数据 delete语句不能删除某一列的值（可使用update设为null或者’’) 仅能删除记录，不能删除表本省 24.14 select 语句1234567891011121314151617-- select 语句【重点 难点】CREATE TABLE student(id INT NOT NULL DEFAULT 1, NAME VARCHAR(20) NOT NULL DEFAULT &#x27;&#x27;, chinese FLOAT NOT NULL DEFAULT 0.0, english FLOAT NOT NULL DEFAULT 0.0, math FLOAT NOT NULL DEFAULT 0.0);INSERT INTO student(id,NAME,chinese,english,math) VALUES(1,&#x27;韩顺平&#x27;,89,78,90);INSERT INTO student(id,NAME,chinese,english,math) VALUES(2,&#x27;张飞&#x27;,67,98,56);INSERT INTO student(id,NAME,chinese,english,math) VALUES(3,&#x27;宋江&#x27;,87,78,77);INSERT INTO student(id,NAME,chinese,english,math) VALUES(4,&#x27;关羽&#x27;,88,98,90);INSERT INTO student(id,NAME,chinese,english,math) VALUES(5,&#x27;赵云&#x27;,82,84,67);INSERT INTO student(id,NAME,chinese,english,math) VALUES(6,&#x27;欧阳锋&#x27;,55,85,45);INSERT INTO student(id,NAME,chinese,english,math) VALUES(7,&#x27;黄蓉&#x27;,75,65,30);INSERT INTO student(id,NAME,chinese,english,math) VALUES(8,&#x27;韩信&#x27;,45,65,99);SELECT * FROM student; -- 查询表中所有学生的信息。SELECT * FROM student; -- 查询表中所有学生的姓名和对应的英语成绩。SELECT `name`,english FROM student; -- 过滤表中重复数据 distinct 。SELECT DISTINCT english FROM student; -- 要查询的记录，每个字段都相同，才会去重SELECT DISTINCT `name`, english FROM student","categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"算法随记","slug":"2algorithm","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:48:29.917Z","comments":true,"path":"2022/09/01/2algorithm/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/2algorithm/","excerpt":"","text":"基础知识 阿全,天堂 实用技巧Map.putIfAbsent();随机数常用的两种 Random random &#x3D; new Random（）; random.nextInt(l.size()) Math.random*l.size() Math.randow [0,1) Lambda(int[] pair1, int[] pair2) -&gt; { return pair2[1] - pair1[1]; } 比较数组的第二个 HashMap getOrDefault() 方法getOrDefault() 法获取指定 key 对应的 value，如果找不到 key ，则返回设置的默认值。 hashMap.containsKey(); string.substring(); HashMap遍历先取出 所有的 KeySet keyset &#x3D; map.keySet(); for(Object :keyset){ ​ map.get(key); } Iterator iterator&#x3D; keyset.iterator(); while(iterator.hasNext()){ Object key &#x3D; iterator.next(); map.get(key); } 取出所有valueCollection values &#x3D; map.values(); EntrySet 获取k-vSet entrySet &#x3D; map.entrySet(); 123456789101112131415161718192021222324252627282930313233343536373839404142Set entrySet = map.entrySet();class Solution &#123; public int mostFrequentEven(int[] nums) &#123; Map&lt;Integer,Integer&gt; map = new HashMap&lt;&gt;(); for (int num : nums) &#123; if (num % 2 == 0) &#123; map.put(num,map.getOrDefault(num,0)+1); &#125; &#125; Set&lt;Map.Entry&lt;Integer,Integer&gt;&gt; entrySet = map.entrySet(); int ans = -1; int max = 0; for (Map.Entry&lt;Integer,Integer&gt; entry : entrySet) &#123; if (entry.getValue() &gt; max) &#123; ans = entry.getKey(); max = entry.getValue(); &#125; &#125; return ans; &#125;&#125;/(1) 增强 forSystem.out.println(&quot;----使用 EntrySet 的 for 增强(第 3 种)----&quot;);for (Object entry : entrySet) &#123;//将 entry 转成 Map.EntryMap.Entry m = (Map.Entry) entry;System.out.println(m.getKey() + &quot;-&quot; + m.getValue());&#125;//(2) 迭代器System.out.println(&quot;----使用 EntrySet 的 迭代器(第 4 种)----&quot;);Iterator iterator3 = entrySet.iterator();while (iterator3.hasNext()) &#123;Object entry = iterator3.next();//System.out.println(next.getClass());//HashMap$Node -实现-&gt; Map.Entry (getKey,getValue)//向下转型 Map.EntryMap.Entry m = (Map.Entry) entry;System.out.println(m.getKey() + &quot;-&quot; + m.getValue());&#125; 一些值得学习的处理147. 对链表进行插入排序方法一 直观模拟123456789101112131415161718192021222324252627282930313233343536373839404142434445//方法一 直观模拟class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //分割成单结点,去除链表的影响 所有都是单节点操作 //不分割可能会成环 if (head == null) return null; ListNode newHead = head; head = head.next; ListNode p = newHead; newHead.next = null; ListNode q = head; // 梳理下思路 //遍历输入结点,找输入结点在新链表的位置,小于等于新结点的头结点 直接头插 大于则往后遍历, // 直到 p.next.val &gt; q.val // 如果 p.next == null 直接尾插 while (q != null) &#123; //头插 ListNode t = q.next; q.next = null; if (q.val &lt; newHead.val) &#123; //头插法 q.next = newHead; newHead = q; q = t; p = newHead; continue; &#125; while (p == null || q.val &gt; p.val) &#123; if (p.next == null || p.next.val &gt; q.val ) &#123; break; &#125; p = p.next; &#125; // 插入 ListNode temp = p.next; p.next = q; q.next = temp; q = t; p = newHead; &#125; return newHead; &#125;&#125; 方法二12345678910111213141516171819202122232425class Solution &#123; public ListNode insertionSortList(ListNode head) &#123; //在一条环上处理 ListNode dummy = new ListNode(-1,head); ListNode newLast = dummy.next; ListNode cur = newLast.next; while (cur != null) &#123; if (cur.val &gt;= newLast.val) &#123; newLast = cur; &#125;else &#123; //从头找到p.next.val &gt; cur.val //因为last.val &gt; cur.val 所以.next不会为空的 ListNode p = dummy; while (p.next.val &lt;= cur.val) &#123; p = p.next; &#125; newLast.next = cur.next; cur.next = p.next; p.next = cur; &#125; cur = newLast.next; &#125; return dummy.next; &#125;&#125; 排序快排剑指 Offer 45. 把数组排成最小的数快速选择K 找到分界点x q[l], q[r+l &gt;&gt; 1], q [r]; 左边所有数Left &lt;&#x3D; x 右边所有数Right &gt;&#x3D; x 注:分界点不一定等于x 递归排序Left,递归排序 Right k &lt;&#x3D; Leftnum 递归left k &gt; Lnum 递归right 找 k - Lnum 时间复杂度 n(1 + 1&#x2F;2 + 1&#x2F; 4….. &lt;&#x3D; 2) 总和 &lt;&#x3D; 2n 215. 数组中的第K个最大元素第K个数12345678910111213141516171819202122232425262728293031class Solution &#123; public int findKthLargest(int[] nums, int k) &#123; //快选 int n = nums.length; return quickSelect(nums,0,n-1,n-k+1); &#125; public int quickSelect(int[] q, int l, int r, int k) &#123; if (l &gt;= r) return q[l]; int x = q[l], i = l - 1, j = r + 1; while (i &lt; j) &#123; while (q[ ++ i] &lt; x); while (q[ -- j] &gt; x); if (i &lt; j) &#123; int t = q[i]; q[i] = q[j]; q[j] = t; &#125; &#125; // 分界点为 j // 如果舍去左边 K也要变 // int ls = j - l + 1; // if (k &lt;= ls) return quickSelect(q,l,j,k); // return quickSelect(q,j+1,r,k - ls); //简单写法 if (k &gt; j) return quickSelect(q,j+1,r,k); return quickSelect(q,l,j,k); &#125;&#125; 归并逆序对二分leetcode 34 .数的范围剑指 Offer II 058. 日程表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class MyCalendar &#123; // 排序 二分 记录每个的起点 与 end ,将 end 排序 能插则插 找到索引进行插入 List&lt;int[]&gt; calendar; public MyCalendar() &#123; calendar = new ArrayList&lt;&gt;(); //查找与插入都多 综合 用 arr &#125; public boolean book(int start, int end) &#123; int[] map = new int[]&#123;end,start&#125;; if(calendar.size() == 0) &#123; calendar.add(map); &#125;else &#123; // 找到小于等于start的右边界 int r = calendar.size() - 1; int l = 0; // 要插入的坐标 int index = -1; while(l &lt;= r) &#123; int mid = l + (r - l) / 2; if(calendar.get(mid)[0] == start) &#123; index = mid+1; break; &#125;else if(calendar.get(mid)[0] &lt; start) &#123; l = mid + 1; &#125;else &#123; r = mid - 1; &#125; //处理左溢出 index = r + 1; &#125; //处理右溢出 if(index &gt; calendar.size() - 1)&#123; calendar.add(map); return true; &#125; if(calendar.get(index)[1] &lt; end) return false; calendar.add(index,map);//插入后其他坐标向后移 &#125; return true; &#125;&#125;/** * Your MyCalendar object will be instantiated and called as such: * MyCalendar obj = new MyCalendar(); * boolean param_1 = obj.book(start,end); */ 前缀和1.前缀和 返回preSum[right+1] - preSum[left]; 求 left 到right 细节 构建多一个 前缀和 并查集547. 省份数量难度中等873收藏分享切换为英文接收动态反馈 有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。 省份 是一组直接或间接相连的城市，组内不含其他没有相连的城市。 给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。 返回矩阵中 省份 的数量。 示例 1： 12输入：isConnected = [[1,1,0],[1,1,0],[0,0,1]]输出：2 示例 2： 12输入：isConnected = [[1,0,0],[0,1,0],[0,0,1]]输出：3 提示： 1 &lt;= n &lt;= 200 n == isConnected.length n == isConnected[i].length isConnected[i][j] 为 1 或 0 isConnected[i][i] == 1 isConnected[i][j] == isConnected[j][i] 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution &#123; public int findCircleNum(int[][] isConnected) &#123; int n = isConnected.length; UF uf = new UF(n); for (int i = 0; i &lt; n; i++) &#123; for (int j = 0; j &lt; n; j++) &#123; if (isConnected[i][j] == 1) &#123; uf.union(i,j); &#125; &#125; &#125; return uf.sz; &#125;&#125;class UF &#123; public int[] id; public int sz; // 维护有几个 // 只有根的值 id[i] = i public UF(int n) &#123; id = new int[n]; sz = n; for (int i = 0; i &lt; n; i++) &#123; id[i] = i; &#125; &#125; public void union(int a, int b) &#123; if (! connected(a,b)) sz--; int i = root(a); int j = root(b); id[i] = j; &#125; public boolean connected(int a, int b) &#123; return root(a) == root(b); &#125; public int root(int a) &#123; while (id[a] != a) &#123; id[a] = id[id[a]]; a = id[a]; &#125; return a; &#125;&#125; 一个数组长度为n 0-n-1个自连接结点, 连接操作: 选取两个结点所属组的任意一个的标记 如果是属于这个的就变为另一个的标记 每个结点记录自己的父节点信息,只有root自己指向自己,缺点是树太高,解决:将小树连接到大树上 单词rotate 旋转 contents 目录，内容 You may have been using Java for a while 您可能已经使用Java一段时间了 Rotate an array of n elements to the right by k steps 将n个元素的数组向右旋转k步 For example, with n &#x3D; 7 and k &#x3D; 3, the array [1,2,3,4,5,6,7] is rotated to [5,6,7,1,2,3,4]. 2022.3.6 抢银行只需要预先计算出第 i天前警卫数目连续非递增的天数以及第 i 天后警卫数目连续非递减的天数 1234567891011121314151617181920212223242526class Solution &#123; public List&lt;Integer&gt; goodDaysToRobBank(int[] security, int time) &#123; int n = security.length; int[] left = new int[n]; int[] right = new int[n]; for (int i = 1; i &lt; n; i++) &#123; if (security[i] &lt;= security[i - 1]) &#123; left[i] = left[i - 1] + 1; //连续非递增的天数 第i天前 left[i] = left[i-1] +1 //要不然等于0 都初始为0 &#125; if (security[n - i - 1] &lt;= security[n - i]) &#123; right[n - i - 1] = right[n - i] + 1;//连续非递减的天数 第i天后 从右往左 连续非递增 &#125; &#125; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); for (int i = time; i &lt; n - time; i++) &#123; if (left[i] &gt;= time &amp;&amp; right[i] &gt;= time) &#123; ans.add(i); &#125; &#125; return ans; &#125;&#125; 简单题两数之和跳出双重循环flag 确定返回类型 优化遍历 i!&#x3D;j i&#x3D;0 j&#x3D;0 到i&#x3D;0,j&#x3D;i+1 哈希表待学 删除有序数组重复项双指针 最大子数组和动态规划 sum是动态前一状态影响后续 class Solution { public int maxSubArray(int[] nums) { int ans &#x3D; nums[0]; int sum &#x3D; 0; for(int num: nums) { if(sum &gt; 0) { sum +&#x3D; num; } else { sum &#x3D; num; } ans &#x3D; Math.max(ans, sum); } return ans; }} 化成字符串 翻转数组 481.汉明距离 原始问题 x与y 做异或运算 二进制 相同为0 不同为1 转为位计数 法一：内置位计数法 1return Integer.bitCount(x ^ y); 法二：移位计数法 +&#x3D; &gt;&gt;&#x3D; 这些符号要连在一起写 法三：对法二的改进 \\text{Brian Kernighan}Brian Kernighan 算法进行优化，具体地，该算法可以被描述为这样一个结论：记 f(x)f(x) 表示 xx 和 x-1x−1 进行与运算所得的结果（即 f(x)&#x3D;x&amp;(x-1)f(x)&#x3D;x &amp; (x−1)），那么 f(x)f(x) 恰为 xx 删去其二进制表示中最右侧的 1 的结果。 法四：求所有位的和 分治 类似题 338.比特位计算 1234567891011121314151617181920class Solution &#123; public int[] countBits(int n) &#123; int[] res = new int[n+1]; int _count = 0; for(int i =0;i&lt;=res.length-1;i++)&#123; int j=i; while(i!=0)&#123; i &amp;= i-1; _count++; &#125; i=j; res[i]=_count; _count=0; &#125; return res; &#125;&#125; 20.有效的括号 12345678910111213141516171819class Solution &#123; public boolean isValid(String s) &#123; Deque&lt;Character&gt; stack = new LinkedList(); for(char c:s.toCharArray())&#123; if(c==&#x27;(&#x27;) stack.push(&#x27;)&#x27;); else if(c==&#x27;&#123;&#x27;) stack.push(&#x27;&#125;&#x27;); else if(c==&#x27;[&#x27;) stack.push(&#x27;]&#x27;); else if(stack.isEmpty()||c!=stack.pop()) return false; &#125; if(stack.isEmpty()) return true; return false; &#125;&#125; 买股票给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 法一： 12345678910111213class Solution &#123; public int maxProfit(int[] prices) &#123; int[] dp = new int[prices.length+1]; int max = 0; dp[0] = prices[0]; for(int i = 1; i&lt;prices.length;i++)&#123; dp[i]=dp[i-1]&gt;prices[i]?prices[i]:dp[i-1];// max= prices[i]-dp[i]&gt;max?prices[i]-dp[i]:max;// &#125; return max; &#125;&#125; 法二： 复数乘法public String[] split(String regex, int limit) &#x2F;&#x2F;regex 正则表达式分隔符 limit 分割的份数 &#x2F;&#x2F;return 字符串数组 integer.parseInt（)是将整型数据Integer转换为基本数据类型int 12345678910111213class Solution &#123; public String complexNumberMultiply(String num1, String num2) &#123; String[] temp1 = num1.split(&quot;\\\\+|i&quot;); String[] temp2 = num2.split(&quot;\\\\+|i&quot;); int real1 = Integer.parseInt(temp1[0]); int real2 = Integer.parseInt(temp2[0]); int image1 = Integer.parseInt(temp1[1]); int image2 = Integer.parseInt(temp2[1]); return String.format(&quot;%d+%di&quot;,real1*real2-image1*image2,real1*image2+image1*real2); &#125;&#125; 杨辉三角1234567891011121314151617181920public List&lt;Integer&gt; getRow(int rowIndex) &#123; //左上方 和右上方 //保存所有数据 List&lt;List&lt;Integer&gt;&gt; c = new ArrayList&lt;List&lt;Integer&gt;&gt;(); for(int i =0;i&lt;rowIndex+1;++i)&#123; List&lt;Integer&gt; row = new ArrayList(); for(int j =0 ;j&lt;i+1;++j)&#123; if(j==0 || j==i)&#123; row.add(1); &#125;else&#123; row.add(c.get(i-1).get(j-1)+c.get(i-1).get(j)); &#125; &#125; c.add(row); &#125; return c.get(rowIndex); &#125; 优化思路 只需要保存上一行 1234567891011121314151617181920public List&lt;Integer&gt; getRow(int rowIndex) &#123; //左上方 和右上方 //保存所有数据 // List&lt;List&lt;Integer&gt;&gt; c = new ArrayList&lt;List&lt;Integer&gt;&gt;(); List&lt;Integer&gt; pre = new ArrayList(); for(int i =0;i&lt;rowIndex+1;++i)&#123; List&lt;Integer&gt; row = new ArrayList(); for(int j =0 ;j&lt;i+1;++j)&#123; if(j==0 || j==i)&#123; row.add(1); &#125;else&#123; row.add(pre.get(j-1)+pre.get(j)); &#125; &#125; pre = row; &#125; return pre; &#125; 动态规划双指针6.反转数组选择循环 是 i 还是while 123456789101112void reverseString(char[] arr)&#123; int left = 0; int right = arr.length-1; while(left&lt;right)&#123; char temp = char[right]; char[right] = char[left]; char[left] = temp; right--; left++; &#125;&#125; 7.滑动窗口 76 最小覆盖子串1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public String minWindow(String s, String t) &#123; //需要记录窗口中的字符 若装满子串所有 //和需要凑齐的字符 need 子串 //需要存储频次 每 // left 和right 是左闭右开 开始里面没有元素 //操作 //right 扩大 做什么？ 判断是否包含所有子串的字符 包含全部扩大 否则缩小 // //结果怎么更新 收缩结果 char[] sc = s.toCharArray(); char[] tc = t.toCharArray(); int left = 0 ; int right = 0; HashMap&lt;Character,Integer&gt; need = new HashMap(); HashMap&lt;Character,Integer&gt; window = new HashMap(); int start = 0; int valid = 0; int len = Integer.MAX_VALUE; for(int i=0;i&lt;tc.length;i++)&#123; need.put(tc[i],need.getOrDefault(tc[i], 0) + 1); // 在need中 记录频次 没有设为1 有的话返回记录值 &#125; while(right&lt;sc.length)&#123; char c = sc[right]; right++; if(need.containsKey(c))&#123; //若need中有 则在window中记录频次 window.put(c,window.getOrDefault(c, 0) +1 ); if(window.get(c).equals(need.get(c)))&#123; valid++;//频次相等 说明一个字母完成了 &#125; &#125; while(valid == need.size())&#123; //窗口满足条件 进行记录 记录最小值 if(right-left&lt;len)&#123; start = left; len = right - left; &#125; char d = sc[left]; //进行优化 left++; //若左边 不在need中 继续循环优化 if(need.containsKey(d))&#123; if(window.get(d).equals(need.get(d)))&#123; valid--;//有一个字母的频次不满足了 需要移动右边窗口 &#125; window.put(d, window.getOrDefault(d, 0) - 1);//减少频次 &#125; &#125; &#125; return len == Integer.MAX_VALUE?&quot;&quot;:s.substring(start,start+len); &#125; 8. 567.字符串的排列123456789101112131415161718192021222324252627282930313233343536373839public boolean checkInclusion(String s1, String s2) &#123; //排列是有顺序的 //s1的排列之一是s2的子串 Map&lt;Character,Integer&gt; need = new HashMap();//记录频次 Map&lt;Character,Integer&gt; window = new HashMap(); char[] s1c = s1.toCharArray(); char[] s2c = s2.toCharArray(); for ( int i = 0; i&lt;s1c.length;i++)&#123; need.put(s1c[i],need.getOrDefault(s1c[i],0)+1); &#125; int left = 0,right=0; int valid = 0; while(right&lt;s2c.length)&#123; char c = s2c[right]; right++; if(need.containsKey(c))&#123; window.put(c,window.getOrDefault(c,0)+1); if(window.get(c).equals(need.get(c)))&#123; //频次满足一个字母 累计结果 valid++; &#125; &#125; while(right-left&gt;=s1c.length)&#123;// 当个数大于排列需要收缩窗口 if(valid==need.size()) return true; char d = s2c[left]; left++; if(need.containsKey(d))&#123; if(window.get(d).equals(need.get(d)))&#123; valid--; &#125; window.put(d,window.getOrDefault(d,0)-1); &#125; &#125; &#125; return false; &#125; 9 .待做 438 找到字符串所有字母异位词11. 3412345678910111213141516171819202122232425262728293031323334353637383940414243444546public int[] searchRange(int[] nums, int target) &#123; //左边界和右边界 int left = 0 ; int right = nums.length - 1; int left2 = 0 ; int right2 = nums.length - 1; int[] res = new int[2]; while(left &lt;= right)&#123;// left = right +1 跳出循环 int mid = right -(right - left)/2; if(nums[mid]==target)&#123; //求左边界向左缩 right = mid-1; &#125; else if(nums[mid]&gt;target)&#123; right = mid -1; &#125;else if (nums[mid]&lt;target)&#123; left = mid +1; &#125; &#125; //比较的是left 只需判left if(left&gt;=nums.length||target != nums[left])&#123; res[0] =-1; res[1]= -1; &#125;else res[0] = left; while(left2 &lt;= right2)&#123; int mid = right2 -(right2 - left2)/2; if(nums[mid]==target)&#123; //求右边界向左缩 left2 = mid+1; &#125; else if(nums[mid]&gt;target)&#123; right2 = mid -1; &#125;else if (nums[mid]&lt;target)&#123; left2 = mid +1; &#125; &#125; //比较的是right 只需判right if(right2&lt;0||target != nums[right2])&#123; res[1] =-1; &#125;else res[1]= right2; return res; &#125; 12. 875 爱吃香蕉的珂珂二分 自变量x 要求的值 target f(x) &#x3D; target 12345678910111213141516171819202122232425262728293031323334public int minEatingSpeed(int[] piles, int h) &#123; //k根 每小时 一小时最多吃一堆 //吃完所有香蕉 //速度区间 1 到 int n = piles.length;//h&gt;=n Arrays.sort(piles); //nlogn if(h==n ) return piles[n-1]; int left= 1; int right = piles[n-1]; //以mid的速度吃是多少时间 while(left&lt;=right)&#123; int mid = right - (right - left)/2; if(f(piles,mid)==h)&#123; right = mid -1; &#125;else if(f(piles,mid)&gt;=h)&#123; left = mid +1; &#125;else if(f(piles,mid)&lt;=h)&#123; right = mid -1; &#125; &#125; return left; &#125;//抽象单调函数 H K &gt;= SUM 限定H 纵坐标吃完所有香蕉要用的时间 target = H 横坐标 根/每小时 吃的越快 f(x) 越小int f(int[] piles,int mid)&#123; int res = 0; for(int i = 0 ;i&lt;piles.length;i++)&#123; res += piles[i]/mid; if(piles[i]%mid!=0)&#123; res++; &#125; &#125; return res;&#125; 13.1011 在D天内送达包裹的能力12345678910111213141516171819202122232425262728293031323334353637383940414243public int shipWithinDays(int[] weights, int days) &#123; //按给出的顺序 运载能力 5天 // 运载能力越大 天数越小 目标天数 days //用二分找运载能力 0~sum int n = weights.length; int right = 0; for(int i = 0 ;i&lt;weights.length;i++)&#123; right += weights[i]; &#125; int left = 0 ; //以mid运载返回的天数 while(left&lt;=right)&#123; int mid = right - (right-left)/2; if(d(weights,mid)==days)&#123; left = mid+1; &#125;else if(d(weights,mid)&gt;=days)&#123; left = mid +1; &#125;else if(d(weights,mid)&lt;=days)&#123; right = mid -1; &#125; &#125; return left; &#125; //mid 运载用的天数 int d(int[] weights,int mid)&#123; /4 int res = 0; int temp=0; for(int i = 0 ;i&lt;weights.length;i++)&#123; temp += weights[i]; if(temp&gt;mid)&#123; i--; res++; temp=0; &#125; &#125; return res; &#125; 14.优势洗牌 田忌赛马二叉堆 （用数组实现的排序二叉树）可以排序 数组由大到小 用一个数组保存下标和值 存到优先队列里 15.380 常数时间插入、删除和获取随机元素&#x2F;&#x2F;队尾操作 16. 原地修改数组 26. 删除有序数组的重复项通用解法 快慢指针 17.移除元素75.颜色分类 中等输入：nums &#x3D; [2,0,2,1,1,0]输出：[0,0,1,1,2,2] 单指针 确保遍历每一个 后面的与前面的分割 想清楚操作 每一遍交换 把0换到前面 把1换到接下来的位置 双指针 0 与前面一样 找到2 放到后面 &#x2F;&#x2F;小心交换两遍 如果 交换两个一样的 !&#x3D;1 也可以桶排序 统计 0 ，1 ，2个数 字符串字符串相加12345678910111213141516171819class Solution &#123; public String addStrings(String num1, String num2) &#123; StringBuilder sb = new StringBuilder(); int i = num1.length()-1; int j = num2.length()-1; int carry = 0; while(i&gt;=0||j&gt;=0||carry!=0)&#123; if(i&gt;=0)&#123; carry += num1.charAt(i--)-&#x27;0&#x27;; //&#x27;0&#x27;代表字符0的ascii值 48 转换字符为int &#125; if(j&gt;=0)&#123; carry +=num2.charAt(j--)-&#x27;0&#x27;; &#125; sb.append(carry%10); carry /=10; &#125; return sb.reverse().toString(); &#125;&#125; 9.3 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution &#123; boolean[] visted; int ans; public int maximumRows(int[][] mat, int cols) &#123; // 判断对于每一行来说需要覆盖哪些列 12位 才能覆盖这一行 // 选定col 后覆盖的行数 // 回溯 // int n = mat.length; int m = mat[0].length; if (cols == m) return n; visted = new boolean[m]; ans = 0; backtrack(0,0,cols,mat); return ans; &#125; private void backtrack(int start, int num, int cols, int[][] mat) &#123; if (num == cols) &#123; ans = Math.max(ans,cal(mat)); return; &#125; int t = num; for (int i = start; i &lt; mat[0].length; i++) &#123; if (visted[i]) continue; visted[i] = true; num++; backtrack(i,num,cols,mat); num--; visted[i] = false; &#125; &#125; private int cal(int[][] mat) &#123; int n = mat.length; boolean[] res = new boolean[mat.length]; for (int i = 0; i &lt; mat[0].length; i++ ) &#123; if (!visted[i]) &#123; for (int j = 0; j &lt; n; j++) &#123; if (mat[j][i] == 1) &#123; res[j] = true; &#125; &#125; &#125; &#125; int ret = 0; for (int i = 0; i &lt; n; i++) &#123; if (!res[i]) &#123; ret++; &#125; &#125; return ret; &#125;&#125; 12345678910111213141516171819202122232425262728class Solution &#123; public boolean isStrictlyPalindromic(int n) &#123; // 2 - n- 2 // 9 2 - 7 进制转换 // 10 进制转 n进制 for (int i = 2; i &lt;= n - 2; i++) &#123; if (!isTrue(change(n,i))) &#123; return false; &#125; &#125; return true; &#125; private String change(int x, int n) &#123; // 除n取余 在倒过来 StringBuilder sb = new StringBuilder(); while (x != 0) &#123; sb.append(x % n); x /= n; &#125; return sb.reverse().toString(); &#125; private boolean isTrue(String s) &#123; StringBuilder sb = new StringBuilder(s); return sb.reverse().toString().equals(s); &#125;&#125; 滑动窗口维护区间最大值 单调队列 + 滑动窗口 你有 n 个机器人，给你两个下标从 0 开始的整数数组 chargeTimes 和 runningCosts ，两者长度都为 n 。第 i 个机器人充电时间为 chargeTimes[i] 单位时间，花费 runningCosts[i] 单位时间运行。再给你一个整数 budget 。 运行 k 个机器人 总开销 是 max(chargeTimes) + k * sum(runningCosts) ，其中 max(chargeTimes) 是这 k 个机器人中最大充电时间，sum(runningCosts) 是这 k 个机器人的运行时间之和。 请你返回在 不超过 budget 的前提下，你 最多 可以 连续 运行的机器人数目为多少。 示例 1： 输入：chargeTimes &#x3D; [3,6,1,3,4], runningCosts &#x3D; [2,1,3,4,5], budget &#x3D; 25输出：3解释：可以在 budget 以内运行所有单个机器人或者连续运行 2 个机器人。选择前 3 个机器人，可以得到答案最大值 3 。总开销是 max(3,6,1) + 3 * sum(2,1,3) &#x3D; 6 + 3 * 6 &#x3D; 24 ，小于 25 。可以看出无法在 budget 以内连续运行超过 3 个机器人，所以我们返回 3 。示例 2： 输入：chargeTimes &#x3D; [11,12,19], runningCosts &#x3D; [10,8,7], budget &#x3D; 19输出：0解释：即使运行任何一个单个机器人，还是会超出 budget，所以我们返回 0 。 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/maximum-number-of-robots-within-budget著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// n方超时class Solution &#123; public int maximumRobots(int[] chargeTimes, int[] runningCosts, long budget) &#123; // dp[i][j] 背包 连续装 以一个为起点最多连续装的 // 枚举起点不行吗 int ans = 0; int n = chargeTimes.length; for (int i = 0; i &lt; n; i++) &#123; long t = budget; int max = chargeTimes[i]; int runSum = runningCosts[i]; int j = i; while (t -(max + (j-i+1) * runSum) &gt;= 0) &#123; if (j &lt; n - 1 &amp;&amp; chargeTimes[++j] &gt; max) &#123; max = chargeTimes[j]; &#125; runSum += runningCosts[j]; &#125; ans = Math.max(ans,j-i); if (j == n) break; &#125; return ans; &#125;&#125;// 滑动窗口优化 维护区间最大值 从大到小用栈的操作保证 (即经典做法: 优先队列)class Solution &#123; public int maximumRobots(int[] x, int[] y, long m) &#123; // dp[i][j] 背包 连续装 以一个为起点最多连续装的 // 枚举起点不行吗 int ans = 0; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); int n = x.length; long sum = 0; for (int i = 0, j = 0; i &lt; n; i++) &#123; sum += y[i]; while(!q.isEmpty() &amp;&amp; x[q.peekLast()] &lt;= x[i]) q.removeLast(); q.addLast(i); while (!q.isEmpty() &amp;&amp; (x[q.peekFirst()] + (i-j+1) * sum) &gt; m) &#123; sum -= y[j]; if (q.peekFirst() == j) &#123; q.removeFirst(); &#125; j++; &#125; ans = Math.max(ans,i - j + 1); &#125; return ans; &#125;&#125; DP状态机模型抢家劫舍为例 统计1-n 每一个数1出现的个数 13 : 1 2 3 4 5 6 7 8 9 10, 11, 12 ,13 答案为 6 n 为10 的8次方 剑指 Offer 13. 机器人的运动范围1234567891011121314151617181920212223242526272829303132333435class Solution &#123; int res; public int movingCount(int m, int n, int k) &#123; res = 0; //bfs,dfs(递归)四个方向 走回头路 坐标 boolean[][] vis = new boolean[m][n]; dfs(m,n,0,0,k,vis); return res; &#125; private void dfs(int m, int n, int i, int j, int k,boolean[][] vis)&#123; if(i&lt;0 || j&lt;0 || i==m || j==n) return ; if(!checkIj(i,j,k)) return ; if(vis[i][j]) return; //干脆不走回头路 res++; vis[i][j] = true; //缩小了规模 总有一天会跳出 dfs(m,n,i+1,j,k,vis); dfs(m,n,i,j+1,k,vis); //下面这两步回溯会帮你干 // dfs(m,n,i-1,j,k,vis); // dfs(m,n,i,j-1,k,vis); &#125; //判断下标的数位之和 private boolean checkIj(int i, int j, int k)&#123; int comp = 0; while(i != 0 || j != 0)&#123; comp += i%10; comp += j%10; j /= 10; i /= 10; &#125; return comp &lt;= k; &#125;&#125; 329. 矩阵中的最长递增路径 1234567891011121314151617181920212223242526272829class Solution &#123; static final int[][] dirs = new int[][]&#123;&#123;1,0&#125;,&#123;-1,0&#125;,&#123;0,-1&#125;,&#123;0,1&#125;&#125;; public int longestIncreasingPath(int[][] matrix) &#123; // dfs 记忆化 int n = matrix.length; int m = matrix[0].length; int ans = 1; int[][] memo = new int[n][m]; for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; m; j++) &#123; ans = Math.max(ans,dfs(matrix,memo,i,j)); &#125; return ans; &#125; private int dfs(int[][] matrix, int[][] memo, int row, int col) &#123; if (memo[row][col] != 0) &#123; return memo[row][col]; &#125; memo[row][col]++; int n = matrix.length; int m = matrix[0].length; for (int[] dir : dirs) &#123; int newRow = row + dir[0], newCol = col + dir[1]; if (newRow &lt; 0 || newRow == n || newCol &lt; 0 || newCol == m || matrix[newRow][newCol] &lt;= matrix[row][col]) continue; memo[row][col] = Math.max(memo[row][col],dfs(matrix,memo,newRow,newCol) + 1); &#125; return memo[row][col]; &#125;&#125; 514. 自由之路12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; int[][] memo; Map&lt;Character,List&lt;Integer&gt;&gt; map; public int findRotateSteps(String ring, String key) &#123; //逆转顺转 每一步都最短 是不行的,比如DI 需要知道所有代价 map = new HashMap&lt;&gt;(); int n = ring.length(); for (int i = 0; i &lt; n; i++) &#123; char c = ring.charAt(i); if (map.containsKey(c)) &#123; map.get(c).add(i); &#125; else &#123; map.put(c,new ArrayList&lt;&gt;()); map.get(c).add(i); &#125; &#125; memo = new int[ring.length()][key.length()]; return dp(ring,key,0,0); &#125; //i 当前12点 j 需要输入的字符位置 返回 包括j之后的最优 public int dp(String ring, String key, int i, int j) &#123; if (j == key.length()) return 0; if (memo[i][j] != 0) return memo[i][j]; int best = Integer.MAX_VALUE; char c = key.charAt(j); for (int index : map.get(c)) &#123; int next = dp(ring,key,index,j+1); //处理循环,因为next 固定 只需要找到最小的dealt 分支是由for循环完成的 int delta = (int)Math.abs(index - i); delta = Math.min(delta,ring.length() - delta); best = Math.min(best,delta + 1 + next); &#125; memo[i][j] = best; return best; &#125;&#125; 图 图这种数据结构有⼀些⽐较特殊的算法，⽐如⼆分图判断，有环图⽆环图的判断，拓扑排序，以及最经典的最⼩⽣成树，单源最短路径问题，更难的就是类似⽹络流这样的问题。不过以我的经验呢，像⽹络流这种问题，你⼜不是打竞赛的，没时间的话就没必要学了；像 最⼩⽣成树 和最短路径问题，虽然从刷题的⻆度⽤到的不多，但它们属于经典算法，学有余⼒可以掌握⼀下；像 ⼆分图判定、拓扑排序这⼀类，属于⽐较基本且有⽤的算法，应该⽐较熟练地掌握. 建图邻接表邻接矩阵环检测算法 dfs即可 拓扑排序的一部分 bfs也可 拓扑排序 两种方式DFS与BFS DFS模板现在你总共有 numCourses 门课需要选，记为 0 到 numCourses - 1。给你一个数组 prerequisites ，其中 prerequisites[i] &#x3D; [ai, bi] ，表示在选修课程 ai 前 必须 先选修 bi 。 例如，想要学习课程 0 ，你需要先完成课程 1 ，我们用一个匹配来表示：[0,1] 。返回你为了学完所有课程所安排的学习顺序。可能会有多个正确的顺序，你只要返回 任意一种 就可以了。如果不可能完成所有课程，返回 一个空数组 。 示例 1： 输入：numCourses &#x3D; 2, prerequisites &#x3D; [[1,0]]输出：[0,1]解释：总共有 2 门课程。要学习课程 1，你需要先完成课程 0。因此，正确的课程顺序为 [0,1] 。示例 2： 输入：numCourses &#x3D; 4, prerequisites &#x3D; [[1,0],[2,0],[3,1],[3,2]]输出：[0,2,1,3]解释：总共有 4 门课程。要学习课程 3，你应该先完成课程 1 和课程 2。并且课程 1 和课程 2 都应该排在课程 0 之后。因此，一个正确的课程顺序是 [0,1,2,3] 。另一个正确的排序是 [0,2,1,3] 。示例 3： 输入：numCourses &#x3D; 1, prerequisites &#x3D; []输出：[0] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; List&lt;Integer&gt; seq; public int[] findOrder(int numCourses, int[][] prerequisites) &#123; topo(prerequisites,numCourses); int n = seq.size(); int[] ans = new int[n]; for (int i = 0; i &lt; n; i++) &#123; ans[i] = seq.get(i)-1; &#125; if (loop) return new int[]&#123;&#125;; return ans; &#125; List&lt;Integer&gt; g[]; boolean[] instk; boolean[] vis; boolean loop; public void topo(int[][] edges, int n) &#123; g = new List[n+1]; for (int i = 1; i &lt;= n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[0]+1].add(edge[1]+1); &#125; instk = new boolean[n+1]; vis = new boolean[n+1]; seq = new ArrayList&lt;&gt;(); //防止不连通 for (int i = 1; i &lt;= n; i++) &#123; dfs(i); if (loop) break; &#125; &#125; public void dfs(int root) &#123; if (vis[root]) &#123; if (instk[root]) &#123; loop = true; &#125; return ; &#125; instk[root] = vis[root] = true; for (int node : g[root]) &#123; dfs(node); &#125; seq.add(root); instk[root] = false; &#125;&#125; BFS模板123456789101112131415161718192021222324252627282930313233343536373839public int[] findOrder(int n, int[][] edges) &#123; if (!topo(edges,n)) return new int[]&#123;&#125;; return seq; &#125; List&lt;Integer&gt; g[]; // d[i] 存储 结点i的入度 int[] d; int[] seq; public boolean topo(int[][] edges, int n) &#123; g = new List[n]; d = new int[n]; seq = new int[n]; for (int i = 0; i&lt; n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[1]].add(edge[0]); d[edge[0]] ++; &#125; Deque&lt;Integer&gt; q = new ArrayDeque&lt;&gt;(); for (int i = 0; i &lt; n; i++) &#123; if (d[i] == 0) &#123; q.offer(i); &#125; &#125; int index = 0; while (!q.isEmpty()) &#123; int t = q.poll(); seq[index++] = t; for (int node : g[t]) &#123; if (--d[node] == 0) &#123; q.offer(node); &#125; &#125; &#125; return index == n; &#125;&#125; 直观地说就是，让你把⼀幅图「拉平」，⽽且这个「拉平」的图⾥⾯，所有箭头⽅向都是⼀致的，⽐如上图 所有箭头都是朝右的。 很显然，如果⼀幅有向图中存在环，是⽆法进⾏拓扑排序的，因为肯定做不到所有箭头⽅向⼀致；反过来， 如果⼀幅图是「有向⽆环图」，那么⼀定可以进⾏拓扑排序。 拓扑排序是正常建有向图后序遍历的逆 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657class Solution &#123; public int[][] buildMatrix(int k, int[][] rowConditions, int[][] colConditions) &#123; int[][] ans = new int[k][k]; int[] xs = new int[k + 1]; int[] ys = new int[k + 1]; List&lt;Integer&gt; rows = topo(rowConditions, k); List&lt;Integer&gt; cols = topo(colConditions, k); if(rows == null || cols == null) &#123; return new int[0][]; &#125; for(int i = 0; i &lt; k; i++) &#123; xs[rows.get(i)] = i; ys[cols.get(i)] = i; &#125; for(int i = 1; i &lt;= k; i++) &#123; ans[xs[i]][ys[i]] = i; &#125; return ans; &#125; List&lt;Integer&gt;[] g; boolean[] instk; boolean[] visited; List&lt;Integer&gt; seq; boolean loop; public void dfs(int root) &#123; if(visited[root]) &#123; if(instk[root]) &#123; loop = true; &#125; return; &#125; visited[root] = instk[root] = true; for(int node : g[root]) &#123; dfs(node); &#125; seq.add(root); instk[root] = false; &#125; public List&lt;Integer&gt; topo(int[][] edges, int n) &#123; loop = false; g = new List[n + 1]; for(int i = 0; i &lt;= n; i++) &#123; g[i] = new ArrayList(); &#125; for(int[] e : edges) &#123; g[e[1]].add(e[0]); &#125; instk = new boolean[n + 1]; visited = new boolean[n + 1]; seq = new ArrayList(); for(int i = 1; i &lt;= n; i++) &#123; dfs(i); &#125; return loop ? null : seq; &#125;&#125; 二分图最小生成树克鲁斯卡尔 运用并查集最短路径743. 网络延迟时间难度中等596收藏分享切换为英文接收动态反馈 有 n 个网络节点，标记为 1 到 n。 给你一个列表 times，表示信号经过 有向 边的传递时间。 times[i] = (ui, vi, wi)，其中 ui 是源节点，vi 是目标节点， wi 是一个信号从源节点传递到目标节点的时间。 现在，从某个节点 K 发出一个信号。需要多久才能使所有节点都收到信号？如果不能使所有节点收到信号，返回 -1 。 示例 1： 12输入：times = [[2,1,1],[2,3,1],[3,4,1]], n = 4, k = 2输出：2 示例 2： 12输入：times = [[1,2,1]], n = 2, k = 1输出：1 示例 3： 12输入：times = [[1,2,1]], n = 2, k = 2输出：-1 提示： 1 &lt;= k &lt;= n &lt;= 100 1 &lt;= times.length &lt;= 6000 times[i].length == 3 1 &lt;= ui, vi &lt;= n ui != vi 0 &lt;= wi &lt;= 100 所有 (ui, vi) 对都 互不相同（即，不含重复边） dijkstra朴素1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class Solution &#123; int[][] g; boolean[] st; int[] dist; int INF = Integer.MAX_VALUE / 2; public int networkDelayTime(int[][] times, int n, int k) &#123; g = new int[n+1][n+1]; for (int i = 1; i &lt;= n; i++) &#123; for (int j = 1; j &lt;= n; j++) &#123; g[i][j] = g[j][i] = i == j ? 0 : INF; &#125; &#125; for (int[] t : times) &#123; g[t[0]][t[1]] = t[2]; &#125; dijkstra(n,k); int ans = -1; for (int i = 1; i &lt;= n; i++) &#123; if (dist[i] == INF) return -1; ans = Math.max(ans,dist[i]); &#125; return ans; &#125; public void dijkstra(int n, int k) &#123; dist = new int[n+1]; st = new boolean[n+1]; Arrays.fill(dist,INF); dist[k] = 0; for (int i = 1; i &lt;= n; i++) &#123; int t = -1; for (int j = 1; j &lt;= n; j++) &#123; if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) &#123; t = j; &#125; &#125; st[t] = true; for (int j = 1; j &lt;= n; j++) &#123; dist[j] = Math.min(dist[j],dist[t] + g[t][j]); &#125; &#125; &#125;&#125; 优化1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123; int INF = Integer.MAX_VALUE / 2; public int networkDelayTime(int[][] times, int n, int k) &#123; //给的是边加边权重,n个网络结点 int[] dis = dij(times,n,k); int ans = Integer.MIN_VALUE; for (int i = 1; i &lt;= n ; i++) &#123; if (dis[i] == INF) &#123; return -1; &#125; ans = Math.max(dis[i],ans); &#125; return ans; &#125; int[] dist; List&lt;int[]&gt;[] g; boolean[] st; public int[] dij(int[][] edges, int n, int start) &#123; g = new List[n+1]; st = new boolean[n+1]; for (int i = 0; i &lt;= n; i++) &#123; g[i] = new ArrayList&lt;&gt;(); &#125; for (int[] edge : edges) &#123; g[edge[0]].add(new int[]&#123;edge[1],edge[2]&#125;); &#125; dist = new int[n+1]; // g是 n+1 Arrays.fill(dist, INF); Queue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a,b)-&gt;&#123; return a[1] - b[1]; &#125;); dist[start] = 0; pq.offer(new int[]&#123;start,0&#125;); while (!pq.isEmpty()) &#123; int[] cur = pq.poll(); int t = cur[0]; int distance = cur[1]; if (st[t]) continue; st[t] = true; for (int[] next : g[t]) &#123; int j = next[0]; int d = distance + next[1]; if (dist[j] &gt; d) &#123; dist[j] = d; pq.offer(new int[]&#123;j,d&#125;); &#125; &#125; &#125; return dist; &#125;&#125; 树状数组 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution &#123; public int reversePairs(int[] nums) &#123; // 离散化 只记录rank int res = 0; int n = nums.length; int[] tmp = new int[n]; System.arraycopy(nums, 0, tmp, 0, n); // 离散化 Arrays.sort(tmp); for (int i = 0; i &lt; n; ++i) &#123; nums[i] = Arrays.binarySearch(tmp, nums[i]) + 1; &#125; // 树状数组统计逆序对 Bit bit = new Bit(n); for (int i = 0; i &lt; n; i++) &#123; res += bit.query(n) - bit.query(nums[i]); bit.add(nums[i],1); &#125; return res; &#125; class Bit &#123; int[] tree; int n; Bit(int n) &#123; this.n = n; this.tree = new int[n+1]; // 下标0 忽略不用 &#125; public int lowbit(int x) &#123; return x &amp; (-x); &#125; // 更新前缀和 自下向上 // 子节点到父节点 只要+ lowbit 反之 则 - public void add(int x, int v) &#123; for (int i = x; i &lt;= n; i += lowbit(i)) tree[i] += v; &#125; // 比如查询 5 则查询的是比5大的出现的个数 // 查询的是 5的前缀和 public int query(int x) &#123; int ret = 0; for (int i = x; i &gt; 0; i -= lowbit(i)) ret += tree[i]; return ret; &#125; &#125;&#125; https://www.bilibili.com/read/cv9904414?spm_id_from=333.999.0.0 计划 ​ 二叉树 ​ 回溯 ​ dp ​ 双指针 复写0 二叉树94. 二叉树的中序遍历12345678910111213141516171819class Solution &#123; public List&lt;Integer&gt; inorderTraversal(TreeNode root) &#123; Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); TreeNode p = root; while(p != null || !stack.isEmpty() )&#123; //左根右 while(p!=null)&#123; stack.addLast(p); p = p.left; &#125; TreeNode cur = stack.removeLast(); res.add(cur.val); p = cur.right; &#125; return res; &#125;&#125; 1234567891011121314while(p != null || !stack.isEmpty())&#123; while(p != null )&#123; //res.add(p.val) 前序 stack.addLast(p); p = p.left; //p = p.right &#125; TreeNode cur = stack.removeLast(); //res.add(cur.val);中序 p = cur.right; //p = cur.left;&#125;//Collections.reverse(res);// return res; //后序 左右根 根右左 98 验证二叉搜索树 123456789101112131415161718192021222324252627282930313233343536373839class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //中序遍历 是从小到大 TreeNode p = root; ArrayDeque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); double pre = Double.MAX_VALUE; while(p!=null || !stack.isEmpty())&#123; while(p!=null)&#123; stack.push(p); p = p.left; &#125; TreeNode cur = stack.pop(); if(pre!=Double.MAX_VALUE &amp;&amp; cur.val&lt;=pre) return false; pre = (double)cur.val; p = cur.right; &#125; return true; &#125;&#125;//递归class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //递归 一个子树是否是二叉树 如果自身是left 就要小于父节点之上的 最小值 // right 大于父节点 之上的最大值 // return isValidBSTHelp(root,Long.MIN_VALUE,Long.MAX_VALUE); &#125; private boolean isValidBSTHelp(TreeNode root , long min , long max)&#123; if(root == null ) return true; if(root.val &lt;= min || root.val &gt;= max) return false; boolean l = isValidBSTHelp(root.left,min,root.val); boolean r = isValidBSTHelp(root.right,root.val,max); return l &amp;&amp; r; &#125; &#125; 123456789101112131415161718192021222324252627282930思路错误 丢失了 传递信息 需要将上上层的也记住 而不是只是当前值 最大值最小值得 改变 需要写到一起 class Solution &#123; public boolean isValidBST(TreeNode root) &#123; //递归 一个子树是否是二叉树 如果自身是left 就要小于父节点 // right 大于父节点 // boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l&amp;&amp;r; &#125; private boolean isValidBSTHelpL(TreeNode root , int max)&#123; if(root == null) return true; if(root.val &gt;= max) return false; boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l &amp;&amp; r; &#125; private boolean isValidBSTHelpR(TreeNode root , int min)&#123; if(root == null) return true; if(root.val &lt;= min) return false; boolean l = isValidBSTHelpL(root.left,root.val); boolean r = isValidBSTHelpR(root.right,root.val); return l &amp;&amp; r; &#125;&#125; 递归 ​ 信息的传递 ,需要的信息 和需要传递的信息 非递归 ​ 两种方式的差别 可以提前跳出 非递归的 对集合的操作配套使用 144. 二叉树的前序遍历12345678910111213141516171819202122232425262728293031323334class Solution &#123; public List&lt;Integer&gt; preorderTraversal(TreeNode root) &#123; //递归栈 Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); TreeNode p = root; // preorder(root,res); while(!stack.isEmpty() || p != null)&#123; //根左右 while(p!=null)&#123; res.add(p.val); stack.addLast(p); p = p.left; &#125; // if(stack.isEmpty())&#123; // break; // &#125;else&#123; TreeNode cur = stack.removeLast(); p = cur.right; // &#125; &#125; //代码简化 只要进入while stack必不为空 return res; &#125; // private void preorder(TreeNode root ,List&lt;Integer&gt; res)&#123; // if(root == null) return ; // res.add(root.val); // preorder(root.left,res); // preorder(root.right,res); // &#125;&#125; 99.恢复搜索二叉树 12345678910111213141516171819202122232425262728293031323334353637383940class Solution &#123; public void recoverTree(TreeNode root) &#123; //用中序遍历 如果 遇到大于的则进行处理 栈 // 1 2 3 4 5 6 7 // 相邻或者不相邻 2 1 3 3 2 1 // 处理 遇到第一个错误的时候 err1 = pre err2 = cur 遇到第二个错误 err2 = cur Deque&lt;TreeNode&gt; stack = new ArrayDeque&lt;&gt;(); TreeNode err1 = null; TreeNode err2 = null; TreeNode p = root; TreeNode pre = null; // stack.addLast(root); while(p != null || !stack.isEmpty())&#123; while( p != null)&#123; stack.addLast(p); p = p.left; &#125; //少了 判左右 TreeNode cur = stack.removeLast(); if(pre != null &amp;&amp; pre.val &gt;= cur.val)&#123; if(err1 == null)&#123; err1 = pre; err2 = cur; &#125;else&#123; err2 = cur; break; &#125; &#125; pre = cur; p = cur.right; &#125; int temp = err1.val; err1.val = err2.val; err2.val = temp; &#125; &#125; 257.二叉树的所有路径 1234567891011121314151617181920212223class Solution &#123; public List&lt;String&gt; binaryTreePaths(TreeNode root) &#123; List&lt;String&gt; res = new ArrayList&lt;&gt;(); StringBuilder sb = new StringBuilder(); backtrack(root,res,sb); return res; &#125; private void backtrack(TreeNode root , List&lt;String&gt; res , StringBuilder sb)&#123; if(root == null ) return ; if(root.left == null &amp;&amp; root.right == null )&#123; sb.append(root.val); res.add(sb.toString()); return ; &#125; sb.append(root.val).append(&quot;-&gt;&quot;); backtrack(root.left,res,new StringBuilder(sb)); backtrack(root.right,res,new StringBuilder(sb)); &#125;&#125; 95.不同的二叉搜索树 123456789101112131415161718192021222324252627282930313233class Solution &#123; public List&lt;TreeNode&gt; generateTrees(int n) &#123; if(n == 0) return new ArrayList&lt;TreeNode&gt;(); return help(1,n); &#125; private List&lt;TreeNode&gt; help(int start , int end)&#123; List&lt;TreeNode&gt; res = new ArrayList&lt;&gt;(); if(start&gt;end)&#123; res.add(null); return res; &#125; if(start == end)&#123; res.add(new TreeNode(start)); return res; &#125; for(int i = start ; i &lt;= end ; i++)&#123; List&lt;TreeNode&gt; leftt = help(start,i-1); List&lt;TreeNode&gt; rightt = help(i+1,end); for(TreeNode l : leftt)&#123; for(TreeNode r : rightt)&#123; TreeNode root = new TreeNode(i); root.left = l; root.right = r; res.add(root); &#125; &#125; &#125; return res; &#125;&#125; 96. 不同二叉搜索树 1234567891011121314151617181920class Solution &#123; public int numTrees(int n) &#123; int[] dp = new int[n+1]; return help(1,n,dp); &#125; private int help(int start, int end,int[] dp)&#123; int res = 0; if(start &gt;= end ) return 1; if(dp[end-start]!=0) return dp[end-start]; for(int i = start ; i &lt;= end ; i++)&#123; int l = help(start,i-1,dp); int r = help(i+1,end,dp); res += l*r; dp[end-start] = res; &#125; return res; &#125;&#125; 101. 对称的二叉树123456789101112class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; return isSymmetric(root.left,root.right); &#125; public boolean isSymmetric(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null) return true; if (p == null || q == null) return false; if (p.val != q.val) return false; return isSymmetric(p.left,q.right) &amp;&amp; isSymmetric(p.right,q.left); &#125;&#125; 124.二叉树中的最大路径和 12345678910111213141516171819class Solution &#123; int max = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) &#123; // 左右子树的 最大路径和 是否包含根节点 //区分的操作 !!! 携带信息带根 结果判断是否带根 //为什么全 :对每个节点都假设存在于最大边 // 则要不左右单边(包含只有自己) 或者 左右相连 postOrder(root); return max; &#125; private int postOrder(TreeNode root)&#123; if(root == null ) return 0; int l = Math.max(postOrder(root.left),0); int r = Math.max(postOrder(root.right),0); max = Math.max(max,l+r+root.val); return root.val + Math.max(l,r); &#125;&#125; 235二叉搜索树的最近公共祖先,236二叉树的~12345678910111213141516171819202122232425262728293031323334class Solution &#123; //运用二叉搜索树性质 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; // if(p.val == q.val ) return p; if(root.val==q.val || root.val == p.val) return root; TreeNode gt = p.val &gt; q.val ? p : q; TreeNode lt = p.val &lt; q.val ? p : q; if(lt.val&lt;root.val &amp;&amp; gt.val &gt; root.val) return root; else if(lt.val &gt; root.val)&#123; return lowestCommonAncestor(root.right,p,q); &#125;else&#123; return lowestCommonAncestor(root.left,p,q); &#125; &#125;&#125;class Solution &#123; public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) &#123; if(root== null) return null; if(root==p || root==q) return root; //这条好好思考,返回值 TreeNode left = lowestCommonAncestor(root.left,p,q); //pq的位置 TreeNode right = lowestCommonAncestor(root.right,p,q); if(left!=null&amp;&amp;right!=null)&#123; // 在左右两棵子树上 return root; &#125;else if(left!=null)&#123; //只在一颗子树 上 return left; &#125;else&#123; return right; &#125; &#125;&#125; 剑指offer28. 对称的二叉树123456789101112class Solution &#123; public boolean isSymmetric(TreeNode root) &#123; if (root == null) return true; return isSymmetric(root.left,root.right); &#125; public boolean isSymmetric(TreeNode p, TreeNode q) &#123; if (p == null &amp;&amp; q == null) return true; if (p == null || q == null) return false; if (p.val != q.val) return false; return isSymmetric(p.left,q.right) &amp;&amp; isSymmetric(p.right,q.left); &#125;&#125; DFS&#x2F;BFSBFS ​ 优化双向 板子1 回溯决策树 选择链表 改变选择链表 优化 去重 板子78. 子集 可重复12345678910111213141516171819202122232425class Solution &#123; List&lt;List&lt;Integer&gt;&gt; res; List&lt;Integer&gt; subset; public List&lt;List&lt;Integer&gt;&gt; subsets(int[] nums) &#123; res = new ArrayList&lt;&gt;(); subset = new ArrayList&lt;&gt;(); res.add(new ArrayList&lt;&gt;()); backtrack(nums,0); return res; &#125; private void backtrack(int[] nums, int start)&#123; // int sz = nums.length; 只用一次优化掉 // if(start&gt;sz-1) return ; for循环会带 for(int i = start ; i &lt; nums.length ; i++)&#123; subset.add(nums[i]); //每一次改变 res.add(new ArrayList&lt;&gt;(subset)); backtrack(nums,i+1); subset.remove(subset.size()-1); &#125; &#125;&#125;subsetsWithDup 90 子集2 12345678910111213141516171819202122232425class Solution &#123; List&lt;List&lt;Integer&gt;&gt; res; List&lt;Integer&gt; subset; public List&lt;List&lt;Integer&gt;&gt; subsetsWithDup(int[] nums) &#123; res = new ArrayList&lt;&gt;(); subset = new ArrayList&lt;&gt;(); res.add(new ArrayList&lt;&gt;()); Arrays.sort(nums); backtrack(nums,0); return res; &#125; private void backtrack(int[] nums, int start)&#123; //重复情况 1223 123 123 下层和上层可以同步 本层不能一样 同组合总数 for(int i = start ; i &lt; nums.length ; i++)&#123; if(i!=start&amp;&amp;i!=0&amp;&amp;nums[i]==nums[i-1]) continue; subset.add(nums[i]); //每一次改变 res.add(new ArrayList&lt;&gt;(subset)); backtrack(nums,i+1); subset.remove(subset.size()-1); &#125; &#125;&#125; ​ ​ 线段树二进制001. 整数除法 难度简单189收藏分享切换为英文接收动态反馈 给定两个整数 a 和 b ，求它们的除法的商 a/b ，要求不得使用乘号 &#39;*&#39;、除号 &#39;/&#39; 以及求余符号 &#39;%&#39; 。 注意： 整数除法的结果应当截去（truncate）其小数部分，例如：truncate(8.345) = 8 以及 truncate(-2.7335) = -2 假设我们的环境只能存储 32 位有符号整数，其数值范围是 [−231, 231−1]。本题中，如果除法结果溢出，则返回 231 − 1 示例 1： 123输入：a = 15, b = 2输出：7解释：15/2 = truncate(7.5) = 7 示例 2： 123输入：a = 7, b = -3输出：-2解释：7/-3 = truncate(-2.33333..) = -2 示例 3： 12输入：a = 0, b = 1输出：0 示例 4： 12输入：a = 1, b = 1输出：1 12345678910111213141516171819202122232425262728// 用 - 法 22/3(从大到小测试) = 7(都可以用二进制表示,)...1class Solution &#123; public int divide(int a, int b) &#123; // //二进制模拟除法 &amp; | ^ &gt;&gt; &lt;&lt; 正常左右移不动符号位 &gt;&gt;&gt;(移动符号位) // 用 - 法 22/3(从大到小测试) = 7(都可以用二进制表示,)...1 // 都转化为整数 考虑特殊情况 MIN_VALUE // 3 &lt;&lt; 31 位的话存在越界 用 a &gt;&gt; int sign = (a &gt; 0) ^ (b &gt; 0) ? -1 : 1; if(a == Integer.MIN_VALUE &amp;&amp; b == -1) return Integer.MAX_VALUE; a = Math.abs(a); // abs(MIN)还为MIN 右移采用 &gt;&gt;&gt; 变为正数进行处理 b = Math.abs(b); // 如果 MIN // 如果都是正数 // 减法代替除法 int res = 0; for(int i = 31 ; i &gt;= 0 ; i--) &#123; // i == 0 时 变成 MIN // 从最大的二进制开始减 // 比如 22 / 3 // 20 - (3 &lt;&lt;2) res += (1&lt;&lt;2) 10 - 6 res+=(1&lt;&lt;1) 4 - 3 = 1 res+=(1&lt;&lt;0) if((a &gt;&gt;&gt; i) - b &gt;= 0) &#123; a -= (b &lt;&lt; i); res += (1 &lt;&lt; i); &#125; &#125; return sign == 1 ? res : -res; &#125;&#125; 002. 二进制加法1234567891011121314151617181920212223242526class Solution &#123; public String addBinary(String a, String b) &#123; // 处理进位 三个数相加 更新 进位的值 1 || 0 int len1 = a.length(); int len2 = b.length(); int i = len1; int j = len2; int more = 0; StringBuilder sb = new StringBuilder(); while(i &gt; 0 || j &gt; 0 || more &gt; 0) &#123; int a1 = 0; int b1 = 0; if(i &gt; 0) &#123; a1 = a.charAt(--i) - &#x27;0&#x27;; &#125; if(j &gt; 0) &#123; b1 = b.charAt(--j) - &#x27;0&#x27;; &#125; int temp = a1 + b1 + more; sb.append(temp % 2); more = temp / 2; &#125; return sb.reverse().toString(); &#125;&#125; 003. 前 n 个数字二进制中 1 的个数12345678910class Solution &#123; public int[] countBits(int n) &#123; // 如果知道 i &lt;&lt; 1 的个数 那么 i的个数就是 i&amp;1 + int[] ans = new int[n+1]; for (int i = 1; i &lt;= n; i++) &#123; ans[i] = ans[i&gt;&gt;1] + (i &amp; 1); &#125; return ans; &#125;&#125; 004. 只出现一次的数字 给你一个整数数组 nums ，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 。请你找出并返回那个只出现了一次的元素。 示例 1： 输入：nums &#x3D; [2,2,3,2]输出：3示例 2： 输入：nums &#x3D; [0,1,0,1,0,1,100]输出：100 提示： 1 &lt;&#x3D; nums.length &lt;&#x3D; 3 * 104-231 &lt;&#x3D; nums[i] &lt;&#x3D; 231 - 1nums 中，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 进阶：你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 123456789101112131415161718192021222324252627282930313233343536class Solution &#123; public int singleNumber(int[] nums) &#123; // 二进制 % 3 每一位 int[] num = new int[32]; for(int i = 0 ; i &lt; nums.length ; i++) &#123; for(int j = 0 ; j &lt; 32 ; j++) &#123; num[j] += nums[i] &amp; 1; nums[i] &gt;&gt;&gt;= 1; &#125; &#125; int res = 0; for(int i = 0 ; i &lt; 32 ; i++) &#123; num[i] %= 3; res += (num[i] &lt;&lt; i); &#125; return res; &#125;&#125;class Solution &#123; public int singleNumber(int[] nums) &#123; Map&lt;Integer, Integer&gt; freq = new HashMap&lt;Integer, Integer&gt;(); for (int num : nums) &#123; freq.put(num, freq.getOrDefault(num, 0) + 1); &#125; int ans = 0; for (Map.Entry&lt;Integer, Integer&gt; entry : freq.entrySet()) &#123; int num = entry.getKey(), occ = entry.getValue(); if (occ == 1) &#123; ans = num; break; &#125; &#125; return ans; &#125;&#125; 005.单词长度的最大乘积 给定一个字符串数组 words，请计算当两个字符串 words[i] 和 words[j] 不包含相同字符时，它们长度的乘积的最大值。假设字符串中只包含英语的小写字母。如果没有不包含相同字符的一对字符串，返回 0。 示例 1: 输入: words &#x3D; [“abcw”,”baz”,”foo”,”bar”,”fxyz”,”abcdef”]输出: 16解释: 这两个单词为 “abcw”, “fxyz”。它们不包含相同字符，且长度的乘积最大。示例 2: 输入: words &#x3D; [“a”,”ab”,”abc”,”d”,”cd”,”bcd”,”abcd”]输出: 4解释: 这两个单词为 “ab”, “cd”。示例 3: 输入: words &#x3D; [“a”,”aa”,”aaa”,”aaaa”]输出: 0解释: 不存在这样的两个单词。 提示： 2 &lt;&#x3D; words.length &lt;&#x3D; 10001 &lt;&#x3D; words[i].length &lt;&#x3D; 1000words[i] 仅包含小写字母 来源：力扣（LeetCode）链接：https://leetcode.cn/problems/aseY1I著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 123456789101112131415161718192021class Solution &#123; public int maxProduct(String[] words) &#123; int n = words.length; int[] mark = new int[n]; for (int i = 0; i &lt; n; i++) &#123; int m = words[i].length(); for (int j = 0; j &lt; m; j++) &#123; mark[i] |= (1 &lt;&lt; words[i].charAt(j) - &#x27;a&#x27;); &#125; &#125; int res = 0; for (int i = 0; i &lt; n; i++) &#123; for (int j = i+1; j &lt; n; j++) &#123; if ((mark[i] &amp; mark[j]) == 0) &#123; res = Math.max(words[i].length() * words[j].length(),res); &#125; &#125; &#125; return res; &#125;&#125; 双指针006. 双数之和007. 三数之和12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; threeSum(int[] nums) &#123; //先排序 固定一个 最小的要为&lt;=0(第一个不能重复) 最大的要为正 // 三个指针都要去重 int len = nums.length; Arrays.sort(nums); if(len == 0 || nums[len-1] &lt; 0) return new ArrayList&lt;&gt;(); int index = 0; List&lt;List&lt;Integer&gt;&gt; res = new ArrayList&lt;&gt;(); while(nums[index] &lt;= 0) &#123; if(index == len - 1) return res; if(index!=0 &amp;&amp; nums[index] == nums[index-1])&#123; index++; continue; &#125; int l = index + 1, r = len - 1; int target = -nums[index]; while(l &lt; r) &#123; int sum = nums[l] + nums[r]; if(sum == target) &#123; List&lt;Integer&gt; sub = new ArrayList&lt;&gt;(); sub.add(nums[index]); sub.add(nums[l]); sub.add(nums[r]); res.add(new ArrayList(sub)); r--; l++; while(l &lt; r &amp;&amp; nums[r] == nums[r+1]) &#123; r--; &#125; while(l &lt; r &amp;&amp; nums[l] == nums[l-1]) &#123; l++; &#125; &#125;else if(sum &gt; target) &#123; r--; &#125;else &#123; l++; &#125; &#125; index++; &#125; return res; &#125;&#125; 008. 和大于等于 target 的最短子数组123456789101112131415161718class Solution &#123; public int minSubArrayLen(int target, int[] nums) &#123; // 滑动窗口 每一步操作之后进行判断进行记录 一增一减 // 改变target 来变化 int n = nums.length; int res = Integer.MAX_VALUE; for (int i = 0, j = 0; i &lt; n; i++) &#123; target -= nums[i]; while (target &lt;= 0) &#123; res = Math.min(res,i - j + 1); target += nums[j++]; &#125; &#125; return res == Integer.MAX_VALUE ? 0 : res; &#125;&#125; 009. 乘积小于K的子数组1234567891011121314151617181920212223class Solution &#123; public int numSubarrayProductLessThanK(int[] nums, int k) &#123; // 滑动窗口 int len = nums.length; int l = 0, r = -1; int target = 1; int res = 0; while(r &lt; len -1) &#123; r++; int in = nums[r]; // if(in &lt; k) res++; target *= in; while(l &lt;= r &amp;&amp; target &gt;= k) &#123; int out = nums[l++]; target /= out; &#125; //固定右边 则所有子数组都符合 res += r - l + 1; &#125; return res; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[]},{"title":"设计模式","slug":"3design pattern","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:48:51.946Z","comments":true,"path":"2022/09/01/3design pattern/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/3design%20pattern/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mybatis随记","slug":"6Mybatis","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:55:17.784Z","comments":true,"path":"2022/09/01/6Mybatis/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/6Mybatis/","excerpt":"","text":"前类中查找一个方法快捷键: Ctrl + F12 常用注解1、@TableIdMyBatis-Plus在实现增删改查时，会默认将id作为主键列，并在插入数据时，默认基于雪花算法的策略生成id，这个雪花算法在这里就不明讲了。 当使用@TableId(value &#x3D; “id”)语句时，若实体类和表中表示主键的不是id，而是其他字段，例如代码中的uid，MyBatis-Plus会自动识别uid为主键列，否则就会报这样的错误： 当使用@TableId(value &#x3D; “id”,type &#x3D; IdType.AUTO)语句时，代表着使用数据库的自增策略，注意，该类型请确保数据库设置了id自增，否则无效！ 当然呢，@TableId的功能，也可以写在application.yml配置文件中，配置如下： 1234567891011mybatis-plus: global-config: banner: false db-config: # 配置MyBatis-Plus操作表的默认前缀 table-prefix: &quot;t_&quot; # 配置MyBatis-Plus的主键策略 id-type: auto # 配置MyBatis日志 configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 2、@TableFieldMyBatis-Plus在执行SQL语句时，要保证实体类中的属性名和表中的字段名一致，否则就会报错，语句@TableField(value &#x3D; “is_deleted”)代表着让数据库表中is_deleted与实体类中isDeleted字段名一样。 注意： 若实体类中的属性使用的是驼峰命名风格，而表中的字段使用的是下划线命名风格例如实体类属性userName，表中字段user_name，此时MyBatis-Plus会自动将下划线命名风格转化为驼峰命名风格 若实体类中的属性和表中的字段不满足上述条件，例如实体类属性name，表中字段username，此时需要在实体类属性上使用@TableField(“username”)设置属性所对应的字段名 3、@TableLogic在讲这个注解之前，我们先认识一下逻辑删除。 物理删除：真实删除，将对应数据从数据库中删除，之后查询不到此条被删除的数据 逻辑删除：假删除，将对应数据中代表是否被删除字段的状态修改为“被删除状态”，之后在数据库中仍旧能看到此条数据记录 使用场景：可以进行数据恢复 在我的数据库表中，is_delete为1时，代表着逻辑上的删除，is_delete为0时，表示没有删除 注解@TableLogic的使用，就代表着该类中的属性是逻辑删除的属性 注意： 在测试逻辑删除的时候，真正执行的是修改UPDATE t_user SET is_deleted&#x3D;1 WHERE id&#x3D;? AND is_deleted&#x3D;0 测试查询功能，被逻辑删除的数据默认不会被查询SELECT id,username AS name,age,email,is_deleted FROM t_user WHERE is_deleted&#x3D;0 在学习mybatis-plus分页插件的时候，我们需要配置拦截器，看代码： 123456789101112@Configurationpublic class MybatisPlusConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() &#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); interceptor.addInnerInterceptor (new PaginationInnerInterceptor(DbType.MYSQL)); return interceptor; &#125;&#125; 4、@Version在我们学习乐观锁的时候，肯定见过如下代码： 1234567891011121314151617181920212223@Data@TableName(&quot;t_product&quot;)public class Product &#123; private Long id; private String name; private Integer price; @Version private Integer version;&#125;@Configurationpublic class MybatisPlusConfig &#123; @Bean public MybatisPlusInterceptor mybatisPlusInterceptor() &#123; MybatisPlusInterceptor interceptor = new MybatisPlusInterceptor(); //分页插件 interceptor.addInnerInterceptor (new PaginationInnerInterceptor(DbType.MYSQL)); //乐观锁插件 interceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor()); return interceptor; &#125;&#125; 而这个注解@Version就是实现乐观锁的重要注解，当要更新数据库中的数据时，例如价格，version 就会加 1，如果where语句中的version版本不对，则更新失败。 CRUD 接口#Service CRUD 接口说明: 通用 Service CRUD 封装IService (opens new window)接口，进一步封装 CRUD 采用 get 查询单行 remove 删除 list 查询集合 page 分页 前缀命名方式区分 Mapper 层避免混淆， 泛型 T 为任意实体对象 建议如果存在自定义通用 Service 方法的可能，请创建自己的 IBaseService 继承 Mybatis-Plus 提供的基类 对象 Wrapper 为 条件构造器 #Save123456// 插入一条记录（选择字段，策略插入）boolean save(T entity);// 插入（批量）boolean saveBatch(Collection&lt;T&gt; entityList);// 插入（批量）boolean saveBatch(Collection&lt;T&gt; entityList, int batchSize); #参数说明 类型 参数名 描述 T entity 实体对象 Collection entityList 实体对象集合 int batchSize 插入批次数量 #SaveOrUpdate12345678// TableId 注解存在更新记录，否插入一条记录boolean saveOrUpdate(T entity);// 根据updateWrapper尝试更新，否继续执行saveOrUpdate(T)方法boolean saveOrUpdate(T entity, Wrapper&lt;T&gt; updateWrapper);// 批量修改插入boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList);// 批量修改插入boolean saveOrUpdateBatch(Collection&lt;T&gt; entityList, int batchSize); 12345678 #参数说明 类型 参数名 描述 T entity 实体对象 Wrapper updateWrapper 实体对象封装操作类 UpdateWrapper Collection entityList 实体对象集合 int batchSize 插入批次数量 #Remove12345678// 根据 entity 条件，删除记录boolean remove(Wrapper&lt;T&gt; queryWrapper);// 根据 ID 删除boolean removeById(Serializable id);// 根据 columnMap 条件，删除记录boolean removeByMap(Map&lt;String, Object&gt; columnMap);// 删除（根据ID 批量删除）boolean removeByIds(Collection&lt;? extends Serializable&gt; idList); 12345678 #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体包装类 QueryWrapper Serializable id 主键 ID Map&lt;String, Object&gt; columnMap 表字段 map 对象 Collection&lt;? extends Serializable&gt; idList 主键 ID 列表 #Update12345678910// 根据 UpdateWrapper 条件，更新记录 需要设置sqlsetboolean update(Wrapper&lt;T&gt; updateWrapper);// 根据 whereWrapper 条件，更新记录boolean update(T updateEntity, Wrapper&lt;T&gt; whereWrapper);// 根据 ID 选择修改boolean updateById(T entity);// 根据ID 批量更新boolean updateBatchById(Collection&lt;T&gt; entityList);// 根据ID 批量更新boolean updateBatchById(Collection&lt;T&gt; entityList, int batchSize); 12345678910 #参数说明 类型 参数名 描述 Wrapper updateWrapper 实体对象封装操作类 UpdateWrapper T entity 实体对象 Collection entityList 实体对象集合 int batchSize 更新批次数量 #Get12345678910// 根据 ID 查询T getById(Serializable id);// 根据 Wrapper，查询一条记录。结果集，如果是多个会抛出异常，随机取一条加上限制条件 wrapper.last(&quot;LIMIT 1&quot;)T getOne(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper，查询一条记录T getOne(Wrapper&lt;T&gt; queryWrapper, boolean throwEx);// 根据 Wrapper，查询一条记录Map&lt;String, Object&gt; getMap(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper，查询一条记录&lt;V&gt; V getObj(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper); 12345678910 #参数说明 类型 参数名 描述 Serializable id 主键 ID Wrapper queryWrapper 实体对象封装操作类 QueryWrapper boolean throwEx 有多个 result 是否抛出异常 T entity 实体对象 Function&lt;? super Object, V&gt; mapper 转换函数 #List1234567891011121314151617181920// 查询所有List&lt;T&gt; list();// 查询列表List&lt;T&gt; list(Wrapper&lt;T&gt; queryWrapper);// 查询（根据ID 批量查询）Collection&lt;T&gt; listByIds(Collection&lt;? extends Serializable&gt; idList);// 查询（根据 columnMap 条件）Collection&lt;T&gt; listByMap(Map&lt;String, Object&gt; columnMap);// 查询所有列表List&lt;Map&lt;String, Object&gt;&gt; listMaps();// 查询列表List&lt;Map&lt;String, Object&gt;&gt; listMaps(Wrapper&lt;T&gt; queryWrapper);// 查询全部记录List&lt;Object&gt; listObjs();// 查询全部记录&lt;V&gt; List&lt;V&gt; listObjs(Function&lt;? super Object, V&gt; mapper);// 根据 Wrapper 条件，查询全部记录List&lt;Object&gt; listObjs(Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录&lt;V&gt; List&lt;V&gt; listObjs(Wrapper&lt;T&gt; queryWrapper, Function&lt;? super Object, V&gt; mapper); 1234567891011121314151617181920 #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper Collection&lt;? extends Serializable&gt; idList 主键 ID 列表 Map&lt;String, Object&gt; columnMap 表字段 map 对象 Function&lt;? super Object, V&gt; mapper 转换函数 #Page12345678// 无条件分页查询IPage&lt;T&gt; page(IPage&lt;T&gt; page);// 条件分页查询IPage&lt;T&gt; page(IPage&lt;T&gt; page, Wrapper&lt;T&gt; queryWrapper);// 无条件分页查询IPage&lt;Map&lt;String, Object&gt;&gt; pageMaps(IPage&lt;T&gt; page);// 条件分页查询IPage&lt;Map&lt;String, Object&gt;&gt; pageMaps(IPage&lt;T&gt; page, Wrapper&lt;T&gt; queryWrapper); 12345678 #参数说明 类型 参数名 描述 IPage page 翻页对象 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper #Count1234// 查询总记录数int count();// 根据 Wrapper 条件，查询总记录数int count(Wrapper&lt;T&gt; queryWrapper); 1234 #参数说明 类型 参数名 描述 Wrapper queryWrapper 实体对象封装操作类 QueryWrapper #Chain#query12345678// 链式查询 普通QueryChainWrapper&lt;T&gt; query();// 链式查询 lambda 式。注意：不支持 KotlinLambdaQueryChainWrapper&lt;T&gt; lambdaQuery();// 示例：query().eq(&quot;column&quot;, value).one();lambdaQuery().eq(Entity::getId, value).list(); 12345678 #update12345678// 链式更改 普通UpdateChainWrapper&lt;T&gt; update();// 链式更改 lambda 式。注意：不支持 KotlinLambdaUpdateChainWrapper&lt;T&gt; lambdaUpdate();// 示例：update().eq(&quot;column&quot;, value).remove();lambdaUpdate().eq(Entity::getId, value).update(entity); 12345678 #Mapper CRUD 接口说明: 通用 CRUD 封装BaseMapper (opens new window)接口，为 Mybatis-Plus 启动时自动解析实体表关系映射转换为 Mybatis 内部对象注入容器 泛型 T 为任意实体对象 参数 Serializable 为任意类型主键 Mybatis-Plus 不推荐使用复合主键约定每一张表都有自己的唯一 id 主键 对象 Wrapper 为 条件构造器 #Insert12// 插入一条记录int insert(T entity); 12 #参数说明 类型 参数名 描述 T entity 实体对象 #Delete12345678// 根据 entity 条件，删除记录int delete(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; wrapper);// 删除（根据ID 批量删除）int deleteBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 ID 删除int deleteById(Serializable id);// 根据 columnMap 条件，删除记录int deleteByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap); 12345678 #参数说明 类型 参数名 描述 Wrapper wrapper 实体对象封装操作类（可以为 null） Collection&lt;? extends Serializable&gt; idList 主键 ID 列表(不能为 null 以及 empty) Serializable id 主键 ID Map&lt;String, Object&gt; columnMap 表字段 map 对象 #Update1234// 根据 whereWrapper 条件，更新记录int update(@Param(Constants.ENTITY) T updateEntity, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; whereWrapper);// 根据 ID 修改int updateById(@Param(Constants.ENTITY) T entity); 1234 #参数说明 类型 参数名 描述 T entity 实体对象 (set 条件值,可为 null) Wrapper updateWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） #Select12345678910111213141516171819202122// 根据 ID 查询T selectById(Serializable id);// 根据 entity 条件，查询一条记录T selectOne(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据ID 批量查询）List&lt;T&gt; selectBatchIds(@Param(Constants.COLLECTION) Collection&lt;? extends Serializable&gt; idList);// 根据 entity 条件，查询全部记录List&lt;T&gt; selectList(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 查询（根据 columnMap 条件）List&lt;T&gt; selectByMap(@Param(Constants.COLUMN_MAP) Map&lt;String, Object&gt; columnMap);// 根据 Wrapper 条件，查询全部记录List&lt;Map&lt;String, Object&gt;&gt; selectMaps(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录。注意： 只返回第一个字段的值List&lt;Object&gt; selectObjs(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 entity 条件，查询全部记录（并翻页）IPage&lt;T&gt; selectPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询全部记录（并翻页）IPage&lt;Map&lt;String, Object&gt;&gt; selectMapsPage(IPage&lt;T&gt; page, @Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper);// 根据 Wrapper 条件，查询总记录数Integer selectCount(@Param(Constants.WRAPPER) Wrapper&lt;T&gt; queryWrapper); 12345678910111213141516171819202122 #参数说明 类型 参数名 描述 Serializable id 主键 ID Wrapper queryWrapper 实体对象封装操作类（可以为 null） Collection&lt;? extends Serializable&gt; idList 主键 ID 列表(不能为 null 以及 empty) Map&lt;String, Object&gt; columnMap 表字段 map 对象 IPage page 分页查询条件（可以为 RowBounds.DEFAULT） #mapper 层 选装件说明: 选装件位于 com.baomidou.mybatisplus.extension.injector.methods 包下 需要配合Sql 注入器使用,案例(opens new window)使用详细见源码注释(opens new window) #AlwaysUpdateSomeColumnById(opens new window)1int alwaysUpdateSomeColumnById(T entity); 1 #insertBatchSomeColumn(opens new window)1int insertBatchSomeColumn(List&lt;T&gt; entityList); 1 #logicDeleteByIdWithFill(opens new window)1int logicDeleteByIdWithFill(T entity); 1 #ActiveRecord 模式说明: 实体类只需继承 Model 类即可进行强大的 CRUD 操作 需要项目中已注入对应实体的BaseMapper #操作步骤： 继承 Model(opens new window) 123class User extends Model&lt;User&gt;&#123; // fields...&#125; 123 调用CRUD方法(演示部分api，仅供参考) 123456User user = new User();user.insert();user.selectAll();user.updateById();user.deleteById();// ... 123456 #SimpleQuery 工具类说明: 对selectList查询后的结果用Stream流进行了一些封装，使其可以返回一些指定结果，简洁了api的调用 需要项目中已注入对应实体的BaseMapper 使用方式见: 测试用例(opens new window) 对于下方参数peeks，其类型为Consumer...，可一直往后叠加操作例如：List&lt;Long&gt; ids = SimpleQuery.list(Wrappers.lambdaQuery(), Entity::getId, System.out::println, user -&gt; userNames.add(user.getName())); #keyMap1234// 查询表内记录，封装返回为Map&lt;属性,实体&gt;Map&lt;A, E&gt; keyMap(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,实体&gt;，考虑了并行流的情况Map&lt;A, E&gt; keyMap(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, boolean isParallel, Consumer&lt;E&gt;... peeks); 1234 #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是map中key的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 实体中属性的getter,用于封装后map中作为key的条件 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #map1234// 查询表内记录，封装返回为Map&lt;属性,属性&gt;Map&lt;A, P&gt; map(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; keyFunc, SFunction&lt;E, P&gt; valueFunc, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,属性&gt;，考虑了并行流的情况Map&lt;A, P&gt; map(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; keyFunc, SFunction&lt;E, P&gt; valueFunc, boolean isParallel, Consumer&lt;E&gt;... peeks); 1234 #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是map中key的类型 P attribute 实体属性类型,也是map中value的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; keyFunc 封装后map中作为key的条件 SFunction&lt;E, P&gt; valueFunc 封装后map中作为value的条件 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #group12345678// 查询表内记录，封装返回为Map&lt;属性,List&lt;实体&gt;&gt;Map&lt;K, List&lt;T&gt;&gt; group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, A&gt; sFunction, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,List&lt;实体&gt;&gt;，考虑了并行流的情况Map&lt;K, List&lt;T&gt;&gt; group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, boolean isParallel, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,分组后对集合进行的下游收集器&gt;M group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, Collector&lt;? super T, A, D&gt; downstream, Consumer&lt;T&gt;... peeks);// 查询表内记录，封装返回为Map&lt;属性,分组后对集合进行的下游收集器&gt;，考虑了并行流的情况M group(LambdaQueryWrapper&lt;T&gt; wrapper, SFunction&lt;T, K&gt; sFunction, Collector&lt;? super T, A, D&gt; downstream, boolean isParallel, Consumer&lt;T&gt;... peeks); 12345678 #参数说明 类型 参数名 描述 T entity 实体对象 K attribute 实体属性类型,也是map中key的类型 D - 下游收集器返回类型,也是map中value的类型 A - 下游操作中间类型 M - 最终结束返回的Map&lt;K, D&gt; LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 分组依据，封装后map中作为key的条件 Collector&lt;T, A, D&gt; downstream 下游收集器 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 #list1234// 查询表内记录，封装返回为List&lt;属性&gt;List&lt;A&gt; list(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, Consumer&lt;E&gt;... peeks);// 查询表内记录，封装返回为List&lt;属性&gt;，考虑了并行流的情况List&lt;A&gt; list(LambdaQueryWrapper&lt;E&gt; wrapper, SFunction&lt;E, A&gt; sFunction, boolean isParallel, Consumer&lt;E&gt;... peeks); 1234 #参数说明 类型 参数名 描述 E entity 实体对象 A attribute 实体属性类型,也是list中元素的类型 LambdaQueryWrapper wrapper 支持lambda的条件构造器 SFunction&lt;E, A&gt; sFunction 封装后list中的元素 boolean isParallel 为true时底层使用并行流执行 Consumer… peeks 可叠加的后续操作 条件构造器说明: 以下出现的第一个入参boolean condition表示该条件是否加入最后生成的sql中，例如：query.like(StringUtils.isNotBlank(name), Entity::getName, name) .eq(age!&#x3D;null &amp;&amp; age &gt;&#x3D; 0, Entity::getAge, age) 以下代码块内的多个方法均为从上往下补全个别boolean类型的入参,默认为true 以下出现的泛型Param均为Wrapper的子类实例(均具有AbstractWrapper的所有方法) 以下方法在入参中出现的R为泛型,在普通wrapper中是String,在LambdaWrapper中是函数(例:Entity::getId,Entity为实体类,getId为字段id的getMethod) 以下方法入参中的R column均表示数据库字段,当R具体类型为String时则为数据库字段名(字段名是数据库关键字的自己用转义符包裹!)!而不是实体类数据字段名!!!,另当R具体类型为SFunction时项目runtime不支持eclipse自家的编译器!!! 以下举例均为使用普通wrapper,入参为Map和List的均以json形式表现! 使用中如果入参的Map或者List为空,则不会加入最后生成的sql中!!! 有任何疑问就点开源码看,看不懂函数的点击我学习新知识(opens new window) 警告: 不支持以及不赞成在 RPC 调用中把 Wrapper 进行传输 wrapper 很重 传输 wrapper 可以类比为你的 controller 用 map 接收值(开发一时爽,维护火葬场) 正确的 RPC 调用姿势是写一个 DTO 进行传输,被调用方再根据 DTO 执行相应的操作 我们拒绝接受任何关于 RPC 传输 Wrapper 报错相关的 issue 甚至 pr #AbstractWrapper说明: QueryWrapper(LambdaQueryWrapper) 和 UpdateWrapper(LambdaUpdateWrapper) 的父类用于生成 sql 的 where 条件, entity 属性也用于生成 sql 的 where 条件注意: entity 生成的 where 条件与 使用各个 api 生成的 where 条件没有任何关联行为 #allEq123allEq(Map&lt;R, V&gt; params)allEq(Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, Map&lt;R, V&gt; params, boolean null2IsNull) 123 全部 eq (或个别 isNull ) 个别参数说明: params : key为数据库字段名,value为字段值null2IsNull : 为true则在map的value为null时调用 isNull 方法,为false时则忽略value为null的 例1: allEq(&#123;id:1,name:&quot;老王&quot;,age:null&#125;)—&gt;id = 1 and name = &#39;老王&#39; and age is null 例2: allEq(&#123;id:1,name:&quot;老王&quot;,age:null&#125;, false)—&gt;id = 1 and name = &#39;老王&#39; 123allEq(BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params)allEq(BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull)allEq(boolean condition, BiPredicate&lt;R, V&gt; filter, Map&lt;R, V&gt; params, boolean null2IsNull) 123 个别参数说明: filter : 过滤函数,是否允许字段传入比对条件中params 与 null2IsNull : 同上 例1: allEq((k,v) -&gt; k.indexOf(&quot;a&quot;) &gt;= 0, &#123;id:1,name:&quot;老王&quot;,age:null&#125;)—&gt;name = &#39;老王&#39; and age is null 例2: allEq((k,v) -&gt; k.indexOf(&quot;a&quot;) &gt;= 0, &#123;id:1,name:&quot;老王&quot;,age:null&#125;, false)—&gt;name = &#39;老王&#39; #eq12eq(R column, Object val)eq(boolean condition, R column, Object val) 12 等于 &#x3D; 例: eq(&quot;name&quot;, &quot;老王&quot;)—&gt;name = &#39;老王&#39; #ne12ne(R column, Object val)ne(boolean condition, R column, Object val) 12 不等于 &lt;&gt; 例: ne(&quot;name&quot;, &quot;老王&quot;)—&gt;name &lt;&gt; &#39;老王&#39; #gt12gt(R column, Object val)gt(boolean condition, R column, Object val) 12 大于 &gt; 例: gt(&quot;age&quot;, 18)—&gt;age &gt; 18 #ge12ge(R column, Object val)ge(boolean condition, R column, Object val) 12 大于等于 &gt;&#x3D; 例: ge(&quot;age&quot;, 18)—&gt;age &gt;= 18 #lt12lt(R column, Object val)lt(boolean condition, R column, Object val) 12 小于 &lt; 例: lt(&quot;age&quot;, 18)—&gt;age &lt; 18 #le12le(R column, Object val)le(boolean condition, R column, Object val) 12 小于等于 &lt;&#x3D; 例: le(&quot;age&quot;, 18)—&gt;age &lt;= 18 #between12between(R column, Object val1, Object val2)between(boolean condition, R column, Object val1, Object val2) 12 BETWEEN 值1 AND 值2 例: between(&quot;age&quot;, 18, 30)—&gt;age between 18 and 30 #notBetween12notBetween(R column, Object val1, Object val2)notBetween(boolean condition, R column, Object val1, Object val2) 12 NOT BETWEEN 值1 AND 值2 例: notBetween(&quot;age&quot;, 18, 30)—&gt;age not between 18 and 30 #like12like(R column, Object val)like(boolean condition, R column, Object val) 12 LIKE ‘%值%’ 例: like(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;%王%&#39; #notLike12notLike(R column, Object val)notLike(boolean condition, R column, Object val) 12 NOT LIKE ‘%值%’ 例: notLike(&quot;name&quot;, &quot;王&quot;)—&gt;name not like &#39;%王%&#39; #likeLeft12likeLeft(R column, Object val)likeLeft(boolean condition, R column, Object val) 12 LIKE ‘%值’ 例: likeLeft(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;%王&#39; #likeRight12likeRight(R column, Object val)likeRight(boolean condition, R column, Object val) 12 LIKE ‘值%’ 例: likeRight(&quot;name&quot;, &quot;王&quot;)—&gt;name like &#39;王%&#39; #isNull12isNull(R column)isNull(boolean condition, R column) 12 字段 IS NULL 例: isNull(&quot;name&quot;)—&gt;name is null #isNotNull12isNotNull(R column)isNotNull(boolean condition, R column) 12 字段 IS NOT NULL 例: isNotNull(&quot;name&quot;)—&gt;name is not null #in12in(R column, Collection&lt;?&gt; value)in(boolean condition, R column, Collection&lt;?&gt; value) 12 字段 IN (value.get(0), value.get(1), …) 例: in(&quot;age&quot;,&#123;1,2,3&#125;)—&gt;age in (1,2,3) 12in(R column, Object... values)in(boolean condition, R column, Object... values) 12 字段 IN (v0, v1, …) 例: in(&quot;age&quot;, 1, 2, 3)—&gt;age in (1,2,3) #notIn12notIn(R column, Collection&lt;?&gt; value)notIn(boolean condition, R column, Collection&lt;?&gt; value) 12 字段 NOT IN (value.get(0), value.get(1), …) 例: notIn(&quot;age&quot;,&#123;1,2,3&#125;)—&gt;age not in (1,2,3) 12notIn(R column, Object... values)notIn(boolean condition, R column, Object... values) 12 字段 NOT IN (v0, v1, …) 例: notIn(&quot;age&quot;, 1, 2, 3)—&gt;age not in (1,2,3) #inSql12inSql(R column, String inValue)inSql(boolean condition, R column, String inValue) 12 字段 IN ( sql语句 ) 例: inSql(&quot;age&quot;, &quot;1,2,3,4,5,6&quot;)—&gt;age in (1,2,3,4,5,6) 例: inSql(&quot;id&quot;, &quot;select id from table where id &lt; 3&quot;)—&gt;id in (select id from table where id &lt; 3) #notInSql12notInSql(R column, String inValue)notInSql(boolean condition, R column, String inValue) 12 字段 NOT IN ( sql语句 ) 例: notInSql(&quot;age&quot;, &quot;1,2,3,4,5,6&quot;)—&gt;age not in (1,2,3,4,5,6) 例: notInSql(&quot;id&quot;, &quot;select id from table where id &lt; 3&quot;)—&gt;id not in (select id from table where id &lt; 3) #groupBy12groupBy(R... columns)groupBy(boolean condition, R... columns) 12 分组：GROUP BY 字段, … 例: groupBy(&quot;id&quot;, &quot;name&quot;)—&gt;group by id,name #orderByAsc12orderByAsc(R... columns)orderByAsc(boolean condition, R... columns) 12 排序：ORDER BY 字段, … ASC 例: orderByAsc(&quot;id&quot;, &quot;name&quot;)—&gt;order by id ASC,name ASC #orderByDesc12orderByDesc(R... columns)orderByDesc(boolean condition, R... columns) 12 排序：ORDER BY 字段, … DESC 例: orderByDesc(&quot;id&quot;, &quot;name&quot;)—&gt;order by id DESC,name DESC #orderBy1orderBy(boolean condition, boolean isAsc, R... columns) 1 排序：ORDER BY 字段, … 例: orderBy(true, true, &quot;id&quot;, &quot;name&quot;)—&gt;order by id ASC,name ASC #having12having(String sqlHaving, Object... params)having(boolean condition, String sqlHaving, Object... params) 12 HAVING ( sql语句 ) 例: having(&quot;sum(age) &gt; 10&quot;)—&gt;having sum(age) &gt; 10 例: having(&quot;sum(age) &gt; &#123;0&#125;&quot;, 11)—&gt;having sum(age) &gt; 11 #func12func(Consumer&lt;Children&gt; consumer)func(boolean condition, Consumer&lt;Children&gt; consumer) 12 func 方法(主要方便在出现if…else下调用不同方法能不断链) 例: func(i -&gt; if(true) &#123;i.eq(&quot;id&quot;, 1)&#125; else &#123;i.ne(&quot;id&quot;, 1)&#125;) #or12or()or(boolean condition) 12 拼接 OR 注意事项: 主动调用or表示紧接着下一个方法不是用and连接!(不调用or则默认为使用and连接) 例: eq(&quot;id&quot;,1).or().eq(&quot;name&quot;,&quot;老王&quot;)—&gt;id = 1 or name = &#39;老王&#39; 12or(Consumer&lt;Param&gt; consumer)or(boolean condition, Consumer&lt;Param&gt; consumer) 12 OR 嵌套 例: or(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;or (name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #and12and(Consumer&lt;Param&gt; consumer)and(boolean condition, Consumer&lt;Param&gt; consumer) 12 AND 嵌套 例: and(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;and (name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #nested12nested(Consumer&lt;Param&gt; consumer)nested(boolean condition, Consumer&lt;Param&gt; consumer) 12 正常嵌套 不带 AND 或者 OR 例: nested(i -&gt; i.eq(&quot;name&quot;, &quot;李白&quot;).ne(&quot;status&quot;, &quot;活着&quot;))—&gt;(name = &#39;李白&#39; and status &lt;&gt; &#39;活着&#39;) #apply12apply(String applySql, Object... params)apply(boolean condition, String applySql, Object... params) 12 拼接 sql 注意事项: 该方法可用于数据库函数 动态入参的params对应前面applySql内部的&#123;index&#125;部分.这样是不会有sql注入风险的,反之会有! 例: apply(&quot;id = 1&quot;)—&gt;id = 1 例: apply(&quot;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;)—&gt;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;) 例: apply(&quot;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#123;0&#125;&quot;, &quot;2008-08-08&quot;)—&gt;date_format(dateColumn,&#39;%Y-%m-%d&#39;) = &#39;2008-08-08&#39;&quot;) #last12last(String lastSql)last(boolean condition, String lastSql) 12 无视优化规则直接拼接到 sql 的最后 注意事项: 只能调用一次,多次调用以最后一次为准 有sql注入的风险,请谨慎使用 例: last(&quot;limit 1&quot;) #exists12exists(String existsSql)exists(boolean condition, String existsSql) 12 拼接 EXISTS ( sql语句 ) 例: exists(&quot;select id from table where age = 1&quot;)—&gt;exists (select id from table where age = 1) #notExists12notExists(String notExistsSql)notExists(boolean condition, String notExistsSql) 12 拼接 NOT EXISTS ( sql语句 ) 例: notExists(&quot;select id from table where age = 1&quot;)—&gt;not exists (select id from table where age = 1) #QueryWrapper说明: 继承自 AbstractWrapper ,自身的内部属性 entity 也用于生成 where 条件及 LambdaQueryWrapper, 可以通过 new QueryWrapper().lambda() 方法获取 #select123select(String... sqlSelect)select(Predicate&lt;TableFieldInfo&gt; predicate)select(Class&lt;T&gt; entityClass, Predicate&lt;TableFieldInfo&gt; predicate) 123 设置查询字段 说明: 以上方法分为两类.第二类方法为:过滤查询字段(主键除外),入参不包含 class 的调用前需要wrapper内的entity属性有值! 这两类方法重复调用以最后一次为准 例: select(&quot;id&quot;, &quot;name&quot;, &quot;age&quot;) 例: select(i -&gt; i.getProperty().startsWith(&quot;test&quot;)) #UpdateWrapper说明: 继承自 AbstractWrapper ,自身的内部属性 entity 也用于生成 where 条件及 LambdaUpdateWrapper, 可以通过 new UpdateWrapper().lambda() 方法获取! #set12set(String column, Object val)set(boolean condition, String column, Object val) 12 SQL SET 字段 例: set(&quot;name&quot;, &quot;老李头&quot;) 例: set(&quot;name&quot;, &quot;&quot;)—&gt;数据库字段值变为空字符串 例: set(&quot;name&quot;, null)—&gt;数据库字段值变为null #setSql1setSql(String sql) 1 设置 SET 部分 SQL 例: setSql(&quot;name = &#39;老李头&#39;&quot;) #lambda 获取 LambdaWrapper在QueryWrapper中是获取LambdaQueryWrapper在UpdateWrapper中是获取LambdaUpdateWrapper #使用 Wrapper 自定义SQL注意事项: 需要mybatis-plus版本 &gt;&#x3D; 3.0.7 param 参数名要么叫ew,要么加上注解@Param(Constants.WRAPPER) 使用$&#123;ew.customSqlSegment&#125; 不支持 Wrapper 内的entity生成where语句 #kotlin持久化对象定义最佳实践Mybatis 缓存机制 将mybatis的接口调用通过与数据库建立SQLsession(内含Executor里有localcache)转化为SQL语句 一级缓存,针对一个事务里的查询,每个事务简历一个SQLsession 如果查询ID相同(xml中的表示SQL语句的ID相同)并且预编译的SQL相同,查询的参数相同,则会从localcache中找到这个缓存,因为是根据这三个生成的hash作为key,结果作为value存储, 二级缓存,caching Executor作为装饰器门面 Mapper的xml中会有namespace级的缓存,难以支持服务器集群","categories":[],"tags":[]},{"title":"redis","slug":"5redis","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:50:31.733Z","comments":true,"path":"2022/09/01/5redis/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/5redis/","excerpt":"","text":"redis日常基础使用 一些配置 与说明 (守护进程)daemonize yes 启动: 123456#可能需要进入redis目录redis-server /usr/local/redis/redis.confredis-cliauth (可以不写用户)密码keys* 关闭 1redis-cli shutdown 使用RedisDesktopManager连接远程服务器redis 修改 .conf ​ bind 0.0.0.0 允许所有主机 127.~ 只允许本机 ​ protectmode yes 只允许本机 no 允许所有主机 ​ requierpass 密码 服务器防火墙开放 6379端口就ok 第三章 redis多线程VS单线程Redis工作线程是单线程的,整个Redis来说，是多线程的；主要是指Redis的网络IO和键值对读写是由一个线程来完成的，Redis在处理客户端的请求时包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。这也是Redis对外提供键值存储服务的主要流程。 但Redis的其他功能，比如持久化、异步删除、集群数据同步等等，其实是由额外的线程执行的。Redis工作线程是单线程的，但是，整个Redis来说，是多线程的； 单线程快的原因1 避免上下文切换：因为是单线程模型，因此就避免了不必要的上下文切换和多线程竞争，这就省去了多线程切换带来的时间和性能上的消耗，而且单线程不会导致死锁问题的发生 2 即使使用单线程模型也并发的处理多客户端的请求，主要使用的是多路复用和非阻塞 IO； 3 对于 Redis 系统来说，主要的性能瓶颈是内存或者网络带宽而并非 CPU。 单线程的弊病正常情况下使用 del 指令可以很快的删除数据，而当被删除的 key 是一个非常大的对象时，例如时包含了成千上万个元素的 hash 集合时，那么 del 指令就会造成 Redis 主线程卡顿。 这就是redis3.x单线程时代最经典的故障，大key删除的头疼问题， 由于redis是单线程的，del bigKey …..等待很久这个线程才会释放，类似加了一个synchronized锁，你可以想象高并发下，程序堵成什么样子？ 在Redis 4.0就引入了多个线程来实现数据的异步惰性删除等功能，但是其处理读写请求的仍然只有一个线程，所以仍然算是狭义上的单线程 Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理8W到10W的QPS， 这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。 默认单线程在Redis6.0中，多线程机制默认是关闭的，如果需要使用多线程功能，需要在redis.conf中完成两个设置 1.设置io-thread-do-reads配置项为yes，表示启动多线程。 2。设置线程个数。关于线程数的设置，官方的建议是如果为 4 核的 CPU，建议线程数设置为 2 或 3，如果为 8 核 CPU 建议线程数设置为 6，线程数一定要小于机器核数，线程数并不是越大越好。 第五章 经典五种数据类型(都是针对value)介绍及落地应用1. String1.1 常用指令12345678910111213141516171819202122set key valueget key同时设置/获取多个键值 MGET key [key ....]MSET key value [key value ....]数值增减INCR key增加指定的整数 INCRBY key increment递减数值 DECR key减少指定的整数DECRBY key decrement获取字符串长度STRLEN key分布式锁setnx key valueset key value [EX seconds] [PX milliseconds] [NX|XX] 1.2 应用场景比如抖音无限点赞某个视频或者商品，点一下加一次 2. HashMap&lt;String,Map&lt;Object,Object&gt;&gt; 2.1常用指令1234567891011121314一次设置一个字段值HSET key field value一次获取一个字段值HGET key field一次设置多个字段值HMSET key field value [field value ...]一次获取多个字段值HMGET key field [field ....]获取所有字段值hgetall key获取某个key内的全部数量hlen删除一个keyhdel 2.2应用场景​ Map&lt;String,Map&lt;Object,Object&gt;&gt; ​ hset key field value ​ JD早期购物车 中小场可用 ​ 新增商品 → hset shopcar:uid1024 334488 1 ​ 新增商品 → hset shopcar:uid1024 334477 1 ​ 增加商品数量 → hincrby shopcar:uid1024 334477 1 ​ 商品总数 → hlen shopcar:uid1024 ​ 全部选择 → hgetall shopcar:uid1024 3. List一个双端链表的结构，容量是2的32次方减1个元素，大概40多亿，主要功能有push&#x2F;pop等，一般用在栈、队列、消息队列等场景。 3.1 常用指令12345678向列表左边添加元素LPUSH key value [value ...]向列表右边添加元素RPUSH key value [value ....]查看列表LRANGE key start stop获取列表中元素的个数LLEN key 3.2 应用场景微信订阅号消息,关注了作者,如果作者发布了文章 就会向关注的list添加 ​ 1 订阅了的公众号和CSDN发布了文章分别是 11 和 22 ​ 2 ggq关注了他们两个，只要他们发布了新文章，就会安装进ggq的List lpush likearticle:ggqid 11 22 ​ 3 查看ggq自己的号订阅的全部文章，类似分页，下面0~10就是一次显示10条 ​ lrange likearticle:ggqid 0 9 ​ 商品评论 ​ 商品ID key 和 value评论信息 ​ 按时间顺序 4. Set4.1 常用指令1234567891011121314151617181920添加元素SADD key member [member ...]删除元素SREM key member [member ...]遍历集合中的所有元素SMEMBERS key判断元素是否在集合中SISMEMBER key member获取集合中的元素总数SCARD key从集合中随机弹出一个元素，元素不删除SRANDMEMBER key [数字]从集合中随机弹出一个元素，出一个删一个SPOP key [数字]集合运算SDIFF key [key ...] # 属于A但不属于B的元素构成的集合SINTER key [key ...] # 属于A同时也属于B的共同拥有的元素构成的集合SUNION key [key ...] # 属于A或者属于B的元素合并后的集合 4.2 应用场景抽奖 1 用户ID，立即参与按钮 sadd key 用户ID 2 显示已经有多少人参与了，上图23208人参加 SCARD key 3 抽奖(从set中任意选取N个中奖人) SRANDMEMBER key 2 随机抽奖2个人，元素不删除 SPOP key 3 随机抽奖3个人，元素会删除 ​ 集合运算 共同好友,共同关注 ​ 可能认识的人 5. sortedSet(Zset)向有序集合中加入一个元素和该元素的分数 5.1123456789101112131415161718192021ZADD key score member [score member ...]按照元素分数从小到大的顺序返回索引从start到stop之间的所有元素ZRANGE key start stop [WITHSCORES]获取元素的分数ZSCORE key member删除元素ZREM key member [member ...]获取指定分数范围的元素ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]增加某个元素的分数ZINCRBY key increment member获取集合中元素的数量ZCARD key获得指定分数范围内的元素个数ZCOUNT key min max按照排名范围删除元素ZREMRANGEBYRANK key start stop获取元素的排名ZRANK key member # 从小到大ZREVRANK key member # 从大到小 5.2 应用场景​ 商品排行 ​ 抖音热搜 incrby ​ 展示多少条 zrange min max 5.3 案例实战第六章 redis新类型bitmap&#x2F;hyperloglgo&#x2F;GEO存的进+取得快+多统计 亿级系统中常见的四种统计统计的类型 聚合统计 统计多个集合元素的聚合结果，就是前面讲解过的交差并等集合统计 交并差集和聚合函数的应用 排序统计 抖音视频最新评论留言的场景，请你设计一个展现列表。 考察你的数据结构和设计思路 设计案例和回答思路 每个商品评价对应一个List集合，这个List包含了对这个商品的所有评论，而且会按照评论时间保存这些评论， 每来一个新评论就用LPUSH命令把它插入List的队头。但是，如果在演示第二页前，又产生了一个新评论， 第2页的评论不一样了。原因： List是通过元素在List中的位置来排序的，当有一个新元素插入时，原先的元素在List中的位置都后移了一位， 原来在第1位的元素现在排在了第2位，当LRANGE读取时，就会读到旧元素。 在⾯对需要展示最新列表、排行榜等场景时， 如果数据更新频繁或者需要分页显示，建议使⽤ZSet 二值统计 集合元素的取值就只有0和1两种。 在钉钉上班签到打卡的场景中，我们只用记录有签到(1)或没签到(0) bitmap 基数统计 指统计⼀个集合中不重复的元素个数 见hyperloglog 1. BitMap1.1 常用指令1.2应用场景​ 按字节(8位)扩容 ​ setbit key offset value 偏移位从0开始算 value 只能是01 ​ 由0和1状态表现的二进制位的bit数组 ​ 用于状态统计 ​ 2. HyperLogLog2.1 常用指令2.2应用场景3. GEO3.1 常用指令3.2应用场景第七 章 布隆过滤器​ 由一个初值都为零的bit数组和多个哈希函数构成， ​ 用来快速判断某个数据是否存在 ​ 判断结果没有的一定没有,有的大概率有 只添加不删除 ​ 多重hash linux安装布隆过滤器的两种方式 采用docker安装RedisBloom，推荐 Redis 在 4.0 之后有了插件功能（Module），可以使用外部的扩展功能， 可以使用 RedisBloom 作为 Redis 布隆过滤器插件。 docker run -p 6379:6379 –name&#x3D;redis6379bloom -d redislabs&#x2F;rebloom docker exec -it redis6379bloom &#x2F;bin&#x2F;bash redis-cli 常用指令123456bf.reserve key error_rate的值 initial_size 的值 默认的error_rate是 0.01，默认的initial_size是 100。bf.add key 值bf.exists key 值bf.madd 一次添加多个元素bf.mexists 一次查询多个元素是否存在 Redis高级学习目标目标1：能够说出redis中的数据删除策与略淘汰策略 目标2：能够说出主从复制的概念，工作流程以及场景问题及解决方案 目标3：能够说出哨兵的作用以及工作原理，以及如何启用哨兵 目标4：能够说出集群的架构设计，完成集群的搭建 目标5：能够说出缓存预热，雪崩，击穿，穿透的概念，能说出redis的相关监控指标 1.数据删除与淘汰策略1.1 过期数据1.1.1 Redis中的数据特征Redis是一种内存级数据库，所有数据均存放在内存中，内存中的数据可以通过TTL指令获取其状态 TTL返回的值有三种情况：正数，-1，-2 正数：代表该数据在内存中还能存活的时间 -1：永久有效的数据 2 ：已经过期的数据 或被删除的数据 或 未定义的数据 删除策略就是针对已过期数据的处理策略，已过期的数据是真的就立即删除了吗？其实也不是，我们会有多种删除策略，是分情况的，在不同的场景下使用不同的删除方式会有不同效果，这也正是我们要将的数据的删除策略的问题 1.1.2 时效性数据的存储结构在Redis中，如何给数据设置它的失效周期呢？数据的时效在redis中如何存储呢？看下图： 过期数据是一块独立的存储空间，Hash结构，field是内存地址，value是过期时间，保存了所有key的过期描述，在最终进行过期处理的时候，对该空间的数据进行检测， 当时间到期之后通过field找到内存该地址处的数据，然后进行相关操作。 1.2 数据删除策略1.2.1 数据删除策略的目标在内存占用与CPU占用之间寻找一种平衡，顾此失彼都会造成整体redis性能的下降，甚至引发服务器宕机或 内存泄露 针对过期数据要进行删除的时候都有哪些删除策略呢？ 1.定时删除 2.惰性删除 3.定期删除 1.2.2 定时删除创建一个定时器，当key设置有过期时间，且过期时间到达时，由定时器任务立即执行对键的删除操作 优点：节约内存，到时就删除，快速释放掉不必要的内存占用 缺点：CPU压力很大，无论CPU此时负载量多高，均占用CPU，会影响redis服务器响应时间和指令吞吐量 总结：用处理器性能换取存储空间（拿时间换空间） 1.2.3 惰性删除数据到达过期时间，不做处理。等下次访问该数据时，我们需要判断 如果未过期，返回数据 发现已过期，删除，返回不存在 优点：节约CPU性能，发现必须删除的时候才删除 缺点：内存压力很大，出现长期占用内存的数据 总结：用存储空间换取处理器性能（拿时间换空间） 1.2.4 定期删除定时删除和惰性删除这两种方案都是走的极端，那有没有折中方案？ 我们来讲redis的定期删除方案： Redis启动服务器初始化时，读取配置server.hz的值，默认为10 即每100ms一次 每秒钟执行server.hz次serverCron()——–&gt;databasesCron()———&gt;activeExpireCycle() **activeExpireCycle()*对每个expires[]逐一进行检测，每次执行耗时：250ms&#x2F;server.hz 对某个expires[*]检测时，随机挑选W个key检测 1234567如果key超时，删除key如果一轮中删除的key的数量&gt;W*25%，循环该过程如果一轮中删除的key的数量≤W*25%，检查下一个expires[*]，0-15循环W取值=ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP属性值 参数current_db用于记录activeExpireCycle() 进入哪个expires[*] 执行 如果activeExpireCycle()执行时间到期，下次从current_db继续向下执行 总的来说：定期删除就是周期性轮询redis库中的时效性数据，采用随机抽取的策略，利用过期数据占比的方式控制删除频度 特点1：CPU性能占用设置有峰值，检测频度可自定义设置 特点2：内存压力不是很大，长期占用内存的冷数据会被持续清理 总结：周期性抽查存储空间（随机抽查，重点抽查） 1.2.5 删除策略对比1：定时删除： 123节约内存，无占用,不分时段占用CPU资源，频度高,拿时间换空间 2：惰性删除： 123内存占用严重延时执行，CPU利用率高拿空间换时间 3：定期删除： 123内存定期随机清理每秒花费固定的CPU资源维护内存随机抽查，重点抽查 1.3 数据淘汰策略（逐出算法）1.3.1 淘汰策略概述什么叫数据淘汰策略？什么样的应用场景需要用到数据淘汰策略？ 当新数据进入redis时，如果内存不足怎么办？在执行每一个命令前，会调用**freeMemoryIfNeeded()**检测内存是否充足。如果内存不满足新 加入数据的最低存储要求，redis要临时删除一些数据为当前指令清理存储空间。清理数据的策略称为逐出算法。 注意：逐出数据的过程不是100%能够清理出足够的可使用的内存空间，如果不成功则反复执行。当对所有数据尝试完毕， 如不能达到内存清理的要求，将出现错误信息如下 1(error) OOM command not allowed when used memory &gt;&#x27;maxmemory&#x27; 1.3.2 策略配置影响数据淘汰的相关配置如下： 1：最大可使用内存，即占用物理内存的比例，默认值为0，表示不限制。生产环境中根据需求设定，通常设置在50%以上 1maxmemory ?mb 2：每次选取待删除数据的个数，采用随机获取数据的方式作为待检测删除数据 1maxmemory-samples count 3：对数据进行删除的选择策略 1maxmemory-policy policy 那数据删除的策略policy到底有几种呢？一共是3类8种 第一类：检测易失数据（可能会过期的数据集server.db[i].expires ） 1234volatile-lru：挑选最近最少使用的数据淘汰volatile-lfu：挑选最近使用次数最少的数据淘汰volatile-ttl：挑选将要过期的数据淘汰volatile-random：任意选择数据淘汰 第二类：检测全库数据（所有数据集server.db[i].dict ） 123allkeys-lru：挑选最近最少使用的数据淘汰allkeLyRs-lfu：：挑选最近使用次数最少的数据淘汰allkeys-random：任意选择数据淘汰，相当于随机 第三类：放弃数据驱逐 1no-enviction（驱逐）：禁止驱逐数据(redis4.0中默认策略)，会引发OOM(Out Of Memory) 注意：这些策略是配置到哪个属性上？怎么配置？如下所示 1maxmemory-policy volatile-lru 数据淘汰策略配置依据 使用INFO命令输出监控信息，查询缓存 hit 和 miss 的次数，根据业务需求调优Redis配置 2.主从复制2.1 主从复制简介2.1.1 高可用首先我们要理解互联网应用因为其独有的特性我们演化出的三高架构 高并发 应用要提供某一业务要能支持很多客户端同时访问的能力，我们称为并发，高并发意思就很明确了 高性能 性能带给我们最直观的感受就是：速度快，时间短 高可用 可用性：一年中应用服务正常运行的时间占全年时间的百分比，如下图：表示了应用服务在全年宕机的时间 ![](D:&#x2F;BaiduNetdiskDownload&#x2F;黑马微服务&#x2F;1.Redis高级【海量资源尽在 】&#x2F;1.Redis高级&#x2F;讲义-md版本&#x2F;img&#x2F;5.png 我们把这些时间加在一起就是全年应用服务不可用的时间，然后我们可以得到应用服务全年可用的时间 4小时27分15秒+11分36秒+2分16秒&#x3D;4小时41分7秒&#x3D;16867秒 1年&#x3D;3652460*60&#x3D;31536000秒 可用性&#x3D;（31536000-16867）&#x2F;31536000*100%&#x3D;99.9465151% 业界可用性目标**5个9，即99.999%**，即服务器年宕机时长低于315秒，约5.25分钟 2.1.2 主从复制概念知道了三高的概念之后，我们想：你的“Redis”是否高可用？那我们要来分析单机redis的风险与问题 问题1.机器故障 现象：硬盘故障、系统崩溃 本质：数据丢失，很可能对业务造成灾难性打击 结论：基本上会放弃使用redis. 问题2.容量瓶颈 现象：内存不足，从16G升级到64G，从64G升级到128G，无限升级内存 本质：穷，硬件条件跟不上 结论：放弃使用redis 结论： 为了避免单点Redis服务器故障，准备多台服务器，互相连通。将数据复制多个副本保存在不同的服务器上，连接在一起，并保证数据是同步的。即使有其中一台服务器宕机，其他服务器依然可以继续提供服务，实现Redis的高可用，同时实现数据冗余备份。 多台服务器连接方案： 提供数据方：master 主服务器，主节点，主库主客户端 接收数据方：slave 从服务器，从节点，从库 从客户端 需要解决的问题： 数据同步（master的数据复制到slave中） 这里我们可以来解释主从复制的概念： 概念：主从复制即将master中的数据即时、有效的复制到slave中 特征：一个master可以拥有多个slave，一个slave只对应一个master 职责：master和slave各自的职责不一样 master: 12345写数据执行写操作时，将出现变化的数据自动同步到slave读数据（可忽略） slave: 123读数据写数据（禁止） 2.1.3 主从复制的作用 读写分离：master写、slave读，提高服务器的读写负载能力 负载均衡：基于主从结构，配合读写分离，由slave分担master负载，并根据需求的变化，改变slave的数 量，通过多个从节点分担数据读取负载，大大提高Redis服务器并发量与数据吞吐量 故障恢复：当master出现问题时，由slave提供服务，实现快速的故障恢复 数据冗余：实现数据热备份，是持久化之外的一种数据冗余方式 高可用基石：基于主从复制，构建哨兵模式与集群，实现Redis的高可用方案 2.2 主从复制工作流程主从复制过程大体可以分为3个阶段 建立连接阶段（即准备阶段） 数据同步阶段 命令传播阶段（反复同步） 而命令的传播其实有4种，分别如下： 2.2.1 主从复制的工作流程（三个阶段）2.2.1.1 阶段一：建立连接建立slave到master的连接，使master能够识别slave，并保存slave端口号 流程如下： 步骤1：设置master的地址和端口，保存master信息 步骤2：建立socket连接 步骤3：发送ping命令（定时器任务） 步骤4：身份验证 步骤5：发送slave端口信息 至此，主从连接成功！ 当前状态： slave：保存master的地址与端口 master：保存slave的端口 总体：之间创建了连接的socket master和slave互联 接下来就要通过某种方式将master和slave连接到一起 方式一：客户端发送命令 1slaveof masterip masterport 方式二：启动服务器参数 1redis-server --slaveof masterip masterport 方式三：服务器配置（主流方式） 1slaveof masterip masterport slave系统信息 12master_link_down_since_secondsmasterhost &amp; masterport master系统信息 1uslave_listening_port(多个) 主从断开连接 断开slave与master的连接，slave断开连接后，不会删除已有数据，只是不再接受master发送的数据 1slaveof no one 授权访问 master客户端发送命令设置密码 1requirepass password master配置文件设置密码 12config set requirepass passwordconfig get requirepass slave客户端发送命令设置密码 1auth password slave配置文件设置密码 1masterauth password slave启动服务器设置密码 1redis-server –a password 2.2.1.2 阶段二：数据同步 在slave初次连接master后，复制master中的所有数据到slave 将slave的数据库状态更新成master当前的数据库状态 同步过程如下： 步骤1：请求同步数据 步骤2：创建RDB同步数据 步骤3：恢复RDB同步数据 步骤4：请求部分同步数据 步骤5：恢复部分同步数据 至此，数据同步工作完成！ 当前状态： slave：具有master端全部数据，包含RDB过程接收的数据 master：保存slave当前数据同步的位置 总体：之间完成了数据克隆 数据同步阶段master说明 1：如果master数据量巨大，数据同步阶段应避开流量高峰期，避免造成master阻塞，影响业务正常执行 2：复制缓冲区大小设定不合理，会导致数据溢出。如进行全量复制周期太长，进行部分复制时发现数据已经存在丢失的情况，必须进行第二次全量复制，致使slave陷入死循环状态。 1repl-backlog-size ?mb master单机内存占用主机内存的比例不应过大，建议使用50%-70%的内存，留下30%-50%的内存用于执 行bgsave命令和创建复制缓冲区 数据同步阶段slave说明 为避免slave进行全量复制、部分复制时服务器响应阻塞或数据不同步，建议关闭此期间的对外服务 1slave-serve-stale-data yes|no 数据同步阶段，master发送给slave信息可以理解master是slave的一个客户端，主动向slave发送命令 多个slave同时对master请求数据同步，master发送的RDB文件增多，会对带宽造成巨大冲击，如果master带宽不足，因此数据同步需要根据业务需求，适量错峰 slave过多时，建议调整拓扑结构，由一主多从结构变为树状结构，中间的节点既是master，也是 slave。注意使用树状结构时，由于层级深度，导致深度越高的slave与最顶层master间数据同步延迟 较大，数据一致性变差，应谨慎选择 2.2.1.3 阶段三：命令传播 当master数据库状态被修改后，导致主从服务器数据库状态不一致，此时需要让主从数据同步到一致的状态，同步的动作称为命令传播 master将接收到的数据变更命令发送给slave，slave接收命令后执行命令 命令传播阶段的部分复制 命令传播阶段出现了断网现象： 网络闪断闪连：忽略 短时间网络中断：部分复制 长时间网络中断：全量复制 这里我们主要来看部分复制，部分复制的三个核心要素 服务器的运行 id（run id） 主服务器的复制积压缓冲区 主从服务器的复制偏移量 服务器运行ID（runid） 12345678910概念：服务器运行ID是每一台服务器每次运行的身份识别码，一台服务器多次运行可以生成多个运行id组成：运行id由40位字符组成，是一个随机的十六进制字符例如：fdc9ff13b9bbaab28db42b3d50f852bb5e3fcdce作用：运行id被用于在服务器间进行传输，识别身份如果想两次操作均对同一台服务器进行，必须每次操作携带对应的运行id，用于对方识别实现方式：运行id在每台服务器启动时自动生成的，master在首次连接slave时，会将自己的运行ID发送给slave，slave保存此ID，通过info Server命令，可以查看节点的runid 复制缓冲区 123456概念：复制缓冲区，又名复制积压缓冲区，是一个先进先出（FIFO）的队列，用于存储服务器执行过的命令，每次传播命令，master都会将传播的命令记录下来，并存储在复制缓冲区 复制缓冲区默认数据存储空间大小是1M 当入队元素的数量大于队列长度时，最先入队的元素会被弹出，而新元素会被放入队列作用：用于保存master收到的所有指令（仅影响数据变更的指令，例如set，select）数据来源：当master接收到主客户端的指令时，除了将指令执行，会将该指令存储到缓冲区中 复制缓冲区内部工作原理： 组成 偏移量 概念：一个数字，描述复制缓冲区中的指令字节位置 分类： master复制偏移量：记录发送给所有slave的指令字节对应的位置（多个） slave复制偏移量：记录slave接收master发送过来的指令字节对应的位置（一个） 作用：同步信息，比对master与slave的差异，当slave断线后，恢复数据使用 数据来源： master端：发送一次记录一次 slave端：接收一次记录一次 字节值 工作原理 通过offset区分不同的slave当前数据传播的差异 master记录已发送的信息对应的offset slave记录已接收的信息对应的offset 2.2.2 流程更新(全量复制&#x2F;部分复制)我们再次的总结一下主从复制的三个阶段的工作流程： 2.2.3 心跳机制什么是心跳机制？ 进入命令传播阶段候，master与slave间需要进行信息交换，使用心跳机制进行维护，实现双方连接保持在线 master心跳： 内部指令：PING 周期：由repl-ping-slave-period决定，默认10秒 作用：判断slave是否在线 查询：INFO replication 获取slave最后一次连接时间间隔，lag项维持在0或1视为正常 slave心跳任务 内部指令：REPLCONF ACK {offset} 周期：1秒 作用1：汇报slave自己的复制偏移量，获取最新的数据变更指令 作用2：判断master是否在线 心跳阶段注意事项： 当slave多数掉线，或延迟过高时，master为保障数据稳定性，将拒绝所有信息同步 12min-slaves-to-write 2min-slaves-max-lag 8 slave数量少于2个，或者所有slave的延迟都大于等于8秒时，强制关闭master写功能，停止数据同步 slave数量由slave发送REPLCONF ACK命令做确认 slave延迟由slave发送REPLCONF ACK命令做确认 至此：我们可以总结出完整的主从复制流程： 2.3 主从复制常见问题2.3.1 频繁的全量复制 伴随着系统的运行，master的数据量会越来越大，一旦master重启，runid将发生变化，会导致全部slave的全量复制操作 内部优化调整方案： 1：master内部创建master_replid变量，使用runid相同的策略生成，长度41位，并发送给所有slave 2：在master关闭时执行命令shutdown save，进行RDB持久化,将runid与offset保存到RDB文件中 123repl-id repl-offset通过redis-check-rdb命令可以查看该信息 3：master重启后加载RDB文件，恢复数据，重启后，将RDB文件中保存的repl-id与repl-offset加载到内存中 123master_repl_id=repl master_repl_offset =repl-offset通过info命令可以查看该信息 作用：本机保存上次runid，重启后恢复该值，使所有slave认为还是之前的master 第二种出现频繁全量复制的问题现象：网络环境不佳，出现网络中断，slave不提供服务 问题原因：复制缓冲区过小，断网后slave的offset越界，触发全量复制 最终结果：slave反复进行全量复制 解决方案：修改复制缓冲区大小 1repl-backlog-size ?mb 建议设置如下： 1.测算从master到slave的重连平均时长second 2.获取master平均每秒产生写命令数据总量write_size_per_second 3.最优复制缓冲区空间 &#x3D; 2 * second * write_size_per_second 2.3.2 频繁的网络中断 问题现象：master的CPU占用过高 或 slave频繁断开连接 问题原因 12345slave每1秒发送REPLCONFACK命令到master当slave接到了慢查询时（keys * ，hgetall等），会大量占用CPU性能master每1秒调用复制定时函数replicationCron()，比对slave发现长时间没有进行响应 最终结果：master各种资源（输出缓冲区、带宽、连接等）被严重占用 解决方案：通过设置合理的超时时间，确认是否释放slave 1repl-timeout seconds 该参数定义了超时时间的阈值（默认60秒），超过该值，释放slave 问题现象：slave与master连接断开 问题原因 12345master发送ping指令频度较低master设定超时时间较短ping指令在网络中存在丢包 解决方案：提高ping指令发送的频度 1repl-ping-slave-period seconds 超时时间repl-time的时间至少是ping指令频度的5到10倍，否则slave很容易判定超时 2.3.3 数据不一致问题现象：多个slave获取相同数据不同步 问题原因：网络信息不同步，数据发送有延迟 解决方案 123优化主从间的网络环境，通常放置在同一个机房部署，如使用阿里云等云服务器时要注意此现象监控主从节点延迟（通过offset）判断，如果slave延迟过大，暂时屏蔽程序对该slave的数据访问 1slave-serve-stale-data yes|no 开启后仅响应info、slaveof等少数命令（慎用，除非对数据一致性要求很高） 3.哨兵模式3.1 哨兵简介3.1.1 哨兵概念首先我们来看一个业务场景：如果redis的master宕机了，此时应该怎么办？ 那此时我们可能需要从一堆的slave中重新选举出一个新的master，那这个操作过程是什么样的呢？这里面会有什么问题出现呢？ 要实现这些功能，我们就需要redis的哨兵，那哨兵是什么呢？ 哨兵 哨兵(sentinel) 是一个分布式系统，用于对主从结构中的每台服务器进行监控，当出现故障时通过投票机制选择新的master并将所有slave连接到新的master。 3.1.2 哨兵作用哨兵的作用： 监控：监控master和slave 不断的检查master和slave是否正常运行 master存活检测、master与slave运行情况检测 通知（提醒）：当被监控的服务器出现问题时，向其他（哨兵间，客户端）发送通知 自动故障转移：断开master与slave连接，选取一个slave作为master，将其他slave连接新的master，并告知客户端新的服务器地址 注意：哨兵也是一台redis服务器，只是不提供数据相关服务，通常哨兵的数量配置为单数 3.2 启用哨兵配置哨兵 配置一拖二的主从结构（利用之前的方式启动即可） 配置三个哨兵（配置相同，端口不同），参看sentinel.conf 1：设置哨兵监听的主服务器信息， sentinel_number表示参与投票的哨兵数量 1sentinel monitor master_name master_host master_port sentinel_number 2：设置判定服务器宕机时长，该设置控制是否进行主从切换 1sentinel down-after-milliseconds master_name million_seconds 3：设置故障切换的最大超时时 1sentinel failover-timeout master_name million_seconds 4：设置主从切换后，同时进行数据同步的slave数量，数值越大，要求网络资源越高，数值越小，同步时间越长 1sentinel parallel-syncs master_name sync_slave_number 启动哨兵 1redis-sentinel filename 3.3 哨兵工作原理哨兵在进行主从切换过程中经历三个阶段 监控 通知 故障转移 3.3.1 监控用于同步各个节点的状态信息 获取各个sentinel的状态（是否在线） 获取master的状态 1234master属性 prunid prole：master各个slave的详细信息 获取所有slave的状态（根据master中的slave信息） 12345slave属性 prunid prole：slave pmaster_host、master_port poffset 其内部的工作原理具体如下： 3.3.2 通知sentinel在通知阶段要不断的去获取master&#x2F;slave的信息，然后在各个sentinel之间进行共享，具体的流程如下： 3.3.3 故障转移当master宕机后sentinel是如何知晓并判断出master是真的宕机了呢？我们来看具体的操作流程 当sentinel认定master下线之后，此时需要决定更换master，那这件事由哪个sentinel来做呢？这时候sentinel之间要进行选举，如下图所示： 在选举的时候每一个人手里都有一票，而每一个人的又都想当这个处理事故的人，那怎么办？大家就开始抢，于是每个人都会发出一个指令，在内网里边告诉大家我要当选举人，比如说现在的sentinel1和sentinel4发出这个选举指令了，那么sentinel2既能接到sentinel1的也能接到sentinel4的，接到了他们的申请以后呢，sentinel2他就会把他的一票投给其中一方，投给谁呢？谁先过来我投给谁，假设sentinel1先过来，所以这个票就给到了sentinel1。那么给过去以后呢，现在sentinel1就拿到了一票，按照这样的一种形式，最终会有一个选举结果。对应的选举最终得票多的，那自然就成为了处理事故的人。需要注意在这个过程中有可能会存在失败的现象，就是一轮选举完没有选取，那就会接着进行第二轮第三轮直到完成选举。 接下来就是由选举胜出的sentinel去从slave中选一个新的master出来的工作，这个流程是什么样的呢？ 首先它有一个在服务器列表中挑选备选master的原则 不在线的OUT 响应慢的OUT 与原master断开时间久的OUT 优先原则 ​ 优先级​ offset​ runid 选出新的master之后，发送指令（ sentinel ）给其他的slave： 向新的master发送slaveof no one 向其他slave发送slaveof 新masterIP端口 总结：故障转移阶段 发现问题，主观下线与客观下线 竞选负责人 优选新master 新master上任，其他slave切换master，原master作为slave故障恢复后连接 4.集群cluster现状问题：业务发展过程中遇到的峰值瓶颈 redis提供的服务OPS可以达到10万&#x2F;秒，当前业务OPS已经达到10万&#x2F;秒 内存单机容量达到256G，当前业务需求内存容量1T 使用集群的方式可以快速解决上述问题 4.1 集群简介集群就是使用网络将若干台计算机联通起来，并提供统一的管理方式，使其对外呈现单机的服务效果 集群作用： 分散单台服务器的访问压力，实现负载均衡 分散单台服务器的存储压力，实现可扩展性 降低单台服务器宕机带来的业务灾难 4.2 Cluster集群结构设计数据存储设计： 通过算法设计，计算出key应该保存的位置 将所有的存储空间计划切割成16384份，每台主机保存一部分 注意：每份代表的是一个存储空间，不是一个key的保存空间 将key按照计算出的结果放到对应的存储空间 那redis的集群是如何增强可扩展性的呢？譬如我们要增加一个集群节点 当我们查找数据时，集群是如何操作的呢？ 各个数据库相互通信，保存各个库中槽的编号数据 一次命中，直接返回 一次未命中，告知具体位置 4.3 Cluster集群结构搭建首先要明确的几个要点： 配置服务器（3主3从） 建立通信（Meet） 分槽（Slot） 搭建主从（master-slave） Cluster配置 是否启用cluster，加入cluster节点 1cluster-enabled yes|no cluster配置文件名，该文件属于自动生成，仅用于快速查找文件并查询文件内容 1cluster-config-file filename 节点服务响应超时时间，用于判定该节点是否下线或切换为从节点 1cluster-node-timeout milliseconds master连接的slave最小数量 1cluster-migration-barrier min_slave_number Cluster节点操作命令 查看集群节点信息 1cluster nodes 更改slave指向新的master 1cluster replicate master-id 发现一个新节点，新增master 1cluster meet ip:port 忽略一个没有solt的节点 1cluster forget server_id 手动故障转移 1cluster failover 集群操作命令： 创建集群 1redis-cli –-cluster create masterhost1:masterport1 masterhost2:masterport2 masterhost3:masterport3 [masterhostn:masterportn …] slavehost1:slaveport1 slavehost2:slaveport2 slavehost3:slaveport3 -–cluster-replicas n 注意：master与slave的数量要匹配，一个master对应n个slave，由最后的参数n决定 master与slave的匹配顺序为第一个master与前n个slave分为一组，形成主从结构 添加master到当前集群中，连接时可以指定任意现有节点地址与端口 1redis-cli --cluster add-node new-master-host:new-master-port now-host:now-port 添加slave 1redis-cli --cluster add-node new-slave-host:new-slave-port master-host:master-port --cluster-slave --cluster-master-id masterid 删除节点，如果删除的节点是master，必须保障其中没有槽slot 1redis-cli --cluster del-node del-slave-host:del-slave-port del-slave-id 重新分槽，分槽是从具有槽的master中划分一部分给其他master，过程中不创建新的槽 1redis-cli --cluster reshard new-master-host:new-master:port --cluster-from src- master-id1, src-master-id2, src-master-idn --cluster-to target-master-id -- cluster-slots slots 注意：将需要参与分槽的所有masterid不分先后顺序添加到参数中，使用，分隔 指定目标得到的槽的数量，所有的槽将平均从每个来源的master处获取 重新分配槽，从具有槽的master中分配指定数量的槽到另一个master中，常用于清空指定master中的槽 1redis-cli --cluster reshard src-master-host:src-master-port --cluster-from src- master-id --cluster-to target-master-id --cluster-slots slots --cluster-yes 5.企业级解决方案5.1 缓存预热场景：“宕机” 服务器启动后迅速宕机 问题排查： 1.请求数量较高，大量的请求过来之后都需要去从缓存中获取数据，但是缓存中又没有，此时从数据库中查找数据然后将数据再存入缓存，造成了短期内对redis的高强度操作从而导致问题 2.主从之间数据吞吐量较大，数据同步操作频度较高 解决方案： 前置准备工作： 1.日常例行统计数据访问记录，统计访问频度较高的热点数据 2.利用LRU数据删除策略，构建数据留存队列例如：storm与kafka配合 准备工作： 1.将统计结果中的数据分类，根据级别，redis优先加载级别较高的热点数据 2.利用分布式多服务器同时进行数据读取，提速数据加载过程 3.热点数据主从同时预热 实施： 4.使用脚本程序固定触发数据预热过程 5.如果条件允许，使用了CDN（内容分发网络），效果会更好 总的来说：缓存预热就是系统启动前，提前将相关的缓存数据直接加载到缓存系统。避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！ 5.2 缓存雪崩场景：数据库服务器崩溃，一连串的场景会随之儿来 1.系统平稳运行过程中，忽然数据库连接量激增 2.应用服务器无法及时处理请求 3.大量408，500错误页面出现 4.客户反复刷新页面获取数据 5.数据库崩溃 6.应用服务器崩溃 7.重启应用服务器无效 8.Redis服务器崩溃 9.Redis集群崩溃 10.重启数据库后再次被瞬间流量放倒 问题排查： 1.在一个较短的时间内，缓存中较多的key集中过期 2.此周期内请求访问过期的数据，redis未命中，redis向数据库获取数据 3.数据库同时接收到大量的请求无法及时处理 4.Redis大量请求被积压，开始出现超时现象 5.数据库流量激增，数据库崩溃 6.重启后仍然面对缓存中无数据可用 7.Redis服务器资源被严重占用，Redis服务器崩溃 8.Redis集群呈现崩塌，集群瓦解 9.应用服务器无法及时得到数据响应请求，来自客户端的请求数量越来越多，应用服务器崩溃 10.应用服务器，redis，数据库全部重启，效果不理想 总而言之就两点：短时间范围内，大量key集中过期 解决方案 思路： 1.更多的页面静态化处理 2.构建多级缓存架构 ​ Nginx缓存+redis缓存+ehcache缓存 3.检测Mysql严重耗时业务进行优化 ​ 对数据库的瓶颈排查：例如超时查询、耗时较高事务等 4.灾难预警机制 ​ 监控redis服务器性能指标 ​ CPU占用、CPU使用率 ​ 内存容量 ​ 查询平均响应时间 ​ 线程数 5.限流、降级 短时间范围内牺牲一些客户体验，限制一部分请求访问，降低应用服务器压力，待业务低速运转后再逐步放开访问 落地实践： 1.LRU与LFU切换 2.数据有效期策略调整 ​ 根据业务数据有效期进行分类错峰，A类90分钟，B类80分钟，C类70分钟 ​ 过期时间使用固定时间+随机值的形式，稀释集中到期的key的数量 3.超热数据使用永久key 4.定期维护（自动+人工） ​ 对即将过期数据做访问量分析，确认是否延时，配合访问量统计，做热点数据的延时 5.加锁：慎用！ 总的来说：缓存雪崩就是瞬间过期数据量太大，导致对数据库服务器造成压力。如能够有效避免过期时间集中，可以有效解决雪崩现象的 出现（约40%），配合其他策略一起使用，并监控服务器的运行数据，根据运行记录做快速调整。 5.3 缓存击穿场景：还是数据库服务器崩溃，但是跟之前的场景有点不太一样 1.系统平稳运行过程中 2.数据库连接量瞬间激增 3.Redis服务器无大量key过期 4.Redis内存平稳，无波动 5.Redis服务器CPU正常 6.数据库崩溃 问题排查： 1.Redis中某个key过期，该key访问量巨大 2.多个数据请求从服务器直接压到Redis后，均未命中 3.Redis在短时间内发起了大量对数据库中同一数据的访问 总而言之就两点：单个key高热数据，key过期 解决方案： 1.预先设定 ​ 以电商为例，每个商家根据店铺等级，指定若干款主打商品，在购物节期间，加大此类信息key的过期时长 注意：购物节不仅仅指当天，以及后续若干天，访问峰值呈现逐渐降低的趋势 2.现场调整 ​ 监控访问量，对自然流量激增的数据延长过期时间或设置为永久性key 3.后台刷新数据 ​ 启动定时任务，高峰期来临之前，刷新数据有效期，确保不丢失 4.二级缓存 ​ 设置不同的失效时间，保障不会被同时淘汰就行 5.加锁 ​ 分布式锁，防止被击穿，但是要注意也是性能瓶颈，慎重！ 总的来说：缓存击穿就是单个高热数据过期的瞬间，数据访问量较大，未命中redis后，发起了大量对同一数据的数据库访问，导致对数 据库服务器造成压力。应对策略应该在业务数据分析与预防方面进行，配合运行监控测试与即时调整策略，毕竟单个key的过 期监控难度较高，配合雪崩处理策略即可。 5.4 缓存穿透场景：数据库服务器又崩溃了，跟之前的一样吗？ 1.系统平稳运行过程中 2.应用服务器流量随时间增量较大 3.Redis服务器命中率随时间逐步降低 4.Redis内存平稳，内存无压力 5.Redis服务器CPU占用激增 6.数据库服务器压力激增 7.数据库崩溃 问题排查： 1.Redis中大面积出现未命中 2.出现非正常URL访问 问题分析： 获取的数据在数据库中也不存在，数据库查询未得到对应数据 Redis获取到null数据未进行持久化，直接返回 下次此类数据到达重复上述过程 出现黑客攻击服务器 解决方案： 1.缓存null ​ 对查询结果为null的数据进行缓存（长期使用，定期清理），设定短时限，例如30-60秒，最高5分钟 2.白名单策略 ​ 提前预热各种分类数据id对应的bitmaps，id作为bitmaps的offset，相当于设置了数据白名单。当加载正常数据时放行，加载异常数据时直接拦截（效率偏低） ​ 使用布隆过滤器（有关布隆过滤器的命中问题对当前状况可以忽略） 2.实施监控 ​ 实时监控redis命中率（业务正常范围时，通常会有一个波动值）与null数据的占比 ​ 非活动时段波动：通常检测3-5倍，超过5倍纳入重点排查对象 ​ 活动时段波动：通常检测10-50倍，超过50倍纳入重点排查对象 ​ 根据倍数不同，启动不同的排查流程。然后使用黑名单进行防控（运营） 4.key加密 ​ 问题出现后，临时启动防灾业务key，对key进行业务层传输加密服务，设定校验程序，过来的key校验 ​ 例如每天随机分配60个加密串，挑选2到3个，混淆到页面数据id中，发现访问key不满足规则，驳回数据访问 总的来说：缓存击穿是指访问了不存在的数据，跳过了合法数据的redis数据缓存阶段，每次访问数据库，导致对数据库服务器造成压力。通常此类数据的出现量是一个较低的值，当出现此类情况以毒攻毒，并及时报警。应对策略应该在临时预案防范方面多做文章。 无论是黑名单还是白名单，都是对整体系统的压力，警报解除后尽快移除。 5.5 性能指标监控redis中的监控指标如下： 性能指标：Performance 响应请求的平均时间: 1&gt;latency 平均每秒处理请求总数 1&gt;instantaneous_ops_per_sec 缓存查询命中率（通过查询总次数与查询得到非nil数据总次数计算而来） 12&gt;hit_rate(calculated) 内存指标：Memory 当前内存使用量 1&gt;used_memory 内存碎片率（关系到是否进行碎片整理） 1&gt;mem_fragmentation_ratio 为避免内存溢出删除的key的总数量 1&gt;evicted_keys 基于阻塞操作（BLPOP等）影响的客户端数量 1&gt;blocked_clients 基本活动指标：Basic_activity 当前客户端连接总数 1&gt;connected_clients 当前连接slave总数 1&gt;connected_slaves 最后一次主从信息交换距现在的秒 1&gt;master_last_io_seconds_ago key的总数 1&gt;keyspace 持久性指标：Persistence 当前服务器最后一次RDB持久化的时间 1&gt;rdb_last_save_time 当前服务器最后一次RDB持久化后数据变化总量 1&gt;rdb_changes_since_last_save 错误指标：Error 被拒绝连接的客户端总数（基于达到最大连接值的因素） 1&gt;rejected_connections key未命中的总次数 1&gt;keyspace_misses 主从断开的秒数 1&gt;master_link_down_since_seconds 要对redis的相关指标进行监控，我们可以采用一些用具： CloudInsight Redis Prometheus Redis-stat Redis-faina RedisLive zabbix 也有一些命令工具： benchmark 测试当前服务器的并发性能 1&gt;redis-benchmark [-h ] [-p ] [-c ] [-n &lt;requests]&gt; [-k ] 范例1：50个连接，10000次请求对应的性能 1&gt;redis-benchmark 范例2：100个连接，5000次请求对应的性能 1&gt;redis-benchmark -c 100 -n 5000 redis-cli ​ monitor：启动服务器调试信息 1&gt;monitor slowlog：慢日志 获取慢查询日志 1&gt;slowlog [operator] ​ get ：获取慢查询日志信息 ​ len ：获取慢查询日志条目数 ​ reset ：重置慢查询日志 相关配置 12&gt;slowlog-log-slower-than 1000 #设置慢查询的时间下线，单位：微妙&gt;slowlog-max-len 100 #设置慢查询命令对应的日志显示长度，单位：命令数 Redis的高效在于其纯内存运算，但是有得就有失，数据全部存在内存中意味着一旦宕机，数据将会全部丢失，因此必须需要一种机制来保证Redis中的数据不会因为故障而丢失，这就需要Redis拥有数据持久化的能力。 6. 持久化Redis的持久化机制有两种，一种是快照，也就是RDB（Redis DataBase），一种是AOF（Append Only File）日志。 1.1 快照（RDB）快照是一次性的全量备份，将某一时刻的全量数据以二进制序列化的形式存储，在空间上非常紧凑，能大大缩小存储所用的空间。 Redis是单线程，而文件IO操作是不支持多路复用的。这难道意味着在进行内存快照时Redis需要停止服务？这当然是不行的，那有指令时服务，没指令时持久化这样边持久化边服务？可是这样的话持久化的同时内存数据还在被指令修改，如果在持有化一个大的Hash字典时，过来一个指令把这个字段删了，这个可怎么办? 显然这样也不行。 为此，Redis使用操作系统的多进程COW（Copy On Write）机制来实现快照持久化。 Redis在持久化时会调用glibc的函数fork产生一个子进程，快照持久化交给子进程处理，父进程继续提供服务。子进程生成时和父进程共用代码段和数据段。也就是说这时间父子进程共享内存数据，因此在分离的一瞬间，内存消耗几乎没有。接下来子内存进行数据持久化，他仅仅是读取，不会修改内存。而父进程对外提供服务，修改数据，但是操作系统的COW机制会进行数据段页面的分离，数据段由操作系统的页面组合而成，父进程修改数据时，COW机制就将数据所在页复制一份出来，父进程在这个复制出来的数据也修改，此时原数据页也就是子线程访问的数据页还是原样，也就是子进程所看到的数据在子进程产生的一瞬间就已经凝固了，可以安心复制，这也是为什么这种持久化方法称为快照原因。 随着父进程的修改，会有越来越多的页面被赋值，但是最多也就是全复制，达到原内存空间的二倍，但是这在大数据量情况下很难发生，因为总会有冷数据存在，而且可能占据多数，所以复制的一般只会是其中的一部分。另外提一下：一个页面的大小是4K。 1.2 AOF日志AOF日志是连续性的增量备份，记录的是修改内存数据的指令记录文本。这样就可以通过对一个空的Redis实例顺序执行记录的命令，也就是重放，来复原实例。Redis在收到修改指令后，会先进行校验，如果没问题，会首先把指令追加记录磁盘上的AOF日志中，然后再执行指令，这样即使突发宕机，重放时也能重放到这个指令。 AOF日志随着运行时间的增长会变的越来越庞大，Redis重启时需要加载AOF日志进行指令重放所需的时间也会更加漫长，所以需要定期对AOF重写，进行瘦身。 1.2.1 AOF重写AOF重写原理就是开辟一个子进程，然后将内存数据遍历并转换成指令，再记录到一个新的AOF文件中，完毕后再将期间发生的增量AOF日志追加到新的AOF日志中，替换旧的AOF文件，就完成了AOF重写的工作，完成了瘦身。 1.2.2 fsyncAOF日志是以文件方式存在的，程序对AOF日志进行操作时，实际上是先将内容写到内核为文件描述符分配的一块内存缓存上，然后内核异步将数据写入磁盘的。 但是如果机器突然宕机，内存缓存中的数据还没来的及写入磁盘，就会出现日志的丢失。Linux的gilbc提了了fsync(int fd)函数来强制把指定文件的内存缓存数据写入到磁盘中，实时使用fsync就能保证AOF日志不丢失。但是fsync涉及到磁盘写入，相较于内存操作会慢很多，如果每一个指令都fsync一次，Redis纯内存操作所带来的优势就不存在了。 因此目前主流的做法是Redis每隔1s执行一个fsync，1s是可配置的，可以根据需要配置。这样就在保持高效能的同时尽可能的减少日志丢失。Redis也提供了另外两种策略：一种是永不fsync，由操作系统决定什么时间将内存缓存同步到磁盘，这样无法掌控，很不安全。另一种是一次指令fsync一次，然后不会丢日志，单缺点上面也说过了，生产并不推荐。 Redis4.0新增了异步模型，可以打开fsync的异步处理开关，此时主线程不进行fsync，而是生成任务放到专门的fsync队列中去，由专门的fsync异步线程处理。 1.3 持久化选在从节点无论是快照还是AOF，都比较消耗资源。快照需要遍历整个内存，大块磁盘读写加重系统负载。AOF的fsync是一个耗时的IO操作，也会影响Redis性能，加重系统IO负担。因此Redis的持久化一般并不安排在主节点，而是在从节点进行，从节点没有客户端请求的压力，资源比较充足。但是如果出现网络分区，从节点连不上主节点，而主节点又宕机了，就会出现数据丢失产生数据一致性的问题。因此生产环境需要做好网络连通性检测，保证出现问题时能快速修复，除此之外可以再挂一个从节点，这样只要有一个从节点数据同步正常，数据就不会丢失。 1.4 Redis4.0的混合持久化Redis重启时，很少使用RDB来恢复数据，因为会丢失最后一次快照之后的数据。但是使用AOF日志重放，效率上又会慢很多。因此Redis4.0提供了混合持久化的策略，就是RDB和AOF同时使用。RDB正常持久化，而AOF不在记录全量指令，而是记录每次RDB快照之后的增量AOF，这样Redis重启时就可以先加载RDB的内容，然后再重放AOF日志，效率大大提升。 总结说出redis中的数据删除策与略淘汰策略 说出主从复制的概念，工作流程以及场景问题及解决方案 说出哨兵的作用以及工作原理，以及如何启用哨兵 说出集群的架构设计，完成集群的搭建 说出缓存预热，雪崩，击穿，穿透的概念，能说出redis的相关监控指标 说出持久化策略的其中的一些细节 第八章 缓存预热+缓存雪崩+缓存击穿+缓存穿透缓存预热统计热点数据(访问频率高的)提前存入缓存中(数据多 多服务并行写) 具体 nginx + lua 将访问量上报到消息队列(?) ​ 要统计出来当前最新的实时的热数据是哪些，我们就得将商品详情页访问的请求对应的流量，日志，实时上报 到kafka中 缓存雪崩发生: ​ redis主机挂了，Redis 全盘崩溃, ​ 比如缓存中有大量数据同时过期 解决: ​ 主从+哨兵 ​ 集群 ​ ehcache本地缓存 + Hystrix或者阿里sentinel限流&amp;降级 ​ 开启Redis持久化机制aof&#x2F;rdb，尽快恢复缓存集群 缓存穿透发生: ​ 请求去查询一条记录，先redis后mysql发现都查询不到该条记录， ​ 但是请求每次都会打到数据库上面去，导致后台数据库压力暴增， ​ 这种现象我们称为缓存穿透，这个redis变成了一个摆设。。。。。。 ​ 简单说就是本来无一物，既不在Redis缓存中，也不在数据库中 危害: ​ 第一次来查询后，一般我们有回写redis机制 ​ 第二次来查的时候redis就有了，偶尔出现穿透现象一般情况无关紧要 解决: 方案1：空对象缓存或者缺省值 一般ok but 黑客会对你的系统进行攻击，拿一个不存在的id 去查询数据，会产生大量的请求到数据库去查询。 可能会导致你的数据库由于压力过大而宕掉 方案2：Google布隆过滤器Guava解决缓存穿透 只能单机使用 Guava 中布隆过滤器的实现算是比较权威的， 所以实际项目中我们不需要手动实现一个布隆过滤器 方案3：Redis布隆过滤器解决缓存穿透 缓存击穿发生: ​ 大量的请求同时查询一个 key 时， ​ 此时这个key正好失效了，就会导致大量的请求都打到数据库上面去 ​ 简单说就是热点key突然失效了，暴打mysql 危害: ​ 会造成某一时刻数据库请求量过大，压力剧增。 解决 方案2：对于访问频繁的热点key，干脆就不设置过期时间 方案3：互斥独占锁防止击穿 多个线程同时去查询数据库的这条数据，那么我们可以在第一个查询数据的请求上使用一个 互斥锁来锁住它。 其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。 案例第九章 Redis分布式锁第十一 章 经典五种数据类型底层实现dictEntry启动流程到内部的表: 源码结构体及解析 将dictEntry理解 &lt;String(sds),redisObject&gt; 不一定全对 key 是字符串，但是 Redis 没有直接使用 C 的字符数组，而是存储在redis自定义的 SDS(简单动态字符串,simple dynimic string)中。 value 既不是直接作为字符串存储，也不是直接存储在 SDS 中，而是存储在redisObject 中。 set hello word为例，因为Redis是KV键值对的数据库，每个键值对都会有一个dictEntry(源码位置：dict.h)， 里面指向了key和value的指针，next 指向下一个 dictEntry。 redisObject 后面的类型会加深对redisObject的理解 SringString的三种编码格式 int当字符串键值的内容可以用一个64位有符号整形来表示时，Redis会将键值转化为long型来进行存储，此时即对应 OBJ_ENCODING_INT 编码类型。内部的内存结构表示如下: Redis 启动时会预先建立 10000 个分别存储 09999 的 redisObject 变量作为共享对象，这就意味着如果 set字符串的键值在 010000 之间的话，则可以 直接指向共享对象 而不需要再建立新对象，此时键值不占空间！ set k1 123 set k2 123 保存long 型(长整型)的64位(8个字节)有符号整数 最多19位只能整数浮点数就是字符串值了 embstr对于长度小于 44的字符串，Redis 对键值采用OBJ_ENCODING_EMBSTR 方式，EMBSTR 顾名思义即：embedded string，表示嵌入式的String。从内存结构上来讲 即字符串 sds结构体与其对应的 redisObject 对象分配在同一块连续的内存空间，字符串sds嵌入在redisObject对象之中一样。 代表 embstr 格式的 SDS(Simple Dynamic String 简单动态字符串),保存长度小于44字节的字符串 Redis中字符串的实现,SDS有多种结构（sds.h）： sdshdr5、(2^5&#x3D;32byte) sdshdr8、(2 ^ 8&#x3D;256byte) sdshdr16、(2 ^ 16&#x3D;65536byte&#x3D;64KB) sdshdr32、 (2 ^ 32byte&#x3D;4GB) sdshdr64，2的64次方byte＝17179869184G用于存储不同的长度的字符串。 len 表示 SDS 的长度，使我们在获取字符串长度的时候可以在 O(1)情况下拿到，而不是像 C 那样需要遍历一遍字符串。 alloc 可以用来计算 free 就是字符串已经分配的未使用的空间，有了这个值就可以引入预分配空间的算法了，而不用去考虑内存分配的问题。 buf 表示字符串数组，真存数据的。 raw保存长度大于44字节的字符串 当字符串的键值为长度大于44的超长字符串时，Redis 则会将键值的内部编码方式改为OBJ_ENCODING_RAW格式，这与OBJ_ENCODING_EMBSTR编码方式的不同之处在于，此时动态字符串sds的内存与其依赖的redisObject的内存不再连续了 set hello 观察以String为例的redisObject实际上五种常用的数据类型的任何一种，都是通过 redisObject 来存储的。 debug指令可能出现异常 (error) ERR DEBUG command not allowed. If the enable-debug-command option is set to “local”, you can run it from a local connection, otherwise you need to set this option in the configuration file, and then restart the server 需要设置参数 直接加一行 raw &gt;&#x3D; 44位 流程图 Hash概述hash-max-ziplist-entries：使用压缩列表保存时哈希集合中的最大元素个数。 hash-max-ziplist-value：使用压缩列表保存哈希集合中单个元素的最大长度。 结论: 1.哈希对象保存的键值对数量小于 512 个； 2.所有的键值对的健和值的字符串长度都小于等于 64byte（一个英文字母一个字节） 时用ziplist，反之用hashtable ziplist升级到hashtable可以，反过来降级不可以 一旦从压缩列表转为了哈希表，Hash类型就会一直用哈希表进行保存而不会再转回压缩列表了。 在节省内存空间方面哈希表就没有压缩列表高效了。 后面会讲 回来再看 hash的两种编码格式ziplist(压缩列表)Ziplist 压缩列表是一种紧凑编码格式，总体思想是多花时间来换取节约空间，即以部分读写性能为代价，来换取极高的内存空间利用率， 因此只会用于 字段个数少，且字段值也较小 的场景。压缩列表内存利用率极高的原因与其连续内存的特性是分不开的 想想我们的学过的一种GC垃圾回收机制：标记–压缩算法 当一个 hash对象 只包含少量键值对且每个键值对的键和值要么就是小整数要么就是长度比较短的字符串，那么它用 ziplist 作为底层实现 不懂 todo ziplist是一个经过特殊编码的双向链表，它不存储指向上一个链表节点和指向下一个链表节点的指针，而是存储上一个节点长度和当前节点长度，通过牺牲部分读写性能，来换取高效的内存空间利用率，节约内存，是一种时间换空间的思想。只用在字段个数少，字段值小的场景里面 ZipList结构本质上是字节数组 zlend是一个单字节255(1111 1111)，用做ZipList的结尾标识符。见下：压缩列表结构：由zlbytes、zltail、zllen、entry、zlend这五部分组成 ziplistEntry结构 压缩列表zlentry节点结构：每个zlentry由前一个节点的长度、encoding和entry-data三部分组成 前节点：(前节点占用的内存字节数)表示前1个zlentry的长度，prev_len有两种取值情况：1字节或5字节。取值1字节时，表示上一个entry的长度小于254字节。虽然1字节的值能表示的数值范围是0到255，但是压缩列表中zlend的取值默认是255，因此，就默认用255表示整个压缩列表的结束，其他表示长度的地方就不能再用255这个值了。所以，当上一个entry长度小于254字节时，prev_len取值为1字节，否则，就取值为5字节。 enncoding：记录节点的content保存数据的类型和长度。 content：保存实际数据内容 123456789101112131415161718192021222324252627typedef struct zlentry &#123; // 压缩列表节点 // prevrawlen是前一个节点的长度 //prevrawlensize是指prevrawlen的大小，有1字节和5字节两种 unsigned int prevrawlensize, prevrawlen; // len为当前节点长度 lensize为编码len所需的字节大小 unsigned int lensize, len; // 当前节点的header大小 unsigned int headersize; // 节点的编码方式 unsigned char encoding; // 指向节点的指针 unsigned char *p; &#125; zlentry; 压缩列表的遍历通过指向表尾节点的位置指针p1, 减去节点的previous_entry_length(前一个结点的长度)，得到前一个节点起始地址的指针。如此循环，从表尾遍历到表头节点。从表尾向表头遍历操作就是使用这一原理实现的，只要我们拥有了一个指向某个节点起始地址的指针，那么通过这个指针以及这个节点的previous_entry_length属性程序就可以一直向前一个节点回溯，最终到达压缩列表的表头节点。 存取情况 hashtable 在 Redis 中，hashtable 被称为字典（dictionary），它是一个数组+链表的结构 OBJ_ENCODING_HT源码分析OBJ_ENCODING_HT 这种编码方式内部才是真正的哈希表结构，或称为字典结构，其可以实现O(1)复杂度的读写操作，因此效率很高。 在 Redis内部，从 OBJ_ENCODING_HT类型到底层真正的散列表数据结构是一层层嵌套下去的，组织关系见面图： 源代码：dict.h Listlist的一种编码格式list用quicklist来存储，quicklist存储了一个双向链表，每个节点都是一个ziplist 在低版本的Redis中，list采用的底层数据结构是ziplist+linkedList； 高版本的Redis中底层数据结构是quicklist(它替换了ziplist+linkedList)，而quicklist也用到了ziplist quicklist 实际上是 zipList 和 linkedList 的混合体，它将 linkedList按段切分，每一段使用 zipList 来紧凑存储，多个 zipList 之间使用双向指针串接起来。 案例: (1) ziplist压缩配置：list-compress-depth 0 表示一个quicklist两端不被压缩的节点个数。这里的节点是指quicklist双向链表的节点，而不是指ziplist里面的数据项个数 参数list-compress-depth的取值含义如下：两端各有x个端点不压缩 0: 是个特殊值，表示都不压缩。这是Redis的默认值。 1: 表示quicklist两端各有1个节点不压缩，中间的节点压缩。 2: 表示quicklist两端各有2个节点不压缩，中间的节点压缩。 3: 表示quicklist两端各有3个节点不压缩，中间的节点压缩。 依此类推… (2) ziplist中entry配置：list-max-ziplist-size -2 当取正值的时候，表示按照数据项个数来限定每个quicklist节点上的ziplist长度。比如，当这个参数配置成5的时候，表示每个quicklist节点的ziplist最多包含5个数据项。当取负值的时候，表示按照占用字节数来限定每个quicklist节点上的ziplist长度。这时，它只能取-1到-5这五个值， 每个值含义如下： -5: 每个quicklist节点上的ziplist大小不能超过64 Kb。（注：1kb &#x3D;&gt; 1024 bytes） -4: 每个quicklist节点上的ziplist大小不能超过32 Kb。 -3: 每个quicklist节点上的ziplist大小不能超过16 Kb。 -2: 每个quicklist节点上的ziplist大小不能超过8 Kb。（-2是Redis给出的默认值） -1: 每个quicklist节点上的ziplist大小不能超过4 Kb。 源码分析 SetSet的两种编码格式intset hashtable 案例 Redis用intset或hashtable存储set。如果元素都是整数类型，就用intset存储。 如果不是整数类型，就用hashtable（数组+链表的存来储结构）。key就是元素的值，value为null。 源码分析 ZsetZSet的两种编码格式ziplist skiplist 案例: 当有序集合中包含的元素数量超过服务器属性 server.zset_max_ziplist_entries 的值（默认值为 128 ）， 或者有序集合中新添加元素的 member 的长度大于服务器属性 server.zset_max_ziplist_value 的值（默认值为 64 ）时， redis会使用跳跃表作为有序集合的底层实现。 否则会使用ziplist作为有序集合的底层实现 源码分析 skipList跳表是可以实现二分查找的有序链表 skiplist是一种以空间换取时间的结构。 由于链表，无法进行二分查找，因此借鉴数据库索引的思想，提取出链表中关键节点（索引），先在关键节点上查找，再进入下层链表查找。 提取多层关键节点，就形成了跳跃表 总结来讲 跳表 &#x3D; 链表 + 多级索引 解决方法：升维，也叫空间换时间。 跳表查询的时间复杂度分析 首先每一级索引我们提升了2倍的跨度，那就是减少了2倍的步数，所以是n&#x2F;2、n&#x2F;4、n&#x2F;8以此类推； 第 k 级索引结点的个数就是 n&#x2F;(2^k)； 假设索引有 h 级， 最高的索引有2个结点；n&#x2F;(2^h) &#x3D; 2, 从这个公式我们可以求得 h &#x3D; log2(N)-1； 所以最后得出跳表的时间复杂度是O(logN) 跳表查询的空间复杂度分析 首先原始链表长度为n 如果索引是每2个结点有一个索引结点，每层索引的结点数：n&#x2F;2, n&#x2F;4, n&#x2F;8 … , 8, 4, 2 以此类推； 或者所以是每3个结点有一个索引结点，每层索引的结点数：n&#x2F;3, n&#x2F;9, n&#x2F;27 … , 9, 3, 1 以此类推； 所以空间复杂度是O(n)； 跳表是一个最典型的空间换时间解决方案，而且只有在数据量较大的情况下才能体现出来优势。而且应该是读多写少的情况下才能使用，所以它的适用范围应该还是比较有限的 维护成本相对要高 - 新增或者删除时需要把所有索引都更新一遍； 最后在新增和删除的过程中的更新，时间复杂度也是O(log n) 第十二章 Redis与MySQL数据双写一致性工程落地案例1. canal是什么Canal是基于MySQL变更日志增量订阅和消费的组件 canal [kə’næl]，中文翻译为 水道&#x2F;管道&#x2F;沟渠&#x2F;运河，主要用途是用于 MySQL 数据库增量日志数据的订阅、消费和解析，是阿里巴巴开发并开源的，采用Java语言开发； 历史背景是早期阿里巴巴因为杭州和美国双机房部署，存在跨机房数据同步的业务需求，实现方式主要是基于业务 trigger（触发器） 获取增量变更。从2010年开始，阿里巴巴逐步尝试采用解析数据库日志获取增量变更进行同步，由此衍生出了canal项目； 能干嘛 数据库镜像 数据库实时备份 索引构建和实时维护(拆分异构索引、倒排索引等) 业务 cache 刷新 带业务逻辑的增量数据处理 2. 相关面试2.1 MySQL的主从复制 MySQL的主从复制将经过如下步骤： 1、当 master 主服务器上的数据发生改变时，则将其改变写入二进制事件日志文件中； 2、salve 从服务器会在一定时间间隔内对 master 主服务器上的二进制日志进行探测，探测其是否发生过改变， 如果探测到 master 主服务器的二进制事件日志发生了改变，则开始一个 I&#x2F;O Thread 请求 master 二进制事件日志； 3、同时 master 主服务器为每个 I&#x2F;O Thread 启动一个dump Thread，用于向其发送二进制事件日志； 4、slave 从服务器将接收到的二进制事件日志保存至自己本地的中继日志文件中； 5、salve 从服务器将启动 SQL Thread 从中继日志中读取二进制日志，在本地重放，使得其数据和主服务器保持一致； 6、最后 I&#x2F;O Thread 和 SQL Thread 将进入睡眠状态，等待下一次被唤醒； 2.2 canal工作原理canal 工作原理 canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) canal 解析 binary log 对象(原始为 byte 流) mysql-canal-redis双写一致性Coding第十三章 缓存双写一致性之更新策略探讨缓存双写一致性，谈谈你的理解 如果redis中有数据 需要和数据库中的值相同 如果redis中无数据 数据库中的值要是最新值 缓存按照操作来分，有细分2种 ​ 只读缓存 ​ 读写缓存 ​ 同步直写策略：写缓存时也同步写数据库，缓存和数据库中的数据⼀致； ​ 对于读写缓存来说，要想保证缓存和数据库中的数据⼀致，就要采⽤同步直写策略 数据库和缓存一致性的几种更新策略 目的给缓存设置过期时间，是保证最终一致性的解决方案。 我们可以对存入缓存的数据设置过期时间，所有的写操作以数据库为准，对缓存操作只是尽最大努力即可。也就是说如果数据库写成功，缓存更新失败，那么只要到达过期时间，则后面的读请求自然会从数据库中读取新值然后回填缓存，达到一致性，切记以mysql的数据库写入库为准。 上述方案和后续落地案例是调研后的主流+成熟的做法，但是考虑到各个公司业务系统的差距， 不是100%绝对正确，不保证绝对适配全部情况，请同学们自行酌情选择打法，合适自己的最好。 先更新数据库，再更新缓存问题 1 先更新mysql的某商品的库存，当前商品的库存是100，更新为99个。 2 先更新mysql修改为99成功，然后更新redis。 3 此时假设异常出现，更新redis失败了，这导致mysql里面的库存是99而redis里面的还是100 。 4 上述发生，会让数据库里面和缓存redis里面数据不一致，读到脏数据 先删除缓存，再更新数据库问题表示更新数据库可能失败 1 A线程先成功删除了redis里面的数据，然后去更新mysql，此时mysql正在更新中，还没有结束。（比如网络延时） B突然出现要来读取缓存数据。 异常问题2: 2 此时redis里面的数据是空的，B线程来读取，先去读redis里数据(已经被A线程delete掉了)，此处出来2个问题： 2.1 B从mysql获得了旧值 ​ B线程发现redis里没有(缓存缺失)马上去mysql里面读取，从数据库里面读取来的是旧值。 2.2 B会把获得的旧值写回redis 获得旧值数据后返回前台并回写进redis(刚被A线程删除的旧数据有极大可能又被写回了)。 3 A线程更新完mysql，发现redis里面的缓存是脏数据，A线程直接懵逼了，o(╥﹏╥)o 两个并发操作，一个是更新操作，另一个是查询操作，A更新操作删除缓存后，B查询操作没有命中缓存，B先把老数据读出来后放到缓存中，然后A更新操作更新了数据库。 于是，在缓存中的数据还是老的数据，导致缓存中的数据是脏的，而且还一直这样脏下去了。 4 总结流程： （1）请求A进行写操作，删除缓存后，工作正在进行中……A还么有彻底更新完 （2）请求B开工，查询redis发现缓存不存在 （3）请求B继续，去数据库查询得到了myslq中的旧值 （4）请求B将旧值写入redis缓存 （5）请求A将新值写入mysql数据库 上述情况就会导致不一致的情形出现。 时间 线程A 线程B 出现的问题 t1 请求A进行写操作，删除缓存后，工作正在进行中…… t2 1 缓存中读取不到，立刻读mysql，由于A还没有对mysql更新完，读到的是旧值。 2 还把从mysql读取的旧值，写回了redis 1 A还更新完mysql，导致B读到了旧值 2 线程B遵守回写机制，把旧值写回redis，导致其它请求读取的还是旧值，A白干了。 t3 更新mysql数据库的值，over redis是被B写回的旧值， mysql是被A更新的新值。 出现了，数据不一致问题。 总结 先删除缓存，再更新数据库 如果数据库更新失败，导致B线程请求再次访问缓存时，发现redis里面没数据，缓存缺失，再去读取mysql时，从数据库中读取到旧值 解决方案多个线程同时去查询数据库的这条数据，那么我们可以在第一个查询数据的请求上使用一个 互斥锁来锁住它。 其他的线程走到这一步拿不到锁就等着，等第一个线程查询到了数据，然后做缓存。 后面的线程进来发现已经有缓存了，就直接走缓存。 延时双删 双删方案面试题这个删除该休眠多久呢线程Asleep的时间，就需要大于线程B读取数据再写入缓存的时间。 这个时间怎么确定呢？ 在业务程序运行的时候，统计下线程读数据和写缓存的操作时间，自行评估自己的项目的读数据业务逻辑的耗时， 以此为基础来进行估算。然后写数据的休眠时间则在读数据业务逻辑的耗时基础上加百毫秒即可。 这么做的目的，就是确保读请求结束，写请求可以删除读请求造成的缓存脏数据。 当前演示的效果是mysql单机，如果mysql主从读写分离架构如何？（1）请求A进行写操作，删除缓存 （2）请求A将数据写入数据库了， （3）请求B查询缓存发现，缓存没有值 （4）请求B去从库查询，这时，还没有完成主从同步，因此查询到的是旧值 （5）请求B将旧值写入缓存 （6）数据库完成主从同步，从库变为新值 上述情形，就是数据不一致的原因。还是使用双删延时策略。 只是，睡眠时间修改为在主从同步的延时时间基础上，加几百ms 这种同步淘汰策略，吞吐量降低怎么办？ 先更新数据库，再删除缓存问题时间 线程A 线程B 出现的问题 t1 删除数据库中的值 t2 缓存中立刻命中，此时B读取的是缓存旧值。 A还没有来得及删除缓存的值，导致B缓存命中读到旧值。 t3 更新缓存的数据，over 先更新数据库，再删除缓存 假如缓存删除失败或者来不及，导致请求再次访问redis时缓存命中，读取到的是缓存旧值 解决方案 1 可以把要删除的缓存值或者是要更新的数据库值暂存到消息队列中（例如使用Kafka&#x2F;RabbitMQ等）。 2 当程序没有能够成功地删除缓存值或者是更新数据库值时，可以从消息队列中重新读取这些值，然后再次进行删除或更新。 3 如果能够成功地删除或更新，我们就要把这些值从消息队列中去除，以免重复操作，此时，我们也可以保证数据库和缓存的数据一致了，否则还需要再次进行重试 4 如果重试超过的一定次数后还是没有成功，我们就需要向业务层发送报错信息了，通知运维人员。 总结方案2和方案3用那个？利弊如何在大多数业务场景下，我们会把Redis作为只读缓存使用。假如定位是只读缓存来说， 理论上我们既可以先删除缓存值再更新数据库，也可以先更新数据库再删除缓存，但是没有完美方案，两害相衡趋其轻的原则 个人建议是，优先使用先更新数据库，再删除缓存的方案。理由如下： 1 先删除缓存值再更新数据库，有可能导致请求因缓存缺失而访问数据库，给数据库带来压力，严重导致打满mysql。 2 如果业务应用中读取数据库和写缓存的时间不好估算，那么，延迟双删中的等待时间就不好设置。 多补充一句：如果使用先更新数据库，再删除缓存的方案 如果业务层要求必须读取一致性的数据，那么我们就需要在更新数据库时，先在Redis缓存客户端暂存并发读请求，等数据库更新完、缓存值删除后，再读取数据，从而保证数据一致性。","categories":[{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"tags":[{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"JUC","slug":"8JUC","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:54:37.581Z","comments":true,"path":"2022/09/01/8JUC/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/8JUC/","excerpt":"","text":"多线程一创建Thread类（子类）的对象1 定义Thread类的子类123456789101112131415161718Public class MyThread extends Thread&#123;@Overridepublic void run()&#123; System.out.println(&quot;重写Thread父类中的run（）&quot;);&#125;public class Test &#123; public static void main(String[] args)&#123; System.out.println(&quot;JVM启动main线程，main线程执行main方法&quot;); MyThread thread = new MyThread(); thread.start(); //启动线程的实质是请求将JVM运行相应的线程，这个线程具体在什么时候运行由（Scheduler）线程调度器来决定 //start（）方法调用结束不意味着子线程开始运行 //新开启的线程会执行run（）方法 &#125;&#125; 2，当Thread已经有子类了，就不能用第一种方式了 可以使用实现Runnable接口的形式123456789101112131415161718192021222324public class MyRunnable implements Runnable&#123; //重写Runnable接口中的抽象方法 run（），run()方法就是子线程要执行的代码 @Override public void run()&#123; System.out.println(&quot;必须通过线程start开启&quot;); &#125;&#125;public class Test&#123; public static void main(String[] args)&#123; MyRunnable runnable = new MyRunnable(); Thread thread = new Thread(runnable); thread.start(); // Thread(Runnable) 实参可以是匿名内部类对象（不需要变量名） Thread tread2 = new Thread(new Runnable()&#123; @Override public void run()&#123; //.... &#125; &#125;); thread2.start(); &#125; &#125; 线程常用方法1.Thread.currentThread() 类方法 获得当前进程， Java中任何一段代码都是执行在某个线程中的，执行当前代码的线程就是当前线程 ，返回的是在代码实际运行时候的线程 2.thread.setName(线程名称） 设置线程名称 对象方法thread.getName() 返回线程名称通过设置线程名称有助于程序调试，提高可读性 3.thread.isAlive() 判断线程是否处于活动状态 活动状态就是线程以启动且尚未终止 4.Thread.sleep(millis) 让当前线程休眠指定的毫秒数（currentThread返回的线程） 5.thread.getId()可以活的线程的唯一标识，某个编号的现成运行结束后，该编号可能被后续创建的线程使用，重启JVM后同一个线程的编号可能不一样 6.Thread.yield()方法的作用是放弃当前的CPU资源 7.thread.setPriority(num) ;设置线程的优先级Java的线程优先级取值范围是1~10 如果超出这个范围会抛出异常8.illegalArgumentException(非法参数异常）线程优先具有继承性，在线程A创建了线程B 则B线程的优先级与A线程是一样的 9.thread.interrupt() &#x2F;&#x2F;仅是给线程标记中断 线程有thread.isInterrupted()方法，该方法返回线程的中断标志 true or false 10.thread.setDaemon()设置守护线程 守护线程是为其他线程提供服务的线程，如GC就是一个典型的守护线程，守护线程不能单独运行，当JVM中没有其他用户线程，只有守护线程，守护线程会自动销毁，JVM会退出 设置守护线程应该在线程启动之前 12345678public class Test&#123; public static void main(String[] args)&#123; SubDaemonThread thread = new SubDaemonThread(); thread.setDaemon(true); //设置守护线程的代码应该在线程启动之前否则会报非法线程状态异常 thread.start(); //当main线程结束，守护线程thread也销毁了 &#125;&#125; 11.Thread.state 枚举类型可通过getState()方法获得 Java中的线程的生命周期1 NEW新建状态 在调用start（）启动之前的状态2 RUNNABLE 可运行状态 包含READY和RUNNING yield方法将RUNNABLE转为READY3 BLOCKED 线程发起阻塞的IO操作或者申请由其他线程占用的独占资源 转为阻塞满足条件转换为RUNNABLE4WAITING 等待状态 线程执行了object.wait()(可以使执行当前代码的线程等待，暂停执行，直到接到通知或被中断为止，只能由锁对象调用，调用wait（）方法当前线程会释放锁),thread.join方法（挂起调用线程的执行，直到被调用的对象完成他的执行，T1，T2，T3三个线程怎么保证按一定顺序执行， 按顺序try join 例如1-2-3 则在2中try T1.join,3中try T2.join）然后依次（代码）启动三个线程） 会把看成转为WAITING等待状态，执行object.notify() 方法 或者加入的线程执行完毕，当前线程都会转换为RUNNABLE状态5TIMED_WAITING状态，与WAITING类似 区别在于不会无限等待 如果线程没有在指定的时间范围内完成期望的操作，该线程自动转化为RUNNABLE 如sleep(long)wait(long) 6TERMINATED 终止状态 线程结束处于终止状态 多线程二CPU 内存 IO设备速度严重差异 提高效率的方式：（1）CPU增加了缓存（2）操作系统增加了进程，线程以分时复用CPU 进而均衡CPU 与iO设备的速度差异 Java采用抢占式线程调度：如果一个线程申请IO 或者申请一个被其他线程占用的资源 就会进入阻塞状态 让出CPU 待准备完成OS会把这个休眠的线程唤醒，唤醒后就有机会重新获得CPU使用权（3）编译程序优化指令顺序 这三种方式也带来了问题可见性：工作区和共享区数据更新应该立刻能被看到原子性：该操作要么已经执行完成要么尚未发生，其他线程不能得到操作的中间结果有序性：重排序可能会导致多线程程序出现非预期操作 （重排序）是对内存访问有序操作的一种优化 ：1 指令重排序 ：主要是由JIT编译器，处理器引起的，指程序顺序与执行顺序不一致 2存储子系统重排序：由高速缓存（是CPU中为了匹配与主内存处理速度不匹配而舍设计一个高速缓存），写缓冲器（用来提高高速缓存操作的效率）引起的，感知顺序与执行顺序不一致 重排序要保证单线程程序的正确性（貌似串行语义） 所以有数据依赖关系的指令不会重排 如果两个指令（操作）访问同一个变量，其中一个有写操作这两个指令就存在数据依赖关系 volatile synchronizevolatilevolatile 修饰变量关键字可以保证可见性与有序性（1）当对volatile变量执行写操作后JMM会把工作内存中的最新变量值强行刷新到主内存，写操作会导致其他线程里的缓存无效（CPU嗅探总线，主存中更改的数据地址与自己缓存对比，若一致则失效）（2） 防止指令重排 在volatile前后加上内存屏障 （各种屏障都是保证同步，简单来说在屏障之后的写操作必须等待屏障之前的写操作完成才可以执行，读操作则不受影响）缺点不具有原子性volatile的实现是轻量级的 性能优于 synchronized synchronize1 synchronized 隐性锁 依赖monitor2 每个对象会与一个monitor相关联 （1）当监视器被占用时，就会处于锁定状态，监视器的获得过程是排他的。如果某线程已经占用了监视器，则其他线程会进入阻塞状态等待锁的释放 （2）执行完成退出监视器修饰实例方法 修饰类方法 修饰代码块 （注意锁粒度） 同步器AQS底层CAS（抽象队列同步器）定义了一套多线程访问共享资源的同步器框架利用CLH队列锁实现 将获取不到线程的进程放入 JMM（Java内存模型)JVM中的共享数据可能被分配到CPU中的寄存器中，主内存RAM中若分配到寄存器中，每个CPU都有自己的 一个CPU不能读取其他CPU上的内容，如果两个线程分别运行在不同CPU上，无法看到数据的变化 CPU不直接从主存读取数据，先把RAM中数据读到Cache缓存中再把Cache的数据读到寄存器中，CPU中线程对数据更新，可能只是更新到写缓冲器，还没有到达Cache更不用说主存 分配到主存中 运行在另一个CPU中的线程无法看到共享数据的更新 CPU具有缓存同步 共享数据的更新必须被写入cache 这个过程就是冲刷处理缓存 JMM对这些进行规定 ：每个线程之间的共享数据都存储在主内存中 每个线程都有一个私有的工作内存（是一个抽象的概念，他涵盖寄存器，写缓冲器，其他硬件的优化） 每个线程从主内存中把数据读取到本地工作内存中，在工作内存中保存共享数据的副本，工作内存仅对当前线程可见 程序、进程、线程的理解 1、程序(programm)概念：是为完成特定任务、用某种语言编写的一组指令的集合。即指一段静态的代码。 2、进程(process)概念：程序的一次执行过程，或是正在运行的一个程序。说明：进程作为资源分配的单位，系统在运行时会为每个进程分配不同的内存区域 3、线程(thread)概念：进程可进一步细化为线程，是一个程序内部的一条执行路径。说明：线程作为CPU调度和执行的单位，每个线程拥独立的运行栈和程序计数器(pc)，线程切换的开销小。 补充： 进程可以细化为多个线程。每个线程，拥有自己独立的：栈、程序计数器多个线程，共享同一个进程中的结构：方法区、堆。 并行与并发单核CPU与多核CPU的理解 单核CPU，其实是一种假的多线程，因为在一个时间单元内，也只能执行一个线程的任务。例如：虽然有多车道，但是收费站只有一个工作人员在收费，只有收了费才能通过，那么CPU就好比收费人员。如果某个人不想交钱，那么收费人员可以把他“挂起”（晾着他，等他想通了，准备好了钱，再去收费。）但是因为CPU时间单元特别短，因此感觉不出来。 如果是多核的话，才能更好的发挥多线程的效率。（现在的服务器都是多核的） 一个Java应用程序java.exe，其实至少三个线程：main()主线程，gc()垃圾回收线程，异常处理线程。当然如果发生异常，会影响主线程。 并行与并发的理解并行：多个CPU同时执行多个任务。比如：多个人同时做不同的事。并发：一个CPU(采用时间片)同时执行多个任务。比如：秒杀、多个人做同一件事 创建线程的几种方法继承Thread类创建线程多线程的创建，方式一：继承于Thread类 创建一个继承于Thread类的子类 重写Thread类的run() –&gt; 将此线程执行的操作声明在run()中 创建Thread类的子类的对象 通过此对象调用start() 例子：遍历100以内的所有的偶数 123456789101112131415161718192021222324252627282930313233343536373839404142434445JAVA//1. 创建一个继承于Thread类的子类class MyThread extends Thread &#123; //2. 重写Thread类的run() @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; if(i % 2 == 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + i); &#125; &#125; &#125;&#125;public class ThreadTest &#123; public static void main(String[] args) &#123; //3. 创建Thread类的子类的对象 MyThread t1 = new MyThread(); //4.通过此对象调用start():①启动当前线程 ② 调用当前线程的run() t1.start(); //问题一：我们不能通过直接调用run()的方式启动线程。// t1.run(); /* 问题二：再启动一个线程，遍历100以内的偶数。不可以还让已经start()的线程去执行。 会报IllegalThreadStateException */ // t1.start(); //我们需要重新创建一个线程的对象 MyThread t2 = new MyThread(); t2.start(); //如下操作仍然是在main线程中执行的。 for (int i = 0; i &lt; 100; i++) &#123; if(i % 2 == 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + i + &quot;***********main()************&quot;); &#125; &#125; &#125;&#125; 实现Runnable接口创建线程1、创建多线程的方式二：实现Runnable接口 创建一个实现了Runnable接口的类 实现类去实现Runnable中的抽象方法：run() 创建实现类的对象 将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象 通过Thread类的对象调用start() 2、 比较创建线程的两种方式。开发中：优先选择：实现Runnable接口的方式原因：实现的方式没有类的单继承性的局限性，实现的方式更适合来处理多个线程有共享数据的情况。 123456789101112131415161718192021222324252627282930313233343536373839JAVA//1. 创建一个实现了Runnable接口的类class MThread implements Runnable&#123; //2. 实现类去实现Runnable中的抽象方法：run() @Override public void run() &#123; for (int i = 0; i &lt; 100; i++) &#123; if(i % 2 == 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + i); &#125; &#125; &#125;&#125;public class ThreadTest1 &#123; public static void main(String[] args) &#123; //3. 创建实现类的对象 MThread mThread = new MThread(); //4. 将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象 Thread t1 = new Thread(mThread); t1.setName(&quot;线程1&quot;); /* 5. 通过Thread类的对象调用start():① 启动线程 ②调用当前线程的run()--&gt; 调用了Runnable类型的target的run() */ t1.start(); //再启动一个线程，遍历100以内的偶数 Thread t2 = new Thread(mThread); t2.setName(&quot;线程2&quot;); t2.start(); &#125;&#125; Thread和Runnable的关系联系：public class Thread implements Runnable相同点：两种方式都需要重写run(),将线程要执行的逻辑声明在run()中。 Runnable接口构造线程源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788JAVA/*下面是Thread类的部分源码*///1.用Runnable接口创建线程时会进入这个方法public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0); &#125;//2.接着调用这个方法private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true); &#125;//3.再调用这个方法private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name cannot be null&quot;); &#125; this.name = name; Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) &#123; /* Determine if it&#x27;s an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) &#123; g = security.getThreadGroup(); &#125; /* If the security doesn&#x27;t have a strong opinion of the matter use the parent thread group. */ if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); //4.最后在这里将Runnable接口(target)赋值给Thread自己的target成员属性 this.target = target; setPriority(priority); if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); &#125;/*如果你是实现了runnable接口，那么在上面的代码中target便不会为null，那么最终就会通过重写的规则去调用真正实现了Runnable接口(你之前传进来的那个Runnable接口实现类)的类里的run方法*/@Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 1、多线程的设计之中，使用了代理设计模式的结构，用户自定义的线程主体只是负责项目核心功能的实现，而所有的辅助实现全部交由Thread类来处理。2、在进行Thread启动多线程的时候调用的是start()方法，而后找到的是run()方法，但通过Thread类的构造方法传递了一个Runnable接口对象的时候，那么该接口对象将被Thread类中的target属性所保存，在start()方法执行的时候会调用Thread类中的run()方法。而这个run()方法去调用实现了Runnable接口的那个类所重写过run()方法，进而执行相应的逻辑。多线程开发的本质实质上是在于多个线程可以进行同一资源的抢占，那么Thread主要描述的是线程，而资源的描述是通过Runnable完成的。如下图所示： Thread类构造线程源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384JAVAMyThread t2 = new MyThread(); //这个构造函数会默认调用Super();也就是Thread类的无参构造JAVA//代码从上往下顺序执行public Thread() &#123; init(null, null, &quot;Thread-&quot; + nextThreadNum(), 0); &#125; private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true); &#125; private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; if (name == null) &#123; throw new NullPointerException(&quot;name cannot be null&quot;); &#125; this.name = name; Thread parent = currentThread(); SecurityManager security = System.getSecurityManager(); if (g == null) &#123; /* Determine if it&#x27;s an applet or not */ /* If there is a security manager, ask the security manager what to do. */ if (security != null) &#123; g = security.getThreadGroup(); &#125; /* If the security doesn&#x27;t have a strong opinion of the matter use the parent thread group. */ if (g == null) &#123; g = parent.getThreadGroup(); &#125; &#125; /* checkAccess regardless of whether or not threadgroup is explicitly passed in. */ g.checkAccess(); /* * Do we have the required permissions? */ if (security != null) &#123; if (isCCLOverridden(getClass())) &#123; security.checkPermission(SUBCLASS_IMPLEMENTATION_PERMISSION); &#125; &#125; g.addUnstarted(); this.group = g; this.daemon = parent.isDaemon(); this.priority = parent.getPriority(); if (security == null || isCCLOverridden(parent.getClass())) this.contextClassLoader = parent.getContextClassLoader(); else this.contextClassLoader = parent.contextClassLoader; this.inheritedAccessControlContext = acc != null ? acc : AccessController.getContext(); this.target = target; setPriority(priority); if (inheritThreadLocals &amp;&amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); /* Stash the specified stack size in case the VM cares */ this.stackSize = stackSize; /* Set thread ID */ tid = nextThreadID(); &#125;/*由于这里是通过继承Thread类来实现的线程，那么target这个东西就是Null。但是因为你继承了Runnable接口并且重写了run()，所以最终还是调用子类的run()*/ @Override public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 最直观的代码描述1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677JAVAclass Window extends Thread&#123; private int ticket = 100; @Override public void run() &#123; while(true)&#123; if(ticket &gt; 0)&#123; System.out.println(getName() + &quot;：卖票，票号为：&quot; + ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125;&#125;public class WindowTest &#123; public static void main(String[] args) &#123; Window t1 = new Window(); Window t2 = new Window(); Window t3 = new Window(); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125;JAVAclass Window1 implements Runnable&#123; private int ticket = 100; @Override public void run() &#123; while(true)&#123; if(ticket &gt; 0)&#123; System.out.println(Thread.currentThread().getName() + &quot;:卖票，票号为：&quot; + ticket); ticket--; &#125;else&#123; break; &#125; &#125; &#125;&#125;public class WindowTest1 &#123; public static void main(String[] args) &#123; Window1 w = new Window1(); Thread t1 = new Thread(w); Thread t2 = new Thread(w); Thread t3 = new Thread(w); t1.setName(&quot;窗口1&quot;); t2.setName(&quot;窗口2&quot;); t3.setName(&quot;窗口3&quot;); t1.start(); t2.start(); t3.start(); &#125;&#125; 1、继承Thread类的方式，new了三个Thread，实际上是有300张票。 2、实现Runnable接口的方式，new了三个Thread，实际上是有100张票。 3、也就是说实现Runnable接口的线程中，成员属性是所有线程共有的。但是继承Thread类的线程中，成员属性是各个线程独有的，其它线程看不到，除非采用static的方式才能使各个线程都能看到。 4、就像上面说的Runnable相当于资源，Thread才是线程。用Runnable创建线程时，new了多个Thread，但是传进去的参数都是同一个Runnable（资源）。用Thread创建线程时，就直接new了多个线程，每个线程都有自己的Runnable（资源）。在Thread源码中就是用target变量（这是一个Runnable类型的变量）来表示这个资源。 5、同时因为这两个的区别，在并发编程中，继承了Thread的子类在进行线程同步时不能将成员变量当做锁，因为多个线程拿到的不是同一把锁，不过用static变量可以解决这个问题。而实现了Runnable接口的类在进行线程同步时没有这个问题。 实现Callable接口创建线程123456789101112131415161718192021JAVA//Callable实现多线程class MyThread implements Callable&lt;String&gt; &#123;//线程的主体类 @Override public String call() throws Exception &#123; for (int x = 0; x &lt; 10; x++) &#123; System.out.println(&quot;*******线程执行，x=&quot; + x + &quot;********&quot;); &#125; return &quot;线程执行完毕&quot;; &#125;&#125;public class Demo1 &#123; public static void main(String[] args) throws Exception &#123; FutureTask&lt;String&gt; task = new FutureTask&lt;&gt;(new MyThread()); new Thread(task).start(); System.out.println(&quot;线程返回数据&quot; + task.get()); &#125;&#125; Callable最主要的就是提供带有返回值的call方法来创建线程。不过Callable要和Future实现类连着用，关于Future的一系列知识会在后面几个系列讲到。 策略模式在Thread和Runnable中的应用Runnable接口最重要的方法—–run方法，使用了策略者模式将执行的逻辑(run方法)和程序的执行单元(start0方法)分离出来，使用户可以定义自己的程序处理逻辑，更符合面向对象的思想。 Thread的构造方法 创建线程对象Thread，默认有一个线程名，以Thread-开头，从0开始计数，如“Thread-0、Thread-1、Thread-2 …” 如果没有传递Runnable或者没有覆写Thread的run方法，该Thread不会调用任何方法 如果传递Runnable接口的实例或者覆写run方法，则会执行该方法的逻辑单元（逻辑代码） 如果构造线程对象时，未传入ThreadGroup，Thread会默认获取父线程的ThreadGroup作为该线程的ThreadGroup，此时子线程和父线程会在同一个ThreadGroup中 stackSize可以提高线程栈的深度，放更多栈帧，但是会减少能创建的线程数目 stackSize默认是0，如果是0，代表着被忽略，该参数会被JNI函数调用，但是注意某些平台可能会失效，可以通过“-Xss10m”设置 具体的介绍可以看Java的API文档 123456789101112131415161718192021222324252627282930313233343536JAVA/*下面是Thread 的部分源码*/public Thread(Runnable target) &#123; init(null, target, &quot;Thread-&quot; + nextThreadNum(), 0);&#125;public Thread(String name) &#123; init(null, null, name, 0);&#125; ↓ ↓ ↓ ↓ ↓ ↓ private void init(ThreadGroup g, Runnable target, String name, long stackSize) &#123; init(g, target, name, stackSize, null, true);&#125; ↓ ↓ ↓ ↓ ↓ ↓ private void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) &#123; //中间源码省略 this.target = target;//①&#125;/* What will be run. */private Runnable target; //Thread类中的target属性@Overridepublic void run() &#123; if (target != null) &#123; //② target.run(); &#125;&#125; 源码标记解读： 1、如果Thread类的构造方法传递了一个Runnable接口对象 ①那么该接口对象将被Thread类中的target属性所保存。 ②在start()方法执行的时候会调用Thread类中的run()方法。因为target不为null， target.run()就去调用实现Runnable接口的子类重写的run()。 2、如果Thread类的构造方传没传Runnable接口对象 ①Thread类中的target属性保存的就是null。 ②在start()方法执行的时候会调用Thread类中的run()方法。因为target为null，只能去调用继承Thread的子类所重写的run()。 JVM一旦启动，虚拟机栈的大小已经确定了。但是如果你创建Thread的时候传了stackSize（该线程占用的stack大小），该参数会被JNI函数去使用。如果没传这个参数，就默认为0，表示忽略这个参数。注：stackSize在有一些平台上是无效的。 start()源码12345678910111213141516171819202122232425262728293031323334JAVApublic synchronized void start() &#123; if (threadStatus != 0) throw new IllegalThreadStateException();//① group.add(this); boolean started = false; try &#123; start0(); started = true; &#125; finally &#123; try &#123; if (!started) &#123; group.threadStartFailed(this); &#125; &#125; catch (Throwable ignore) &#123; /* do nothing. If start0 threw a Throwable then it will be passed up the call stack */ &#125; &#125;&#125;private native void start0();@Overridepublic void run() &#123; if (target != null) &#123; target.run(); &#125;&#125; 源码标记解读： ①当多次调用start()，会抛出throw new IllegalThreadStateException()异常。也就是每一个线程类的对象只允许启动一次，如果重复启动则就抛出此异常。 为什么线程的启动不直接使用run()而必须使用start()呢?1、如果直接调用run()方法，相当于就是简单的调用一个普通方法。 2、run()的调用是在start0()这个Native C++方法里调用的 线程生命周期Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态，这几个状态在Java源码中用枚举来表示。 线程在生命周期中并不是固定处于某一个状态而是随着代码的执行在不同状态之间切换。Java 线程状态变迁如下图所示。 图中 wait到 runnable状态的转换中，join实际上是Thread类的方法，但这里写成了Object。 1、由上图可以看出：线程创建之后它将处于 NEW（新建） 状态，调用 start() 方法后开始运行，线程这时候处于 READY（可运行） 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 RUNNING（运行） 状态。 2、操作系统隐藏 Java 虚拟机（JVM）中的 READY 和 RUNNING 状态，它只能看到 RUNNABLE 状态，所以 Java 系统一般将这两个状态统称为 RUNNABLE（运行中） 状态 。 3、调用sleep()方法，会进入Blocked状态。sleep()结束之后，Blocked状态首先回到的是Runnable状态中的Ready（也就是可运行状态，但并未运行）。只有拿到了cpu的时间片才会进入Runnable中的Running状态。 Thread常用API 获取当前存活的线程数：public int activeCount() 获取当前线程组的线程的集合：public int enumerate(Thread[] list) 一个Java程序有哪些线程？1、当你调用一个线程start()方法的时候，此时至少有两个线程，一个是调用你的线程，还有一个是被你创建出来的线程。 例子： 12345678910JAVApublic static void main(String[] args) &#123; Thread t1 = new Thread() &#123; @Override public void run() &#123; System.out.println(&quot;==========&quot;); &#125; &#125;; t1.start();&#125; 这里面就是一个调用你的线程（main线程），一个被你创建出来的线程（t1，名字可能是Thread-0） 2、当JVM启动后，实际有多个线程，但是至少有一个非守护线程（比如main线程）。 Finalizer：GC守护线程 RMI：Java自带的远程方法调用（秋招面试，有个面试官问过） Monitor ：是一个守护线程，负责监听一些操作，也在main线程组中 其它：我用的是IDEA，其它的应该是IDEA的线程，比如鼠标监听啥的。 守护线程12345678910111213141516171819202122JAVApublic static void main(String[] args) throws InterruptedException &#123; Thread t = new Thread() &#123; @Override public void run() &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot; running&quot;); Thread.sleep(100000);//① System.out.println(Thread.currentThread().getName() + &quot; done.&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; //new t.setDaemon(true);//② t.start(); Thread.sleep(5_000); //JDK1.7 System.out.println(Thread.currentThread().getName());&#125; 源码标记解读： ①变量名为t的线程Thread-0，睡眠100秒。 ②但是在主函数里Thread-0设置成了main线程的守护线程。所以5秒之后main线程结束了，即使在①这里守护线程还是处于睡眠100秒的状态，但由于他是守护线程，非守护线程main结束了，守护线程也必须结束。 1、但是如果Thread-0线程不是守护线程，即使main线程结束了，Thread-0线程仍然会睡眠100秒再结束。 当主线程死亡后，守护线程会跟着死亡 可以帮助做一些辅助性的东西，如“心跳检测” 设置守护线程：public final void setDaemon(boolean on) 用处A和B之间有一条网络连接，可以用守护线程来进行发送心跳，一旦A和B连接断开，非守护线程就结束了，守护线程（也就是心跳没有必要再发送了）也刚好断开。 123456789101112131415161718192021222324252627282930313233JAVApublic static void main(String[] args) &#123; Thread t = new Thread(() -&gt; &#123; Thread innerThread = new Thread(() -&gt; &#123; try &#123; while (true) &#123; System.out.println(&quot;Do some thing for health check.&quot;); Thread.sleep(1_000); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); // innerThread.setDaemon(true); innerThread.start(); try &#123; Thread.sleep(1_000); System.out.println(&quot;T thread finish done.&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); //t.setDaemon(true); t.start();&#125;/*设置该线程为守护线程必须在启动它之前。如果t.start()之后，再t.setDaemon(true);会抛出IllegalThreadStateException*/ 输出结果： Do some thing for health check.Do some thing for health check.T thread finish done. &#x2F;&#x2F;此时main线程已经结束，但是由于innerThread还在发送心跳，应用不会关闭Do some thing for health check.Do some thing for health check.Do some thing for health check.Do some thing for health check. 守护线程还有其它很多用处，在后面的文章里还会有出现。 join方法例子1 1234567891011121314151617181920JAVApublic static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); Thread t2 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); t1.start(); t2.start(); t1.join(); t2.join(); Optional.of(&quot;All of tasks finish done.&quot;).ifPresent(System.out::println); IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i));&#125; 默认传入的数字为0，这里是在main线程里调用了两个线程的join()，所以main线程会等到Thread-0和Thread-1线程执行完再执行它自己。 join必须在start方法之后，并且join()是对wait()的封装。（源码中可以清楚的看到） 也就是说，t.join()方法阻塞调用此方法的线程(calling thread)进入 TIMED_WAITING或WAITING 状态。直到线程t完成，此线程再继续。 join也有人理解成插队，比如在main线程中调用t.join()，就是t线程要插main线程的队，main线程要去等待。 例子2 12345678910111213141516171819202122232425JAVApublic static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); Thread t2 = new Thread(() -&gt; &#123; try &#123; t1.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125;); t1.start(); t2.start();// t1.join(); t2.join(); Optional.of(&quot;All of tasks finish done.&quot;).ifPresent(System.out::println); IntStream.range(1, 1000) .forEach(i -&gt; System.out.println(Thread.currentThread().getName() + &quot;-&gt;&quot; + i)); &#125; 这里是在t2（我们以后就都用变量名来称呼线程了）线程了。t1.join()了。所以t2线程会等待t1线程打印完，t2自己才会打印。然后t2.join()，main线程也要等待t2线程。总体执行顺序就是t1–&gt;t2–&gt;main 通过上方例子可以用join实现类似于CompletableFuture的异步任务编排。（后面会讲） 中断1、Java 中的中断和操作系统的中断还不一样，这里就按照状态来理解吧，不要和操作系统的中断联系在一起 2、记住中断只是一个状态，Java的方法可以选择对这个中断进行响应，也可以选择不响应。响应的意思就是写相对应的代码执行相对应的操作，不响应的意思就是什么代码都不写。 几个方法12345678910111213JAVA// Thread 类中的实例方法，持有线程实例引用即可检测线程中断状态public boolean isInterrupted() &#123;&#125;/*1、Thread 中的静态方法，检测调用这个方法的线程是否已经中断2、注意：这个方法返回中断状态的同时，会将此线程的中断状态重置为 false如果我们连续调用两次这个方法的话，第二次的返回值肯定就是 false 了*/public static boolean interrupted() &#123;&#125;// Thread 类中的实例方法，用于设置一个线程的中断状态为 truepublic void interrupt() &#123;&#125; 小tip1234JAVApublic static boolean interrupted()public boolean isInterrupted()//这个会清除中断状态 为什么要这么设置呢？原因在于： interrupted()是一个静态方法，可以在Runnable接口实例中使用 isInterrupted()是一个Thread的实例方法，在重写Thread的run方法时使用 12345678910111213141516JAVApublic class ThreadInterrupt &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(Thread.interrupted()); &#125;); //这个new Thread用的是runnable接口那个构造函数 Thread t2 = new Thread()&#123; @Override public void run() &#123; System.out.println(isInterrupted()); &#125; &#125;;//这个new Thread用的就是Thread的空参构造 &#125;&#125; 也就是说接口中不能调用Thread的实例方法，只能通过静态方法来判断是否发生中断 重难点当然，中断除了是线程状态外，还有其他含义，否则也不需要专门搞一个这个概念出来了。 初学者肯定以为 thread.interrupt() 方法是用来暂停线程的，主要是和它对应中文翻译的“中断”有关。中断在并发中是常用的手段，请大家一定好好掌握。可以将中断理解为线程的状态，它的特殊之处在于设置了中断状态为 true 后，这几个方法会感知到： wait(), wait(long), wait(long, int), join(), join(long), join(long, int), sleep(long), sleep(long, int) 这些方法都有一个共同之处，方法签名上都有throws InterruptedException，这个就是用来响应中断状态修改的。 如果线程阻塞在 InterruptibleChannel 类的 IO 操作中，那么这个 channel 会被关闭。 如果线程阻塞在一个 Selector 中，那么 select 方法会立即返回。 对于以上 3 种情况是最特殊的，因为他们能自动感知到中断（这里说自动，当然也是基于底层实现），并且在做出相应的操作后都会重置中断状态为 false。然后执行相应的操作（通常就是跳到 catch 异常处）。 如果不是以上3种情况，那么，线程的 interrupt() 方法被调用，会将线程的中断状态设置为 true。 那是不是只有以上 3 种方法能自动感知到中断呢？不是的，如果线程阻塞在 LockSupport.park(Object obj) 方法，也叫挂起，这个时候的中断也会导致线程唤醒，但是唤醒后不会重置中断状态，所以唤醒后去检测中断状态将是 true。 并发编程中的三个问题可见性可见性概念可见性（Visibility）：是指一个线程对共享变量进行修改，另一个先立即得到修改后的新值。 可见性演示1234567891011121314151617181920212223242526272829303132333435363738JAVA/* 笔记 * 1.当没有加Volatile的时候,while循环会一直在里面循环转圈 * 2.当加了之后Volatile,由于可见性,一旦num改了之后,就会通知其他线程 * 3.还有注意的时候不能用if,if不会重新拉回来再判断一次。(也叫做虚假唤醒) * 4.案例演示:一个线程对共享变量的修改,另一个线程不能立即得到新值 * */public class Video04_01 &#123; public static void main(String[] args) &#123; MyData myData = new MyData(); new Thread(() -&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;\\t come in &quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //睡3秒之后再修改num,防止A线程先修改了num,那么到while循环的时候就会直接跳出去了 myData.addTo60(); System.out.println(Thread.currentThread().getName() + &quot;\\t come out&quot;); &#125;,&quot;A&quot;).start(); while(myData.num == 0)&#123; //只有当num不等于0的时候,才会跳出循环 &#125; &#125;&#125;class MyData&#123; int num = 0; public void addTo60()&#123; this.num = 60; &#125;&#125; 由上面代码可以看出，并发编程时，会出现可见性问题，当一个线程对共享变量进行了修改，另外的线程并没有立即看到修改后的最新值。 原子性原子性概念原子性（Atomicity）：在一次或多次操作中，要么所有的操作都成功执行并且不会受其他因素干扰而中 断，要么所有的操作都不执行或全部执行失败。不会出现中间状态 原子性演示案例演示:5个线程各执行1000次 i++; 1234567891011121314151617181920212223242526272829303132333435363738JAVA/** * @Author: 吕 * @Date: 2019/9/23 15:50 * &lt;p&gt; * 功能描述: volatile不保证原子性的代码验证 */public class Video05_01 &#123; public static void main(String[] args) &#123; MyData03 myData03 = new MyData03(); for (int i = 0; i &lt; 20; i++) &#123; new Thread(() -&gt;&#123; for (int j = 0; j &lt; 1000; j++) &#123; myData03.increment(); &#125; &#125;,&quot;线程&quot; + String.valueOf(i)).start(); &#125; //需要等待上面的20个线程计算完之后再查看计算结果 while(Thread.activeCount() &gt; 2)&#123; Thread.yield(); &#125; System.out.println(&quot;20个线程执行完之后num:\\t&quot; + myData03.num); &#125;&#125;class MyData03&#123; static int num = 0; public void increment()&#123; num++; &#125;&#125; 1、控制台输出：（由于并发不安全，每次执行的结果都可能不一样） 20个线程执行完之后num: 19706 正常来说，如果保证原子性的话，20个线程执行完，结果应该是20000。控制台输出的值却不是这个，说明出现了原子性的问题。 2、使用javap反汇编class文件，对于num++可以得到下面的字节码指令： 12345CODE9: getstatic #12 // Field number:I 取值操作12: iconst_1 13: iadd 14: putstatic #12 // Field number:I 赋值操作 由此可见num++是由多条语句组成，以上多条指令在一个线程的情况下是不会出问题的，但是在多线程情况下就可能会出现问题。 比如num刚开始值是7。A线程在执行13: iadd时得到num值是8，B线程又执行9: getstatic得到前一个值是7。马上A线程就把8赋值给了num变量。但是B线程已经拿到了之前的值7，B线程是在A线程真正赋值前拿到的num值。即使A线程最终把值真正的赋给了num变量，但是B线程已经走过了getstaitc取值的这一步，B线程会继续在7的基础上进行++操作，最终的结果依然是8。本来两个线程对7进行分别进行++操作，得到的值应该是9，因为并发问题，导致结果是8。 3、并发编程时，会出现原子性问题，当一个线程对共享变量操作到一半时，另外的线程也有可能来操作共 享变量，干扰了前一个线程的操作。 有序性有序性概念有序性（Ordering）：是指程序中代码的执行顺序，Java在编译时和运行时会对代码进行优化（重排序）来加快速度，会导致程序终的执行顺序不一定就是我们编写代码时的顺序 12345JAVAinstance = new SingletonDemo() 是被分成以下 3 步完成 memory = allocate(); 分配对象内存空间 instance(memory); 初始化对象 instance = memory; 设置 instance 指向刚分配的内存地址，此时 instance != null 步骤2 和 步骤3 不存在数据依赖关系，重排与否的执行结果单线程中是一样的。这种指令重排是被 Java 允许的。当 3 在前时，instance 不为 null，但实际上初始化工作还没完成，会变成一个返回 null 的getInstance。这时候数据就出现了问题。 有序性演示jcstress是java并发压测工具。https://wiki.openjdk.java.net/display/CodeTools/jcstress 修改pom文件，添加依赖： 12345678910111213141516171819202122232425262728293031323334353637CODE&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jcstress&lt;/groupId&gt; &lt;artifactId&gt;jcstress-core&lt;/artifactId&gt; &lt;version&gt;$&#123;jcstress.version&#125;&lt;/version&gt; &lt;/dependency&gt;JAVAimport org.openjdk.jcstress.annotations.*;import org.openjdk.jcstress.infra.results.I_Result; @JCStressTest // @Outcome: 如果输出结果是1或4，我们是接受的(ACCEPTABLE)，并打印ok @Outcome(id = &#123;&quot;1&quot;, &quot;4&quot;&#125;, expect = Expect.ACCEPTABLE, desc = &quot;ok&quot;) //如果输出结果是0，我们是接受的并且感兴趣的，并打印danger @Outcome(id = &quot;0&quot;, expect = Expect.ACCEPTABLE_INTERESTING, desc = &quot;danger&quot;) @Statepublic class Test03Ordering &#123; int num = 0; boolean ready = false; // 线程1执行的代码 @Actor //@Actor：表示会有多个线程来执行这个方法 public void actor1(I_Result r) &#123; if (ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125; &#125; // 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; ready = true; &#125;&#125; 1、实际上上面两个方法会有很多线程来执行，为了讲解方便，我们只提出线程1和线程2来讲解。 2、I_Result 是一个保存int类型数据的对象，有一个属性 r1 用来保存结果，在多线程情况下可能出现几种结果？ 情况1：线 程1先执行actor1，这时ready &#x3D; false，所以进入else分支结果为1。 情况2：线程2执行到actor2，执行了num &#x3D; 2;和ready &#x3D; true，线程1执行，这回进入 if 分支，结果为 4。 情况3：线程2先执行actor2，只执行num &#x3D; 2；但没来得及执行 ready &#x3D; true，线程1执行，还是进入 else分支，结果为1。 情况4：0，发生了指令重排 12345678JAVA// 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; //pos_1 ready = true;//pos_2 &#125; pos_1处代码和pos_2处代码没有什么数据依赖关系，或者说没有因果关系。Java可能对其进行指令重排，排成下面的顺序。 1234567JAVA// 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; ready = true;//pos_2 num = 2; //pos_1 &#125; 此时如果线程2先执行到ready = true;还没来得及执行 num = 2; 。线程1执行，直接进入if分支，此时num默认值为0。 得到的结果也就是0。 volatile 1、关于可见性，重排序等等的硬件原理，MESI缓存一致性，内存屏障，JMM等等这些，请看我的后面文章。第一阶段只是介绍下用法，不涉及原理。 2、如果你在第一篇文章没有找到你想要的内容，请看我后面的内容。并发的体系，我自认为讲的还是比较全面的。 volatile保证可见性代码 读者可以把两个代码运行一下，就能明显看到不加volatile的死循环（就是程序一直显示没结束） 1234567891011121314151617181920212223242526272829303132333435363738JAVA/* 笔记 * 1.当没有加Volatile的时候,while循环会一直在里面转圈 * 2.当加了之后Volatile,由于可见性,一旦num改了之后,就会通知其他线程 * 3.还有注意的时候不能用if,if不会重新拉回来再判断一次 * */public class Video04_02 &#123; public static void main(String[] args) &#123; MyData2 myData = new MyData2(); new Thread(() -&gt;&#123; System.out.println(Thread.currentThread().getName() + &quot;\\t come in &quot;); try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //睡3秒之后再修改num,防止A线程先修改了num,那么到while循环的时候就会直接跳出去了 myData.addTo60(); System.out.println(Thread.currentThread().getName() + &quot;\\t come out&quot;); &#125;,&quot;A&quot;).start(); while(myData.num == 0)&#123; //只有当num不等于0的时候,才会跳出循环 &#125; &#125;&#125;class MyData2&#123; volatile int num = 0; public void addTo60()&#123; this.num = 60; &#125;&#125; volatile保证有序性代码12345678910111213141516171819202122232425262728293031JAVAimport org.openjdk.jcstress.annotations.*;import org.openjdk.jcstress.infra.results.I_Result; @JCStressTest // @Outcome: 如果输出结果是1或4，我们是接受的(ACCEPTABLE)，并打印ok @Outcome(id = &#123;&quot;1&quot;, &quot;4&quot;&#125;, expect = Expect.ACCEPTABLE, desc = &quot;ok&quot;) //如果输出结果是0，我们是接受的并且感兴趣的，并打印danger @Outcome(id = &quot;0&quot;, expect = Expect.ACCEPTABLE_INTERESTING, desc = &quot;danger&quot;) @Statepublic class Test03Ordering &#123; volatile int num = 0; volatile boolean ready = false; // 线程1执行的代码 @Actor //@Actor：表示会有多个线程来执行这个方法 public void actor1(I_Result r) &#123; if (ready) &#123; r.r1 = num + num; &#125; else &#123; r.r1 = 1; &#125; &#125; // 线程2执行的代码 // @Actor public void actor2(I_Result r) &#123; num = 2; ready = true; &#125;&#125; 读者可以将运行结果对比着来看，就能发现区别。 volatile只能保证可见性和有序性（禁止指令重排），无法保证原子性。 CASvolatile自己虽然不能保证原子性，但是和CAS结合起来就可以保证原子性了。CAS+volatile一起用就可以同时解决并发编程中的三个问题了，保证并发安全。 CAS 是什么？ CAS：比较并交换compareAndSet,它是一条 CPU 并发原语，它的功能是判断内存某个位置的值是否为预期值，如果是则更改为新的值，这个过程是原子性的。 例: AtomicInteger 的 compareAndSet(‘期望值’,’设置值’) 方法，期望值与目标值一致时，修改目标变量为设置值，期望值与目标值不一致时，返回 false 和最新主存的变量值 CAS 的底层原理 1234例: AtomicInteger.getAndIncrement() 调用 Unsafe 类中的 CAS 方法，JVM 会帮我们实现出 CAS 汇编指令 这是一种完全依赖于硬件的功能，通过它实现原子操作。 原语的执行必须是连续的，在执行过程中不允许被中断，CAS 是 CPU 的一条原子指令。 CAS的思想就是乐观锁的思想 AtomicInteger在JUC并发包中，CAS和AtomicInteger（原子类的value值都被volatile修饰了）一起保证了并发安全。下面我们以AtomicInteger.getAndIncrement() 方法讲一下。 12345678910111213141516171819202122232425262728293031323334353637JAVA/** * unsafe: rt.jar/sun/misc/Unsafe.class * Unsafe 是 CAS 的核心类，由于 Java 无法直接访问底层系统，需要通过本地&lt;native&gt;方法来访问 * Unsafe 相当于一个后门，基于该类可以直接操作特定内存的数据 * Unsafe 其内部方法都是 native 修饰的，可以像 C 的指针一样直接操作内存 * Java 中的 CAS 操作执行依赖于 Unsafe 的方法，直接调用操作系统底层资源执行程序 * * this: 当前对象 * 变量 value 由 volatile 修饰，保证了多线程之间的内存可见性、禁止重排序 * * valueOffset: 内存地址 * 表示该变量值在内存中的偏移地址，因为 Unsafe 就是根据内存偏移地址获取数据 * * 1: 固定写死，原值加1 */public final int getAndIncrement()&#123; return unsafe.getAndAddInt(this,valueOffset,1);&#125;/** * Unsafe.getAndAddInt() * getIntVolatile: 通过内存地址去主存中取对应数据 * * while(!this.compareAndSwapInt(var1,var2,var5,var5 + var4)): * 将本地 value 与主存中取出的数据对比，如果相同，对其作运算， * 此时返回 true，取反后 while 结束，返回最终值。 * 如果不相同，此时返回 false，取反后 while 循环继续运行，此时为自旋锁&lt;重复尝试&gt; * 由于 value 是被 volatile 修饰的，所以拿到主存中最新值，再循环直至成功。 */public final int getAndAddInt(Object var1,long var2,int var4)&#123; int var5; do&#123; var5 = this.getIntVolatile(var1,var2); // 从主存中拷贝变量到本地内存 &#125; while(!this.compareAndSwapInt(var1,var2,var5,var5 + var4)); return var5;&#125; CAS 代码演示123456789JAVApublic class CASDemo &#123; public static void main(String[] args) &#123; AtomicInteger num = new AtomicInteger(5); // TODO System.out.println(num.compareAndSet(5, 1024) + &quot;\\t current num&quot; + num.get()); System.out.println(num.compareAndSet(5, 2019) + &quot;\\t current num&quot; + num.get()); &#125; CAS三大问题 如果 CAS 长时间一直不成功，会给 CPU 带来很大的开销，在Java的实现中是一直通过while循环自旋CAS获取锁。 只能保证一个共享变量的原子操作 引出了 ABA 问题 ABA问题什么是ABA问题？1234567891011121314151617181920212223242526272829JAVA/** * @Author: 吕 * @Date: 2019/9/24 16:43 * &lt;p&gt; * 功能描述: CAS引发的ABA问题 */public class Video19_01 &#123; static AtomicReference&lt;Integer&gt; num = new AtomicReference&lt;&gt;(100); public static void main(String[] args) &#123; new Thread(() -&gt;&#123; num.compareAndSet(100, 101); num.compareAndSet(101,100); &#125;,&quot;线程A&quot;).start(); new Thread(() -&gt;&#123; //保证A线程已经修改完 try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean b = num.compareAndSet(100, 2019); System.out.println(b + &quot;\\t 当前最新值&quot; + num.get().toString()); &#125;,&quot;线程B&quot;).start(); &#125;&#125; CAS 会导致 ABA 问题： 例: A、B线程从主存取出变量 value -&gt; A 在 N次计算中改变 value 的值-&gt; A 最终计算结果还原 value 最初的值-&gt; B 计算后，比较主存值与自身 value 值一致，修改成功 尽管各个线程的 CAS 都操作成功，但是并不代表这个过程就是没有问题的。 ABA问题的解决123456789101112131415161718192021222324252627282930313233JAVA/** * @Author: 吕 * @Date: 2019/9/24 16:49 * &lt;p&gt; * 功能描述: ABA问题的解决 */public class Video19_02 &#123; static AtomicStampedReference&lt;Integer&gt; num = new AtomicStampedReference&lt;&gt;(100,1); public static void main(String[] args) &#123; int stamp = num.getStamp();//初始版本号 new Thread(() -&gt;&#123; num.compareAndSet(100,101,num.getStamp(),num.getStamp() + 1); System.out.println(Thread.currentThread().getName() + &quot;\\t 版本号&quot; + num.getStamp()); num.compareAndSet(101,100,num.getStamp(),num.getStamp() + 1); System.out.println(Thread.currentThread().getName() + &quot;\\t 版本号&quot; + num.getStamp()); &#125;,&quot;线程A&quot;).start(); new Thread(() -&gt;&#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; boolean b = num.compareAndSet(100, 209, stamp, num.getStamp() + 1); System.out.println(b + &quot;\\t 当前版本号: \\t&quot; + num.getStamp()); System.out.println(&quot;当前最新值 \\t&quot; + num.getReference().toString()); &#125;,&quot;线程B&quot;).start(); &#125;&#125; 思想很简单，可以很明显的看出来用版本号的方式解决了ABA的问题。 除了对象值，AtomicStampedReference内部还维护了一个“状态戳”。 状态戳可类比为时间戳，是一个整数值，每一次修改对象值的同时，也要修改状态戳，从而区分相同对象值的不同状态。 当AtomicStampedReference设置对象值时，对象值以及状态戳都必须满足期望值，写入才会成功。 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁。还有一个方法，就是把多个共享变量合并成一个共享变量来操作。比如，有两个共享变量i&#x3D;2,j&#x3D;a合并一下ij&#x3D;2a，然后用CAS来操作ij。从java1.5开始，JDK提供了AtomicReference类来保证引用对象之间的原子性，就可以把多个变量放在一个对象里来进行CAS操作。 所以一般来说为了同时解决ABA问题和只能保证一个共享变量，原子类使用时大部分使用的是AtomicStampedReference UnSafeUnsafe类是在sun.misc包下，不属于Java标准。但是很多Java的基础类库，包括一些被广泛使用的高性能开发库都是基于Unsafe类开发的，比如Netty、Cassandra、Hadoop、Kafka等。Unsafe类在提升Java运行效率，增强Java语言底层操作能力方面起了很大的作用。 Java和C++语言的一个重要区别就是Java中我们无法直接操作一块内存区域，不能像C++中那样可以自己申请内存和释放内存。 Java中的Unsafe类为我们提供了类似C++手动管理内存的能力，同时也有了指针的问题。 首先，Unsafe类是”final”的，不允许继承。且构造函数是private的: 123456789101112JAVApublic final class Unsafe &#123; private static final Unsafe theUnsafe; public static final int INVALID_FIELD_OFFSET = -1; private static native void registerNatives(); private Unsafe() &#123; &#125; ...&#125; 因此我们无法在外部对Unsafe进行实例化。 获取UnsafeUnsafe无法实例化，那么怎么获取Unsafe呢？答案就是通过反射来获取Unsafe： 1234567JAVApublic Unsafe getUnsafe() throws IllegalAccessException &#123; Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); return unsafe;&#125; Unsafe的功能如下图： CAS相关JUC中大量运用了CAS操作，可以说CAS操作是JUC的基础，因此CAS操作是非常重要的。Unsafe中提供了int,long和Object的CAS操作： 123456JAVApublic final native boolean compareAndSwapObject(Object var1, long var2, Object var4, Object var5);public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);public final native boolean compareAndSwapLong(Object var1, long var2, long var4, long var6); 偏移量相关1234JAVApublic native long staticFieldOffset(Field var1);public native long objectFieldOffset(Field var1); staticFieldOffset方法用于获取静态属性Field在对象中的偏移量，读写静态属性时必须获取其偏移量。 objectFieldOffset方法用于获取非静态属性Field在对象实例中的偏移量，读写对象的非静态属性时会用到这个偏移量 类加载12345678910JAVApublic native Class&lt;?&gt; defineClass(String var1, byte[] var2, int var3, int var4, ClassLoader var5, ProtectionDomain var6);public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; var1, byte[] var2, Object[] var3);public native Object allocateInstance(Class&lt;?&gt; var1) throws InstantiationException;public native boolean shouldBeInitialized(Class&lt;?&gt; var1);public native void ensureClassInitialized(Class&lt;?&gt; var1); defineClass方法定义一个类，用于动态地创建类。 defineAnonymousClass用于动态的创建一个匿名内部类。 allocateInstance方法用于创建一个类的实例，但是不会调用这个实例的构造方法，如果这个类还未被初始化，则初始化这个类。 shouldBeInitialized方法用于判断是否需要初始化一个类。 ensureClassInitialized方法用于保证已经初始化过一个类。 举例 1234567891011121314151617181920212223242526272829303132333435363738JAVApublic class UnsafeFooTest &#123; private static Unsafe geUnsafe() &#123; try &#123; Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); f.setAccessible(true); return (Unsafe) f.get(null); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return null; &#125; static class Simple &#123; private long l = 0; public Simple() &#123; this.l = 1; System.out.println(&quot;我被初始化了&quot;); &#125; public long getL() &#123; return l; &#125; &#125; public static void main(String[] args) throws Exception &#123; Unsafe unsafe = geUnsafe(); Simple s = (Simple) unsafe.allocateInstance(Simple.class); System.out.println(s.getL()); &#125;&#125; 结果： 0 可以发现，利用Unsafe获取实例，不会调用构造方法 普通读写通过Unsafe可以读写一个类的属性，即使这个属性是私有的，也可以对这个属性进行读写。 读写一个Object属性的相关方法 1234JAVApublic native int getInt(Object var1, long var2);public native void putInt(Object var1, long var2, int var4); getInt用于从对象的指定偏移地址处读取一个int。 putInt用于在对象指定偏移地址处写入一个int。其他的primitive type也有对应的方法。 举例 123456789101112131415161718192021222324252627282930313233343536373839404142JAVApublic class UnsafeFooTest &#123; private static Unsafe geUnsafe() &#123; try &#123; Field f = Unsafe.class.getDeclaredField(&quot;theUnsafe&quot;); f.setAccessible(true); return (Unsafe) f.get(null); &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; return null; &#125; static class Guard&#123; private int ACCESS_ALLOWED = 1; private boolean allow()&#123; return 50 == ACCESS_ALLOWED; &#125; public void work()&#123; if (allow())&#123; System.out.println(&quot;我被允许工作....&quot;); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Unsafe unsafe = geUnsafe(); Guard guard = new Guard(); Field f = guard.getClass().getDeclaredField(&quot;ACCESS_ALLOWED&quot;); unsafe.putInt(guard,unsafe.objectFieldOffset(f),50); System.out.println(&quot;强行赋值...&quot;); guard.work(); &#125;&#125; 结果 强行赋值… 我被允许工作… 类加载12345678910JAVApublic native Class&lt;?&gt; defineClass(String var1, byte[] var2, int var3, int var4, ClassLoader var5, ProtectionDomain var6);public native Class&lt;?&gt; defineAnonymousClass(Class&lt;?&gt; var1, byte[] var2, Object[] var3);public native Object allocateInstance(Class&lt;?&gt; var1) throws InstantiationException;public native boolean shouldBeInitialized(Class&lt;?&gt; var1);public native void ensureClassInitialized(Class&lt;?&gt; var1); defineClass方法定义一个类，用于动态地创建类。 defineAnonymousClass用于动态的创建一个匿名内部类。 allocateInstance方法用于创建一个类的实例，但是不会调用这个实例的构造方法，如果这个类还未被初始化，则初始化这个类。 shouldBeInitialized方法用于判断是否需要初始化一个类。 ensureClassInitialized方法用于保证已经初始化过一个类。 内存屏障123456JAVApublic native void loadFence();public native void storeFence();public native void fullFence(); loadFence：保证在这个屏障之前的所有读操作都已经完成。 storeFence：保证在这个屏障之前的所有写操作都已经完成。 fullFence：保证在这个屏障之前的所有读写操作都已经完成。 线程调度12345678910JAVApublic native void unpark(Object var1);public native void park(boolean var1, long var2);public native void monitorEnter(Object var1);public native void monitorExit(Object var1);public native boolean tryMonitorEnter(Object var1); park方法和unpark方法相信看过LockSupport类的都不会陌生，这两个方法主要用来挂起和唤醒线程。 LockSupport中的park和unpark方法正是通过Unsafe来实现的： 123456789101112JAVApublic static void park(Object blocker) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null);&#125;public static void unpark(Thread thread) &#123; if (thread != null) UNSAFE.unpark(thread);&#125; monitorEnter方法和monitorExit方法用于加锁，Java中的synchronized锁就是通过这两个指令来实现的。 synchronized优化 synchronized可以同时保证可见性，有序性，原子性。这个东西就不讲了 从JDk 1.6开始，JVM就对synchronized锁进行了很多的优化。synchronized说是锁，但是他的底层加锁的方式可能不同，偏向锁的方式来加锁，自旋锁的方式来加锁，轻量级锁的方式来加锁 锁消除锁消除是JIT编译器对synchronized锁做的优化，在编译的时候，JIT会通过逃逸分析技术，来分析synchronized锁对象，是不是只可能被一个线程来加锁，没有其他的线程来竞争加锁，这个时候编译就不用加入monitorenter和monitorexit的指令。这就是，仅仅一个线程争用锁的时候，就可以消除这个锁了，提升这段代码的执行的效率，因为可能就只有一个线程会来加锁，不涉及到多个线程竞争锁 锁粗化12345678910111213141516JAVAsynchronized(this) &#123;&#125; synchronized(this) &#123; &#125; synchronized(this) &#123;&#125; 这个意思就是，JIT编译器如果发现有代码里连续多次加锁释放锁的代码，会给合并为一个锁，就是锁粗化，把一个锁给搞粗了，避免频繁多次加锁释放锁 偏向锁这个意思就是说，monitorenter和monitorexit是要使用CAS操作加锁和释放锁的，开销较大，因此如果发现大概率只有一个线程会主要竞争一个锁，那么会给这个锁维护一个偏好（Bias），后面他加锁和释放锁，基于Bias来执行，不需要通过CAS，性能会提升很多。但是如果有偏好之外的线程来竞争锁，就要收回之前分配的偏好。可能只有一个线程会来竞争一个锁，但是也有可能会有其他的线程来竞争这个锁，但是其他线程唉竞争锁的概率很小。如果有其他的线程来竞争这个锁，此时就会收回之前那个线程分配的那个Bias偏好 轻量级锁如果偏向锁没能成功实现，就是因为不同线程竞争锁太频繁了，此时就会尝试采用轻量级锁的方式来加锁，就是将对象头的Mark Word里有一个轻量级锁指针，尝试指向持有锁的线程，然后判断一下是不是自己加的锁，如果是自己加的锁，那就执行代码就好了。如果不是自己加的锁，那就是加锁失败，说明有其他人加了锁，这个时候就是升级为重量级锁 适应性锁这是JIT编译器对锁做的另外一个优化，如果各个线程持有锁的时间很短，那么一个线程竞争锁不到，就会暂停，发生上下文切换，让其他线程来执行。但是其他线程很快释放锁了，然后暂停的线程再次被唤醒。也就是说在这种情况下，线程会频繁的上下文切换，导致开销过大。所以对这种线程持有锁时间很短的情况，是可以采取忙等策略的，也就是一个线程没竞争到锁，进入一个while循环不停等待，不会暂停不会发生线程上下文切换，等到机会获取锁就继续执行好了 指令重排计算机在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排。 为什么指令重排序可以提高性能？简单地说，每一个指令都会包含多个步骤，每个步骤可能使用不同的硬件。因此，流水线技术产生了，它的原理是指令1还没有执行完，就可以开始执行指令2，而不用等到指令1执行结束之后再执行指令2，这样就大大提高了效率。 但是，流水线技术最害怕中断，恢复中断的代价是比较大的，所以我们要想尽办法不让流水线中断。指令重排就是减少中断的一种技术。 我们分析一下下面这个代码的执行情况： 123JAVAa = b + c;d = e - f ; 先加载b、c（注意，即有可能先加载b，也有可能先加载c），但是在执行add(b,c)的时候，需要等待b、c装载结束才能继续执行，也就是增加了停顿，那么后面的指令也会依次有停顿,这降低了计算机的执行效率。 为了减少这个停顿，我们可以先加载e和f,然后再去加载add(b,c),这样做对程序（串行）是没有影响的,但却减少了停顿。既然add(b,c)需要停顿，那还不如去做一些有意义的事情。 综上所述，指令重排对于提高CPU处理性能十分必要。虽然由此带来了乱序的问题，但是这点牺牲是值得的。 指令重排一般分为以下三种： 编译器优化重排 编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 指令并行重排 现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性(即后一个执行的语句无需依赖前面执行的语句的结果)，处理器可以改变语句对应的机器指令的执行顺序。 内存系统重排 由于处理器使用缓存和读写缓存冲区，这使得加载(load)和存储(store)操作看上去可能是在乱序执行，因为三级缓存的存在，导致内存与缓存的数据同步存在时间差。 指令重排可以保证串行语义一致，但是没有义务保证多线程间的语义也一致。所以在多线程下，指令重排序可能会导致一些问题。 as-if-serial语义as-if-serial语义的意思是：不管编译器和CPU如何重排序，必须保证在单线程情况下程序的结果是正确的。 以下数据有依赖关系，不能重排序。 写后读： 123CODEint a = 1; int b = a; 写后写： 123CODEint a = 1; int a = 2; 读后写： 1234CODEint a = 1; int b = a; int a = 2; 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 1234CODEint a = 1; int b = 2; int c = a + b; Java内存模型(JMM)在介绍Java内存模型之前，先来看一下到底什么是计算机内存模型。 计算机结构计算机结构简介冯诺依曼，提出计算机由五大组成部分，输入设备，输出设备存储器，控制器，运算器。 输入设备：鼠标，键盘等等 输出设备：显示器，打印机等等 存储器：内存条 运算器和控制器组成CPU CPU中央处理器，是计算机的控制和运算的核心，我们的程序终都会变成指令让CPU去执行，处理程序中 的数据。 内存我们的程序都是在内存中运行的，内存会保存程序运行时的数据，供CPU处理。 缓存CPU的运算速度和内存的访问速度相差比较大。这就导致CPU每次操作内存都要耗费很多等待时间。内 存的读写速度成为了计算机运行的瓶颈。于是就有了在CPU和主内存之间增加缓存的设计。靠近CPU 的缓存称为L1，然后依次是 L2，L3和主内存，CPU缓存模型如图下图所示。 CPU Cache分成了三个级别: L1， L2， L3。级别越小越接近CPU，速度也更快，同时也代表着容量越小。速度越快的价格越贵。 1、L1是接近CPU的，它容量小，例如32K，速度快，每个核上都有一个L1 Cache。 2、L2 Cache 更大一些，例如256K，速度要慢一些，一般情况下每个核上都有一个独立的L2 Cache。 3、L3 Cache是三级缓存中大的一级，例如12MB，同时也是缓存中慢的一级，在同一个CPU插槽 之间的核共享一个L3 Cache。 上面的图中有一个Latency指标。比如Memory这个指标为59.4ns，表示CPU在操作内存的时候有59.4ns的延迟，一级缓存最快只有1.2ns。 CPU处理数据的流程 Cache的出现是为了解决CPU直接访问内存效率低下问题的。 1、程序在运行的过程中，CPU接收到指令 后，它会先向CPU中的一级缓存（L1 Cache）去寻找相关的数据，如果命中缓存，CPU进行计算时就可以直接对CPU Cache中的数据进行读取和写人，当运算结束之后，再将CPUCache中的新数据刷新 到主内存当中，CPU通过直接访问Cache的方式替代直接访问主存的方式极大地提高了CPU 的吞吐能 力。 2、但是由于一级缓存（L1 Cache）容量较小，所以不可能每次都命中。这时CPU会继续向下一级的二 级缓存（L2 Cache）寻找，同样的道理，当所需要的数据在二级缓存中也没有的话，会继续转向L3 Cache、内存(主存)和硬盘。 Java内存模型1、Java Memory Molde (Java内存模型&#x2F;JMM)，千万不要和Java内存结构（JVM划分的那个堆，栈，方法区）混淆。关于“Java内存模型”的权威解释，参考 https://download.oracle.com/otn-pub/jcp/memory_model1.0-pfd-spec-oth-JSpec/memory_model-1_0-pfd-spec.pdf。 2、 Java内存模型，是Java虚拟机规范中所定义的一种内存模型，Java内存模型是标准化的，屏蔽掉了底层不同计算机的区别。 Java内存模型是一套规范，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量存储到内存和从内存中读取变量这样的底层细节，具体如下。 3、Java内存模型根据官方的解释，主要是在说两个关键字，一个是volatile，一个是synchronized。 主内存 主内存是所有线程都共享的，都能访问的。所有的共享变量都存储于主内存。 工作内存 每一个线程有自己的工作内存，工作内存只存储该线程对共享变量的副本。线程对变量的所有的操 作(读，取)都必须在工作内存中完成，而不能直接读写主内存中的变量，不同线程之间也不能直接 访问对方工作内存中的变量。 Java的线程不能直接在主内存中操作共享变量。而是首先将主内存中的共享变量赋值到自己的工作内存中，再进行操作，操作完成之后，刷回主内存。 Java内存模型的作用 Java内存模型是一套在多线程读写共享数据时，对共享数据的可见性、有序性、和原子性的规则和保障。 synchronized,volatile CPU缓存，内存与Java内存模型的关系 通过对前面的CPU硬件内存架构、Java内存模型以及Java多线程的实现原理的了解，我们应该已经意识到，多线程的执行终都会映射到硬件处理器上进行执行。 但Java内存模型和硬件内存架构并不完全一致。 对于硬件内存来说只有寄存器、缓存内存、主内存的概念，并没有工作内存和主内存之分，也就是说Java内存模型对内存的划分对硬件内存并没有任何影响， 因为JMM只是一种抽象的概念，是一组规则，不管是工作内存的数据还是主内存的数据，对于计算机硬 件来说都会存储在计算机主内存中，当然也有可能存储到CPU缓存或者寄存器中，因此总体上来说， Java内存模型和计算机硬件内存架构是一个相互交叉的关系，是一种抽象概念划分与真实物理硬件的交叉。 JMM内存模型与CPU硬件内存架构的关系： 工作内存：可能对应CPU寄存器，也可能对应CPU缓存，也可能对应内存。 Java内存模型是一套规范，描述了Java程序中各种变量(线程共享变量)的访问规则，以及在JVM中将变量 存储到内存和从内存中读取变量这样的底层细节，Java内存模型是对共享数据的可见性、有序性、和原子性的规则和保障。 再谈可见性 1、图中所示是 个双核 CPU 系统架构 ，每个核有自己的控制器和运算器，其中控制器包含一组寄存器和操作控制器，运算器执行算术逻辅运算。每个核都有自己的1级缓存，在有些架构里面还有1个所有 CPU 共享的2级缓存。 那么 Java 内存模型里面的工作内存，就对应这里的 Ll 或者 L2 存或者 CPU 寄存器。 2、一个线程操作共享变量时，它首先从主内存复制共享变量到自己的工作内存，然后对工作内存里的变量进行处理，处理完后将变量值更新到主内存。 3、那么假如线程A和线程B同时处理一个共享变量，会出现什么情况?我们使用图所示CPU架构，假设线程A和线程B使用不同CPU执行，并且当前两级Cache都为空，那么这时候由于Cache的存在，将会导致内存不可见问题，具体看下面的分析。 线程A首先获取共享变量X的值，由于两级Cache都没有命中，所以加载主内存中X的值，假如为0。然后把X&#x3D;0的值缓存到两级缓存，线程A修改X的值为1，然后将其写入两级Cache，并且刷新到主内存。线程A操作完毕后，线程A所在的CPU的两级Cache 内和主内存里面的X的值都是1。 线程B获取X的值，首先一级缓存没有命中，然后看二级缓存，二级缓存命中了，所以返回X&#x3D;1;到这里一切都是正常的，因为这时候主内存中也是X&#x3D;1。然后线程B修改X的值为2，并将其存放到线程2所在的一级Cache和共享二级Cache中，最后更新主内存中X 的值为2;到这里一切都是好的。 线程A 这次又需要修改X的值，获取时一级缓存命中，并且X&#x3D;1，到这里问题就出现了，明明线程B已经把X的值修改为了2，为何线程A获取的还是1呢?这就是共享变量的内存不可见问题，也就是线程B写入的值对线程A不可见。那么如何解决共享变量内存不可见问题?使用Java中的volatile和synchronized关键字就可以解决这个问题，下面会有讲解。 主内存与工作内存之间的交互为了保证数据交互时数据的正确性，Java内存模型中定义了8种操作来完成这个交互过程，这8种操作本身都是原子性的。虚拟机实现时必须保证下面 提及的每一种操作都是原子的、不可再分的。 (1)lock:作用于主内存的变量，它把一个变量标识为一条线程独占的状态。 (2)unlock:作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其它线程锁定。 (3)read:作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用。 (4)load:作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 (5)use:作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时都会执行这个操作。 (6)assign:作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 (7)store:作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write使用。 (8)write:作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中。 注意: 如果对一个变量执行lock操作，将会清空工作内存中此变量的值 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中 lock和unlock操作只有加锁才会有。synchronized就是通过这样来保证可见性的。 如果没有synchronized，那就是下面这样的 happens-before什么是happens-before?一方面，程序员需要JMM提供一个强的内存模型来编写代码；另一方面，编译器和处理器希望JMM对它们的束缚越少越好，这样它们就可以最可能多的做优化来提高性能，希望的是一个弱的内存模型。 JMM考虑了这两种需求，并且找到了平衡点，对编译器和处理器来说，只要不改变程序的执行结果（单线程程序和正确同步了的多线程程序），编译器和处理器怎么优化都行。 而对于程序员，JMM提供了happens-before规则（JSR-133规范），满足了程序员的需求——简单易懂，并且提供了足够强的内存可见性保证。换言之，程序员只要遵循happens-before规则，那他写的程序就能保证在JMM中具有强的内存可见性。 JMM使用happens-before的概念来定制两个操作之间的执行顺序。这两个操作可以在一个线程以内，也可以是不同的线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证。 happens-before关系的定义如下： 如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么JMM也允许这样的重排序。 happens-before关系本质上和as-if-serial语义是一回事。 as-if-serial语义保证单线程内重排序后的执行结果和程序代码本身应有的结果是一致的，happens-before关系保证正确同步的多线程程序的执行结果不被重排序改变。 总之，如果操作A happens-before操作B，那么操作A在内存上所做的操作对操作B都是可见的，不管它们在不在一个线程。 天然的happens-before关系在Java中，有以下天然的happens-before关系： 1、程序次序规则：一个线程内，按照代码顺序，书写在前面的操作先行发生于书写在后面的操作 2、锁定规则：一个unLock操作先行发生于后面对同一个锁的lock操作，比如说在代码里有先对一个lock.lock()，lock.unlock()，lock.lock() 3、volatile变量规则：对一个volatile变量的写操作先行发生于后面对这个volatile变量的读操作，volatile变量写，再是读，必须保证是先写，再读 4、传递规则：如果操作A先行发生于操作B，而操作B又先行发生于操作C，则可以得出操作A先行发生于操作C 5、线程启动规则：Thread对象的start()方法先行发生于此线程的每个一个动作，thread.start()，thread.interrupt() 6、线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生 7、线程终结规则：线程中所有的操作都先行发生于线程的终止检测，我们可以通过Thread.join()方法结束、Thread.isAlive()的返回值手段检测到线程已经终止执行 8、对象终结规则：一个对象的初始化完成先行发生于他的finalize()方法的开始 上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。 举例1： 12345JAVAint a = 1; // A操作int b = 2; // B操作int sum = a + b;// C 操作System.out.println(sum); 上面这8条原则的意思很显而易见，就是程序中的代码如果满足这个条件，就一定会按照这个规则来保证指令的顺序。 举例1： 12345JAVAint a = 1; // A操作int b = 2; // B操作int sum = a + b;// C 操作System.out.println(sum); 根据以上介绍的happens-before规则，假如只有一个线程，那么不难得出： 1234CODE1&gt; A happens-before B 2&gt; B happens-before C 3&gt; A happens-before C 注意，真正在执行指令的时候，其实JVM有可能对操作A &amp; B进行重排序，因为无论先执行A还是B，他们都对对方是可见的，并且不影响执行结果。 如果这里发生了重排序，这在视觉上违背了happens-before原则，但是JMM是允许这样的重排序的。 所以，我们只关心happens-before规则，不用关心JVM到底是怎样执行的。只要确定操作A happens-before操作B就行了。 重排序有两类，JMM对这两类重排序有不同的策略： 会改变程序执行结果的重排序，比如 A -&gt; C，JMM要求编译器和处理器都禁止这种重排序。 不会改变程序执行结果的重排序，比如 A -&gt; B，JMM对编译器和处理器不做要求，允许这种重排序。 举例2： 123456789101112131415JAVA//伪代码volatile boolean flag = false; //线程1 prepare(); flag = false; //线程2 while(!flag)&#123; sleep(); &#125; //基于准备好的资源进行操作 execute(); 这8条原则是避免说出现乱七八糟扰乱秩序的指令重排，要求是这几个重要的场景下，比如是按照顺序来，但是8条规则之外，可以随意重排指令。 比如这个例子，如果用volatile来修饰flag变量，一定可以让prepare()指令在flag &#x3D; true之前先执行，这就禁止了指令重排。 因为volatile要求的是，volatile前面的代码一定不能指令重排到volatile变量操作后面，volatile后面的代码也不能指令重排到volatile前面。 volatilevolatile不保证原子性，只保证可见性和禁止指令重排 CPU术语介绍 123456789101112131415161718JAVAprivate static volatile SingletonDemo instance = null; private SingletonDemo() &#123; System.out.println(Thread.currentThread().getName() + &quot;\\t 执行单例构造函数&quot;); &#125; public static SingletonDemo getInstance()&#123; if(instance == null)&#123; synchronized (SingletonDemo.class)&#123; if(instance == null)&#123; instance = new SingletonDemo(); //pos_1 &#125; &#125; &#125; return instance; &#125; pos_1处的代码转换成汇编代码如下 123SHELL0x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); volatile保证可见性原理有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架 构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情。 1）将当前处理器缓存行的数据写回到系统内存。 2）这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。 为了提高处理速度，处理器不直接和主内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的 变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现MESI缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。 注意：lock前缀指令是同时保证可见性和有序性（也就是禁止指令重排）的 注意：lock前缀指令相当于一个内存屏障【后文讲】 volatile禁止指令重排的原理12345678910111213141516JAVApublic class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // step 1 flag = true; // step 2 &#125; public void reader() &#123; if (flag) &#123; // step 3 System.out.println(a); // step 4 &#125; &#125;&#125; 在JSR-133之前的旧的Java内存模型中，是允许volatile变量与普通变量重排序的。那上面的案例中，可能就会被重排序成下列时序来执行： 线程A写volatile变量，step 2，设置flag为true； 线程B读同一个volatile，step 3，读取到flag为true； 线程B读普通变量，step 4，读取到 a &#x3D; 0； 线程A修改普通变量，step 1，设置 a &#x3D; 1； 可见，如果volatile变量与普通变量发生了重排序，虽然volatile变量能保证内存可见性，也可能导致普通变量读取错误。 所以在旧的内存模型中，volatile的写-读就不能与锁的释放-获取具有相同的内存语义了。为了提供一种比锁更轻量级的线程间的通信机制，JSR-133专家组决定增强volatile的内存语义：严格限制编译器和处理器对volatile变量与普通变量的重排序。 编译器还好说，JVM是怎么还能限制处理器的重排序的呢？它是通过内存屏障来实现的。 什么是内存屏障？硬件层面，内存屏障分两种：读屏障（Load Barrier）和写屏障（Store Barrier）。内存屏障有两个作用： 阻止屏障两侧的指令重排序； 强制把写缓冲区&#x2F;高速缓存中的脏数据等写回主内存，或者让缓存中相应的数据失效。 注意这里的缓存主要指的是上文说的CPU缓存，如L1，L2等 保守策略下 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的前面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。编译器选择了一个比较保守的JMM内存屏障插入策略，但它可以保证在任意处理器平台，任意的程序中都能 得到正确的volatile内存语义。 再逐个解释一下这几个屏障。注：下述Load代表读操作，Store代表写操作 LoadLoad屏障：对于这样的语句Load1; LoadLoad; Load2，在Load2及后续读取操作要读取的数据被访问前，保证Load1要读取的数据被读取完毕。StoreStore屏障：对于这样的语句Store1; StoreStore; Store2，在Store2及后续写入操作执行前，这个屏障会吧Store1强制刷新到内存，保证Store1的写入操作对其它处理器可见。LoadStore屏障：对于这样的语句Load1; LoadStore; Store2，在Store2及后续写入操作被刷出前，保证Load1要读取的数据被读取完毕。StoreLoad屏障：对于这样的语句Store1; StoreLoad; Load2，在Load2及后续所有读取操作执行前，保证Store1的写入对所有处理器可见。它的开销是四种屏障中最大的（冲刷写缓冲器，清空无效化队列）。在大多数处理器的实现中，这个屏障是个万能屏障，兼具其它三种内存屏障的功能 对于连续多个volatile变量读或者连续多个volatile变量写，编译器做了一定的优化来提高性能，比如： 第一个volatile读; LoadLoad屏障； 第二个volatile读； LoadStore屏障 1、下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图 图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任 意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与 后面可能有的volatile读&#x2F;写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面 是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确 实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile 读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个 写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时， 选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率 2、下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图 图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。 LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。 上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 优化举例： 123456789101112131415JAVAclass VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; // 第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; // 普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; // 第二个 volatile写 &#125; // 其他方法 &#125; &#125; 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。 上面的优化针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以X86处理器为例，图中除最后的StoreLoad屏障外，其他的屏障都会被省略。 X86处理器优化前面保守策略下的volatile读和写，在X86处理器平台可以优化成如下图所示。 X86处理器仅会对写-读操作做重排序。X86不会对读-读、读-写和写-写操作 做重排序，因此在X86处理器中会省略掉这3种操作类型对应的内存屏障。在X86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在X86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 volatile的用途 下面的代码在前面可能已经写过了，这里总结一下 从volatile的内存语义上来看，volatile可以保证内存可见性且禁止重排序。 在保证内存可见性这一点上，volatile有着与锁相同的内存语义，所以可以作为一个“轻量级”的锁来使用。但由于volatile仅仅保证对单个volatile变量的读&#x2F;写具有原子性，而锁可以保证整个临界区代码的执行具有原子性。所以在功能上，锁比volatile更强大；在性能上，volatile更有优势。 在禁止重排序这一点上，volatile也是非常有用的。比如我们熟悉的单例模式，其中有一种实现方式是“双重锁检查”，比如这样的代码： 1234567891011121314151617JAVApublic class Singleton &#123; private static Singleton instance; // 不使用volatile关键字 // 双重锁检验 public static Singleton getInstance() &#123; if (instance == null) &#123; // 第7行 synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); // 第10行 &#125; &#125; &#125; return instance; &#125;&#125; 如果这里的变量声明不使用volatile关键字，是可能会发生错误的。它可能会被重排序： 123456789101112JAVAinstance = new Singleton(); // 第10行// 可以分解为以下三个步骤1 memory=allocate();// 分配内存 相当于c的malloc2 ctorInstanc(memory) //初始化对象3 s=memory //设置s指向刚分配的地址// 上述三个步骤可能会被重排序为 1-3-2，也就是：1 memory=allocate();// 分配内存 相当于c的malloc3 s=memory //设置s指向刚分配的地址2 ctorInstanc(memory) //初始化对象 而一旦假设发生了这样的重排序，比如线程A在第10行执行了步骤1和步骤3，但是步骤2还没有执行完。这个时候另一个线程B执行到了第7行，它会判定instance不为空，然后直接返回了一个未初始化完成的instance！ 所以JSR-133对volatile做了增强后，volatile的禁止重排序功能还是非常有用的。 中断什么是中断首先一个线程不应该由其他线程来强制中断或停止，而是应该由线程自己自行停止。所以，Thread.stop, Thread.suspend, Thread.resume 都已经被废弃了。 其次在Java中没有办法立即停止一条线程，然而停止线程却显得尤为重要，如取消一个耗时操作。因此，Java提供了一种用于停止线程的机制——中断。 中断只是一种协作机制，Java没有给中断增加任何语法，中断的过程完全需要程序员自己实现。若要中断一个线程，你需要手动调用该线程的interrupt方法，该方法也仅仅是将线程对象的中断标识设成true；接着你需要自己写代码不断地检测当前线程的标识位，如果为true，表示别的线程要求这条线程中断，此时究竟该做什么需要你自己写代码实现。 每个线程对象中都有一个标识，用于表示线程是否被中断；该标识位为true表示中断，为false表示未中断；通过调用线程对象的interrupt方法将该线程的标识位设为true；可以在别的线程中调用，也可以在自己的线程中调用。 中断API 如何停止线程使用中断标志位12345678910111213141516171819202122232425262728293031323334package com.atguigu.ggq.juc.interrupt;import java.util.concurrent.TimeUnit;public class ThreeApi &#123; public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(true)&#123;// System.out.println(&quot;线程执行&quot;); if(Thread.currentThread().isInterrupted())&#123; // 线程自己实现中断 System.out.println(&quot;线程中断&quot;); break; &#125; System.out.println(&quot;线程执行&quot;);// try &#123;// TimeUnit.SECONDS.sleep(1);// &#125; catch (InterruptedException e) &#123; //Thread.currentThread().interrupt(); 错误1 不加这行, 错误2 加// e.printStackTrace();// 抛出(注意这里代码是catch 异常已抛出过了)这个异常时 标志位变为false了 // &#125; &#125; &#125;,&quot;t1&quot;); t1.start(); System.out.println(&quot;*************&quot;+t1.isInterrupted()); try &#123; TimeUnit.MILLISECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t1.interrupt(); System.out.println(&quot;*************&quot;+t1.isInterrupted()); &#125;&#125; 12345678910111213正常结果*************false线程执行线程执行线程执行线程执行线程执行线程执行*************true线程中断Process finished with exit code 0 错误结果1(如果线程被block) + sleep 1抛出异常后 线程会一直执行 错误结果2 抛出异常 但线程关闭 添加捕获异常时的中断 12345678910111213141516171819202122232425262728//部分源码public boolean isInterrupted() &#123; return isInterrupted(false); &#125;private native boolean isInterrupted(boolean ClearInterrupted);public static boolean interrupted() &#123; return currentThread().isInterrupted(true); // 返回当前 状态 并设为 false&#125;// 此方法 若线程被处于阻塞 会抛出异常public void interrupt() &#123; if (this != Thread.currentThread()) checkAccess(); synchronized (blockerLock) &#123; Interruptible b = blocker; if (b != null) &#123; interrupt0(); // Just to set the interrupt flag b.interrupt(this); return; &#125; &#125; interrupt0();&#125; interrupt方法源码注释 sleep (优雅的关闭)使用 volatile Boolean12345678910111213public class Vol &#123; private static volatile boolean flag; public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(!flag) &#123; //do &#125; &#125;,&quot;t1&quot;); t1.start(); flag = true; &#125;&#125; 使用AutomaticBoolean12345678910111213public class Ato &#123; private static final AtomicBoolean flag = new AtomicBoolean(true); public static void main(String[] args) &#123; Thread t1 = new Thread(()-&gt;&#123; while(flag.get()) &#123; //do &#125; &#125;,&quot;t1&quot;); t1.start(); flag.set(false); &#125;&#125; LockSupport是什么LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。 下面这句话，后面详细说LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程 3种让线程等待和唤醒的方法方式1：使用Object中的wait()方法让线程等待，使用Object中的notify()方法唤醒线程 创建一个对象作为锁对象 Object类中的wait、notify、notifyAll用于线程等待和唤醒的方法，都必须在synchronized内部执行（必须用到关键字synchronized）。 先wait后notify才OK 方式2：使用JUC包中Condition的await()方法让线程等待，使用signal()方法唤醒线程 Lock lock &#x3D; new ReentrantLock(); lock.lock(); lock.unlock(); 线程先要获得并持有锁，必须在锁块(synchronized或lock)中 必须要先等待后唤醒，线程才能够被唤醒 以上两种 使用不当 会有非法monitor状态异常 方式3：LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程 LockSupport类使用了一种名为Permit（许可）的概念来做到阻塞和唤醒线程的功能， 每个线程都有一个许可(permit)，permit只有两个值1和零，默认是零。可以把许可看成是一种(0,1)信号量（Semaphore），但与 Semaphore 不同的是，许可的累加上限是1。默认0 park 把什么搁置 设为0 不许可 阻塞当前线程 unpark(t) 设为1 可执行 唤醒 t 不会抛什么异常 \\ 123456789101112131415161718public class T1&#123; public static void main(String[] args) &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(3); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()); LockSupport.park(); //执行park无效 System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()+&quot;---被叫醒&quot;); &#125;,&quot;t1&quot;); t1.start(); try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; LockSupport.unpark(t1); // 先执行 System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+System.currentTimeMillis()+&quot;---unpark over&quot;); &#125;&#125; 聊聊ThreadLocalThreadLocal提供线程局部变量。这些变量与正常的变量不同，因为每一个线程在访问ThreadLocal实例的时候（通过其get或set方法）都有自己的、独立初始化的变量副本。ThreadLocal实例通常是类中的私有静态字段，使用它的目的是希望将状态（例如，用户ID或事务ID）与线程关联起来。 实现每一个线程都有自己专属的本地变量副本(自己用自己的变量不麻烦别人，不和其他人共享，人人有份，人各一份)， 主要解决了让每个线程绑定自己的值，通过使用get()和set()方法，获取默认值或将其值更改为当前线程所存的副本的值从而避免了线程安全问题。 使用例子1 群雄逐鹿起纷争 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.concurrent.TimeUnit;class MovieTicket&#123;int number = 50;public synchronized void saleTicket() &#123;if(number &gt; 0) &#123; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;号售票员卖出第： &quot;+(number--)); &#125;else&#123; System.out.println(&quot;--------卖完了&quot;); &#125; &#125;&#125;/** * @auther zzyy * @create 2021-03-23 15:03 * 三个售票员卖完50张票务，总量完成即可，吃大锅饭，售票员每个月固定月薪*/public class ThreadLocalDemo&#123;public static void main(String[] args) &#123; MovieTicket movieTicket = new MovieTicket(); //新建三个线程.start for (int i = 1; i 3; i++) &#123; new Thread(() -&gt; &#123; for (int j = 0; j 20; j++) &#123; movieTicket.saleTicket(); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,String.valueOf(i)).start(); &#125; &#125;&#125; 2 凭本事拿提成(人手一份天下安) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.atguigu.juc.tl;class MovieTicket&#123;int number = 50;public synchronized void saleTicket() &#123; if(number &gt; 0) &#123; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;号售票员卖出第： &quot;+(number--)); &#125;else&#123; System.out.println(&quot;--------卖完了&quot;); &#125; &#125;&#125;class House&#123; ThreadLocal threadLocal = ThreadLocal.withInitial(() -&gt; 0);public void saleHouse() &#123; Integer value = threadLocal.get(); value++; threadLocal.set(value); &#125;&#125;/** * @auther zzyy * @create 2021-03-23 15:03 * 1 三个售票员卖完50张票务，总量完成即可，吃大锅饭，售票员每个月固定月薪* * 2 分灶吃饭，各个销售自己动手，丰衣足食*/public class ThreadLocalDemo&#123;public static void main(String[] args) &#123;/*MovieTicket movieTicket = new MovieTicket(); for (int i = 1; i new Thread(() -&gt; &#123; for (int j = 0; j movieTicket.saleTicket(); try &#123; TimeUnit.MILLISECONDS.sleep(10); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;,String.valueOf(i)).start(); &#125;*/ //===========================================House house = new House();new Thread(() -&gt; &#123;try &#123; for (int i = 1; i 3; i++) &#123; house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123;house.threadLocal.remove();//如果不清理自定义的 ThreadLocal 变量，可能会影响后续业务逻辑和造成内存泄露等问题&#125; &#125;,&quot;t1&quot;).start();new Thread(() -&gt; &#123;try &#123;for (int i = 1; i &lt; 2; i++) &#123;house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123; house.threadLocal.remove(); &#125; &#125;,&quot;t2&quot;).start();new Thread(() -&gt; &#123;try &#123;for (int i = 1; i &lt; 5; i++) &#123;house.saleHouse(); &#125; System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;finally &#123;house.threadLocal.remove(); &#125; &#125;,&quot;t3&quot;).start(); System.out.println(Thread.currentThread().getName()+&quot;\\t&quot;+&quot;---&quot;+house.threadLocal.get()); &#125;&#125; 总结 统一设置初始值，但是每个线程对这个值的修改都是各自线程互相独立的 如何才能不争抢 ​ 1 加入synchronized或者Lock控制资源的访问顺序 ​ 2 人手一份，大家各自安好，没必要抢夺 ​ 由浅入深Java多线程第一章 进程与线程的基本概念进程解决了批处理系统时只能存在一个程序问题 线程解决了进程在一段时间只能做一件事情 进程让操作系统的并发性成为了可能,线程让进程的并发成为了可能 多进程也可以实现并发,为什么我们要使用多线程 进程间通信比较复杂,通常我们需要共享资源,这些资源在线程间通信比较简单 进程是重量级 区别 进程间内存隔离,数据共享复杂,但是同步简单 线程则相反 一个进程出现问题不会影响其他进程,而线程则不同 进程创建不仅需要寄存器和栈信息,还需要资源的分配回收以及页调度,线程只需要保存寄存器和栈信息 另外一个重要区别是进程是操作系统进行资源分配的基本单位,而线程是os进行调度的基本单位即cpu分配时间的单位 第二章 Java多线程入门类和接口2.1 Thread类和Runnable接口2.2 Callable、Future与FutureTast第三章 线程组和线程优先级第四章 Java线程的状态及主要转换方法4.3.4线程中断第五章 Java线程间的通信线程的生命周期os五状态 新建 就绪 运行 等待 终止 java6状态 1. NEW 还未调用start方法 2. RUNABLE表示当前线程正在运行中. 线程在JVM运行,也有可能在等待其他系统资源如IO 包含了os的就绪与运行 3. BLOCKED 阻塞状态 等待锁的释放以进入同步区 4. WAITING 等待状态 被唤醒进入RUNABLE 5. TIMEED_WAITING 限定时间的等待,自动唤醒 6. 终止状态 线程执行完毕 第六章 Java内存模型基础知识第七章 重排序与happens-before第八章 volatile第九章 synchronized与锁synchronized为什么是重量级锁 第十章 乐观锁和悲观锁第十一章 AQS第十二章 线程池原理12.1为什么要使用线程池 创建&#x2F;销毁线程需要消耗系统资源,线程池可复用已创建的线程 控制并发数量(主要原因),并发数量过多可能会导致资源消耗过多导致服务器崩溃 可以对线程做统一管理 12.2 线程池原理线程池顶层接口是Executor,TreadPoolExecutor是实现类 12.2.1 ThreadPoolExecutor提供的构造方法一共四个构造方法 涉及到5~7个参数 都有的 5 个 核心线程数 最大线程数 非核心线程闲置超时时长 时长单位 阻塞队列,维护等待执行的Runnable任务对象 两个非必须参数 指定线程工厂 默认defaultThreadFactory 拒绝处理策略 线程数量大于最大线程数就会采用拒绝处理策略 默认AbortPolicy 丢弃任务并抛出异常 第十三章 阻塞队列第十四章 锁接口和类第十五章 并发集合容器简介第十六章 CopyOnwrite第十七章 通信工具类第十八章 Fork&#x2F;Join框架第十九章 Java8 Stream并行计算原理第二十章 计划任务","categories":[{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}],"tags":[{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"}]},{"title":"SSM总结","slug":"4spring","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:49:52.725Z","comments":true,"path":"2022/09/01/4spring/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/4spring/","excerpt":"","text":"SSM概述2. Spring Framework五大功能模块 功能模块 功能介绍 Core Container 核心容器，在 Spring 环境下使用任何功能都必须基于 IOC 容器。 AOP&amp;Aspects 面向切面编程 Testing 提供了对 junit 或 TestNG 测试框架的整合。 Data Access&#x2F;Integration 提供了对数据访问&#x2F;集成的功能。 Spring MVC 提供了面向Web应用程序的集成功能。 3. 程序中的复杂容器Servlet 容器能够管理 Servlet、Filter、Listener 这样的组件的一生，所以它是一个复杂容器。我们即将要学习的 IOC 容器也是一个复杂容器。它们不仅要负责创建组件的对象、存储组件的对象，还要负责调用组件的方法让它们工作，最终在特定情况下销毁组件。 [1]Servlet生命周期 名称 时机 次数 创建对象 默认情况：接收到第一次请求修改启动顺序后：Web应用启动过程中 一次 初始化操作 创建对象之后 一次 处理请求 接收到请求 多次 销毁操作 Web应用卸载之前 一次 [2]Filter生命周期 生命周期阶段 执行时机 执行次数 创建对象 Web应用启动时 一次 初始化 创建对象后 一次 拦截请求 接收到匹配的请求 多次 销毁 Web应用卸载前 一次 4.JDBCtemplate2. mybatis5、Mybatis特性 MyBatis支持定制化SQL、存储过程以及高级映射 MyBatis避免了几乎所有的JDBC代码和手动设置参数以及结果集解析操作 MyBatis可以使用简单的XML或注解实现配置和原始映射；将接口和Java的POJO（Plain Ordinary Java Object，普通的Java对象）映射成数据库中的记录 Mybatis是一个半自动的ORM（Object Relation Mapping）框架 6、和其它持久化层技术对比 JDBC SQL 夹杂在Java代码中耦合度高，导致硬编码内伤 维护不易且实际开发需求中 SQL 有变化，频繁修改的情况多见 代码冗长，开发效率低 Hibernate 和 JPA 操作简便，开发效率高 程序中的长难复杂 SQL 需要绕过框架 内部自动生产的 SQL，不容易做特殊优化 基于全映射的全自动框架，大量字段的 POJO 进行部分映射时比较困难。 反射操作太多，导致数据库性能下降 MyBatis 轻量级，性能出色 SQL 和 Java 编码分开，功能边界清晰。Java代码专注业务、SQL语句专注数据 开发效率稍逊于HIbernate，但是完全能够接收 3. spring mvc11、概述SpringMVC 是 Spring 为表述层开发提供的一整套完备的解决方案。在表述层框架历经 Strust、WebWork、Strust2 等诸多产品的历代更迭之后，目前业界普遍选择了 SpringMVC 作为 Java EE 项目表述层开发的首选方案。之所以能做到这一点，是因为 SpringMVC 具备如下显著优势： Spring 家族原生产品，与 IOC 容器等基础设施无缝对接 表述层各细分领域需要解决的问题全方位覆盖，提供全面解决方案 代码清新简洁，大幅度提升开发效率 内部组件化程度高，可插拔式组件即插即用，想要什么功能配置相应组件即可 性能卓著，尤其适合现代大型、超大型互联网项目要求 2、表述层框架要解决的基本问题 请求映射 数据输入 视图界面 请求分发 表单回显 会话控制 过滤拦截 异步交互 文件上传 文件下载 数据校验 类型转换 3、SpringMVC 代码对比①基于原生 Servlet API 开发代码片段1234567protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; String userName = request.getParameter(&quot;userName&quot;); System.out.println(&quot;userName=&quot;+userName); &#125; ②基于 SpringMVC 开发代码片段1234567@RequestMapping(&quot;/user/login&quot;)public String login(@RequestParam(&quot;userName&quot;) String userName)&#123; System.out.println(&quot;userName=&quot;+userName); return &quot;result&quot;;&#125; 小插曲打包格式 声明为 HTML5 文档 元素是 HTML 页面的根元素 元素包含了文档的元（meta）数据，如 定义网页编码格式为 utf-8。 元素描述了文档的标题 元素包含了可见的页面内容 元素定义一个大标题 元素定义一个段落 第0章 Spring内置的能够使用的工具速查注册Bean到IOC容器1. BeanDefinition12345AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setBeanClass(User.class);context.registerBeanDefinition(&quot;user&quot;, beanDefinition);System.out.println(context.getBean(&quot;user&quot;)); 我们还可以通过BeanDefinition设置一个Bean的其他属性 123beanDefinition.setScope(&quot;prototype&quot;); // 设置作用域beanDefinition.setInitMethodName(&quot;init&quot;); // 设置初始化方法beanDefinition.setLazyInit(true); // 设置懒加载 和申明式事务、编程式事务类似，通过，@Bean，@Component等申明式方式所定义的Bean，最终都会被Spring解析为对应的BeanDefinition对象，并放入Spring容器中。 BeanDefinitionReaderAnnotatedBeanDefinitionReader可以直接把某个类转换为BeanDefinition，并且会解析该类上的注解，比如 12345678AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);AnnotatedBeanDefinitionReader annotatedBeanDefinitionReader = new AnnotatedBeanDefinitionReader(context);// 将User.class解析为BeanDefinitionannotatedBeanDefinitionReader.register(User.class);System.out.println(context.getBean(&quot;user&quot;)); 注意：它能解析的注解是：@Conditional，**@Scope**、@Lazy、@Primary、@DependsOn、@Role、@Description XmlBeanDefinitionReader可以解析标签 123456AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);XmlBeanDefinitionReader xmlBeanDefinitionReader = new XmlBeanDefinitionReader(context);int i = xmlBeanDefinitionReader.loadBeanDefinitions(&quot;spring.xml&quot;);System.out.println(context.getBean(&quot;user&quot;)); ClassPathBeanDefinitionScannerClassPathBeanDefinitionScanner是扫描器，但是它的作用和BeanDefinitionReader类似，它可以进行扫描，扫描某个包路径，对扫描到的类进行解析，比如，扫描到的类上如果存在@Component注解，那么就会把这个类解析为一个BeanDefinition，比如：​ 1234567AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();context.refresh();ClassPathBeanDefinitionScanner scanner = new ClassPathBeanDefinitionScanner(context);scanner.scan(&quot;com.zhouyu&quot;);System.out.println(context.getBean(&quot;userService&quot;)); 事件发布先定义一个事件监听器 123456789@Beanpublic ApplicationListener applicationListener() &#123; return new ApplicationListener() &#123; @Override public void onApplicationEvent(ApplicationEvent event) &#123; System.out.println(&quot;接收到了一个事件&quot;); &#125; &#125;;&#125; 然后发布一个事件： 12345// 可以用ApplicationEventListener.publishEvent()context.publishEvent(&quot;kkk&quot;);// 这个底层用的就是上面的// 理解: 类似于消息队列 执行到publish之后 事件监听就会执行onApplicationEvent方法 类型转化在Spring源码中，有可能需要把String转成其他类型，所以在Spring源码中提供了一些技术来更方便的做对象的类型转化，关于类型转化的应用场景， 后续看源码的过程中会遇到很多。 123@Value(&quot;ggq&quot;)public User user;// 报错 类型不匹配 PropertyEditor这其实是JDK中提供的类型转化工具类 12345678910111213public class StringToUserPropertyEditor extends PropertyEditorSupport implements PropertyEditor &#123; @Override public void setAsText(String text) throws IllegalArgumentException &#123; User user = new User(); user.setName(text); this.setValue(user); &#125;&#125;StringToUserPropertyEditor propertyEditor = new StringToUserPropertyEditor();propertyEditor.setAsText(&quot;1&quot;);User value = (User) propertyEditor.getValue();System.out.println(value); 如何向Spring中注册PropertyEditor： 12345678910@Beanpublic CustomEditorConfigurer customEditorConfigurer() &#123; CustomEditorConfigurer customEditorConfigurer = new CustomEditorConfigurer(); Map&lt;Class&lt;?&gt;, Class&lt;? extends PropertyEditor&gt;&gt; propertyEditorMap = new HashMap&lt;&gt;(); // 表示StringToUserPropertyEditor可以将String转化成User类型，在Spring源码中，如果发现当前对象是String，而需要的类型是User，就会使用该PropertyEditor来做类型转化 propertyEditorMap.put(User.class, StringToUserPropertyEditor.class); customEditorConfigurer.setCustomEditors(propertyEditorMap); return customEditorConfigurer;&#125; 假设现在有如下Bean: 1234567891011@Componentpublic class UserService &#123; @Value(&quot;xxx&quot;) private User user; public void test() &#123; System.out.println(user); &#125;&#125; 那么test属性就能正常的完成属性赋值 ConversionServiceSpring中提供的类型转化服务，它比PropertyEditor更强大 1234567891011121314151617181920212223public class StringToUserConverter implements ConditionalGenericConverter &#123; @Override public boolean matches(TypeDescriptor sourceType, TypeDescriptor targetType) &#123; return sourceType.getType().equals(String.class) &amp;&amp; targetType.getType().equals(User.class); &#125; @Override public Set&lt;ConvertiblePair&gt; getConvertibleTypes() &#123; return Collections.singleton(new ConvertiblePair(String.class, User.class)); &#125; @Override public Object convert(Object source, TypeDescriptor sourceType, TypeDescriptor targetType) &#123; User user = new User(); user.setName((String)source); return user; &#125;&#125;DefaultConversionService conversionService = new DefaultConversionService();conversionService.addConverter(new StringToUserConverter());User value = conversionService.convert(&quot;1&quot;, User.class);System.out.println(value); 如何向Spring中注册ConversionService： 1234567@Beanpublic ConversionServiceFactoryBean conversionService() &#123; ConversionServiceFactoryBean conversionServiceFactoryBean = new ConversionServiceFactoryBean(); conversionServiceFactoryBean.setConverters(Collections.singleton(new StringToUserConverter())); return conversionServiceFactoryBean;&#125; TypeConverter整合了PropertyEditor和ConversionService的功能，是Spring内部用的 12345SimpleTypeConverter typeConverter = new SimpleTypeConverter();typeConverter.registerCustomEditor(User.class, new StringToUserPropertyEditor());//typeConverter.setConversionService(conversionService);User value = typeConverter.convertIfNecessary(&quot;1&quot;, User.class);System.out.println(value); OrderComparatorOrderComparator是Spring所提供的一种比较器，可以用来根据@Order注解或实现Ordered接口来执行值进行笔记，从而可以进行排序。 比如：​ 123456789101112public class A implements Ordered &#123; @Override public int getOrder() &#123; return 3; &#125; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125; 12345678910111213141516171819202122232425262728293031public class B implements Ordered &#123; @Override public int getOrder() &#123; return 2; &#125; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; A a = new A(); // order=3 B b = new B(); // order=2 OrderComparator comparator = new OrderComparator(); System.out.println(comparator.compare(a, b)); // 1 List list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); // 按order值升序排序 list.sort(comparator); System.out.println(list); // B，A &#125;&#125; 另外，Spring中还提供了一个OrderComparator的子类：AnnotationAwareOrderComparator，它支持用@Order来指定order值。比如：​ 12345678910111213141516171819202122232425262728293031323334353637@Order(3)public class A &#123; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;@Order(2)public class B &#123; @Override public String toString() &#123; return this.getClass().getSimpleName(); &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; A a = new A(); // order=3 B b = new B(); // order=2 AnnotationAwareOrderComparator comparator = new AnnotationAwareOrderComparator(); System.out.println(comparator.compare(a, b)); // 1 List list = new ArrayList&lt;&gt;(); list.add(a); list.add(b); // 按order值升序排序 list.sort(comparator); System.out.println(list); // B，A &#125;&#125; MetadataReader、ClassMetadata、AnnotationMetadata在Spring中需要去解析类的信息，比如类名、类中的方法、类上的注解，这些都可以称之为类的元数据，所以Spring中对类的元数据做了抽象，并提供了一些工具类。 MetadataReader表示类的元数据读取器，默认实现类为SimpleMetadataReader。比如： 123456789101112131415161718192021public class Test &#123; public static void main(String[] args) throws IOException &#123; SimpleMetadataReaderFactory simpleMetadataReaderFactory = new SimpleMetadataReaderFactory(); // 构造一个MetadataReader MetadataReader metadataReader = simpleMetadataReaderFactory.getMetadataReader(&quot;com.zhouyu.service.UserService&quot;); // 得到一个ClassMetadata，并获取了类名 ClassMetadata classMetadata = metadataReader.getClassMetadata(); System.out.println(classMetadata.getClassName()); // 获取一个AnnotationMetadata，并获取类上的注解信息 AnnotationMetadata annotationMetadata = metadataReader.getAnnotationMetadata(); for (String annotationType : annotationMetadata.getAnnotationTypes()) &#123; System.out.println(annotationType); &#125; &#125;&#125; 需要注意的是，SimpleMetadataReader去解析类时，使用的ASM技术。 并不是等Java类加载到JVM在解析,而是直接读取字节码文件 为什么要使用ASM技术，Spring启动的时候需要去扫描，如果指定的包路径比较宽泛，那么扫描的类是非常多的，那如果在Spring启动时就把这些类全部加载进JVM了，这样不太好，所以使用了ASM技术。 ExcludeFilter和IncludeFilter这两个Filter是Spring扫描过程中用来过滤的。ExcludeFilter表示排除过滤器，IncludeFilter表示包含过滤器。 比如以下配置，表示扫描com.zhouyu这个包下面的所有类，但是排除UserService类，也就是就算它上面有@Component注解也不会成为Bean。 123456@ComponentScan(value = &quot;com.zhouyu&quot;, excludeFilters = &#123;@ComponentScan.Filter( type = FilterType.ASSIGNABLE_TYPE, classes = UserService.class)&#125;.)public class AppConfig &#123;&#125; 再比如以下配置，就算UserService类上没有@Component注解，它也会被扫描成为一个Bean。 123456@ComponentScan(value = &quot;com.zhouyu&quot;, includeFilters = &#123;@ComponentScan.Filter( type = FilterType.ASSIGNABLE_TYPE, classes = UserService.class)&#125;)public class AppConfig &#123;&#125; ​ FilterType分为： ANNOTATION：表示是否包含某个注解 ASSIGNABLE_TYPE：表示是否是某个类 ASPECTJ：表示否是符合某个Aspectj表达式 REGEX：表示是否符合某个正则表达式 CUSTOM：自定义 在Spring的扫描逻辑中，默认会添加一个AnnotationTypeFilter给includeFilters，表示默认情况下Spring扫描过程中会认为类上有@Component注解的就是Bean。 第一章 Spring底层核心原理解析Spring串讲,从整体上了解Spring Bean的生命周期底层原理 依赖注入底层原理 初始化底层方法 (推断构造方法底层方法) AOP底层原理 事务底层原理 Spring的使用12345678910@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;)public class SpingBootDemoApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext applicationContext = SpringApplication.run(SpingBootDemoApplication.class, args); //run方法返回一个IOC容器 &#125; Spring中是如何创建一个对象？目前，我们都可以简单的将它们理解为就是用来创建Java对象的，比如调用getBean()就会去创建对象（此处不严谨，getBean可能也不会去创建对象，后续详解 1234567891011@SpringBootApplication(exclude = &#123;DataSourceAutoConfiguration.class&#125;)public class SpingBootDemoApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext ioc = SpringApplication.run(SpingBootDemoApplication.class, args); UserService userService = (UserService) ioc.getBean(&quot;userService&quot;); userService.test(); &#125;&#125; UserService test~~~~ 怎么实现的呢? 在Spring底层会获取类的信息:根据配置类获取扫描路径(SpingBootDemoApplication本身是配置类(@Configuration注解下的类),他的内部内置了SpringBoot本包及其子包的扫描路径) 遍历扫描路径下的所有Java类,如果有@Compoent等注解,Spring就会把这个类记录下来存到BeanDefinitionMap(后续会详解) Spring会根据某个规则生成对应的beanName作为Key存到Map中,当前类定义信息作为Value Bean的创建过程Bean的大概创建生命周期 推断构造函数实例化一个对象 确定构造方法,确定入参的Bean对象 两种情况 1种构造方法和多种构造方法 1种则就选唯一的这一个 多种: 无指定默认找默认空参数构造函数 没有则报错,有指定(@Autowired的构造方法)则选择指定的 选择有参的会根据入参类型和名字去Spring找Bean对象 先根据入参类型找，如果只找到一个，那就直接用来作为入参 如果根据类型找到多个，则再根据入参名字来确定唯一一个 最终如果没有找到，则会报错，无法创建当前Bean对象 依赖注入 之后Aware回调(判断是否实现各种XxxxAware接口,实现接口的各种setXxxx放方法) 初始化前:判断是否存在@PostConstruct方法,存在则调用(初始化前) 初始化:是否实现InitaillizingBean接口,如果实现则要实现其的afterPropertySet()方法 初始化后:最后判断需不需要AOP,如果不需要那么Bean就创建完了,否则进行动态代理生成代理对象 AOP大致流程 如果当前Bean是单例Bean，那么会把该Bean对象存入一个Map&lt;String, Object&gt;，Map的key为beanName，value为Bean对象。这样下次getBean时就可以直接从Map中拿到对应的Bean对象了。（实际上，在Spring源码中，这个Map就是单例池） 如果当前Bean是原型Bean，那么后续没有其他动作，不会存入一个Map，下次getBean时会再次执行上述创建过程，得到一个新的Bean对象。 AOP大致流程AOP就是进行动态代理，在创建一个Bean的过程中，Spring在最后一步会去判断当前正在创建的这个Bean是不是需要进行AOP，如果需要则会进行动态代理。​ 如何判断当前Bean对象需不需要进行AOP: 找出所有的切面Bean 遍历切面中的每个方法，看是否写了@Before、@After等注解 如果写了，则判断所对应的Pointcut是否和当前Bean对象的类是否匹配 如果匹配则表示当前Bean对象有匹配的的Pointcut，表示需要进行AOP 利用cglib进行AOP的大致流程： 生成代理类UserServiceProxy，代理类继承UserService 代理类中重写了父类的方法，比如UserService中的test()方法 代理类中还会有一个target属性，该属性的值为被代理对象（也就是通过UserService类推断构造方法实例化出来的对象，进行了依赖注入、初始化等步骤的对象） 代理类中的test()方法被执行时的逻辑如下： 执行切面逻辑（@Before） 调用target.test() 当我们从Spring容器得到UserService的Bean对象时，拿到的就是UserServiceProxy所生成的对象，也就是代理对象。​ UserService代理对象.test()—&gt;执行切面逻辑—&gt;target.test()，注意target对象不是代理对象，而是被代理对象。 Spring事务当我们在某个方法上加了@Transactional注解后，就表示该方法在调用时会开启Spring事务，而这个方法所在的类所对应的Bean对象会是该类的代理对象。​ Spring事务的代理对象执行某个方法时的步骤： 判断当前执行的方法是否存在@Transactional注解 如果存在，则利用事务管理器（TransactionMananger）新建一个数据库连接 修改数据库连接的autocommit为false 执行target.test()，执行程序员所写的业务逻辑代码，也就是执行sql 执行完了之后如果没有出现异常，则提交，否则回滚 Spring事务是否会失效的判断标准：某个加了@Transactional注解的方法被调用时，要判断到底是不是直接被代理对象调用的，如果是则事务会生效，如果不是则失效。 第二章 Bean生命周期1. 生成BeanDefinition关于Spring启动流程，后续会单独的课详细讲，这里先讲一下Spring扫描的底层实现：​ Spring扫描底层流程：https://www.processon.com/view/link/61370ee60e3e7412ecd95d43​ 首先，通过ResourcePatternResolver获得指定包路径下的所有.class文件（Spring源码中将此文件包装成了Resource对象） 遍历每个Resource对象 利用MetadataReaderFactory解析Resource对象得到MetadataReader（在Spring源码中MetadataReaderFactory具体的实现类为CachingMetadataReaderFactory，MetadataReader的具体实现类为SimpleMetadataReader） 利用MetadataReader进行excludeFilters和includeFilters，以及条件注解@Conditional的筛选（条件注解并不能理解：某个类上是否存在@Conditional注解，如果存在则调用注解中所指定的类的match方法进行匹配，匹配成功则通过筛选，匹配失败则pass掉。） 筛选通过后，基于metadataReader生成ScannedGenericBeanDefinition 再基于metadataReader判断是不是对应的类是不是接口或抽象类 如果筛选通过，那么就表示扫描到了一个Bean，将ScannedGenericBeanDefinition加入结果集 MetadataReader表示类的元数据读取器，主要包含了一个AnnotationMetadata，功能有 获取类的名字、 获取父类的名字 获取所实现的所有接口名 获取所有内部类的名字 判断是不是抽象类 判断是不是接口 判断是不是一个注解 获取拥有某个注解的方法集合 获取类上添加的所有注解信息 获取类上添加的所有注解类型集合 值得注意的是，CachingMetadataReaderFactory解析某个.class文件得到MetadataReader对象是利用的ASM技术，并没有加载这个类到JVM。并且，最终得到的ScannedGenericBeanDefinition对象，beanClass属性存储的是当前类的名字，而不是class对象。（beanClass属性的类型是Object，它即可以存储类的名字，也可以存储class对象）​ 最后，上面是说的通过扫描得到BeanDefinition对象，我们还可以通过直接定义BeanDefinition，或解析spring.xml文件的，或者@Bean注解得到BeanDefinition对象。（后续课程会分析@Bean注解是怎么生成BeanDefinition的）。 2. 合并BeanDefinition通过扫描得到所有BeanDefinition之后，就可以根据BeanDefinition创建Bean对象了，但是在Spring中支持父子BeanDefinition，和Java父子类类似，但是完全不是一回事。 父子BeanDefinition实际用的比较少，使用是这样的，比如： 12&lt;bean id=&quot;parent&quot; class=&quot;com.zhouyu.service.Parent&quot; scope=&quot;prototype&quot;/&gt;&lt;bean id=&quot;child&quot; class=&quot;com.zhouyu.service.Child&quot;/&gt; 这么定义的情况下，child是单例Bean。 12&lt;bean id=&quot;parent&quot; class=&quot;com.zhouyu.service.Parent&quot; scope=&quot;prototype&quot;/&gt;&lt;bean id=&quot;child&quot; class=&quot;com.zhouyu.service.Child&quot; parent=&quot;parent&quot;/&gt; 但是这么定义的情况下，child就是原型Bean了。​ 因为child的父BeanDefinition是parent，所以会继承parent上所定义的scope属性。​ 而在根据child来生成Bean对象之前，需要进行BeanDefinition的合并，得到完整的child的BeanDefinition。​ 3. 加载类BeanDefinition合并之后，就可以去创建Bean对象了，而创建Bean就必须实例化对象，而实例化就必须先加载当前BeanDefinition所对应的class，在AbstractAutowireCapableBeanFactory类的createBean()方法中，一开始就会调用： 1Class&lt;?&gt; resolvedClass = resolveBeanClass(mbd, beanName); 这行代码就是去加载类，该方法是这么实现的： 12345678910111213if (mbd.hasBeanClass()) &#123; return mbd.getBeanClass();&#125;if (System.getSecurityManager() != null) &#123; return AccessController.doPrivileged((PrivilegedExceptionAction&lt;Class&lt;?&gt;&gt;) () -&gt; doResolveBeanClass(mbd, typesToMatch), getAccessControlContext()); &#125;else &#123; return doResolveBeanClass(mbd, typesToMatch);&#125;public boolean hasBeanClass() &#123; return (this.beanClass instanceof Class);&#125; 如果beanClass属性的类型是Class，那么就直接返回，如果不是，则会根据类名进行加载（doResolveBeanClass方法所做的事情） 会利用BeanFactory所设置的类加载器来加载类，如果没有设置，则默认使用**ClassUtils.getDefaultClassLoader()**所返回的类加载器来加载。 ClassUtils.getDefaultClassLoader() 优先返回当前线程中的ClassLoader 线程中类加载器为null的情况下，返回ClassUtils类的类加载器 如果ClassUtils类的类加载器为空，那么则表示是Bootstrap类加载器加载的ClassUtils类，那么则返回系统类加载器 4. 实例化前当前BeanDefinition对应的类成功加载后，就可以实例化对象了，但是…​ 在Spring中，实例化对象之前，Spring提供了一个扩展点，允许用户来控制是否在某个或某些Bean实例化之前做一些启动动作。这个扩展点叫**InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation()**。比如：​ 1234567891011@Componentpublic class ZhouyuBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;实例化前&quot;); &#125; return null; &#125;&#125; 如上代码会导致，在userService这个Bean实例化前，会进行打印。​ 值得注意的是，postProcessBeforeInstantiation()是有返回值的，如果这么实现： 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public Object postProcessBeforeInstantiation(Class&lt;?&gt; beanClass, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;实例化前&quot;); return new UserService(); &#125; return null; &#125;&#125; userService这个Bean，在实例化前会直接返回一个由我们所定义的UserService对象。如果是这样，表示不需要Spring来实例化了，并且后续的Spring依赖注入也不会进行了，会跳过一些步骤，直接执行初始化后这一步。 5. 实例化在这个步骤中就会根据BeanDefinition去创建一个对象了。 5.1 Supplier创建对象首先判断BeanDefinition中是否设置了Supplier，如果设置了则调用Supplier的get()得到对象。​ 得直接使用BeanDefinition对象来设置Supplier，比如： 12345678AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setInstanceSupplier(new Supplier&lt;Object&gt;() &#123; @Override public Object get() &#123; return new UserService(); &#125;&#125;);context.registerBeanDefinition(&quot;userService&quot;, beanDefinition); 5.2 工厂方法创建对象如果没有设置Supplier，则检查BeanDefinition中是否设置了factoryMethod，也就是工厂方法，有两种方式可以设置factoryMethod，比如：​ 方式一： 1&lt;bean id=&quot;userService&quot; class=&quot;com.zhouyu.service.UserService&quot; factory-method=&quot;createUserService&quot; /&gt; 对应的UserService类为： 12345678910111213public class UserService &#123; public static UserService createUserService() &#123; System.out.println(&quot;执行createUserService()&quot;); UserService userService = new UserService(); return userService; &#125; public void test() &#123; System.out.println(&quot;test&quot;); &#125;&#125; 方式二： 12&lt;bean id=&quot;commonService&quot; class=&quot;com.zhouyu.service.CommonService&quot;/&gt;&lt;bean id=&quot;userService1&quot; factory-bean=&quot;commonService&quot; factory-method=&quot;createUserService&quot; /&gt; 对应的CommonService的类为： 123456public class CommonService &#123; public UserService createUserService() &#123; return new UserService(); &#125;&#125; Spring发现当前BeanDefinition方法设置了工厂方法后，就会区分这两种方式，然后调用工厂方法得到对象。​ 值得注意的是，我们通过@Bean所定义的BeanDefinition，是存在factoryMethod和factoryBean的，也就是和上面的方式二非常类似，@Bean所注解的方法就是factoryMethod，AppConfig对象就是factoryBean。如果@Bean所所注解的方法是static的，那么对应的就是方式一。 5.3 推断构造方法第一节已经讲过一遍大概原理了，后面有一节课单独分析源码实现。推断完构造方法后，就会使用构造方法来进行实例化了。​ 额外的，在推断构造方法逻辑中除开会去选择构造方法以及查找入参对象意外，会还判断是否在对应的类中是否存在使用**@Lookup注解了方法。如果存在则把该方法封装为LookupOverride对象并添加到BeanDefinition中。**​ 在实例化时，如果判断出来当前BeanDefinition中没有LookupOverride，那就直接用构造方法反射得到一个实例对象。如果存在LookupOverride对象，也就是类中存在@Lookup注解了的方法，那就会生成一个代理对象。​ @Lookup注解就是方法注入，使用demo如下： 12345678910111213141516@Componentpublic class UserService &#123; private OrderService orderService; public void test() &#123; OrderService orderService = createOrderService(); System.out.println(orderService); &#125; @Lookup(&quot;orderService&quot;) public OrderService createOrderService() &#123; return null; &#125;&#125; 6. BeanDefinition的后置处理Bean对象实例化出来之后，接下来就应该给对象的属性赋值了。在真正给属性赋值之前，Spring又提供了一个扩展点**MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition()**，可以对此时的BeanDefinition进行加工，比如： 12345678910@Componentpublic class ZhouyuMergedBeanDefinitionPostProcessor implements MergedBeanDefinitionPostProcessor &#123; @Override public void postProcessMergedBeanDefinition(RootBeanDefinition beanDefinition, Class&lt;?&gt; beanType, String beanName) &#123; if (&quot;userService&quot;.equals(beanName)) &#123; beanDefinition.getPropertyValues().add(&quot;orderService&quot;, new OrderService()); &#125; &#125;&#125; 在Spring源码中，AutowiredAnnotationBeanPostProcessor就是一个MergedBeanDefinitionPostProcessor，它的postProcessMergedBeanDefinition()中会去查找注入点，并缓存在AutowiredAnnotationBeanPostProcessor对象的一个Map中（injectionMetadataCache）。 7. 实例化后在处理完BeanDefinition后，Spring又设计了一个扩展点：**InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation()**，比如：​ 1234567891011121314@Componentpublic class ZhouyuInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; UserService userService = (UserService) bean; userService.test(); &#125; return true; &#125;&#125; 上述代码就是对userService所实例化出来的对象进行处理。​ 这个扩展点，在Spring源码中基本没有怎么使用。 8. 自动注入这里的自动注入指的是Spring的自动注入，后续依赖注入课程中单独讲​ 9. 处理属性这个步骤中，就会处理@Autowired、@Resource、@Value等注解，也是通过**InstantiationAwareBeanPostProcessor.postProcessProperties()**扩展点来实现的，比如我们甚至可以实现一个自己的自动注入功能，比如： 123456789101112131415161718192021@Componentpublic class ZhouyuInstantiationAwareBeanPostProcessor implements InstantiationAwareBeanPostProcessor &#123; @Override public PropertyValues postProcessProperties(PropertyValues pvs, Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; for (Field field : bean.getClass().getFields()) &#123; if (field.isAnnotationPresent(ZhouyuInject.class)) &#123; field.setAccessible(true); try &#123; field.set(bean, &quot;123&quot;); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; return pvs; &#125;&#125; 关于@Autowired、@Resource、@Value的底层源码，会在后续的依赖注入课程中详解。 10. 执行Aware完成了属性赋值之后，Spring会执行一些回调，包括： BeanNameAware：回传beanName给bean对象。 BeanClassLoaderAware：回传classLoader给bean对象。 BeanFactoryAware：回传beanFactory给对象。 11. 初始化前初始化前，也是Spring提供的一个扩展点：**BeanPostProcessor.postProcessBeforeInitialization()**，比如 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;初始化前&quot;); &#125; return bean; &#125;&#125; 利用初始化前，可以对进行了依赖注入的Bean进行处理。​ 在Spring源码中： InitDestroyAnnotationBeanPostProcessor会在初始化前这个步骤中执行@PostConstruct的方法， ApplicationContextAwareProcessor会在初始化前这个步骤中进行其他Aware的回调： EnvironmentAware：回传环境变量 EmbeddedValueResolverAware：回传占位符解析器 ResourceLoaderAware：回传资源加载器 ApplicationEventPublisherAware：回传事件发布器 MessageSourceAware：回传国际化资源 ApplicationStartupAware：回传应用其他监听对象，可忽略 ApplicationContextAware：回传Spring容器ApplicationContext 12. 初始化 查看当前Bean对象是否实现了InitializingBean接口，如果实现了就调用其afterPropertiesSet()方法 执行BeanDefinition中指定的初始化方法 13. 初始化后这是Bean创建生命周期中的最后一个步骤，也是Spring提供的一个扩展点：**BeanPostProcessor.postProcessAfterInitialization()**，比如： 123456789101112@Componentpublic class ZhouyuBeanPostProcessor implements BeanPostProcessor &#123; @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (&quot;userService&quot;.equals(beanName)) &#123; System.out.println(&quot;初始化后&quot;); &#125; return bean; &#125;&#125; 可以在这个步骤中，对Bean最终进行处理，Spring中的AOP就是基于初始化后实现的，初始化后返回的对象才是最终的Bean对象。 总结BeanPostProcessor InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation() 实例化 MergedBeanDefinitionPostProcessor.postProcessMergedBeanDefinition() InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation() 自动注入 InstantiationAwareBeanPostProcessor.postProcessProperties() Aware对象 BeanPostProcessor.postProcessBeforeInitialization() 初始化 BeanPostProcessor.postProcessAfterInitialization() 第三章 IOC启动前言分析通常，我们说的Spring启动，就是构造ApplicationContext对象以及调用refresh()方法的过程。​ 首先，Spring启动过程主要做了这么几件事情： 构造一个BeanFactory对象 解析配置类，得到BeanDefinition，并注册到BeanFactory中 解析@ComponentScan，此时就会完成扫描 解析@Import 解析@Bean … 因为ApplicationContext还支持国际化，所以还需要初始化MessageSource对象 因为ApplicationContext还支持事件机制，所以还需要初始化ApplicationEventMulticaster对象 把用户定义的ApplicationListener对象添加到ApplicationContext中，等Spring启动完了就要发布事件了 创建非懒加载的单例Bean对象，并存在BeanFactory的单例池中。 调用Lifecycle Bean的start()方法 发布ContextRefreshedEvent事件 由于Spring启动过程中要创建非懒加载的单例Bean对象，那么就需要用到BeanPostProcessor，所以Spring在启动过程中就需要做两件事： 生成默认的BeanPostProcessor对象，并添加到BeanFactory中 AutowiredAnnotationBeanPostProcessor：处理@Autowired、@Value CommonAnnotationBeanPostProcessor：处理@Resource、@PostConstruct、@PreDestroy ApplicationContextAwareProcessor：处理ApplicationContextAware等回调 找到外部用户所定义的BeanPostProcessor对象（类型为BeanPostProcessor的Bean对象），并添加到BeanFactory中 Spring自带的BeanPostProcessor类 BeanPostProcessor InstantiationAwareBeanPostProcessor MergeBeanDefinitionAwareBeanPostProcessor SmartInstantiationAwareBeanPostProcessor spring的启动流程this() 首先要创建一个BeanFactory比较常见的是AnnotationConfigApplicationContext(隐藏会组合一个defaultListableBeanfactory对象来作为一个存储各种信息的工厂) 先创建reader读取各种bean定义信息,后续为了加载底层功能的后置处理器 注册一个注解配置处理器,给工厂准备些解析器等基础组件(传入的是一个自带的rootBeanDefinition)注册到default 注册核心组件() 给工厂准备些基础组件(各种解析器),给工厂注册核心组件 AnnotatitionConfigurationClassBeanfactory(BeanFactoryPostProcessor)处理配置类 AutowiredAnnotationBeanPostProcessor(SmartInstantiationAwareBeanPostProcessor) CommonAnnotationBeanPostProcessor(普通JSR250支持@PostContruct @PreDestroy @Resource) JPA支持后置处理器 EventListenerMethorPostProcessor(事件功能(事件方法)的后置处理器) DefaultListenerFactory(事件工厂) 创建一个scanner准备一些环境变量 register(MainConfig) reader来注册所有配置类, 完善主配置类的定义信息(解析@Lazy @ Primary @DependsOn @ Description) 把主配置注册进去 refresh() 准备上下文环境 获取this()准备好的工厂 预准备工厂 给工厂设置必要的工具比如:el表达式解析器,资源解析器,基本的后置处理器(ApplicationContextAwareProcessor判断当前组件是否实现了xxxaware接口) 还注册了一些单实例Bean: 系统属性,系统环境 后置处理BeanFactory(空方法) 执行工厂的后置增强 ConfigurationClassPostProcessor解析配置类后置处理器在此工作 通过一个代理PostProecessorRegistrationDelegate管理所有后置处理器的执行 在工厂左右定义信息中获取配置类的信息使用parser进行配置类解析 处理@Lazy,@ComponentScan,@PropertySource@Import等注解将所有的Bean定义信息全部准备好 (利用反射把BeanClass的所有元数据准备好放入BeanDefinitionMap中) 注册Bean的后置处理器 for创建所有后置处理器对象 工厂提前保存所有处理器,方便后面创建Bean使用 初始化国际化组件 初始化事件多播器组件 看容器是否有(用户自己定义的) 没有就注册个默认的 放入单例池 OnRefresh()留给子类继续增强处理逻辑 注册监听器 获取容器中定义的所有ApplicationListener并保存起来 完成工厂初始化 遍历所有BeanName创建Bean对象 详情参照Bean初始化 finishRefresh() 最后的一些清理,时间发送等 Bean初始化+生命周期 todo循环引用 Bean的销毁过程Bean销毁是发送在Spring容器关闭过程中的。​ 在Spring容器关闭时，比如： 123456AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);UserService userService = (UserService) context.getBean(&quot;userService&quot;);userService.test();// 容器关闭context.close(); 在Bean创建过程中，在最后（初始化之后），有一个步骤会去判断当前创建的Bean是不是DisposableBean： 当前Bean是否实现了DisposableBean接口 或者，当前Bean是否实现了AutoCloseable接口 BeanDefinition中是否指定了destroyMethod 调用DestructionAwareBeanPostProcessor.requiresDestruction(bean)进行判断 ApplicationListenerDetector中直接使得ApplicationListener是DisposableBean InitDestroyAnnotationBeanPostProcessor中使得拥有@PreDestroy注解了的方法就是DisposableBean 把符合上述任意一个条件的Bean适配成DisposableBeanAdapter对象，并存入disposableBeans中（一个LinkedHashMap） 在Spring容器关闭过程时： 首先发布ContextClosedEvent事件 调用lifecycleProcessor的onCloese()方法 销毁单例Bean 遍历disposableBeans 把每个disposableBean从单例池中移除 调用disposableBean的destroy() 如果这个disposableBean还被其他Bean依赖了，那么也得销毁其他Bean 如果这个disposableBean还包含了inner beans，将这些Bean从单例池中移除掉 (inner bean参考https://docs.spring.io/spring-framework/docs/current/spring-framework-reference/core.html#beans-inner-beans) 清空manualSingletonNames，是一个Set，存的是用户手动注册的单例Bean的beanName 清空allBeanNamesByType，是一个Map，key是bean类型，value是该类型所有的beanName数组 清空singletonBeanNamesByType，和allBeanNamesByType类似，只不过只存了单例Bean 这里涉及到一个设计模式：适配器模式 在销毁时，Spring会找出实现了DisposableBean接口的Bean。​ 但是我们在定义一个Bean时，如果这个Bean实现了DisposableBean接口，或者实现了AutoCloseable接口，或者在BeanDefinition中指定了destroyMethodName，那么这个Bean都属于“DisposableBean”，这些Bean在容器关闭时都要调用相应的销毁方法。 所以，这里就需要进行适配，将实现了DisposableBean接口、或者AutoCloseable接口等适配成实现了DisposableBean接口，所以就用到了DisposableBeanAdapter。 会把实现了AutoCloseable接口的类封装成DisposableBeanAdapter，而DisposableBeanAdapter实现了DisposableBean接口。 BeanFactoryPostProcessorBeanPostProcessor表示Bean的后置处理器，是用来对Bean进行加工的，类似的，BeanFactoryPostProcessor理解为BeanFactory的后置处理器，用来用对BeanFactory进行加工的。​ Spring支持用户定义BeanFactoryPostProcessor的实现类Bean，来对BeanFactory进行加工，比如： 123456789@Componentpublic class ZhouyuBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition beanDefinition = beanFactory.getBeanDefinition(&quot;userService&quot;); beanDefinition.setAutowireCandidate(false); &#125;&#125; 以上代码，就利用了BeanFactoryPostProcessor来拿到BeanFactory，然后获取BeanFactory内的某个BeanDefinition对象并进行修改，注意这一步是发生在Spring启动时，创建单例Bean之前的，所以此时对BeanDefinition就行修改是会生效的。​ 注意：在ApplicationContext内部有一个核心的DefaultListableBeanFactory，它实现了ConfigurableListableBeanFactory和BeanDefinitionRegistry接口，所以ApplicationContext和DefaultListableBeanFactory是可以注册BeanDefinition的，但是ConfigurableListableBeanFactory是不能注册BeanDefinition的，只能获取BeanDefinition，然后做修改。 所以Spring还提供了一个BeanFactoryPostProcessor的子接口：BeanDefinitionRegistryPostProcessor​ BeanDefinitionRegistryPostProcessor12345public interface BeanDefinitionRegistryPostProcessor extends BeanFactoryPostProcessor &#123; void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException;&#125; 我们可以看到BeanDefinitionRegistryPostProcessor继承了BeanFactoryPostProcessor接口，并新增了一个方法，注意方法的参数为BeanDefinitionRegistry，所以如果我们提供一个类来实现BeanDefinitionRegistryPostProcessor，那么在postProcessBeanDefinitionRegistry()方法中就可以注册BeanDefinition了。比如：​ 12345678910111213141516@Componentpublic class ZhouyuBeanDefinitionRegistryPostProcessor implements BeanDefinitionRegistryPostProcessor &#123; @Override public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry) throws BeansException &#123; AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition(); beanDefinition.setBeanClass(User.class); registry.registerBeanDefinition(&quot;user&quot;, beanDefinition); &#125; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; BeanDefinition beanDefinition = beanFactory.getBeanDefinition(&quot;userService&quot;); beanDefinition.setAutowireCandidate(false); &#125;&#125; 如何理解refresh()？1234567891011/** * Load or refresh the persistent representation of the configuration, * which might an XML file, properties file, or relational database schema. * &lt;p&gt;As this is a startup method, it should destroy already created singletons * if it fails, to avoid dangling resources. In other words, after invocation * of that method, either all or no singletons at all should be instantiated. * @throws BeansException if the bean factory could not be initialized * @throws IllegalStateException if already initialized and multiple refresh * attempts are not supported */ void refresh() throws BeansException, IllegalStateException; 这是ConfigurableApplicationContext接口上refresh()方法的注释，意思是：加载或刷新持久化的配置，可能是XML文件、属性文件或关系数据库中存储的。由于这是一个启动方法，如果失败，它应该销毁已经创建的单例，以避免暂用资源。换句话说，在调用该方法之后，应该实例化所有的单例，或者根本不实例化单例 。 有个理念需要注意：ApplicationContext关闭之后不代表JVM也关闭了，ApplicationContext是属于JVM的，说白了ApplicationContext也是JVM中的一个对象。​ 在Spring的设计中，也提供可以刷新的ApplicationContext和不可以刷新的ApplicationContext。比如： 1AbstractRefreshableApplicationContext extends AbstractApplicationContext 就是可以刷新的 1GenericApplicationContext extends AbstractApplicationContext 就是不可以刷新的。 AnnotationConfigApplicationContext继承的是GenericApplicationContext，所以它是不能刷新的。AnnotationConfigWebApplicationContext继承的是AbstractRefreshableWebApplicationContext，所以它是可以刷的。 上面说的不能刷新是指不能重复刷新，只能调用一次refresh方法，第二次时会报错。 refresh()底层原理流程底层原理流程图：https://www.processon.com/view/link/5f60a7d71e08531edf26a919 下面以AnnotationConfigApplicationContext为例子，来介绍refresh的底层原理。 在调用AnnotationConfigApplicationContext的构造方法之前，会调用父类GenericApplicationContext的无参构造方法，会构造一个BeanFactory，为DefaultListableBeanFactory。 构造AnnotatedBeanDefinitionReader（ 主要作用添加一些基础的PostProcessor，同时可以通过reader进行BeanDefinition的注册 ），同时对BeanFactory进行设置和添加 PostProcessor （后置处理器） 设置dependencyComparator：AnnotationAwareOrderComparator，它是一个Comparator，是用来进行排序的，会获取某个对象上的Order注解或者通过实现Ordered接口所定义的值进行排序，在日常开发中可以利用这个类来进行排序。 设置autowireCandidateResolver：ContextAnnotationAutowireCandidateResolver，用来解析某个Bean能不能进行自动注入，比如某个Bean的autowireCandidate属性是否等于true 向BeanFactory中添加ConfigurationClassPostProcessor对应的BeanDefinition，它是一个BeanDefinitionRegistryPostProcessor，并且实现了PriorityOrdered接口 向BeanFactory中添加AutowiredAnnotationBeanPostProcessor对应的BeanDefinition，它是一个InstantiationAwareBeanPostProcessorAdapter，MergedBeanDefinitionPostProcessor 向BeanFactory中添加CommonAnnotationBeanPostProcessor对应的BeanDefinition，它是一个InstantiationAwareBeanPostProcessor，InitDestroyAnnotationBeanPostProcessor 向BeanFactory中添加EventListenerMethodProcessor对应的BeanDefinition，它是一个BeanFactoryPostProcessor，SmartInitializingSingleton 向BeanFactory中添加DefaultEventListenerFactory对应的BeanDefinition，它是一个EventListenerFactory 构造ClassPathBeanDefinitionScanner（ 主要作用可以用来扫描得到并注册BeanDefinition ），同时进行设置： 设置this.includeFilters &#x3D; AnnotationTypeFilter(Component.class) 设置environment 设置resourceLoader 利用reader注册AppConfig为BeanDefinition，类型为AnnotatedGenericBeanDefinition 接下来就是调用refresh方法 prepareRefresh()： 记录启动时间 可以允许子容器设置一些内容到Environment中 验证Environment中是否包括了必须要有的属性 obtainFreshBeanFactory()：进行BeanFactory的refresh，在这里会去调用子类的refreshBeanFactory方法，具体子类是怎么刷新的得看子类，然后再调用子类的getBeanFactory方法，重新得到一个BeanFactory prepareBeanFactory(beanFactory)： 设置beanFactory的类加载器 设置表达式解析器：StandardBeanExpressionResolver，用来解析Spring中的表达式 添加PropertyEditorRegistrar：ResourceEditorRegistrar，PropertyEditor类型转化器注册器，用来注册一些默认的PropertyEditor 添加一个Bean的后置处理器：ApplicationContextAwareProcessor，是一个BeanPostProcessor，用来执行EnvironmentAware、ApplicationEventPublisherAware等回调方法 添加 ignoredDependencyInterface ：可以向这个属性中添加一些接口，如果某个类实现了这个接口，并且这个类中的某些set方法在接口中也存在，那么这个set方法在自动注入的时候是不会执行的，比如EnvironmentAware这个接口，如果某个类实现了这个接口，那么就必须实现它的setEnvironment方法，而这是一个set方法，和Spring中的autowire是冲突的，那么Spring在自动注入时是不会调用setEnvironment方法的，而是等到回调Aware接口时再来调用（注意，这个功能仅限于xml的autowire，@Autowired注解是忽略这个属性的） EnvironmentAware EmbeddedValueResolverAware ResourceLoaderAware ApplicationEventPublisherAware MessageSourceAware ApplicationContextAware 另外其实在构造BeanFactory的时候就已经提前添加了另外三个： BeanNameAware BeanClassLoaderAware BeanFactoryAware 添加 resolvableDependencies ：在byType进行依赖注入时，会先从这个属性中根据类型找bean BeanFactory.class：当前BeanFactory对象 ResourceLoader.class：当前ApplicationContext对象 ApplicationEventPublisher.class：当前ApplicationContext对象 ApplicationContext.class：当前ApplicationContext对象 添加一个Bean的后置处理器：ApplicationListenerDetector，是一个BeanPostProcessor，用来判断某个Bean是不是ApplicationListener，如果是则把这个Bean添加到ApplicationContext中去，注意一个ApplicationListener只能是单例的 添加一个Bean的后置处理器：LoadTimeWeaverAwareProcessor，是一个BeanPostProcessor，用来判断某个Bean是不是实现了LoadTimeWeaverAware接口，如果实现了则把ApplicationContext中的loadTimeWeaver回调setLoadTimeWeaver方法设置给该Bean。 添加一些单例bean到单例池： “environment”：Environment对象 “systemProperties”：System.getProperties()返回的Map对象 “systemEnvironment”：System.getenv()返回的Map对象 postProcessBeanFactory(beanFactory) ： 提供给AbstractApplicationContext的子类进行扩展，具体的子类，可以继续向BeanFactory中再添加一些东西 invokeBeanFactoryPostProcessors(beanFactory)： 执行BeanFactoryPostProcessor 此时在BeanFactory中会存在一个BeanFactoryPostProcessor：ConfigurationClassPostProcessor，它也是一个BeanDefinitionRegistryPostProcessor 第一阶段 从BeanFactory中找到类型为BeanDefinitionRegistryPostProcessor的beanName，也就是ConfigurationClassPostProcessor， 然后调用BeanFactory的getBean方法得到实例对象 执行**ConfigurationClassPostProcessor的postProcessBeanDefinitionRegistry()**方法: 解析AppConfig类 扫描得到BeanDefinition并注册 解析@Import，@Bean等注解得到BeanDefinition并注册 详细的看另外的笔记，专门分析了ConfigurationClassPostProcessor是如何工作的 在这里，我们只需要知道在这一步会去得到BeanDefinition，而这些BeanDefinition中可能存在BeanFactoryPostProcessor和BeanDefinitionRegistryPostProcessor，所以执行完ConfigurationClassPostProcessor的postProcessBeanDefinitionRegistry()方法后，还需要继续执行其他BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行其他BeanDefinitionRegistryPostProcessor的**postProcessBeanDefinitionRegistry()**方法 执行所有BeanDefinitionRegistryPostProcessor的**postProcessBeanFactory()**方法 第二阶段 从BeanFactory中找到类型为BeanFactoryPostProcessor的beanName，而这些BeanFactoryPostProcessor包括了上面的BeanDefinitionRegistryPostProcessor 执行还没有执行过的BeanFactoryPostProcessor的**postProcessBeanFactory()**方法 到此，所有的BeanFactoryPostProcessor的逻辑都执行完了，主要做的事情就是得到BeanDefinition并注册到BeanFactory中 registerBeanPostProcessors(beanFactory)：因为上面的步骤完成了扫描，这个过程中程序员可能自己定义了一些BeanPostProcessor，在这一步就会把BeanFactory中所有的BeanPostProcessor找出来并实例化得到一个对象，并添加到BeanFactory中去（属性beanPostProcessors），最后再重新添加一个ApplicationListenerDetector对象（之前其实就添加了过，这里是为了把ApplicationListenerDetector移动到最后） initMessageSource()：如果BeanFactory中存在一个叫做”messageSource“的BeanDefinition，那么就会把这个Bean对象创建出来并赋值给ApplicationContext的messageSource属性，让ApplicationContext拥有国际化的功能 initApplicationEventMulticaster()：如果BeanFactory中存在一个叫做”applicationEventMulticaster“的BeanDefinition，那么就会把这个Bean对象创建出来并赋值给ApplicationContext的applicationEventMulticaster属性，让ApplicationContext拥有事件发布的功能 onRefresh()：提供给AbstractApplicationContext的子类进行扩展，没用 registerListeners()：从BeanFactory中获取ApplicationListener类型的beanName，然后添加到ApplicationContext中的事件广播器applicationEventMulticaster中去，到这一步因为FactoryBean还没有调用getObject()方法生成Bean对象，所以这里要在根据类型找一下ApplicationListener，记录一下对应的beanName finishBeanFactoryInitialization(beanFactory)：完成BeanFactory的初始化，主要就是实例化非懒加载的单例Bean，单独的笔记去讲。 finishRefresh()：BeanFactory的初始化完后，就到了Spring启动的最后一步了 设置ApplicationContext的lifecycleProcessor，默认情况下设置的是DefaultLifecycleProcessor 调用lifecycleProcessor的onRefresh()方法，如果是DefaultLifecycleProcessor，那么会获取所有类型为Lifecycle的Bean对象，然后调用它的start()方法，这就是ApplicationContext的生命周期扩展机制 发布ContextRefreshedEvent事件 执行BeanFactoryPostProcessor 执行通过ApplicationContext添加进来的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中实现了PriorityOrdered接口的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中实现了Ordered接口的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行BeanFactory中其他的BeanDefinitionRegistryPostProcessor的postProcessBeanDefinitionRegistry()方法 执行上面所有的BeanDefinitionRegistryPostProcessor的postProcessBeanFactory()方法 执行通过ApplicationContext添加进来的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中实现了PriorityOrdered接口的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中实现了Ordered接口的BeanFactoryPostProcessor的postProcessBeanFactory()方法 执行BeanFactory中其他的BeanFactoryPostProcessor的postProcessBeanFactory()方法 Lifecycle的使用Lifecycle表示的是ApplicationContext的生命周期，可以定义一个SmartLifecycle来监听ApplicationContext的启动和关闭： 1234567891011121314151617181920212223@Componentpublic class ZhouyuLifecycle implements SmartLifecycle &#123; private boolean isRunning = false; @Override public void start() &#123; System.out.println(&quot;启动&quot;); isRunning = true; &#125; @Override public void stop() &#123; // 要触发stop()，要调用context.close()，或者注册关闭钩子（context.registerShutdownHook();） System.out.println(&quot;停止&quot;); isRunning = false; &#125; @Override public boolean isRunning() &#123; return isRunning; &#125;&#125; 解析配置类解析配置类流程图：https://www.processon.com/view/link/5f9512d5e401fd06fda0b2dd解析配置类思维脑图：https://www.processon.com/view/link/614c83cae0b34d7b342f6d14 在启动Spring时，需要传入一个AppConfig.class给ApplicationContext，ApplicationContext会根据AppConfig类封装为一个BeanDefinition，这种BeanDefinition我们把它称为配置类BeanDefinition。 ConfigurationClassPostProcessor中会把配置类BeanDefinition取出来 构造一个ConfigurationClassParser用来解析配置类BeanDefinition，并且会生成一个配置类对象ConfigurationClass 如果配置类上存在@Component注解，那么解析配置类中的内部类（这里有递归，如果内部类也是配置类的话） 如果配置类上存在@PropertySource注解，那么则解析该注解，并得到PropertySource对象，并添加到environment中去 如果配置类上存在@ComponentScan注解，那么则解析该注解，进行扫描，扫描得到一系列的BeanDefinition对象，然后判断这些BeanDefinition是不是也是配置类BeanDefinition（只要存在@Component注解就是配置类，所以基本上扫描出来的都是配置类），如果是则继续解析该配置类，（也有递归），并且会生成对应的ConfigurationClass 如果配置类上存在@Import注解，那么则判断Import的类的类型： 如果是ImportSelector，那么调用执行selectImports方法得到类名，然后在把这个类当做配置类进行解析（也是递归） 如果是ImportBeanDefinitionRegistrar，那么则生成一个ImportBeanDefinitionRegistrar实例对象，并添加到配置类对象中（ConfigurationClass）的importBeanDefinitionRegistrars属性中。 如果配置类上存在@ImportResource注解，那么则把导入进来的资源路径存在配置类对象中的importedResources属性中。 如果配置类中存在@Bean的方法，那么则把这些方法封装为BeanMethod对象，并添加到配置类对象中的beanMethods属性中。 如果配置类实现了某些接口，则看这些接口内是否定义了@Bean的默认方法 如果配置类有父类，则把父类当做配置类进行解析 AppConfig这个配置类会对应一个ConfigurationClass，同时在解析的过程中也会生成另外的一些ConfigurationClass，接下来就利用reader来进一步解析ConfigurationClass 如果ConfigurationClass是通过@Import注解导入进来的，则把这个类生成一个BeanDefinition，同时解析这个类上@Scope,@Lazy等注解信息，并注册BeanDefinition 如果ConfigurationClass中存在一些BeanMethod，也就是定义了一些@Bean，那么则解析这些@Bean，并生成对应的BeanDefinition，并注册 如果ConfigurationClass中导入了一些资源文件，比如xx.xml，那么则解析这些xx.xml文件，得到并注册BeanDefinition 如果ConfigurationClass中导入了一些ImportBeanDefinitionRegistrar，那么则执行对应的registerBeanDefinitions进行BeanDefinition的注册 总结一下 解析AppConfig类，生成对应的ConfigurationClass 再扫描，扫描到的类都会生成对应的BeanDefinition，并且同时这些类也是ConfigurationClass 再解析ConfigurationClass的其他信息，比如@ImportResource注解的处理，@Import注解的处理，@Bean注解的处理 第四章 依赖注入Spring中到底有几种依赖注入的方式？首先分两种： 手动注入 自动注入 手动注入在XML中定义Bean时，就是手动注入，因为是程序员手动给某个属性指定了值。 123&lt;bean name=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot;&gt; &lt;property name=&quot;orderService&quot; ref=&quot;orderService&quot;/&gt;&lt;/bean&gt; 上面这种底层是通过set方法进行注入。 123&lt;bean name=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot;&gt; &lt;constructor-arg index=&quot;0&quot; ref=&quot;orderService&quot;/&gt;&lt;/bean&gt; 上面这种底层是通过构造方法进行注入。 所以手动注入的底层也就是分为两种： set方法注入 构造方法注入 自动注入自动注入又分为两种： XML的autowire自动注入 @Autowired注解的自动注入 XML的autowire自动注入在XML中，我们可以在定义一个Bean时去指定这个Bean的自动注入模式： byType byName constructor default no 比如： 1&lt;bean id=&quot;userService&quot; class=&quot;com.luban.service.UserService&quot; autowire=&quot;byType&quot;/&gt; 这么写，表示Spring会自动的给userService中所有的属性自动赋值（不需要这个属性上有@Autowired注解，但需要这个属性有对应的set方法）。 在创建Bean的过程中，在填充属性时，Spring会去解析当前类，把当前类的所有方法都解析出来，Spring会去解析每个方法得到对应的PropertyDescriptor对象，PropertyDescriptor中有几个属性： name：这个name并不是方法的名字，而是拿方法名字进过处理后的名字 如果方法名字以“get”开头，比如“getXXX”,那么name&#x3D;XXX 如果方法名字以“is”开头，比如“isXXX”,那么name&#x3D;XXX 如果方法名字以“set”开头，比如“setXXX”,那么name&#x3D;XXX readMethodRef：表示get方法的Method对象的引用 readMethodName：表示get方法的名字 writeMethodRef：表示set方法的Method对象的引用 writeMethodName：表示set方法的名字 propertyTypeRef：如果有get方法那么对应的就是返回值的类型，如果是set方法那么对应的就是set方法中唯一参数的类型 get方法的定义是： 方法参数个数为0个，并且 （方法名字以”get”开头 或者 方法名字以”is”开头并且方法的返回类型为boolean） set方法的定义是：方法参数个数为1个，并且 （方法名字以”set”开头并且方法返回类型为void） 所以，Spring在通过byName的自动填充属性时流程是： 找到所有set方法所对应的XXX部分的名字 根据XXX部分的名字去获取bean Spring在通过byType的自动填充属性时流程是： 获取到set方法中的唯一参数的参数类型，并且根据该类型去容器中获取bean 如果找到多个，会报错。 以上，分析了autowire的byType和byName情况，那么接下来分析constructor，constructor表示通过构造方法注入，其实这种情况就比较简单了，没有byType和byName那么复杂。​ 如果是constructor，那么就可以不写set方法了，当某个bean是通过构造方法来注入时，spring利用构造方法的参数信息从Spring容器中去找bean，找到bean之后作为参数传给构造方法，从而实例化得到一个bean对象，并完成属性赋值（属性赋值的代码得程序员来写）。 我们这里先不考虑一个类有多个构造方法的情况，后面单独讲推断构造方法。我们这里只考虑只有一个有参构造方法。 其实构造方法注入相当于byType+byName，普通的byType是根据set方法中的参数类型去找bean，找到多个会报错，而constructor就是通过构造方法中的参数类型去找bean，如果找到多个会根据参数名确定。 另外两个： no，表示关闭autowire default，表示默认值，我们一直演示的某个bean的autowire，而也可以直接在标签中设置autowire，如果设置了，那么标签中设置的autowire如果为default，那么则会用标签中设置的autowire。 可以发现XML中的自动注入是挺强大的，那么问题来了，为什么我们平时都是用的@Autowired注解呢？而没有用上文说的这种自动注入方式呢？ @Autowired注解相当于XML中的autowire属性的注解方式的替代。这是在官网上有提到的。 1Essentially, the @Autowired annotation provides the same capabilities as described in Autowiring Collaborators but with more fine-grained control and wider applicability 翻译一下：从本质上讲，@Autowired注解提供了与autowire相同的功能，但是拥有更细粒度的控制和更广泛的适用性。 注意：更细粒度的控制。 XML中的autowire控制的是整个bean的所有属性，而@Autowired注解是直接写在某个属性、某个set方法、某个构造方法上的。 再举个例子，如果一个类有多个构造方法，那么如果用XML的autowire&#x3D;constructor，你无法控制到底用哪个构造方法，而你可以用@Autowired注解来直接指定你想用哪个构造方法。 同时，用@Autowired注解，还可以控制，哪些属性想被自动注入，哪些属性不想，这也是细粒度的控制。 但是@Autowired无法区分byType和byName，@Autowired是先byType，如果找到多个则byName。 那么XML的自动注入底层其实也就是: set方法注入 构造方法注入 @Autowired注解的自动注入上文说了@Autowired注解，是byType和byName的结合。 @Autowired注解可以写在： 属性上：先根据属性类型去找Bean，如果找到多个再根据属性名确定一个 构造方法上：先根据方法参数类型去找Bean，如果找到多个再根据参数名确定一个 set方法上：先根据方法参数类型去找Bean，如果找到多个再根据参数名确定一个 而这种底层到了： 属性注入 set方法注入 构造方法注入 寻找注入点在创建一个Bean的过程中，Spring会利用AutowiredAnnotationBeanPostProcessor的**postProcessMergedBeanDefinition()**找出注入点并缓存，找注入点的流程为： 遍历当前类的所有的属性字段Field 查看字段上是否存在@Autowired、@Value、@Inject中的其中任意一个，存在则认为该字段是一个注入点 如果字段是static的，则不进行注入 怎么判断 获取@Autowired中的required属性的值 将字段信息构造成一个AutowiredFieldElement对象，作为一个注入点对象添加到currElements集合中。 遍历当前类的所有方法Method 判断当前Method是否是桥接方法，如果是找到原方法 查看方法上是否存在@Autowired、@Value、@Inject中的其中任意一个，存在则认为该方法是一个注入点 如果方法是static的，则不进行注入 获取@Autowired中的required属性的值 将方法信息构造成一个AutowiredMethodElement对象，作为一个注入点对象添加到currElements集合中。 遍历完当前类的字段和方法后，将遍历父类的，直到没有父类。 最后将currElements集合封装成一个InjectionMetadata对象，作为当前Bean对于的注入点集合对象，并缓存。 static的字段或方法为什么不支持123456@Component@Scope(&quot;prototype&quot;)public class OrderService &#123;&#125; 123456789101112@Component@Scope(&quot;prototype&quot;)public class UserService &#123; @Autowired private static OrderService orderService; public void test() &#123; System.out.println(&quot;test123&quot;); &#125;&#125; 看上面代码，UserService和OrderService都是原型Bean，假设Spring支持static字段进行自动注入，那么现在调用两次 UserService userService1 &#x3D; context.getBean(“userService”) UserService userService2 &#x3D; context.getBean(“userService”) 问此时，userService1的orderService值是什么？还是它自己注入的值吗？​ 答案是不是，一旦userService2 创建好了之后，static orderService字段的值就发生了修改了，从而出现bug。 桥接方法12345678910111213141516171819public interface UserInterface&lt;T&gt; &#123; void setOrderService(T t);&#125;@Componentpublic class UserService implements UserInterface&lt;OrderService&gt; &#123; private OrderService orderService; @Override @Autowired public void setOrderService(OrderService orderService) &#123; this.orderService = orderService; &#125; public void test() &#123; System.out.println(&quot;test123&quot;); &#125;&#125; UserService对应的字节码为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// class version 52.0 (52)// access flags 0x21// signature Ljava/lang/Object;Lcom/zhouyu/service/UserInterface&lt;Lcom/zhouyu/service/OrderService;&gt;;// declaration: com/zhouyu/service/UserService implements com.zhouyu.service.UserInterface&lt;com.zhouyu.service.OrderService&gt;public class com/zhouyu/service/UserService implements com/zhouyu/service/UserInterface &#123; // compiled from: UserService.java @Lorg/springframework/stereotype/Component;() // access flags 0x2 private Lcom/zhouyu/service/OrderService; orderService // access flags 0x1 public &lt;init&gt;()V L0 LINENUMBER 12 L0 ALOAD 0 INVOKESPECIAL java/lang/Object.&lt;init&gt; ()V RETURN L1 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L1 0 MAXSTACK = 1 MAXLOCALS = 1 // access flags 0x1 public setOrderService(Lcom/zhouyu/service/OrderService;)V @Lorg/springframework/beans/factory/annotation/Autowired;() L0 LINENUMBER 19 L0 ALOAD 0 ALOAD 1 PUTFIELD com/zhouyu/service/UserService.orderService : Lcom/zhouyu/service/OrderService; L1 LINENUMBER 20 L1 RETURN L2 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L2 0 LOCALVARIABLE orderService Lcom/zhouyu/service/OrderService; L0 L2 1 MAXSTACK = 2 MAXLOCALS = 2 // access flags 0x1 public test()V L0 LINENUMBER 23 L0 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; LDC &quot;test123&quot; INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L1 LINENUMBER 24 L1 RETURN L2 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L2 0 MAXSTACK = 2 MAXLOCALS = 1 // access flags 0x1041 public synthetic bridge setOrderService(Ljava/lang/Object;)V @Lorg/springframework/beans/factory/annotation/Autowired;() L0 LINENUMBER 11 L0 ALOAD 0 ALOAD 1 CHECKCAST com/zhouyu/service/OrderService INVOKEVIRTUAL com/zhouyu/service/UserService.setOrderService (Lcom/zhouyu/service/OrderService;)V RETURN L1 LOCALVARIABLE this Lcom/zhouyu/service/UserService; L0 L1 0 MAXSTACK = 2 MAXLOCALS = 2&#125; 可以看到在UserSerivce的字节码中有两个setOrderService方法： public setOrderService(Lcom&#x2F;zhouyu&#x2F;service&#x2F;OrderService;)V public synthetic bridge setOrderService(Ljava&#x2F;lang&#x2F;Object;)V 并且都是存在@Autowired注解的。​ 所以在Spring中需要处理这种情况，当遍历到桥接方法时，得找到原方法。 注入点进行注入Spring在AutowiredAnnotationBeanPostProcessor的**postProcessProperties()**方法中，会遍历所找到的注入点依次进行注入。​ 字段注入 遍历所有的AutowiredFieldElement对象。 将对应的字段封装为DependencyDescriptor对象。 调用BeanFactory的resolveDependency()方法，传入DependencyDescriptor对象，进行依赖查找，找到当前字段所匹配的Bean对象。 将DependencyDescriptor对象和所找到的结果对象beanName封装成一个ShortcutDependencyDescriptor对象作为缓存，比如如果当前Bean是原型Bean，那么下次再来创建该Bean时，就可以直接拿缓存的结果对象beanName去BeanFactory中去那bean对象了，不用再次进行查找了 利用反射将结果对象赋值给字段。 Set方法注入 遍历所有的AutowiredMethodElement对象 遍历将对应的方法的参数，将每个参数封装成MethodParameter对象 将MethodParameter对象封装为DependencyDescriptor对象 调用BeanFactory的resolveDependency()方法，传入DependencyDescriptor对象，进行依赖查找，找到当前方法参数所匹配的Bean对象。 将DependencyDescriptor对象和所找到的结果对象beanName封装成一个ShortcutDependencyDescriptor对象作为缓存，比如如果当前Bean是原型Bean，那么下次再来创建该Bean时，就可以直接拿缓存的结果对象beanName去BeanFactory中去那bean对象了，不用再次进行查找了 利用反射将找到的所有结果对象传给当前方法，并执行。 上节课我们讲了Spring中的自动注入(byName,byType)和@Autowired注解的工作原理以及源码分析，那么今天这节课，我们来分析还没讲完的，剩下的核心的方法： 123@NullableObject resolveDependency(DependencyDescriptor descriptor, @Nullable String requestingBeanName, @Nullable Set&lt;String&gt; autowiredBeanNames, @Nullable TypeConverter typeConverter) throws BeansException; 该方法表示，传入一个依赖描述（DependencyDescriptor），该方法会根据该依赖描述从BeanFactory中找出对应的唯一的一个Bean对象。 下面来分析一下DefaultListableBeanFactory中resolveDependency()方法的具体实现，具体流程图：https://www.processon.com/view/link/5f8d3c895653bb06ef076688 findAutowireCandidates()实现根据类型找beanName的底层流程：https://www.processon.com/view/link/6135bb430e3e7412ecd5d1f2对应执行流程图为：https://www.processon.com/view/link/5f8fdfa8e401fd06fd984f20​ 找出BeanFactory中类型为type的所有的Bean的名字，注意是名字，而不是Bean对象，因为我们可以根据BeanDefinition就能判断和当前type是不是匹配，不用生成Bean对象 把resolvableDependencies中key为type的对象找出来并添加到result中 遍历根据type找出的beanName，判断当前beanName对应的Bean是不是能够被自动注入 先判断beanName对应的BeanDefinition中的autowireCandidate属性，如果为false，表示不能用来进行自动注入，如果为true则继续进行判断 判断当前type是不是泛型，如果是泛型是会把容器中所有的beanName找出来的，如果是这种情况，那么在这一步中就要获取到泛型的真正类型，然后进行匹配，如果当前beanName和当前泛型对应的真实类型匹配，那么则继续判断 如果当前DependencyDescriptor上存在@Qualifier注解，那么则要判断当前beanName上是否定义了Qualifier，并且是否和当前DependencyDescriptor上的Qualifier相等，相等则匹配 经过上述验证之后，当前beanName才能成为一个可注入的，添加到result中 关于依赖注入中泛型注入的实现首先在Java反射中，有一个Type接口，表示类型，具体分类为： raw types：也就是普通Class parameterized types：对应ParameterizedType接口，泛型类型 array types：对应GenericArrayType，泛型数组 type variables：对应TypeVariable接口，表示类型变量，也就是所定义的泛型，比如T、K primitive types：基本类型，int、boolean 大家可以好好看看下面代码所打印的结果：​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class TypeTest&lt;T&gt; &#123; private int i; private Integer it; private int[] iarray; private List list; private List&lt;String&gt; slist; private List&lt;T&gt; tlist; private T t; private T[] tarray; public static void main(String[] args) throws NoSuchFieldException &#123; test(TypeTest.class.getDeclaredField(&quot;i&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;it&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;iarray&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;list&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;slist&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;tlist&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;t&quot;)); System.out.println(&quot;=======&quot;); test(TypeTest.class.getDeclaredField(&quot;tarray&quot;)); &#125; public static void test(Field field) &#123; if (field.getType().isPrimitive()) &#123; System.out.println(field.getName() + &quot;是基本数据类型&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是基本数据类型&quot;); &#125; if (field.getGenericType() instanceof ParameterizedType) &#123; System.out.println(field.getName() + &quot;是泛型类型&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型类型&quot;); &#125; if (field.getType().isArray()) &#123; System.out.println(field.getName() + &quot;是普通数组&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是普通数组&quot;); &#125; if (field.getGenericType() instanceof GenericArrayType) &#123; System.out.println(field.getName() + &quot;是泛型数组&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型数组&quot;); &#125; if (field.getGenericType() instanceof TypeVariable) &#123; System.out.println(field.getName() + &quot;是泛型变量&quot;); &#125; else &#123; System.out.println(field.getName() + &quot;不是泛型变量&quot;); &#125; &#125;&#125; Spring中，但注入点是一个泛型时，也是会进行处理的，比如：​ 1234567891011121314151617@Componentpublic class UserService extends BaseService&lt;OrderService, StockService&gt; &#123; public void test() &#123; System.out.println(o); &#125;&#125;public class BaseService&lt;O, S&gt; &#123; @Autowired protected O o; @Autowired protected S s;&#125; Spring扫描时发现UserService是一个Bean 那就取出注入点，也就是BaseService中的两个属性o、s 接下来需要按注入点类型进行注入，但是o和s都是泛型，所以Spring需要确定o和s的具体类型。 因为当前正在创建的是UserService的Bean，所以可以通过userService.getClass().getGenericSuperclass().getTypeName()获取到具体的泛型信息，比如com.zhouyu.service.BaseService&lt;com.zhouyu.service.OrderService, com.zhouyu.service.StockService&gt; 然后再拿到UserService的父类BaseService的泛型变量： for (TypeVariable&lt;? extends Class&lt;?&gt;&gt; typeParameter : userService.getClass().getSuperclass().getTypeParameters()) &#123; System._out_.println(typeParameter.getName()); &#125; 通过上面两段代码，就能知道，o对应的具体就是OrderService，s对应的具体类型就是StockService 然后再调用oField.getGenericType()就知道当前field使用的是哪个泛型，就能知道具体类型了 @Qualifier的使用定义两个注解： 12345678910@Target(&#123;ElementType.TYPE, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifier(&quot;random&quot;)public @interface Random &#123;&#125;@Target(&#123;ElementType.TYPE, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Qualifier(&quot;roundRobin&quot;)public @interface RoundRobin &#123;&#125; 定义一个接口和两个实现类，表示负载均衡： 123public interface LoadBalance &#123; String select();&#125; 123456789101112131415161718@Component@Randompublic class RandomStrategy implements LoadBalance &#123; @Override public String select() &#123; return null; &#125;&#125;@Component@RoundRobinpublic class RoundRobinStrategy implements LoadBalance &#123; @Override public String select() &#123; return null; &#125;&#125; 使用： 123456789101112@Componentpublic class UserService &#123; @Autowired @RoundRobin private LoadBalance loadBalance; public void test() &#123; System.out.println(loadBalance); &#125;&#125; @Resource@Resource注解底层工作流程图：https://www.processon.com/view/link/5f91275f07912906db381f6e 第五章 循环依赖什么是循环依赖？很简单，就是A对象依赖了B对象，B对象依赖了A对象。 比如： 123456789// A依赖了Bclass A&#123; public B b;&#125;// B依赖了Aclass B&#123; public A a;&#125; 那么循环依赖是个问题吗？ 如果不考虑Spring，循环依赖并不是问题，因为对象之间相互依赖是很正常的事情。 比如 12345A a = new A();B b = new B();a.b = b;b.a = a; 这样，A,B就依赖上了。 但是，在Spring中循环依赖就是一个问题了，为什么？因为，在Spring中，一个对象并不是简单new出来了，而是会经过一系列的Bean的生命周期，就是因为Bean的生命周期所以才会出现循环依赖问题。当然，在Spring中，出现循环依赖的场景很多，有的场景Spring自动帮我们解决了，而有的场景则需要程序员来解决，下文详细来说。 要明白Spring中的循环依赖，得先明白Spring中Bean的生命周期。 Bean的生命周期这里不会对Bean的生命周期进行详细的描述，只描述一下大概的过程。 Bean的生命周期指的就是：在Spring中，Bean是如何生成的？ 被Spring管理的对象叫做Bean。Bean的生成步骤如下： Spring扫描class得到BeanDefinition 根据得到的BeanDefinition去生成bean 首先根据class推断构造方法 根据推断出来的构造方法，反射，得到一个对象（暂时叫做原始对象） 填充原始对象中的属性（依赖注入） 如果原始对象中的某个方法被AOP了，那么则需要根据原始对象生成一个代理对象 把最终生成的代理对象放入单例池（源码中叫做singletonObjects）中，下次getBean时就直接从单例池拿即可 可以看到，对于Spring中的Bean的生成过程，步骤还是很多的，并且不仅仅只有上面的7步，还有很多很多，比如Aware回调、初始化等等，这里不详细讨论。 可以发现，在Spring中，构造一个Bean，包括了new这个步骤（第4步构造方法反射）。 得到一个原始对象后，Spring需要给对象中的属性进行依赖注入，那么这个注入过程是怎样的？ 比如上文说的A类，A类中存在一个B类的b属性，所以，当A类生成了一个原始对象之后，就会去给b属性去赋值，此时就会根据b属性的类型和属性名去BeanFactory中去获取B类所对应的单例bean。如果此时BeanFactory中存在B对应的Bean，那么直接拿来赋值给b属性；如果此时BeanFactory中不存在B对应的Bean，则需要生成一个B对应的Bean，然后赋值给b属性。 问题就出现在第二种情况，如果此时B类在BeanFactory中还没有生成对应的Bean，那么就需要去生成，就会经过B的Bean的生命周期。 那么在创建B类的Bean的过程中，如果B类中存在一个A类的a属性，那么在创建B的Bean的过程中就需要A类对应的Bean，但是，触发B类Bean的创建的条件是A类Bean在创建过程中的依赖注入，所以这里就出现了循环依赖： ABean创建–&gt;依赖了B属性–&gt;触发BBean创建—&gt;B依赖了A属性—&gt;需要ABean（但ABean还在创建过程中） 从而导致ABean创建不出来，BBean也创建不出来。 这是循环依赖的场景，但是上文说了，在Spring中，通过某些机制帮开发者解决了部分循环依赖的问题，这个机制就是三级缓存。 三级缓存三级缓存是通用的叫法。一级缓存为：singletonObjects二级缓存为：earlySingletonObjects三级缓存为：singletonFactories​ 先稍微解释一下这三个缓存的作用，后面详细分析： singletonObjects中缓存的是已经经历了完整生命周期的bean对象。 earlySingletonObjects比singletonObjects多了一个early，表示缓存的是早期的bean对象。早期是什么意思？表示Bean的生命周期还没走完就把这个Bean放入了earlySingletonObjects。 singletonFactories中缓存的是ObjectFactory，表示对象工厂，表示用来创建早期bean对象的工厂。 解决循环依赖思路分析先来分析为什么缓存能解决循环依赖。 上文分析得到，之所以产生循环依赖的问题，主要是： A创建时—&gt;需要B—-&gt;B去创建—&gt;需要A，从而产生了循环 那么如何打破这个循环，加个中间人（缓存） A的Bean在创建过程中，在进行依赖注入之前，先把A的原始Bean放入缓存（提早暴露，只要放到缓存了，其他Bean需要时就可以从缓存中拿了），放入缓存后，再进行依赖注入，此时A的Bean依赖了B的Bean，如果B的Bean不存在，则需要创建B的Bean，而创建B的Bean的过程和A一样，也是先创建一个B的原始对象，然后把B的原始对象提早暴露出来放入缓存中，然后在对B的原始对象进行依赖注入A，此时能从缓存中拿到A的原始对象（虽然是A的原始对象，还不是最终的Bean），B的原始对象依赖注入完了之后，B的生命周期结束，那么A的生命周期也能结束。 因为整个过程中，都只有一个A原始对象，所以对于B而言，就算在属性注入时，注入的是A原始对象，也没有关系，因为A原始对象在后续的生命周期中在堆中没有发生变化。 从上面这个分析过程中可以得出，只需要一个缓存就能解决循环依赖了，那么为什么Spring中还需要singletonFactories呢？ 这是难点，基于上面的场景想一个问题：如果A的原始对象注入给B的属性之后，A的原始对象进行了AOP产生了一个代理对象，此时就会出现，对于A而言，它的Bean对象其实应该是AOP之后的代理对象，而B的a属性对应的并不是AOP之后的代理对象，这就产生了冲突。 B依赖的A和最终的A不是同一个对象。 AOP就是通过一个BeanPostProcessor来实现的，这个BeanPostProcessor就是AnnotationAwareAspectJAutoProxyCreator，它的父类是AbstractAutoProxyCreator，而在Spring中AOP利用的要么是JDK动态代理，要么CGLib的动态代理，所以如果给一个类中的某个方法设置了切面，那么这个类最终就需要生成一个代理对象。 一般过程就是：A类—&gt;生成一个普通对象–&gt;属性注入–&gt;基于切面生成一个代理对象–&gt;把代理对象放入singletonObjects单例池中。 而AOP可以说是Spring中除开IOC的另外一大功能，而循环依赖又是属于IOC范畴的，所以这两大功能想要并存，Spring需要特殊处理。 如何处理的，就是利用了第三级缓存singletonFactories。 首先，singletonFactories中存的是某个beanName对应的ObjectFactory，在bean的生命周期中，生成完原始对象之后，就会构造一个ObjectFactory存入singletonFactories中。这个ObjectFactory是一个函数式接口，所以支持Lambda表达式：**() -&gt; getEarlyBeanReference(beanName, mbd, bean)** 上面的Lambda表达式就是一个ObjectFactory，执行该Lambda表达式就会去执行getEarlyBeanReference方法，而该方法如下： 123456789101112protected Object getEarlyBeanReference(String beanName, RootBeanDefinition mbd, Object bean) &#123; Object exposedObject = bean; if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof SmartInstantiationAwareBeanPostProcessor) &#123; SmartInstantiationAwareBeanPostProcessor ibp = (SmartInstantiationAwareBeanPostProcessor) bp; exposedObject = ibp.getEarlyBeanReference(exposedObject, beanName); &#125; &#125; &#125; return exposedObject;&#125; 该方法会去执行SmartInstantiationAwareBeanPostProcessor中的getEarlyBeanReference方法，而这个接口下的实现类中只有两个类实现了这个方法，一个是AbstractAutoProxyCreator，一个是InstantiationAwareBeanPostProcessorAdapter，它的实现如下： 123456789101112// InstantiationAwareBeanPostProcessorAdapter@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) throws BeansException &#123; return bean;&#125;// AbstractAutoProxyCreator@Overridepublic Object getEarlyBeanReference(Object bean, String beanName) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); this.earlyProxyReferences.put(cacheKey, bean); return wrapIfNecessary(bean, beanName, cacheKey);&#125; 在整个Spring中，默认就只有AbstractAutoProxyCreator真正意义上实现了getEarlyBeanReference方法，而该类就是用来进行AOP的。上文提到的AnnotationAwareAspectJAutoProxyCreator的父类就是AbstractAutoProxyCreator。 那么getEarlyBeanReference方法到底在干什么？首先得到一个cachekey，cachekey就是beanName。然后把beanName和bean（这是原始对象）存入earlyProxyReferences中调用wrapIfNecessary进行AOP，得到一个代理对象。 那么，什么时候会调用getEarlyBeanReference方法呢？回到循环依赖的场景中 左边文字：这个ObjectFactory就是上文说的labmda表达式，中间有getEarlyBeanReference方法，注意存入singletonFactories时并不会执行lambda表达式，也就是不会执行getEarlyBeanReference方法 右边文字：从singletonFactories根据beanName得到一个ObjectFactory，然后执行ObjectFactory，也就是执行getEarlyBeanReference方法，此时会得到一个A原始对象经过AOP之后的代理对象，然后把该代理对象放入earlySingletonObjects中，注意此时并没有把代理对象放入singletonObjects中，那什么时候放入到singletonObjects中呢？ 我们这个时候得来理解一下earlySingletonObjects的作用，此时，我们只得到了A原始对象的代理对象，这个对象还不完整，因为A原始对象还没有进行属性填充，所以此时不能直接把A的代理对象放入singletonObjects中，所以只能把代理对象放入earlySingletonObjects，假设现在有其他对象依赖了A，那么则可以从earlySingletonObjects中得到A原始对象的代理对象了，并且是A的同一个代理对象。 当B创建完了之后，A继续进行生命周期，而A在完成属性注入后，会按照它本身的逻辑去进行AOP，而此时我们知道A原始对象已经经历过了AOP，所以对于A本身而言，不会再去进行AOP了，那么怎么判断一个对象是否经历过了AOP呢？会利用上文提到的earlyProxyReferences，在AbstractAutoProxyCreator的postProcessAfterInitialization方法中，会去判断当前beanName是否在earlyProxyReferences，如果在则表示已经提前进行过AOP了，无需再次进行AOP。 对于A而言，进行了AOP的判断后，以及BeanPostProcessor的执行之后，就需要把A对应的对象放入singletonObjects中了，但是我们知道，应该是要把A的代理对象放入singletonObjects中，所以此时需要从earlySingletonObjects中得到代理对象，然后入singletonObjects中。 整个循环依赖解决完毕。 总结至此，总结一下三级缓存： singletonObjects：缓存经过了完整生命周期的bean earlySingletonObjects：缓存未经过完整生命周期的bean，如果某个bean出现了循环依赖，就会提前把这个暂时未经过完整生命周期的bean放入earlySingletonObjects中，这个bean如果要经过AOP，那么就会把代理对象放入earlySingletonObjects中，否则就是把原始对象放入earlySingletonObjects，但是不管怎么样，就是是代理对象，代理对象所代理的原始对象也是没有经过完整生命周期的，所以放入earlySingletonObjects我们就可以统一认为是未经过完整生命周期的bean。 singletonFactories：缓存的是一个ObjectFactory，也就是一个Lambda表达式。在每个Bean的生成过程中，经过实例化得到一个原始对象后，都会提前基于原始对象暴露一个Lambda表达式，并保存到三级缓存中，这个Lambda表达式可能用到，也可能用不到，如果当前Bean没有出现循环依赖，那么这个Lambda表达式没用，当前bean按照自己的生命周期正常执行，执行完后直接把当前bean放入singletonObjects中，如果当前bean在依赖注入时发现出现了循环依赖（当前正在创建的bean被其他bean依赖了），则从三级缓存中拿到Lambda表达式，并执行Lambda表达式得到一个对象，并把得到的对象放入二级缓存（(如果当前Bean需要AOP，那么执行lambda表达式，得到就是对应的代理对象，如果无需AOP，则直接得到一个原始对象)）。 其实还要一个缓存，就是earlyProxyReferences，它用来记录某个原始对象是否进行过AOP了。 反向分析一下singletonFactories为什么需要singletonFactories？假设没有singletonFactories，只有earlySingletonObjects，earlySingletonObjects是二级缓存，它内部存储的是为经过完整生命周期的bean对象，Spring原有的流程是出现了循环依赖的情况下： 先从singletonFactories中拿到lambda表达式，这里肯定是能拿到的，因为每个bean实例化之后，依赖注入之前，就会生成一个lambda表示放入singletonFactories中 执行lambda表达式，得到结果，将结果放入earlySingletonObjects中 那如果没有singletonFactories，该如何把原始对象或AOP之后的代理对象放入earlySingletonObjects中呢？何时放入呢？​ 首先，将原始对象或AOP之后的代理对象放入earlySingletonObjects中的有两种： 实例化之后，依赖注入之前：如果是这样，那么对于每个bean而言，都是在依赖注入之前会去进行AOP，这是不符合bean生命周期步骤的设计的。 真正发现某个bean出现了循环依赖时：按现在Spring源码的流程来说，就是getSingleton(String beanName, boolean allowEarlyReference)中，是在这个方法中判断出来了当前获取的这个bean在创建中，就表示获取的这个bean出现了循环依赖，那在这个方法中该如何拿到原始对象呢？更加重要的是，该如何拿到AOP之后的代理对象呢？难道在这个方法中去循环调用BeanPostProcessor的初始化后的方法吗？不是做不到，不太合适，代码太丑。最关键的是在这个方法中该如何拿到原始对象呢？还是得需要一个Map，预习把这个Bean实例化后的对象存在这个Map中，那这样的话还不如直接用第一种方案，但是第一种又直接打破了Bean生命周期的设计。 所以，我们可以发现，现在Spring所用的singletonFactories，为了调和不同的情况，在singletonFactories中存的是lambda表达式，这样的话，只有在出现了循环依赖的情况，才会执行lambda表达式，才会进行AOP，也就说只有在出现了循环依赖的情况下才会打破Bean生命周期的设计，如果一个Bean没有出现循环依赖，那么它还是遵守了Bean的生命周期的设计的。 推断构造方法流程图：https://www.processon.com/view/link/5f97bc717d9c0806f291d7eb​ AutowiredAnnotationBeanPostProcessor中推断构造方法不同情况思维脑图：https://www.processon.com/view/link/6146def57d9c08198c58bb26​ Spring中的一个bean，需要实例化得到一个对象，而实例化就需要用到构造方法。 一般情况下，一个类只有一个构造方法： 要么是无参的构造方法 要么是有参的构造方法 如果只有一个无参的构造方法，那么实例化就只能使用这个构造方法了。如果只有一个有参的构造方法，那么实例化时能使用这个构造方法吗？要分情况讨论： 使用AnnotationConfigApplicationContext，会使用这个构造方法进行实例化，那么Spring会根据构造方法的参数信息去寻找bean，然后传给构造方法 使用ClassPathXmlApplicationContext，表示使用XML的方式来使用bean，要么在XML中指定构造方法的参数值(手动指定)，要么配置autowire&#x3D;constructor让Spring自动去寻找bean做为构造方法参数值。 上面是只有一个构造方法的情况，那么如果有多个构造方法呢？ 又分为两种情况，多个构造方法中存不存在无参的构造方法。 分析：一个类存在多个构造方法，那么Spring进行实例化之前，该如何去确定到底用哪个构造方法呢？ 如果开发者指定了想要使用的构造方法，那么就用这个构造方法 如果开发者没有指定想要使用的构造方法，则看开发者有没有让Spring自动去选择构造方法 如果开发者也没有让Spring自动去选择构造方法，则Spring利用无参构造方法，如果没有无参构造方法，则报错 针对第一点，开发者可以通过什么方式来指定使用哪个构造方法呢？ xml中的标签，这个标签表示构造方法参数，所以可以根据这个确定想要使用的构造方法的参数个数，从而确定想要使用的构造方法 通过@Autowired注解，@Autowired注解可以写在构造方法上，所以哪个构造方法上写了@Autowired注解，表示开发者想使用哪个构造方法，当然，它和第一个方式的不同点是，通过xml的方式，我们直接指定了构造方法的参数值，而通过@Autowired注解的方式，需要Spring通过byType+byName的方式去找到符合条件的bean作为构造方法的参数值 再来看第二点，如果开发者没有指定想要使用的构造方法，则看开发者有没有让Spring自动去选择构造方法，对于这一点，只能用在ClassPathXmlApplicationContext，因为通过AnnotationConfigApplicationContext没有办法去指定某个bean可以自动去选择构造方法，而通过ClassPathXmlApplicationContext可以在xml中指定某个bean的autowire为constructor，虽然这个属性表示通过构造方法自动注入，所以需要自动的去选择一个构造方法进行自动注入，因为是构造方法，所以顺便是进行实例化。 当然，还有一种情况，就是多个构造方法上写了@Autowired注解，那么此时Spring会报错。但是，因为@Autowired还有一个属性required，默认为ture，所以一个类中，只有能一个构造方法标注了@Autowired或@Autowired（required&#x3D;true），有多个会报错。但是可以有多个@Autowired（required&#x3D;false）,这种情况下，需要Spring从这些构造方法中去自动选择一个构造方法。 源码思路 AbstractAutowireCapableBeanFactory类中的createBeanInstance()方法会去创建一个Bean实例 根据BeanDefinition加载类得到Class对象 如果BeanDefinition绑定了一个Supplier，那就调用Supplier的get方法得到一个对象并直接返回 如果BeanDefinition中存在factoryMethodName，那么就调用该工厂方法得到一个bean对象并返回 如果BeanDefinition已经自动构造过了，那就调用autowireConstructor()自动构造一个对象 调用SmartInstantiationAwareBeanPostProcessor的determineCandidateConstructors()方法得到哪些构造方法是可以用的 如果存在可用得构造方法，或者当前BeanDefinition的autowired是AUTOWIRE_CONSTRUCTOR，或者BeanDefinition中指定了构造方法参数值，或者创建Bean的时候指定了构造方法参数值，那么就调用**autowireConstructor()**方法自动构造一个对象 最后，如果不是上述情况，就根据无参的构造方法实例化一个对象 autowireConstructor() 先检查是否指定了具体的构造方法和构造方法参数值，或者在BeanDefinition中缓存了具体的构造方法或构造方法参数值，如果存在那么则直接使用该构造方法进行实例化 如果没有确定的构造方法或构造方法参数值，那么 如果没有确定的构造方法，那么则找出类中所有的构造方法 如果只有一个无参的构造方法，那么直接使用无参的构造方法进行实例化 如果有多个可用的构造方法或者当前Bean需要自动通过构造方法注入 根据所指定的构造方法参数值，确定所需要的最少的构造方法参数值的个数 对所有的构造方法进行排序，参数个数多的在前面 遍历每个构造方法 如果不是调用getBean方法时所指定的构造方法参数值，那么则根据构造方法参数类型找值 如果时调用getBean方法时所指定的构造方法参数值，就直接利用这些值 如果根据当前构造方法找到了对应的构造方法参数值，那么这个构造方法就是可用的，但是不一定这个构造方法就是最佳的，所以这里会涉及到是否有多个构造方法匹配了同样的值，这个时候就会用值和构造方法类型进行匹配程度的打分，找到一个最匹配的 为什么分越少优先级越高？主要是计算找到的bean和构造方法参数类型匹配程度有多高。 假设bean的类型为A，A的父类是B，B的父类是C，同时A实现了接口D如果构造方法的参数类型为A，那么完全匹配，得分为0如果构造方法的参数类型为B，那么得分为2如果构造方法的参数类型为C，那么得分为4如果构造方法的参数类型为D，那么得分为1 可以直接使用如下代码进行测试： 12345678910111213Object[] objects = new Object[]&#123;new A()&#125;;// 0System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;A.class&#125;, objects));// 2System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;B.class&#125;, objects));// 4System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;C.class&#125;, objects));// 1System.out.println(MethodInvoker.getTypeDifferenceWeight(new Class[]&#123;D.class&#125;, objects)); 所以，我们可以发现，越匹配分数越低。 @Bean的情况首先，Spring会把@Bean修饰的方法解析成BeanDefinition： 如果方法是static的，那么解析出来的BeanDefinition中： factoryBeanName为AppConfig所对应的beanName，比如”appConfig” factoryMethodName为对应的方法名，比如”aService” factoryClass为AppConfig.class 如果方法不是static的，那么解析出来的BeanDefinition中： factoryBeanName为null factoryMethodName为对应的方法名，比如”aService” factoryClass也为AppConfig.class 在由@Bean生成的BeanDefinition中，有一个重要的属性isFactoryMethodUnique，表示factoryMethod是不是唯一的，在普通情况下@Bean生成的BeanDefinition的isFactoryMethodUnique为true，但是如果出现了方法重载，那么就是特殊的情况，比如： 123456789@Beanpublic static AService aService()&#123; return new AService();&#125;@Beanpublic AService aService(BService bService)&#123; return new AService();&#125; 虽然有两个@Bean，但是肯定只会生成一个aService的Bean，那么Spring在处理@Bean时，也只会生成一个aService的BeanDefinition，比如Spring先解析到第一个@Bean，会生成一个BeanDefinition，此时isFactoryMethodUnique为true，但是解析到第二个@Bean时，会判断出来beanDefinitionMap中已经存在一个aService的BeanDefinition了，那么会把之前的这个BeanDefinition的isFactoryMethodUnique修改为false，并且不会生成新的BeanDefinition了。​ 并且后续在根据BeanDefinition创建Bean时，会根据isFactoryMethodUnique来操作，如果为true，那就表示当前BeanDefinition只对应了一个方法，那也就是只能用这个方法来创建Bean了，但是如果isFactoryMethodUnique为false，那就表示当前BeanDefition对应了多个方法，需要和推断构造方法的逻辑一样，去选择用哪个方法来创建Bean。 第六章 Spring之整合Mybatis mybatis-spring jar包 JDK动态代理返回xxxMapper代理类 注册组件的整合 整合核心思路由很多框架都需要和Spring进行整合，而整合的核心思想就是把其他框架所产生的对象放到Spring容器中，让其成为Bean。 \\ 比如Mybatis，Mybatis框架可以单独使用，而单独使用Mybatis框架就需要用到Mybatis所提供的一些类构造出对应的对象，然后使用该对象，就能使用到Mybatis框架给我们提供的功能，和Mybatis整合Spring就是为了将这些对象放入Spring容器中成为Bean，只要成为了Bean，在我们的Spring项目中就能很方便的使用这些对象了，也就能很方便的使用Mybatis框架所提供的功能了。 实现简易mybatismybatis使用&#x2F;需求分析 根据类型注入Mapper增强类型(将增强类型注册到容器) 多个Mapper写活 给构造器指定值 通过后置处理器增强 写死不好,想要拿到扫描路径 也可以写 这个 接口会给你一些工具 比如这个注解原信息 spring自带Scanner 但是不关心接口 ,可mybatis只关心接口 需要重写扫描器的逻辑 两个判断 带component和另一个sbd才能被扫描器扫入 mybatis解决: 不符合要求 Beandefinition里是接口mapper 需要 是factoryBean里的代理类 解决 Mybatis-Spring 1.3.2版本底层源码执行流程 通过@MapperScan导入了MapperScannerRegistrar类 MapperScannerRegistrar类实现了ImportBeanDefinitionRegistrar接口，所以Spring在启动时会调用MapperScannerRegistrar类中的registerBeanDefinitions方法 在registerBeanDefinitions方法中定义了一个ClassPathMapperScanner对象，用来扫描mapper 设置ClassPathMapperScanner对象可以扫描到接口，因为在Spring中是不会扫描接口的 同时因为ClassPathMapperScanner中重写了isCandidateComponent方法，导致isCandidateComponent只会认为接口是备选者Component 通过利用Spring的扫描后，会把接口扫描出来并且得到对应的BeanDefinition 接下来把扫描得到的BeanDefinition进行修改，把BeanClass修改为MapperFactoryBean，把AutowireMode修改为byType 扫描完成后，Spring就会基于BeanDefinition去创建Bean了，相当于每个Mapper对应一个FactoryBean 在MapperFactoryBean中的getObject方法中，调用了getSqlSession()去得到一个sqlSession对象，然后根据对应的Mapper接口生成一个Mapper接口代理对象，这个代理对象就成为Spring容器中的Bean sqlSession对象是Mybatis中的，一个sqlSession对象需要SqlSessionFactory来产生 MapperFactoryBean的AutowireMode为byType，所以Spring会自动调用set方法，有两个set方法，一个setSqlSessionFactory，一个setSqlSessionTemplate，而这两个方法执行的前提是根据方法参数类型能找到对应的bean，所以Spring容器中要存在SqlSessionFactory类型的bean或者SqlSessionTemplate类型的bean。 如果你定义的是一个SqlSessionFactory类型的bean，那么最终也会被包装为一个SqlSessionTemplate对象，并且赋值给sqlSession属性 而在SqlSessionTemplate类中就存在一个getMapper方法，这个方法中就产生一个Mapper接口代理对象 到时候，当执行该代理对象的某个方法时，就会进入到Mybatis框架的底层执行流程，详细的请看下图 Spring整合Mybatis之后SQL执行流程：https://www.processon.com/view/link/6152cc385653bb6791db436c Mybatis-Spring 2.0.6版本(最新版)底层源码执行流程 通过@MapperScan导入了MapperScannerRegistrar类 MapperScannerRegistrar类实现了ImportBeanDefinitionRegistrar接口，所以Spring在启动时会调用MapperScannerRegistrar类中的registerBeanDefinitions方法 在registerBeanDefinitions方法中注册一个MapperScannerConfigurer类型的BeanDefinition 而MapperScannerConfigurer实现了BeanDefinitionRegistryPostProcessor接口，所以Spring在启动过程中时会调用它的postProcessBeanDefinitionRegistry()方法 在postProcessBeanDefinitionRegistry方法中会生成一个ClassPathMapperScanner对象，然后进行扫描 后续的逻辑和1.3.2版本一样。 带来的好处是，可以不使用@MapperScan注解，而可以直接定义一个Bean，比如： 123456@Beanpublic MapperScannerConfigurer mapperScannerConfigurer() &#123; MapperScannerConfigurer mapperScannerConfigurer = new MapperScannerConfigurer(); mapperScannerConfigurer.setBasePackage(&quot;com.luban&quot;); return mapperScannerConfigurer;&#125; Spring整合Mybatis后一级缓存失效问题先看下图：Spring整合Mybatis之后SQL执行流程：https://www.processon.com/view/link/6152cc385653bb6791db436c​ Mybatis中的一级缓存是基于SqlSession来实现的，所以在执行同一个sql时，如果使用的是同一个SqlSession对象，那么就能利用到一级缓存，提高sql的执行效率。​ 但是在Spring整合Mybatis后，如果没有执行某个方法时，该方法上没有加@Transactional注解，也就是没有开启Spring事务，那么后面在执行具体sql时，没执行一个sql时都会新生成一个SqlSession对象来执行该sql，这就是我们说的一级缓存失效（也就是没有使用同一个SqlSession对象），而如果开启了Spring事务，那么该Spring事务中的多个sql，在执行时会使用同一个SqlSession对象，从而一级缓存生效，具体的底层执行流程在上图。​ 个人理解：实际上Spring整合Mybatis后一级缓存失效并不是问题，是正常的实现，因为，一个方法如果没有开启Spring事务，那么在执行sql时候，那就是每个sql单独一个事务来执行，也就是单独一个SqlSession对象来执行该sql，如果开启了Spring事务，那就是多个sql属于同一个事务，那自然就应该用一个SqlSession来执行这多个sql。所以，在没有开启Spring事务的时候，SqlSession的一级缓存并不是失效了，而是存在的生命周期太短了（执行完一个sql后就被销毁了，下一个sql执行时又是一个新的SqlSession了）。​ 第七章 AOP动态代理代理模式的解释：为其他对象提供一种代理以控制对这个对象的访问，增强一个类中的某个方法，对程序进行扩展。 比如，现在存在一个UserService类： 1234567public class UserService &#123; public void test() &#123; System.out.println(&quot;test...&quot;); &#125;&#125; 此时，我们new一个UserService对象，然后执行test()方法，结果是显而易见的。​ 如果我们现在想在不修改UserService类的源码前提下，给test()增加额外逻辑，那么就可以使用动态代理机制来创建UserService对象了，比如： 12345678910111213141516171819202122UserService target = new UserService();// 通过cglib技术Enhancer enhancer = new Enhancer();enhancer.setSuperclass(UserService.class);// 定义额外逻辑，也就是代理逻辑enhancer.setCallbacks(new Callback[]&#123;new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = methodProxy.invoke(target, objects); System.out.println(&quot;after...&quot;); return result; &#125;&#125;&#125;);// 动态代理所创建出来的UserService对象UserService userService = (UserService) enhancer.create();// 执行这个userService的test方法时，就会额外会执行一些其他逻辑userService.test(); 得到的都是UserService对象，但是执行test()方法时的效果却不一样了，这就是代理所带来的效果。 上面是通过cglib来实现的代理对象的创建，是基于父子类的，被代理类（UserService）是父类，代理类是子类，代理对象就是代理类的实例对象，代理类是由cglib创建的，对于程序员来说不用关心。​ 除开cglib技术，jdk本身也提供了一种创建代理对象的动态代理机制，但是它只能代理接口，也就是UserService得先有一个接口才能利用jdk动态代理机制来生成一个代理对象，比如： 1234567891011public interface UserInterface &#123; public void test();&#125;public class UserService implements UserInterface &#123; public void test() &#123; System.out.println(&quot;test...&quot;); &#125;&#125; 利用JDK动态代理来生成一个代理对象： 123456789101112131415UserService target = new UserService();// UserInterface接口的代理对象Object proxy = Proxy.newProxyInstance(UserService.class.getClassLoader(), new Class[]&#123;UserInterface.class&#125;, new InvocationHandler() &#123; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = method.invoke(target, args); System.out.println(&quot;after...&quot;); return result; &#125;&#125;);UserInterface userService = (UserInterface) proxy;userService.test(); 如果你把new Class[]{UserInterface.class}，替换成new Class[]{UserService.class}，允许代码会直接报错： 1Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: com.zhouyu.service.UserService is not an interface 表示一定要是个接口。​ 由于这个限制，所以产生的代理对象的类型是UserInterface，而不是UserService，这是需要注意的。 ProxyFactory上面我们介绍了两种动态代理技术，那么在Spring中进行了封装，封装出来的类叫做ProxyFactory，表示是创建代理对象的一个工厂，使用起来会比上面的更加方便，比如： 1234567891011121314151617UserService target = new UserService();ProxyFactory proxyFactory = new ProxyFactory();proxyFactory.setTarget(target);proxyFactory.addAdvice(new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; //增强所有的方法 System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125;&#125;);UserInterface userService = (UserInterface) proxyFactory.getProxy();userService.test(); 通过ProxyFactory，我们可以不再关系到底是用cglib还是jdk动态代理了，ProxyFactory会帮我们去判断，如果UserService实现了接口，那么ProxyFactory底层就会用jdk动态代理，如果没有实现接口，就会用cglib技术，上面的代码，就是由于UserService实现了UserInterface接口，所以最后产生的代理对象是UserInterface类型。 &#x2F;&#x2F; 123456789101112131415 //增强逻辑Object result = null; // 判断哪些方法需要增强 if (&quot;selectAll&quot;.equals(method.getName())) &#123; //权限校验 System.out.println(&quot;权限校验&quot;); //执行被代理类的目标方法 result = methodProxy.invokeSuper(proxy, args); //日志记录 System.out.println(&quot;日志记录&quot;); &#125; else &#123; //不是addUser和deleteUser执行目标方法 result = methodProxy.invokeSuper(proxy, args); &#125; return result; Advice的分类 Before Advice：方法之前执行 After returning advice：方法return后执行 After throwing advice：方法抛异常后执行 After (finally) advice：方法执行完finally之后执行，这是最后的，比return更后 Around advice：这是功能最强大的Advice，可以自定义执行顺序 看课上给的代码例子将一目了然​ Advisor的理解跟Advice类似的还有一个Advisor的概念，一个Advisor是有一个Pointcut和一个Advice组成的，通过Pointcut可以指定要需要被代理的逻辑，比如一个UserService类中有两个方法，按上面的例子，这两个方法都会被代理，被增强，那么我们现在可以通过Advisor，来控制到具体代理哪一个方法，比如： 12345678910111213141516171819202122232425262728293031323334353637 UserService target = new UserService(); ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.setTarget(target); proxyFactory.addAdvisor(new PointcutAdvisor() &#123; @Override public Pointcut getPointcut() &#123; return new StaticMethodMatcherPointcut() &#123; @Override public boolean matches(Method method, Class&lt;?&gt; targetClass) &#123; return method.getName().equals(&quot;testAbc&quot;); &#125; &#125;; &#125; @Override public Advice getAdvice() &#123; return new MethodInterceptor() &#123; //注意是覆写invoke方法的方法拦截器 @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;; &#125;// 这一步 @Override public boolean isPerInstance() &#123; return false; &#125; &#125;); UserInterface userService = (UserInterface) proxyFactory.getProxy(); userService.test(); 上面代码表示，产生的代理对象，只有在执行testAbc这个方法时才会被增强，会执行额外的逻辑，而在执行其他方法时是不会增强的。​ 创建代理对象的方式 proxyFactoryBean 类似proxyfacoty(只能单个Bean增强)返回一个代理对象放到IOC容器中 指定一个Advice 根据Beanname (BeanNameAutoProxyCreater) 指定一个Advice 找所有Advisor根据Advisor中的PointCut和Advice信息，确定要代理的Bean以及代理逻辑 上面介绍了Spring中所提供了ProxyFactory、Advisor、Advice、PointCut等技术来实现代理对象的创建，但是我们在使用Spring时，我们并不会直接这么去使用ProxyFactory，比如说，我们希望ProxyFactory所产生的代理对象能直接就是Bean，能直接从Spring容器中得到UserSerivce的代理对象，而这些，Spring都是支持的，只不过，作为开发者的我们肯定得告诉Spring，那些类需要被代理，代理逻辑是什么。 ProxyFactoryBean1234567891011121314151617@Beanpublic ProxyFactoryBean userServiceProxy()&#123; UserService userService = new UserService(); ProxyFactoryBean proxyFactoryBean = new ProxyFactoryBean(); proxyFactoryBean.setTarget(userService); proxyFactoryBean.addAdvice(new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;); return proxyFactoryBean;&#125; 通过这种方法来定义一个UserService的Bean，并且是经过了AOP的。但是这种方式只能针对某一个Bean。它是一个FactoryBean，所以利用的就是FactoryBean技术，间接的将UserService的代理对象作为了Bean。​ ProxyFactoryBean还有额外的功能，比如可以把某个Advise或Advisor定义成为Bean，然后在ProxyFactoryBean中进行设置 12345678910111213141516171819202122@Beanpublic MethodInterceptor zhouyuAroundAdvise()&#123; return new MethodInterceptor() &#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; System.out.println(&quot;before...&quot;); Object result = invocation.proceed(); System.out.println(&quot;after...&quot;); return result; &#125; &#125;;&#125;@Beanpublic ProxyFactoryBean userService()&#123; UserService userService = new UserService(); ProxyFactoryBean proxyFactoryBean = new ProxyFactoryBean(); proxyFactoryBean.setTarget(userService); proxyFactoryBean.setInterceptorNames(&quot;zhouyuAroundAdvise&quot;); return proxyFactoryBean;&#125; BeanNameAutoProxyCreatorProxyFactoryBean得自己指定被代理的对象，那么我们可以通过BeanNameAutoProxyCreator来通过指定某个bean的名字，来对该bean进行代理 123456789@Beanpublic BeanNameAutoProxyCreator beanNameAutoProxyCreator() &#123; BeanNameAutoProxyCreator beanNameAutoProxyCreator = new BeanNameAutoProxyCreator(); beanNameAutoProxyCreator.setBeanNames(&quot;userSe*&quot;); beanNameAutoProxyCreator.setInterceptorNames(&quot;zhouyuAroundAdvise&quot;); beanNameAutoProxyCreator.setProxyTargetClass(true); return beanNameAutoProxyCreator;&#125; 通过BeanNameAutoProxyCreator可以对批量的Bean进行AOP，并且指定了代理逻辑，指定了一个InterceptorName，也就是一个Advise，前提条件是这个Advise也得是一个Bean，这样Spring才能找到的，但是BeanNameAutoProxyCreator的缺点很明显，它只能根据beanName来指定想要代理的Bean。​ DefaultAdvisorAutoProxyCreator12345678910111213141516171819@Beanpublic DefaultPointcutAdvisor defaultPointcutAdvisor()&#123; NameMatchMethodPointcut pointcut = new NameMatchMethodPointcut(); pointcut.addMethodName(&quot;test&quot;); DefaultPointcutAdvisor defaultPointcutAdvisor = new DefaultPointcutAdvisor(); defaultPointcutAdvisor.setPointcut(pointcut); defaultPointcutAdvisor.setAdvice(new ZhouyuAfterReturningAdvise()); return defaultPointcutAdvisor;&#125;@Beanpublic DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator() &#123; DefaultAdvisorAutoProxyCreator defaultAdvisorAutoProxyCreator = new DefaultAdvisorAutoProxyCreator(); return defaultAdvisorAutoProxyCreator;&#125; 通过DefaultAdvisorAutoProxyCreator会直接去找所有Advisor类型的Bean，根据Advisor中的PointCut和Advice信息，确定要代理的Bean以及代理逻辑。 但是，我们发现，通过这种方式，我们得依靠某一个类来实现定义我们的Advisor，或者Advise，或者Pointcut，那么这个步骤能不能更加简化一点呢？​ 对的，通过注解！ 比如我们能不能只定义一个类，然后通过在类中的方法上通过某些注解，来定义PointCut以及Advice，可以的，比如： 12345678910@Aspect@Componentpublic class ZhouyuAspect &#123; @Before(&quot;execution(public void com.zhouyu.service.UserService.test())&quot;) public void zhouyuBefore(JoinPoint joinPoint) &#123; System.out.println(&quot;zhouyuBefore&quot;); &#125;&#125; 通过上面这个类，我们就直接定义好了所要代理的方法(通过一个表达式)，以及代理逻辑（被@Before修饰的方法），简单明了，这样对于Spring来说，它要做的就是来解析这些注解了，解析之后得到对应的Pointcut对象、Advice对象，生成Advisor对象，扔进ProxyFactory中，进而产生对应的代理对象，具体怎么解析这些注解就是**@EnableAspectJAutoProxy注解**所要做的事情了，后面详细分析。 对Spring AOP的理解OOP表示面向对象编程，是一种编程思想，AOP表示面向切面编程，也是一种编程思想，而我们上面所描述的就是Spring为了让程序员更加方便的做到面向切面编程所提供的技术支持，换句话说，就是Spring提供了一套机制，可以让我们更加容易的来进行AOP，所以这套机制我们也可以称之为Spring AOP。​ 但是值得注意的是，上面所提供的注解的方式来定义Pointcut和Advice，Spring并不是首创，首创是AspectJ，而且也不仅仅只有Spring提供了一套机制来支持AOP，还有比如 JBoss 4.0、aspectwerkz等技术都提供了对于AOP的支持。而刚刚说的注解的方式，Spring是依赖了AspectJ的，或者说，Spring是直接把AspectJ中所定义的那些注解直接拿过来用，自己没有再重复定义了，不过也仅仅只是把注解的定义赋值过来了，每个注解具体底层是怎么解析的，还是Spring自己做的，所以我们在用Spring时，如果你想用@Before、@Around等注解，是需要单独引入aspecj相关jar包的，比如： 12compile group: &#x27;org.aspectj&#x27;, name: &#x27;aspectjrt&#x27;, version: &#x27;1.9.5&#x27;compile group: &#x27;org.aspectj&#x27;, name: &#x27;aspectjweaver&#x27;, version: &#x27;1.9.5&#x27; 值得注意的是：AspectJ是在编译时对字节码进行了修改，是直接在UserService类对应的字节码中进行增强的，也就是可以理解为是在编译时就会去解析@Before这些注解，然后得到代理逻辑，加入到被代理的类中的字节码中去的，所以如果想用AspectJ技术来生成代理对象 ，是需要用单独的AspectJ编译器的。我们在项目中很少这么用，我们仅仅只是用了@Before这些注解，而我们在启动Spring的过程中，Spring会去解析这些注解，然后利用动态代理机制生成代理对象的。​ IDEA中使用Aspectj：https://blog.csdn.net/gavin_john/article/details/80156963 AOP中的概念 Aspect：表示切面，比如被@Aspect注解的类就是切面，可以在切面中去定义Pointcut、Advice等等 Join point：表示连接点，表示一个程序在执行过程中的一个点，比如一个方法的执行，比如一个异常的处理，在Spring AOP中，一个连接点通常表示一个方法的执行。 Advice：表示通知，表示在一个特定连接点上所采取的动作。Advice分为不同的类型，后面详细讨论，在很多AOP框架中，包括Spring，会用Interceptor拦截器来实现Advice，并且在连接点周围维护一个Interceptor链 Pointcut：表示切点，用来匹配一个或多个连接点，Advice与切点表达式是关联在一起的，Advice将会执行在和切点表达式所匹配的连接点上 Introduction：可以使用@DeclareParents来给所匹配的类添加一个接口，并指定一个默认实现 Target object：目标对象，被代理对象 AOP proxy：表示代理工厂，用来创建代理对象的，在Spring Framework中，要么是JDK动态代理，要么是CGLIB代理 Weaving：表示织入，表示创建代理对象的动作，这个动作可以发生在编译时期（比如Aspejctj），或者运行时，比如Spring AOP Advice在Spring AOP中对应API上面说到的Aspject中的注解，其中有五个是用来定义Advice的，表示代理逻辑，以及执行时机： @Before @AfterReturning @AfterThrowing @After @Around 我们前面也提到过，Spring自己也提供了类似的执行实际的实现类： 接口MethodBeforeAdvice，继承了接口BeforeAdvice 接口AfterReturningAdvice 接口ThrowsAdvice 接口AfterAdvice 接口MethodInterceptor Spring会把五个注解解析为对应的Advice类： @Before：AspectJMethodBeforeAdvice，实际上就是一个MethodBeforeAdvice @AfterReturning：AspectJAfterReturningAdvice，实际上就是一个AfterReturningAdvice @AfterThrowing：AspectJAfterThrowingAdvice，实际上就是一个MethodInterceptor @After：AspectJAfterAdvice，实际上就是一个MethodInterceptor @Around：AspectJAroundAdvice，实际上就是一个MethodInterceptor TargetSource的使用在我们日常的AOP中，被代理对象就是Bean对象，是由BeanFactory给我们创建出来的，但是Spring AOP中提供了TargetSource机制，可以让我们用来自定义逻辑来创建被代理对象。​ 比如之前所提到的**@Lazy注解，当加在属性上时，会产生一个代理对象赋值给这个属性**，产生代理对象的代码为： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455protected Object buildLazyResolutionProxy(final DependencyDescriptor descriptor, final @Nullable String beanName) &#123; BeanFactory beanFactory = getBeanFactory(); Assert.state(beanFactory instanceof DefaultListableBeanFactory, &quot;BeanFactory needs to be a DefaultListableBeanFactory&quot;); final DefaultListableBeanFactory dlbf = (DefaultListableBeanFactory) beanFactory; TargetSource ts = new TargetSource() &#123; @Override public Class&lt;?&gt; getTargetClass() &#123; return descriptor.getDependencyType(); &#125; @Override public boolean isStatic() &#123; return false; &#125; @Override public Object getTarget() &#123; Set&lt;String&gt; autowiredBeanNames = (beanName != null ? new LinkedHashSet&lt;&gt;(1) : null); Object target = dlbf.doResolveDependency(descriptor, beanName, autowiredBeanNames, null); if (target == null) &#123; Class&lt;?&gt; type = getTargetClass(); if (Map.class == type) &#123; return Collections.emptyMap(); &#125; else if (List.class == type) &#123; return Collections.emptyList(); &#125; else if (Set.class == type || Collection.class == type) &#123; return Collections.emptySet(); &#125; throw new NoSuchBeanDefinitionException(descriptor.getResolvableType(), &quot;Optional dependency not present for lazy injection point&quot;); &#125; if (autowiredBeanNames != null) &#123; for (String autowiredBeanName : autowiredBeanNames) &#123; if (dlbf.containsBean(autowiredBeanName)) &#123; dlbf.registerDependentBean(autowiredBeanName, beanName); &#125; &#125; &#125; return target; &#125; @Override public void releaseTarget(Object target) &#123; &#125; &#125;; ProxyFactory pf = new ProxyFactory(); pf.setTargetSource(ts); Class&lt;?&gt; dependencyType = descriptor.getDependencyType(); if (dependencyType.isInterface()) &#123; pf.addInterface(dependencyType); &#125; return pf.getProxy(dlbf.getBeanClassLoader()); &#125; 这段代码就利用了ProxyFactory来生成代理对象，以及使用了TargetSource，以达到代理对象在执行某个方法时，调用TargetSource的getTarget()方法实时得到一个被代理对象。 ProxyFactory选择cglib或jdk动态代理原理ProxyFactory在生成代理对象之前需要决定到底是使用JDK动态代理还是CGLIB技术： 1234567891011121314151617181920// config就是ProxyFactory对象// optimize为true,或proxyTargetClass为true,或用户没有给ProxyFactory对象添加interfaceif (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException(&quot;TargetSource cannot determine target class: &quot; + &quot;Either an interface or a target is required for proxy creation.&quot;); &#125; // targetClass是接口，直接使用Jdk动态代理 if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; // 使用Cglib return new ObjenesisCglibAopProxy(config);&#125;else &#123; // 使用Jdk动态代理 return new JdkDynamicAopProxy(config);&#125; 代理对象创建过程JdkDynamicAopProxy 在构造JdkDynamicAopProxy对象时，会先拿到被代理对象自己所实现的接口，并且额外的增加SpringProxy、Advised、DecoratingProxy三个接口，组合成一个Class[]，并赋值给proxiedInterfaces属性 并且检查这些接口中是否定义了equals()、hashcode()方法 执行Proxy.newProxyInstance(classLoader, this.proxiedInterfaces, this)，得到代理对象，JdkDynamicAopProxy作为InvocationHandler，代理对象在执行某个方法时，会进入到JdkDynamicAopProxy的**invoke()**方法中 ObjenesisCglibAopProxy 创建Enhancer对象 设置Enhancer的superClass为通过ProxyFactory.setTarget()所设置的对象的类 设置Enhancer的interfaces为通过ProxyFactory.addInterface()所添加的接口，以及SpringProxy、Advised、DecoratingProxy接口 设置Enhancer的Callbacks为DynamicAdvisedInterceptor 最后创建一个代理对象，代理对象在执行某个方法时，会进入到DynamicAdvisedInterceptor的intercept()方法中 代理对象执行过程 在使用ProxyFactory创建代理对象之前，需要往ProxyFactory先添加Advisor 代理对象在执行某个方法时，会把ProxyFactory中的Advisor拿出来和当前正在执行的方法进行匹配筛选 把和方法所匹配的Advisor适配成MethodInterceptor 把和当前方法匹配的MethodInterceptor链，以及被代理对象、代理对象、代理类、当前Method对象、方法参数封装为MethodInvocation对象 调用MethodInvocation的proceed()方法，开始执行各个MethodInterceptor以及被代理对象的对应方法 按顺序调用每个MethodInterceptor的invoke()方法，并且会把MethodInvocation对象传入invoke()方法 直到执行完最后一个MethodInterceptor了，就会调用invokeJoinpoint()方法，从而执行被代理对象的当前方法 各注解对应的MethodInterceptor @Before 对应的是AspectJMethodBeforeAdvice，在进行动态代理时会把AspectJMethodBeforeAdvice转成 MethodBeforeAdviceInterceptor 先执行advice对应的方法 再执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 @After 对应的是AspectJAfterAdvice，直接实现了 MethodInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 再执行advice对应的方法 @Around 对应的是AspectJAroundAdvice，直接实现了 MethodInterceptor 直接执行advice对应的方法，由@Around自己决定要不要继续往后面调用 @AfterThrowing 对应的是AspectJAfterThrowingAdvice，直接实现了 MethodInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 如果上面抛了Throwable，那么则会执行advice对应的方法 @AfterReturning 对应的是AspectJAfterReturningAdvice，在进行动态代理时会把AspectJAfterReturningAdvice转成 AfterReturningAdviceInterceptor 先执行MethodInvocation的proceed()，会执行下一个Interceptor，如果没有下一个Interceptor了，会执行target对应的方法 执行上面的方法后得到最终的方法的返回值 再执行Advice对应的方法 AbstractAdvisorAutoProxyCreatorDefaultAdvisorAutoProxyCreator的父类是AbstractAdvisorAutoProxyCreator。 AbstractAdvisorAutoProxyCreator非常强大以及重要，只要Spring容器中存在这个类型的Bean，就相当于开启了AOP，AbstractAdvisorAutoProxyCreator实际上就是一个BeanPostProcessor，所以在创建某个Bean时，就会进入到它对应的生命周期方法中，比如：在某个Bean初始化之后，会调用wrapIfNecessary()方法进行AOP，底层逻辑是，AbstractAdvisorAutoProxyCreator会找到所有的Advisor，然后判断当前这个Bean是否存在某个Advisor与之匹配（根据Pointcut），如果匹配就表示当前这个Bean有对应的切面逻辑，需要进行AOP，需要产生一个代理对象。 @EnableAspectJAutoProxy这个注解主要就是往Spring容器中添加了一个AnnotationAwareAspectJAutoProxyCreator类型的Bean。AspectJAwareAdvisorAutoProxyCreator继承了AbstractAdvisorAutoProxyCreator，重写了findCandidateAdvisors()方法，AbstractAdvisorAutoProxyCreator只能找到所有Advisor类型的Bean对象，但是AspectJAwareAdvisorAutoProxyCreator除开可以找到所有Advisor类型的Bean对象，还能把@Aspect注解所标注的Bean中的@Before等注解及方法进行解析，并生成对应的Advisor对象。​ 所以，我们可以理解@EnableAspectJAutoProxy，其实就是像Spring容器中添加了一个AbstractAdvisorAutoProxyCreator类型的Bean，从而开启了AOP，并且还会解析@Before等注解生成Advisor。 第八章 事务增强已有类的组合 @EnableTransactionManagement工作原理开启Spring事务本质上就是增加了一个Advisor，但我们使用@EnableTransactionManagement注解来开启Spring事务是，该注解代理的功能就是向Spring容器中添加了两个Bean： AutoProxyRegistrar ProxyTransactionManagementConfiguration AutoProxyRegistrar主要的作用是向Spring容器中注册了一个InfrastructureAdvisorAutoProxyCreator的Bean。而InfrastructureAdvisorAutoProxyCreator继承了AbstractAdvisorAutoProxyCreator，所以这个类的主要作用就是开启自动代理的作用，也就是一个BeanPostProcessor，会在初始化后步骤中去寻找Advisor类型的Bean，并判断当前某个Bean是否有匹配的Advisor，是否需要利用动态代理产生一个代理对象。​ ProxyTransactionManagementConfiguration是一个配置类，它又定义了另外三个bean： BeanFactoryTransactionAttributeSourceAdvisor：一个Advisor AnnotationTransactionAttributeSource：相当于BeanFactoryTransactionAttributeSourceAdvisor中的Pointcut TransactionInterceptor：相当于BeanFactoryTransactionAttributeSourceAdvisor中的Advice AnnotationTransactionAttributeSource就是用来判断某个类上是否存在@Transactional注解，或者判断某个方法上是否存在@Transactional注解的。​ TransactionInterceptor就是代理逻辑，当某个类中存在@Transactional注解时，到时就产生一个代理对象作为Bean，代理对象在执行某个方法时，最终就会进入到TransactionInterceptor的invoke()方法。 Spring事务基本执行原理一个Bean在执行Bean的创建生命周期时，会经过InfrastructureAdvisorAutoProxyCreator的初始化后的方法，会判断当前当前Bean对象是否和BeanFactoryTransactionAttributeSourceAdvisor匹配，匹配逻辑为判断该Bean的类上是否存在@Transactional注解，或者类中的某个方法上是否存在@Transactional注解，如果存在则表示该Bean需要进行动态代理产生一个代理对象作为Bean对象。​ 该代理对象在执行某个方法时，会再次判断当前执行的方法是否和BeanFactoryTransactionAttributeSourceAdvisor匹配，如果匹配则执行该Advisor中的TransactionInterceptor的invoke()方法，执行基本流程为： 利用所配置的PlatformTransactionManager事务管理器新建一个数据库连接 修改数据库连接的autocommit为false 执行MethodInvocation.proceed()方法，简单理解就是执行业务方法，其中就会执行sql 如果没有抛异常，则提交 如果抛了异常，则回滚 Spring事务详细执行流程Spring事务执行流程图：https://www.processon.com/view/link/5fab6edf1e0853569633cc06 Spring事务传播机制在开发过程中，经常会出现一个方法调用另外一个方法，那么这里就涉及到了多种场景，比如a()调用b()： a()和b()方法中的所有sql需要在同一个事务中吗？ a()和b()方法需要单独的事务吗？ a()需要在事务中执行，b()还需要在事务中执行吗？ 等等情况… 所以，这就要求Spring事务能支持上面各种场景，这就是Spring事务传播机制的由来。那Spring事务传播机制是如何实现的呢?​ 先来看上述几种场景中的一种情况，a()在一个事务中执行，调用b()方法时需要新开一个事务执行：​ 首先，代理对象执行a()方法前，先利用事务管理器新建一个数据库连接a 将数据库连接a的autocommit改为false 把数据库连接a设置到ThreadLocal中 执行a()方法中的sql 执行a()方法过程中，调用了b()方法（注意用代理对象调用b()方法） 代理对象执行b()方法前，判断出来了当前线程中已经存在一个数据库连接a了，表示当前线程其实已经拥有一个Spring事务了，则进行挂起 挂起就是把ThreadLocal中的数据库连接a从ThreadLocal中移除，并放入一个挂起资源对象中 挂起完成后，再次利用事务管理器新建一个数据库连接b 将数据库连接b的autocommit改为false 把数据库连接b设置到ThreadLocal中 执行b()方法中的sql b()方法正常执行完，则从ThreadLocal中拿到数据库连接b进行提交 提交之后会恢复所挂起的数据库连接a，这里的恢复，其实只是把在挂起资源对象中所保存的数据库连接a再次设置到ThreadLocal中 a()方法正常执行完，则从ThreadLocal中拿到数据库连接a进行提交 这个过程中最为核心的是：在执行某个方法时，判断当前是否已经存在一个事务，就是判断当前线程的ThreadLocal中是否存在一个数据库连接对象，如果存在则表示已经存在一个事务了。 Spring事务传播机制分类其中，以非事务方式运行，表示以非Spring事务运行，表示在执行这个方法时，Spring事务管理器不会去建立数据库连接，执行sql时，由Mybatis或JdbcTemplate自己来建立数据库连接来执行sql。 案例分析情况112345678910111213141516@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional public void a() &#123; // a方法中的sql &#125;&#125; 默认情况下传播机制为REQUIRED，表示当前如果没有事务则新建一个事务，如果有事务则在当前事务中执行。​ 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 执行conn的commit()方法进行提交 情况2假如是这种情况 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); int result = 100/0; &#125; @Transactional public void a() &#123; // a方法中的sql &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 抛出异常 执行conn的rollback()方法进行回滚，所以两个方法中的sql都会回滚掉 情况3假如是这种情况： 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional public void a() &#123; // a方法中的sql int result = 100/0; &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 执行a方法中的sql 抛出异常 执行conn的rollback()方法进行回滚，所以两个方法中的sql都会回滚掉 情况4如果是这种情况： 1234567891011121314151617@Componentpublic class UserService &#123; @Autowired private UserService userService; @Transactional public void test() &#123; // test方法中的sql userService.a(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW) public void a() &#123; // a方法中的sql int result = 100/0; &#125;&#125; 所以上面这种情况的执行流程如下： 新建一个数据库连接conn 设置conn的autocommit为false 执行test方法中的sql 又新建一个数据库连接conn2 执行a方法中的sql 抛出异常 执行conn2的rollback()方法进行回滚 继续抛异常，对于test()方法而言，它会接收到一个异常，然后抛出 执行conn的rollback()方法进行回滚，最终还是两个方法中的sql都回滚了 Spring事务强制回滚正常情况下，a()调用b()方法时，如果b()方法抛了异常，但是在a()方法捕获了，那么a()的事务还是会正常提交的，但是有的时候，我们捕获异常可能仅仅只是不把异常信息返回给客户端，而是为了返回一些更友好的错误信息，而这个时候，我们还是希望事务能回滚的，那这个时候就得告诉Spring把当前事务回滚掉，做法就是： 12345678910111213141516@Transactionalpublic void test()&#123; // 执行sql try &#123; b(); &#125; catch (Exception e) &#123; // 构造友好的错误信息返回 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); &#125; &#125;public void b() throws Exception &#123; throw new Exception();&#125; TransactionSynchronizationSpring事务有可能会提交，回滚、挂起、恢复，所以Spring事务提供了一种机制，可以让程序员来监听当前Spring事务所处于的状态。​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990@Componentpublic class UserService &#123; @Autowired private JdbcTemplate jdbcTemplate; @Autowired private UserService userService; @Transactional public void test()&#123; TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() &#123; @Override public void suspend() &#123; System.out.println(&quot;test被挂起了&quot;); &#125; @Override public void resume() &#123; System.out.println(&quot;test被恢复了&quot;); &#125; @Override public void beforeCommit(boolean readOnly) &#123; System.out.println(&quot;test准备要提交了&quot;); &#125; @Override public void beforeCompletion() &#123; System.out.println(&quot;test准备要提交或回滚了&quot;); &#125; @Override public void afterCommit() &#123; System.out.println(&quot;test提交成功了&quot;); &#125; @Override public void afterCompletion(int status) &#123; System.out.println(&quot;test提交或回滚成功了&quot;); &#125; &#125;); jdbcTemplate.execute(&quot;insert into t1 values(1,1,1,1,&#x27;1&#x27;)&quot;); System.out.println(&quot;test&quot;); userService.a(); &#125; @Transactional(propagation = Propagation.REQUIRES_NEW) public void a()&#123; TransactionSynchronizationManager.registerSynchronization(new TransactionSynchronization() &#123; @Override public void suspend() &#123; System.out.println(&quot;a被挂起了&quot;); &#125; @Override public void resume() &#123; System.out.println(&quot;a被恢复了&quot;); &#125; @Override public void beforeCommit(boolean readOnly) &#123; System.out.println(&quot;a准备要提交了&quot;); &#125; @Override public void beforeCompletion() &#123; System.out.println(&quot;a准备要提交或回滚了&quot;); &#125; @Override public void afterCommit() &#123; System.out.println(&quot;a提交成功了&quot;); &#125; @Override public void afterCompletion(int status) &#123; System.out.println(&quot;a提交或回滚成功了&quot;); &#125; &#125;); jdbcTemplate.execute(&quot;insert into t1 values(2,2,2,2,&#x27;2&#x27;)&quot;); System.out.println(&quot;a&quot;); &#125;&#125; 第九章 Spring MVC请求流程源码Spring集成Sring MVC (无SpringBoot) 1234567public class MyApplicationInitializer implements WebApplicationInitializer &#123; @Override public void onStartup(ServletContext servletContext) throws ServletException &#123; //注册DispatcherServlet到容器里 &#125;&#125;// todo 启动Tomcat Servlet规范定义接口,tomcat通过SPI机制加载 实现是 @HandlerTypes(感兴趣的类) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@HandlesTypes(WebApplicationInitializer.class)public class SpringServletContainerInitializer implements ServletContainerInitializer &#123; /** * Delegate the &#123;@code ServletContext&#125; to any &#123;@link WebApplicationInitializer&#125; * implementations present on the application classpath. * &lt;p&gt;Because this class declares @&#123;@code HandlesTypes(WebApplicationInitializer.class)&#125;, * Servlet 3.0+ containers will automatically scan the classpath for implementations * of Spring&#x27;s &#123;@code WebApplicationInitializer&#125; interface and provide the set of all * such types to the &#123;@code webAppInitializerClasses&#125; parameter of this method. * &lt;p&gt;If no &#123;@code WebApplicationInitializer&#125; implementations are found on the classpath, * this method is effectively a no-op. An INFO-level log message will be issued notifying * the user that the &#123;@code ServletContainerInitializer&#125; has indeed been invoked but that * no &#123;@code WebApplicationInitializer&#125; implementations were found. * &lt;p&gt;Assuming that one or more &#123;@code WebApplicationInitializer&#125; types are detected, * they will be instantiated (and &lt;em&gt;sorted&lt;/em&gt; if the @&#123;@link * org.springframework.core.annotation.Order @Order&#125; annotation is present or * the &#123;@link org.springframework.core.Ordered Ordered&#125; interface has been * implemented). Then the &#123;@link WebApplicationInitializer#onStartup(ServletContext)&#125; * method will be invoked on each instance, delegating the &#123;@code ServletContext&#125; such * that each instance may register and configure servlets such as Spring&#x27;s * &#123;@code DispatcherServlet&#125;, listeners such as Spring&#x27;s &#123;@code ContextLoaderListener&#125;, * or any other Servlet API componentry such as filters. * @param webAppInitializerClasses all implementations of * &#123;@link WebApplicationInitializer&#125; found on the application classpath * @param servletContext the servlet context to be initialized * @see WebApplicationInitializer#onStartup(ServletContext) * @see AnnotationAwareOrderComparator */ @Override public void onStartup(@Nullable Set&lt;Class&lt;?&gt;&gt; webAppInitializerClasses, ServletContext servletContext) throws ServletException &#123; List&lt;WebApplicationInitializer&gt; initializers = Collections.emptyList();// 这里面会有我们自己写的myWebApplicationInitializer if (webAppInitializerClasses != null) &#123; initializers = new ArrayList&lt;&gt;(webAppInitializerClasses.size()); for (Class&lt;?&gt; waiClass : webAppInitializerClasses) &#123; // Be defensive: Some servlet containers provide us with invalid classes, // no matter what @HandlesTypes says... if (!waiClass.isInterface() &amp;&amp; !Modifier.isAbstract(waiClass.getModifiers()) &amp;&amp; WebApplicationInitializer.class.isAssignableFrom(waiClass)) &#123; try &#123; initializers.add((WebApplicationInitializer) ReflectionUtils.accessibleConstructor(waiClass).newInstance()); &#125; catch (Throwable ex) &#123; throw new ServletException(&quot;Failed to instantiate WebApplicationInitializer class&quot;, ex); &#125; &#125; &#125; &#125; if (initializers.isEmpty()) &#123; servletContext.log(&quot;No Spring WebApplicationInitializer types detected on classpath&quot;); return; &#125; servletContext.log(initializers.size() + &quot; Spring WebApplicationInitializers detected on classpath&quot;); AnnotationAwareOrderComparator.sort(initializers); for (WebApplicationInitializer initializer : initializers) &#123; initializer.onStartup(servletContext); &#125; &#125;&#125; SpringMVC —请求源码流程 有道云链接：http://note.youdao.com/noteshare?id=ec3ca523300ad31d7f6f673b9e92bbeb&amp;sub=1D1BF1D55D0148879F53878CB24F8214 ​ SpringMVC —请求源码流程 ​ 前言 ​ 从Servlet到SpringMVC ​ 传统Servlet： ​ SpringMVC ​ SpringMVC的具体执行流程： ​ HandlerMapping 前言 Spring官网的MVC模块介绍： Spring Web MVC is the original web framework built on the Servlet API and has been included in the Spring Framework from the very beginning. The formal name, “Spring Web MVC,” comes from the name of its source module (spring-webmvc), but it is more commonly known as “Spring MVC”. Spring Web MVC是基于Servlet API构建的原始Web框架，从一开始就已包含在Spring框架中。正式名称“ Spring Web MVC”来自其源模块的名称（spring-webmvc），但它通常被称为“ Spring MVC”。 从Servlet到SpringMVC 最典型的MVC就是JSP + servlet + javabean的模式。 传统Servlet： ​ 弊端： 1.xml下配置servlet的映射非常麻烦 开发效率低 2.必须要继承父类、重写方法 侵入性强 2.如果想在一个Servlet中处理同一业务模块的的功能分发给不同方法进行处理非常麻烦 3.参数解析麻烦:单个参数（转换类型）—&gt;pojo对象 Json文本—&gt;pojo对象 4.**数据响应麻烦:**pojo对象—&gt;json … Content-type 5.跳转页面麻烦, 对path的控制、 如果使用其他模板也很麻烦 、设置编码麻烦…等等… 所以SpringMVC 就是在Servlet的基础上进行了封装，帮我把这些麻烦事都给我们做了。 Web框架的升级是一个不断偷懒的过程 从最开始的Servlet到现在的SpringMVC、SpringBoot等等 SpringMVC 基于xml的实现方式： 1.给Servlet容器配置一个DispatcherServlet（web.xml ) 2.添加SpringMVC的配置信息 继承类&#x2F;实现接口 方式： ​ implements HttpRequestHandler 不同的HandlerMapping ​ simpleController 注解方式： 配置控制器@Controller和处理方法的映射—@RequstMapping 即可 其实SpringMVC请求原理很简单：说白了就是用一个DispatcherServlet 封装了一个Servlet的调度中心， 由调度中心帮我们调用我们的处理方法： 在这个过程中调度中心委托给各个组件执行具体工作 ，比如帮我们映射方法请求、帮我解析参数、调用处理方法、响应数据和页面 等 这就相当于你在家自己做饭和去饭店吃饭的区别了， 在家你买菜、洗菜、蒸饭、炒菜、洗碗都得自己来. 饭店都给你做好了， 你只要分服务员说你吃什么、就能得到响应. 殊不知呢， 你只是说了吃什么（请求）， 后厨（DispatcherServlet）就有配菜员你给找到菜单-对应的食材（映射） 、切菜员切菜（解析参数）、 厨师给你炒菜（调用处理方法）、装盘（处理返回值)、 抄完给你端出来（响应） SpringMVC的具体执行流程： Spring MVC 是围绕前端控制器模式设计的，其中：中央 Servlet DispatcherServlet 为请求处理流程提供统一调度，实际工作则交给可配置组件执行。这个模型是灵活的且开放的，我们可以通过自己去定制这些组件从而进行定制自己的工作流。 ​ DispatcherServlet： 前端调度器 ， 负责将请求拦截下来分发到各控制器方法中 HandlerMapping: 负责根据请求的URL和配置@RequestMapping映射去匹配， 匹配到会返回Handler（具体控制器的方法） HandlerAdaper: 负责调用Handler-具体的方法- 返回视图的名字 Handler将它封装到ModelAndView(封装视图名，request域的数据） ViewReslover: 根据ModelAndView里面的视图名地址去找到具体的jsp封装在View对象中 View：进行视图渲染（将jsp转换成html内容 –这是Servlet容器的事情了） 最终response到的客户端 用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用处理器映射器HandlerMapping。 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter,执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 执行处理器Handler(Controller，也叫页面控制器)。 Handler执行完成返回ModelAndView HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 DispatcherServlet响应用户。 整个调用过程其实都在doDispatch中体现了： 用户发送请求至前端控制器DispatcherServlet 由于它是个Servlet会先进入service方法——&gt;doGet&#x2F;doPost——&gt;processRequestdoService——&gt;doDispatch ↓ 这个doDispatch非常重要–体现了整个请求流程 ​ protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception { try { try { &#x2F;&#x2F; 文件上传相关 processedRequest &#x3D; checkMultipart(request); multipartRequestParsed &#x3D; (processedRequest !&#x3D; request); &#x2F;&#x2F; DispatcherServlet收到请求调用处理器映射器HandlerMapping。 &#x2F;&#x2F; 处理器映射器根据请求url找到具体的处理器，生成处理器执行链HandlerExecutionChain(包括处理器对象和处理器拦截器)一并返回给DispatcherServlet。 mappedHandler &#x3D; getHandler(processedRequest); if (mappedHandler &#x3D;&#x3D; null) { noHandlerFound(processedRequest, response); return; } 4.DispatcherServlet根据处理器Handler获取处理器适配器HandlerAdapter, HandlerAdapter ha &#x3D; getHandlerAdapter(mappedHandler.getHandler()); &#x2F;&#x2F; Process last-modified header, if supported by the handler. HTTP缓存相关 String method &#x3D; request.getMethod(); boolean isGet &#x3D; HttpMethod.GET.matches(method); if (isGet || HttpMethod.HEAD.matches(method)) { long lastModified &#x3D; ha.getLastModified(request, mappedHandler.getHandler()); if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) { return; } } &#x2F;&#x2F; 前置拦截器 if (!mappedHandler.applyPreHandle(processedRequest, response)) { &#x2F;&#x2F; 返回false就不进行后续处理了 return; } &#x2F;&#x2F; 执行HandlerAdapter处理一系列的操作，如：参数封装，数据格式转换，数据验证等操作 &#x2F;&#x2F; 执行处理器Handler(Controller，也叫页面控制器)。 &#x2F;&#x2F; Handler执行完成返回ModelAndView &#x2F;&#x2F; HandlerAdapter将Handler执行结果ModelAndView返回到DispatcherServlet mv &#x3D; ha.handle(processedRequest, response, mappedHandler.getHandler()); if (asyncManager.isConcurrentHandlingStarted()) { return; } &#x2F;&#x2F; 如果没有视图，给你设置默认视图 json忽略 applyDefaultViewName(processedRequest, mv); &#x2F;&#x2F;后置拦截器 mappedHandler.applyPostHandle(processedRequest, response, mv); } catch (Exception ex) { dispatchException &#x3D; ex; } catch (Throwable err) { &#x2F;&#x2F; As of 4.3, we’re processing Errors thrown from handler methods as well, &#x2F;&#x2F; making them available for @ExceptionHandler methods and other scenarios. dispatchException &#x3D; new NestedServletException(“Handler dispatch failed”, err); } &#x2F;&#x2F; DispatcherServlet将ModelAndView传给ViewReslover视图解析器 &#x2F;&#x2F; ViewReslover解析后返回具体View &#x2F;&#x2F; DispatcherServlet对View进行渲染视图（即将模型数据model填充至视图中）。 &#x2F;&#x2F; DispatcherServlet响应用户。 processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); } catch (Exception ex) { triggerAfterCompletion(processedRequest, response, mappedHandler, ex); } catch (Throwable err) { triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(“Handler processing failed”, err)); } finally { if (asyncManager.isConcurrentHandlingStarted()) { &#x2F;&#x2F; Instead of postHandle and afterCompletion if (mappedHandler !&#x3D; null) { mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); } } else { &#x2F;&#x2F; Clean up any resources used by a multipart request. if (multipartRequestParsed) { cleanupMultipart(processedRequest); } } } 详细过程我们课程中分析…. HandlerMapping 在整个过程中，涉及到非常多的组件，每个组件解析各个环节，其中HandlerMapping最为重要它是用来映射请求的，我们就着重介绍下HandlerMapping的解析过程和请求映射过程： 附上流程图： https://www.processon.com/view/link/615ea79e1efad4070b2d6707 ​ 第十章 Spring MVC父子容器1、Spring整合SpringMVC 特性： 说到Spring整合SpringMVC唯一的体现就是父子容器： 通常我们会设置父容器（Spring）管理Service、Dao层的Bean, 子容器(SpringMVC)管理Controller的Bean . 子容器可以访问父容器的Bean, 父容器无法访问子容器的Bean。 实现： 也就是零配置（零xml）的放式来说明SpringMVC的原理！！ 此方式作为我们本文重点介绍，也是很多人缺失的一种方式， 其实早在Spring3+就已经提供， 只不过我们直到SpringBoot才使用该方式进行自动配置， 这也是很多人从xml调到SpringBoot不适应的原因， 因为你缺失了这个版本。 所以我们以这种方式作为源码切入点既可以理解到XML的方式又能兼顾到SpringBoot的方式 。 3、实现基于SPI规范的SpringMVC TulingStarterInitializer 此类继承AbstractAnnotationConfigDispatcherServletInitializer 这是个啥？ 待会我们讲原理来介绍 getRootConfigClasses 提供父容器的配置类 getServletConfigClasses 提供子容器的配置类 getServletMappings 设置DispatcherServlet的映射 ​ public class TulingStarterInitializer extends AbstractAnnotationConfigDispatcherServletInitializer { &#x2F;** * 方法实现说明:IOC 父容器的启动类 * @author:xsls * @date:2019&#x2F;7&#x2F;31 22:12 &#x2F; @Override protected Class&lt;?&gt;[] getRootConfigClasses() { return new Class[]{RootConfig.class}; } &#x2F;* * 方法实现说明 IOC子容器配置 web容器配置 * @author:xsls * @date:2019&#x2F;7&#x2F;31 22:12 &#x2F; @Override protected Class&lt;?&gt;[] getServletConfigClasses() { return new Class[]{WebAppConfig.class}; } &#x2F;* * 方法实现说明 * @author:xsls * @return: 我们前端控制器DispatcherServlet的拦截路径 * @exception: * @date:2019&#x2F;7&#x2F;31 22:16 *&#x2F; @Override protected String[] getServletMappings() { return new String[]{“&#x2F;“}; RootConfig 父容器的配置类 &#x3D;以前的spring.xml 扫描的包排除掉@Controller ​ @Configuration @ComponentScan(basePackages &#x3D; “com.tuling”,excludeFilters &#x3D; { @ComponentScan.Filter(type &#x3D; FilterType.ANNOTATION,value&#x3D;{RestController.class,Controller.class}), @ComponentScan.Filter(type &#x3D; ASSIGNABLE_TYPE,value &#x3D;WebAppConfig.class ), }) public class RootConfig { WebAppConfig 子容器的配置类 &#x3D;以前的spring-mvc.xml 扫描的包：包含掉@Controller ​ @Configuration @ComponentScan(basePackages &#x3D; {“com.tuling”},includeFilters &#x3D; { @ComponentScan.Filter(type &#x3D; FilterType.ANNOTATION,value &#x3D; {RestController.class, Controller.class}) },useDefaultFilters &#x3D;false) @EnableWebMvc &#x2F;&#x2F; ≈mvc:annotation-driven/ public class WebAppConfig implements WebMvcConfigurer{ &#x2F;** * 配置拦截器 * @return &#x2F; @Bean public TulingInterceptor tulingInterceptor() { return new TulingInterceptor(); } &#x2F;* * 文件上传下载的组件 * @return &#x2F; @Bean public MultipartResolver multipartResolver() { CommonsMultipartResolver multipartResolver &#x3D; new CommonsMultipartResolver(); multipartResolver.setDefaultEncoding(“UTF-8”); multipartResolver.setMaxUploadSize(1024102410); return multipartResolver; } &#x2F;* * 注册处理国际化资源的组件 * @return &#x2F; &#x2F; @Bean public AcceptHeaderLocaleResolver localeResolver() { AcceptHeaderLocaleResolver acceptHeaderLocaleResolver &#x3D; new AcceptHeaderLocaleResolver(); return acceptHeaderLocaleResolver; }&#x2F; @Override public void addInterceptors(InterceptorRegistry registry) { registry.addInterceptor(tulingInterceptor()).addPathPatterns(“&#x2F;“); } &#x2F;** * 方法实现说明:配置试图解析器 * @author:xsls * @exception: * @date:2019&#x2F;8&#x2F;6 16:23 *&#x2F; @Bean public InternalResourceViewResolver internalResourceViewResolver() { InternalResourceViewResolver viewResolver &#x3D; new InternalResourceViewResolver(); viewResolver.setSuffix(“.jsp”); viewResolver.setPrefix(“&#x2F;WEB-INF&#x2F;jsp&#x2F;“); return viewResolver; } @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) { converters.add(new MappingJackson2HttpMessageConverter()); } 自己去添加个Controller进行测试 OK， 现在可以访问你的SpringMVC了 4、SPI的方式****SpringMVC启动原理 接着我们来看看SPI方式的原理是什么： SpringMVC 大致可以分为 启动 和请求 2大部分， 所以我们本文先研究启动部分 流程图： ​ 源码流程 外置Tomcat启动的时候通过SPI 找到我们应用中的&#x2F;META-INF&#x2F;service&#x2F;javax.servlet.ServletContainerInitializer ​ 调用SpringServletContainerInitializer.onStartUp() ​ 调用onStartUp()前会先找到@HandlesTypes(WebApplicationInitializer.class) 所有实现了WebApplicationInitializer的类，传入到OnStartup的webAppInitializerClasses参数中，并传入Servlet上下文对象。 重点关注这组类：他们组成了父子容器 ​ 找到所有WebApplicationInitializer的实现类后， 不是接口、不是抽象则通过反射进行实例化（所以，你会发现内部实现类都是抽象的，你想让其起作用我们必须添加一个自定义实现类，在下文提供我的自定义实现类） 调用所有上一步实例化后的对象的onStartup方法 ​ ​ \\1. 首先来到AbstractDispatcherServletInitializer#onStartup再执行super.onStartup(servletContext); ​ @Override public void onStartup(ServletContext servletContext) throws ServletException { &#x2F;&#x2F;实例化我们的spring root上下文 super.onStartup(servletContext); &#x2F;&#x2F;注册我们的DispatcherServlet 创建我们spring web 上下文对象 registerDispatcherServlet(servletContext); 创建父容器——ContextLoaderListener 2.父类AbstractContextLoaderInitializer#onStartup执行registerContextLoaderListener(servletContext); createRootApplicationContext()该方法中会创建父容器 该方法是抽象方法，实现类是AbstractAnnotationConfigDispatcherServletInitializer 调用getRootConfigClasses();方法获取父容器配置类（此抽象方法在我们自定义的子类中实现提供我们自定义的映射路径 ） 创建父容器，注册配置类 ​ 会创建ContextLoaderListener并通过ServletContext注册 ​ 看完大家是不是感觉跟我们XML的配置ContextLoaderListener对上了： ​ 创建子容器——DispatcherServlet 3.回到AbstractDispatcherServletInitializer#onStartup再执行registerDispatcherServlet(servletContext); ​ registerDispatcherServlet方法说明： 调用createServletApplicationContext创建子容器 该方法是抽象方法，实现类是AbstractAnnotationConfigDispatcherServletInitializer 创建子容器（下图很明显不多介绍） 调用抽象方法：getServletConfigClasses();获得配置类（此抽象方法在我们自定义的子类中实现提供我们自定义的配置类 ） 配置类除了可以通过ApplicationContext()构造函数的方式传入 ， 也可以通过这种方式动态添加，不知道了吧~ ​ 调用createDispatcherServlet(servletAppContext);创建DispatcherServlet 设置启动时加载：registration.setLoadOnStartup(1); 调用抽象方法设置映射路径：getServletMappings()（此抽象方法在我们自定义的子类中实现提供我们自定义的映射路径 ） 看完大家是不是感觉跟我们XML的配置DispatcherServlet对上了 ​ 4. 初始化ContextLoaderListener ​ ContextLoaderListener加载过程比较简单： 外置tomcat会帮我们调用ContextLoaderListener#contextInitialized 进行初始化 xml的方式下会判断容器为空时创建父容器 在里面会调用父容器的refresh方法加载 将父容器存入到Servlet域中供子容器使用 ​ 5. 初始化DispatcherServlet ​ 可以看到流程比ContextLoaderListener流程更多 外置tomcat会帮我们调用DispatcherServlet#init() 进行初始化—&gt;重点关注：initWebApplicationContext方法 getWebApplicationContext(getServletContext())获得父容器（从之前的Servlet域中拿到） cwac.setParent(rootContext);给子容器设置父容器 调用configureAndRefreshWebApplicationContext(cwac); ​ 注册一个监听器（该监听会初始化springmvc所需信息） ContextRefreshedEvent可以看到该监听器监听的是容器refreshed事件， 会在finishRefresh中发布 刷新容器 ​ 当执行refresh 即加载ioc容器 完了会调用finishRefresh(): publishEvent(new ContextRefreshedEvent(this));发布ContextRefreshedEvent事件 触发上面的ContextRefreshListener监听器： —-&gt;FrameworkServlet.this.onApplicationEvent(event); ——–&gt;onRefresh(event.getApplicationContext()); ————–&gt;initStrategies(context); ​ protected void initStrategies(ApplicationContext context) { &#x2F;&#x2F;初始化我们web上下文对象的 用于文件上传下载的解析器对象 initMultipartResolver(context); &#x2F;&#x2F;初始化我们web上下文对象用于处理国际化资源的 initLocaleResolver(context); &#x2F;&#x2F;主题解析器对象初始化 initThemeResolver(context); &#x2F;&#x2F;初始化我们的HandlerMapping initHandlerMappings(context); &#x2F;&#x2F;实例化我们的HandlerAdapters initHandlerAdapters(context); &#x2F;&#x2F;实例化我们处理器异常解析器对象 initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); &#x2F;&#x2F;给DispatcherSerlvet的ViewResolvers处理器 initViewResolvers(context); initFlashMapManager(context); 这里面的每一个方法不用太细看， 就是给SpringMVC准备初始化的数据， 为后续SpringMVC处理请求做准备 基本都是从容器中拿到已经配置的Bean（RequestMappingHandlerMapping、RequestMappingHandlerAdapter、HandlerExceptionResolver ）放到dispatcherServlet中做准备: ​ ​ ​ … 但是这些Bean又是从哪来的呢？？ 来来来， 回到我们的WebAppConfig 我们使用的一个@EnableWebMvc 导入了DelegatingWebMvcConfiguration@Import(DelegatingWebMvcConfiguration.class) DelegatingWebMvcConfiguration的父类就配置了这些Bean 而且我告诉你SpringBoot也是用的这种方式， ​ 总结 Tomcat在启动时会通过SPI注册 ContextLoaderListener和DispatcherServlet对象 同时创建父子容器 分别创建在ContextLoaderListener初始化时创建父容器设置配置类 在DispatcherServlet初始化时创建子容器 即2个ApplicationContext实例设置配置类 Tomcat在启动时执行ContextLoaderListener和DispatcherServlet对象的初始化方法， 执行容器refresh进行加载 在子容器加载时 创建SpringMVC所需的Bean和预准备的数据：(通过配置类+@EnableWebMvc配置（DelegatingWebMvcConfiguration）——可实现WebMvcConfigurer进行定制扩展） RequestMappingHandlerMapping，它会处理@RequestMapping 注解 RequestMappingHandlerAdapter，则是处理请求的适配器，确定调用哪个类的哪个方法，并且构造方法参数，返回值。 HandlerExceptionResolver 错误视图解析器 addDefaultHttpMessageConverters 添加默认的消息转换器（解析json、解析xml） 等…. 子容器需要注入父容器的Bean时（比如Controller中需要@Autowired Service的Bean）; 会先从子容器中找，没找到会去父容器中找： 详情见AbstractBeanFactory#doGetBean方法 ​ &#x2F;** * 一般情况下,只有Spring 和SpringMvc整合的时才会有父子容器的概念, * 作用： * 比如我们的Controller中注入Service的时候，发现我们依赖的是一个引用对象，那么他就会调用getBean去把service找出来 * 但是当前所在的容器是web子容器，那么就会在这里的 先去父容器找 *&#x2F; BeanFactory parentBeanFactory &#x3D; getParentBeanFactory(); &#x2F;&#x2F;若存在父工厂,且当前的bean工厂不存在当前的bean定义,那么bean定义是存在于父beanFacotry中 if (parentBeanFactory !&#x3D; null &amp;&amp; !containsBeanDefinition(beanName)) { &#x2F;&#x2F;获取bean的原始名称 String nameToLookup &#x3D; originalBeanName(name); &#x2F;&#x2F;若为 AbstractBeanFactory 类型，委托父类处理 if (parentBeanFactory instanceof AbstractBeanFactory) { return ((AbstractBeanFactory) parentBeanFactory).doGetBean( nameToLookup, requiredType, args, typeCheckOnly); } else if (args !&#x3D; null) { &#x2F;&#x2F; 委托给构造函数 getBean() 处理 return (T) parentBeanFactory.getBean(nameToLookup, args); } else { &#x2F;&#x2F; 没有 args，委托给标准的 getBean() 处理 return parentBeanFactory.getBean(nameToLookup, requiredType); } 用几道面试题做个总结: Spring和SpringMVC为什么需要父子容器？不要不行吗？ 就实现层面来说不用子父容器也可以完成所需功能（参考：SpringBoot就没用子父容器） 所以父子容器的主要作用应该是早期Spring为了划分框架边界。有点单一职责的味道。service、dao层我们一般使用spring框架来管理、controller层交给springmvc管理 规范整体架构 使 父容器service无法访问子容器controller、子容器controller可以访问父容器 service 方便子容器的切换。如果现在我们想把web层从spring mvc替换成struts，那么只需要将spring-mvc.xml替换成Struts的配置文件struts.xml即可，而spring-core.xml不需要改变。 为了节省重复bean创建 是否可以把所有Bean都通过Spring容器来管理？（Spring的applicationContext.xml中配置全局扫描) 不可以，这样会导致我们请求接口的时候产生404。 如果所有的Bean都交给父容器，SpringMVC在初始化HandlerMethods的时候（initHandlerMethods）无法根据Controller的handler方法注册HandlerMethod，并没有去查找父容器的bean； 也就无法根据请求URI 获取到 HandlerMethod来进行匹配. ​ 是否可以把我们所需的Bean都放入Spring-mvc子容器里面来管理（springmvc的spring-servlet.xml中配置全局扫描）? 可以 ， 因为父容器的体现无非是为了获取子容器不包含的bean, 如果全部包含在子容器完全用不到父容器了， 所以是可以全部放在springmvc子容器来管理的。 虽然可以这么做不过一般应该是不推荐这么去做的，一般人也不会这么干的。如果你的项目里有用到事物、或者aop记得也需要把这部分配置需要放到Spring-mvc子容器的配置文件来，不然一部分内容在子容器和一部分内容在父容器,可能就会导致你的事物或者AOP不生效。 所以如果aop或事物如果不生效也有可能是通过父容器(spring)去增强子容器(Springmvc)，也就无法增强 这也是很多同学会遇到的问题。 第十一章 Mybatis本章着重介绍MyBatis执行Sql的流程，关于在执行过程中缓存、动态SQl生成等细节不在本章中体现 还是以之前的查询作为列子： &#x2F;*** * @Author 徐庶 QQ:1092002729 * @Slogan 致敬大师，致敬未来的你 *&#x2F; public class App { public static void main(String[] args) { String resource &#x3D; “mybatis-config.xml”; Reader reader; try { &#x2F;&#x2F;将XML配置文件构建为Configuration配置类 reader &#x3D; Resources.getResourceAsReader(resource); &#x2F;&#x2F; 通过加载配置文件流构建一个SqlSessionFactory DefaultSqlSessionFactory SqlSessionFactory sqlMapper &#x3D; new SqlSessionFactoryBuilder().build(reader); &#x2F;&#x2F; 数据源 执行器 DefaultSqlSession SqlSession session &#x3D; sqlMapper.openSession(); try { &#x2F;&#x2F; 执行查询 底层执行jdbc &#x2F;&#x2F;User user &#x3D; (User)session.selectOne(“com.tuling.mapper.selectById”, 1); UserMapper mapper &#x3D; session.getMapper(UserMapper.class); System.out.println(mapper.getClass()); User user &#x3D; mapper.selectById(1L); System.out.println(user.getUserName()); } catch (Exception e) { e.printStackTrace(); }finally { session.close(); } } catch (IOException e) { e.printStackTrace(); } } } 之前提到拿到sqlSession之后就能进行各种CRUD操作了，所以我们就从sqlSession.getMapper这个方法开始分析，看下整个Sql的执行流程是怎么样的。 openSession的过程: Copy private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx &#x3D; null; try { final Environment environment &#x3D; configuration.getEnvironment(); final TransactionFactory transactionFactory &#x3D; getTransactionFactoryFromEnvironment(environment); tx &#x3D; transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); &#x2F;&#x2F;获取执行器，这边获得的执行器已经代理拦截器的功能（见下面代码） final Executor executor &#x3D; configuration.newExecutor(tx, execType); &#x2F;&#x2F;根据获取的执行器创建SqlSession return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); &#x2F;&#x2F; may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(“Error opening session. Cause: “ + e, e); } finally { ErrorContext.instance().reset(); } } Copy &#x2F;&#x2F;interceptorChain生成代理类，具体参见Plugin这个类的方法 public Executor newExecutor(Transaction transaction, ExecutorType executorType) { executorType &#x3D; executorType &#x3D;&#x3D; null ? defaultExecutorType : executorType; executorType &#x3D; executorType &#x3D;&#x3D; null ? ExecutorType.SIMPLE : executorType; Executor executor; if (ExecutorType.BATCH &#x3D;&#x3D; executorType) { executor &#x3D; new BatchExecutor(this, transaction); } else if (ExecutorType.REUSE &#x3D;&#x3D; executorType) { executor &#x3D; new ReuseExecutor(this, transaction); } else { executor &#x3D; new SimpleExecutor(this, transaction); } if (cacheEnabled) { executor &#x3D; new CachingExecutor(executor); } executor &#x3D; (Executor) interceptorChain.pluginAll(executor); return executor; } Executor分成两大类，一类是CacheExecutor，另一类是普通Executor。 普通Executor又分为三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。 ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map内，供下一次使用。简言之，就是重复使用Statement对象。 BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。 CacheExecutor其实是封装了普通的Executor，和普通的区别是在查询前先会查询缓存中是否存在结果，如果存在就使用缓存中的结果，如果不存在还是使用普通的Executor进行查询，再将查询出来的结果存入缓存。 ​ 到此为止，我们已经获得了SqlSession，拿到SqlSession就可以执行各种CRUD方法了。 简单总结 拿到SqlSessionFactory对象后，会调用SqlSessionFactory的openSesison方法，这个方法会创建一个Sql执行器（Executor），这个Sql执行器会代理你配置的拦截器方法。 获得上面的Sql执行器后，会创建一个SqlSession（默认使用DefaultSqlSession）,这个SqlSession中也包含了Configration对象，所以通过SqlSession也能拿到全局配置； 获得SqlSession对象后就能执行各种CRUD方法了。 SQL的具体执行流程见后续博客。 一些重要类总结： SqlSessionFactory SqlSessionFactoryBuilder SqlSession（默认使用DefaultSqlSession） Executor接口 Plugin、InterceptorChain的pluginAll方法 获取Mapper的流程 进入sqlSession.getMapper方法，会发现调的是Configration对象的getMapper方法： public T getMapper(Class type, SqlSession sqlSession) { &#x2F;&#x2F;mapperRegistry实质上是一个Map，里面注册了启动过程中解析的各种Mapper.xml &#x2F;&#x2F;mapperRegistry的key是接口的Class类型 &#x2F;&#x2F;mapperRegistry的Value是MapperProxyFactory,用于生成对应的MapperProxy（动态代理类） return mapperRegistry.getMapper(type, sqlSession); } 进入getMapper方法： public T getMapper(Class type, SqlSession sqlSession) { final MapperProxyFactory mapperProxyFactory &#x3D; (MapperProxyFactory) knownMappers.get(type); &#x2F;&#x2F;如果配置文件中没有配置相关Mapper,直接抛异常 if (mapperProxyFactory &#x3D;&#x3D; null) { throw new BindingException(“Type “ + type + “ is not known to the MapperRegistry.”); } try { &#x2F;&#x2F;关键方法 return mapperProxyFactory.newInstance(sqlSession); } catch (Exception e) { throw new BindingException(“Error getting mapper instance. Cause: “ + e, e); } } 进入MapperProxyFactory的newInstance方法： public class MapperProxyFactory { private final Class mapperInterface; private final Map methodCache &#x3D; new ConcurrentHashMap(); public MapperProxyFactory(Class mapperInterface) { this.mapperInterface &#x3D; mapperInterface; } public Class getMapperInterface() { return mapperInterface; } public Map getMethodCache() { return methodCache; } &#x2F;&#x2F;生成Mapper接口的动态代理类MapperProxy，MapperProxy实现了InvocationHandler 接口 @SuppressWarnings(“unchecked”) protected T newInstance(MapperProxy mapperProxy) { return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy); } public T newInstance(SqlSession sqlSession) { final MapperProxy mapperProxy &#x3D; new MapperProxy(sqlSession, mapperInterface, methodCache); return newInstance(mapperProxy); } } 获取Mapper的流程总结如下： ​ Mapper方法的执行流程 下面是动态代理类MapperProxy，调用Mapper接口的所有方法都会先调用到这个代理类的invoke方法（注意由于Mybatis中的Mapper接口没有实现类，所以MapperProxy这个代理对象中没有委托类，也就是说MapperProxy干了代理类和委托类的事情）。好了下面重点看下invoke方法。 &#x2F;&#x2F;MapperProxy代理类 public class MapperProxy implements InvocationHandler, Serializable { private static final long serialVersionUID &#x3D; -6424540398559729838L; private final SqlSession sqlSession; private final Class mapperInterface; private final Map methodCache; public MapperProxy(SqlSession sqlSession, Class mapperInterface, Map methodCache) { this.sqlSession &#x3D; sqlSession; this.mapperInterface &#x3D; mapperInterface; this.methodCache &#x3D; methodCache; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { if (Object.class.equals(method.getDeclaringClass())) { return method.invoke(this, args); } else if (isDefaultMethod(method)) { return invokeDefaultMethod(proxy, method, args); } } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } &#x2F;&#x2F;获取MapperMethod，并调用MapperMethod final MapperMethod mapperMethod &#x3D; cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args); } MapperProxy的invoke方法非常简单，主要干的工作就是创建MapperMethod对象或者是从缓存中获取MapperMethod对象。获取到这个对象后执行execute方法。 所以这边需要进入MapperMethod的execute方法：这个方法判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作。（这边以sqlSession.selectOne这种方式进行分析~） public Object execute(SqlSession sqlSession, Object[] args) { Object result; &#x2F;&#x2F;判断是CRUD那种方法 switch (command.getType()) { case INSERT: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.insert(command.getName(), param)); break; } case UPDATE: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.update(command.getName(), param)); break; } case DELETE: { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; rowCountResult(sqlSession.delete(command.getName(), param)); break; } case SELECT: if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) { executeWithResultHandler(sqlSession, args); result &#x3D; null; } else if (method.returnsMany()) { result &#x3D; executeForMany(sqlSession, args); } else if (method.returnsMap()) { result &#x3D; executeForMap(sqlSession, args); } else if (method.returnsCursor()) { result &#x3D; executeForCursor(sqlSession, args); } else { Object param &#x3D; method.convertArgsToSqlCommandParam(args); result &#x3D; sqlSession.selectOne(command.getName(), param); } break; case FLUSH: result &#x3D; sqlSession.flushStatements(); break; default: throw new BindingException(“Unknown execution method for: “ + command.getName()); } if (result &#x3D;&#x3D; null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) { throw new BindingException(“Mapper method ‘“ + command.getName() + “ attempted to return null from a method with a primitive return type (“ + method.getReturnType() + “).”); } return result; } 详细流程图 https://www.processon.com/view/link/5efc23966376891e81f2a37e sqlSession.selectOne方法会会调到DefaultSqlSession的selectList方法。这个方法获取了获取了MappedStatement对象，并最终调用了Executor的query方法。 public List selectList(String statement, Object parameter, RowBounds rowBounds) { try { MappedStatement ms &#x3D; configuration.getMappedStatement(statement); return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception e) { throw ExceptionFactory.wrapException(“Error querying database. Cause: “ + e, e); } finally { ErrorContext.instance().reset(); } } 然后，通过一层一层的调用（这边省略了缓存操作的环节，会在后面的文章中介绍），最终会来到doQuery方法， 这儿咱们就随便找个Excutor看看doQuery方法的实现吧，我这儿选择了SimpleExecutor: Copy public List doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException { Statement stmt &#x3D; null; try { Configuration configuration &#x3D; ms.getConfiguration(); &#x2F;&#x2F;内部封装了ParameterHandler和ResultSetHandler StatementHandler handler &#x3D; configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt &#x3D; prepareStatement(handler, ms.getStatementLog()); &#x2F;&#x2F;StatementHandler封装了Statement, 让 StatementHandler 去处理 return handler.query(stmt, resultHandler); } finally { closeStatement(stmt); } } 接下来，咱们看看StatementHandler 的一个实现类 PreparedStatementHandler（这也是我们最常用的，封装的是PreparedStatement）, 看看它使怎么去处理的： Copy public List query(Statement statement, ResultHandler resultHandler) throws SQLException { &#x2F;&#x2F;到此，原形毕露， PreparedStatement, 这个大家都已经滚瓜烂熟了吧 PreparedStatement ps &#x3D; (PreparedStatement) statement; ps.execute(); &#x2F;&#x2F;结果交给了ResultSetHandler 去处理,处理完之后返回给客户端 return resultSetHandler. handleResultSets(ps); } 到此，整个调用流程结束。 ​ 简单总结 这边结合获取SqlSession的流程，做下简单的总结： SqlSessionFactoryBuilder解析配置文件，包括属性配置、别名配置、拦截器配置、环境（数据源和事务管理器）、Mapper配置等；解析完这些配置后会生成一个Configration对象，这个对象中包含了MyBatis需要的所有配置，然后会用这个Configration对象创建一个SqlSessionFactory对象，这个对象中包含了Configration对象； 拿到SqlSessionFactory对象后，会调用SqlSessionFactory的openSesison方法，这个方法会创建一个Sql执行器（Executor组件中包含了Transaction对象），这个Sql执行器会代理你配置的拦截器方法。 获得上面的Sql执行器后，会创建一个SqlSession（默认使用DefaultSqlSession）,这个SqlSession中也包含了Configration对象和上面创建的Executor对象，所以通过SqlSession也能拿到全局配置； 获得SqlSession对象后就能执行各种CRUD方法了。 以上是获得SqlSession的流程，下面总结下本博客中介绍的Sql的执行流程： 调用SqlSession的getMapper方法，获得Mapper接口的动态代理对象MapperProxy，调用Mapper接口的所有方法都会调用到MapperProxy的invoke方法（动态代理机制）； MapperProxy的invoke方法中唯一做的就是创建一个MapperMethod对象，然后调用这个对象的execute方法，sqlSession会作为execute方法的入参； 往下，层层调下来会进入Executor组件（如果配置插件会对Executor进行动态代理）的query方法，这个方法中会创建一个StatementHandler对象，这个对象中同时会封装ParameterHandler和ResultSetHandler对象。调用StatementHandler预编译参数以及设置参数值，使用ParameterHandler来给sql设置参数。 Executor组件有两个直接实现类，分别是BaseExecutor和CachingExecutor。CachingExecutor静态代理了BaseExecutor。Executor组件封装了Transction组件，Transction组件中又分装了Datasource组件。 调用StatementHandler的增删改查方法获得结果，ResultSetHandler对结果进行封装转换，请求结束。 Executor、StatementHandler 、ParameterHandler、ResultSetHandler，Mybatis的插件会对上面的四个组件进行动态代理。 Mybatis-插件原理 链接：http://note.youdao.com/noteshare?id=80acf548788cef82ffb924f043241365&amp;sub=FAE1C62BE5C4422EBA80EF27A171C067 重要类 MapperRegistry：本质上是一个Map，其中的key是Mapper接口的全限定名，value的MapperProxyFactory； MapperProxyFactory：这个类是MapperRegistry中存的value值，在通过sqlSession获取Mapper时，其实先获取到的是这个工厂，然后通过这个工厂创建Mapper的动态代理类； MapperProxy：实现了InvocationHandler接口，Mapper的动态代理接口方法的调用都会到达这个类的invoke方法； MapperMethod：判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作； SqlSession：作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能； Executor：MyBatis执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护； StatementHandler:封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler:负责对用户传递的参数转换成JDBC Statement 所需要的参数， ResultSetHandler:负责将JDBC返回的ResultSet结果集对象转换成List类型的集合； TypeHandler:负责java数据类型和jdbc数据类型之间的映射和转换 MappedStatement:MappedStatement维护了一条节点的封装， SqlSource:负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql:表示动态生成的SQL语句以及相应的参数信息 Configuration:MyBatis所有的配置信息都维持在Configuration对象之中。 调试主要关注点 MapperProxy.invoke方法：MyBatis的所有Mapper对象都是通过动态代理生成的，任何方法的调用都会调到invoke方法，这个方法的主要功能就是创建MapperMethod对象，并放进缓存。所以调试时我们可以在这个位置打个断点，看下是否成功拿到了MapperMethod对象，并执行了execute方法。 MapperMethod.execute方法：这个方法会判断你当前执行的方式是增删改查哪一种，并通过SqlSession执行相应的操作。Debug时也建议在此打个断点看下。 DefaultSqlSession.selectList方法：这个方法获取了获取了MappedStatement对象，并最终调用了Executor的query方法； 问题： 1.请介绍下MyBatissql语句的解析过程原理 2.请介绍下MyBatis缓存的原理 3.请介绍下MyBatis插件的原理 第十二章Spring容器(Beanfactory整体架构)BeanFactoryBeanFactory表示Bean工厂，所以很明显，BeanFactory会负责创建Bean，并且提供获取Bean的API。​ 而ApplicationContext是BeanFactory的一种，在Spring源码中，是这么定义的：​ 12345public interface ApplicationContext extends EnvironmentCapable, ListableBeanFactory, HierarchicalBeanFactory, MessageSource, ApplicationEventPublisher, ResourcePatternResolver &#123; ...&#125; 所以，我们可以直接来使用DefaultListableBeanFactory，而不用使用ApplicationContext的某个实现类，比如： 12345678DefaultListableBeanFactory beanFactory = new DefaultListableBeanFactory();AbstractBeanDefinition beanDefinition = BeanDefinitionBuilder.genericBeanDefinition().getBeanDefinition();beanDefinition.setBeanClass(User.class);beanFactory.registerBeanDefinition(&quot;user&quot;, beanDefinition);System.out.println(beanFactory.getBean(&quot;user&quot;)); DefaultListableBeanFactory是非常强大的，支持很多功能，可以通过查看DefaultListableBeanFactory的类继承实现结构来看 这部分现在看不懂没关系，源码熟悉一点后回来再来看都可以。 它实现了很多接口，表示，它拥有很多功能： AliasRegistry：支持别名功能，一个名字可以对应多个别名 BeanDefinitionRegistry：可以注册、保存、移除、获取某个BeanDefinition BeanFactory：Bean工厂，可以根据某个bean的名字、或类型、或别名获取某个Bean对象 SingletonBeanRegistry：可以直接注册、获取某个单例Bean SimpleAliasRegistry：它是一个类，实现了AliasRegistry接口中所定义的功能，支持别名功能 ListableBeanFactory：在BeanFactory的基础上，增加了其他功能，可以获取所有BeanDefinition的beanNames，可以根据某个类型获取对应的beanNames，可以根据某个类型获取{类型：对应的Bean}的映射关系 HierarchicalBeanFactory：在BeanFactory的基础上，添加了获取父BeanFactory的功能 DefaultSingletonBeanRegistry：它是一个类，实现了SingletonBeanRegistry接口，拥有了直接注册、获取某个单例Bean的功能 ConfigurableBeanFactory：在HierarchicalBeanFactory和SingletonBeanRegistry的基础上，添加了设置父BeanFactory、类加载器（表示可以指定某个类加载器进行类的加载）、设置Spring EL表达式解析器（表示该BeanFactory可以解析EL表达式）、设置类型转化服务（表示该BeanFactory可以进行类型转化）、可以添加BeanPostProcessor（表示该BeanFactory支持Bean的后置处理器），可以合并BeanDefinition，可以销毁某个Bean等等功能 FactoryBeanRegistrySupport：支持了FactoryBean的功能 AutowireCapableBeanFactory：是直接继承了BeanFactory，在BeanFactory的基础上，支持在创建Bean的过程中能对Bean进行自动装配 AbstractBeanFactory：实现了ConfigurableBeanFactory接口，继承了FactoryBeanRegistrySupport，这个BeanFactory的功能已经很全面了，但是不能自动装配和获取beanNames ConfigurableListableBeanFactory：继承了ListableBeanFactory、AutowireCapableBeanFactory、ConfigurableBeanFactory AbstractAutowireCapableBeanFactory：继承了AbstractBeanFactory，实现了AutowireCapableBeanFactory，拥有了自动装配的功能 DefaultListableBeanFactory：继承了AbstractAutowireCapableBeanFactory，实现了ConfigurableListableBeanFactory接口和BeanDefinitionRegistry接口，所以DefaultListableBeanFactory的功能很强大 ApplicationContext上面有分析到，ApplicationContext是个接口，实际上也是一个BeanFactory，不过比BeanFactory更加强大，比如：​ HierarchicalBeanFactory：拥有获取父BeanFactory的功能 ListableBeanFactory：拥有获取beanNames的功能 ResourcePatternResolver：资源加载器，可以一次性获取多个资源（文件资源等等） EnvironmentCapable：可以获取运行时环境（没有设置运行时环境功能） ApplicationEventPublisher：拥有广播事件的功能（没有添加事件监听器的功能） MessageSource：拥有国际化功能 具体的功能演示，后面会有。 我们先来看ApplicationContext两个比较重要的实现类： AnnotationConfigApplicationContext ClassPathXmlApplicationContext AnnotationConfigApplicationContext 这部分现在看不懂没关系，源码熟悉一点后回来再来看都可以。 ConfigurableApplicationContext：继承了ApplicationContext接口，增加了，添加事件监听器、添加BeanFactoryPostProcessor、设置Environment，获取ConfigurableListableBeanFactory等功能 AbstractApplicationContext：实现了ConfigurableApplicationContext接口 GenericApplicationContext：继承了AbstractApplicationContext，实现了BeanDefinitionRegistry接口，拥有了所有ApplicationContext的功能，并且可以注册BeanDefinition，注意这个类中有一个属性(DefaultListableBeanFactory beanFactory) AnnotationConfigRegistry：可以单独注册某个为类为BeanDefinition（可以处理该类上的**@Configuration注解，已经可以处理@Bean注解**），同时可以扫描 AnnotationConfigApplicationContext：继承了GenericApplicationContext，实现了AnnotationConfigRegistry接口，拥有了以上所有的功能 ClassPathXmlApplicationContext它也是继承了AbstractApplicationContext，但是相对于AnnotationConfigApplicationContext而言，功能没有AnnotationConfigApplicationContext强大，比如不能注册BeanDefinition 国际化 创建两个国际化文件 一个设置为test &#x3D; a 一个设置为test &#x3D; b 先定义一个MessageSource: 123456@Beanpublic MessageSource messageSource() &#123; ResourceBundleMessageSource messageSource = new ResourceBundleMessageSource(); messageSource.setBasename(&quot;messages&quot;); return messageSource;&#125; 有了这个Bean，你可以在你任意想要进行国际化的地方使用该MessageSource。同时，因为ApplicationContext也拥有国家化的功能，所以可以直接这么用： 1context.getMessage(&quot;test&quot;, null, new Locale(&quot;en&quot;)) 输出 b 实际使用 就是一个类实现ApplicationContextAware 来回调进行使用 资源加载ApplicationContext还拥有资源加载的功能，比如，可以直接利用ApplicationContext获取某个文件的内容： 1234AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);// 获取一个文件Resource resource = context.getResource(&quot;file://D:\\\\IdeaProjects\\\\spring-framework\\\\luban\\\\src\\\\main\\\\java\\\\com\\\\luban\\\\entity\\\\User.java&quot;);System.out.println(resource.contentLength()); // 获得字符长度 你可以想想，如果你不使用ApplicationContext，而是自己来实现这个功能，就比较费时间了。 还比如你可以： 12345678910111213AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);Resource resource = context.getResource(&quot;file://D:\\\\IdeaProjects\\\\spring-framework-5.3.10\\\\tuling\\\\src\\\\main\\\\java\\\\com\\\\zhouyu\\\\service\\\\UserService.java&quot;);System.out.println(resource.contentLength());System.out.println(resource.getFilename());Resource resource1 = context.getResource(&quot;https://www.baidu.com&quot;);System.out.println(resource1.contentLength());System.out.println(resource1.getURL());Resource resource2 = context.getResource(&quot;classpath:spring.xml&quot;);System.out.println(resource2.contentLength());System.out.println(resource2.getURL()); 还可以一次性获取多个： 12345Resource[] resources = context.getResources(&quot;classpath:com/zhouyu/*.class&quot;);for (Resource resource : resources) &#123; System.out.println(resource.contentLength()); System.out.println(resource.getFilename());&#125; 获取运行时环境1234567891011121314151617181920212223AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(AppConfig.class);Map&lt;String, Object&gt; systemEnvironment = context.getEnvironment().getSystemEnvironment();// 系统环境变量System.out.println(systemEnvironment);System.out.println(&quot;=======&quot;);Map&lt;String, Object&gt; systemProperties = context.getEnvironment().getSystemProperties();//java环境变量System.out.println(systemProperties);System.out.println(&quot;=======&quot;);MutablePropertySources propertySources = context.getEnvironment().getPropertySources();// 包含前两者和加的配置文件System.out.println(propertySources);System.out.println(&quot;=======&quot;);// 具体的获取值System.out.println(context.getEnvironment().getProperty(&quot;NO_PROXY&quot;));System.out.println(context.getEnvironment().getProperty(&quot;sun.jnu.encoding&quot;));System.out.println(context.getEnvironment().getProperty(&quot;zhouyu&quot;)); 注意，可以利用 123456加载配置类上面@PropertySource(&quot;classpath:spring.properties&quot;) //创建对应的配置文件MutablePropertySources propertySources = context.getEnvironment().getPropertySources();System.out.println(propertySources); 来使得某个properties文件中的参数添加到运行时环境中 第十三章 实践Spring的使用 与 真正实现的逻辑代码(增强代码&#x2F;业务代码) Bean生命周期(重要紧急)理解各个生命周期的作用与记忆, 依赖注入实现通过Spring提供的组件(解析器等)实现具体Bean后置处理器的逻辑,解析注解等 整合Mybatis其他框架注册Bean到容器中并有些定制处理的实现(BeanDefinition,BeanFacory的增强),解析配置类等等 逻辑: AOP与事务(重要紧急)可涉及到aop,事务,对Bean进行增强 aop: AbstractAdvisorAutoProxyCreator实际上就是一个BeanPostProcessor在初始化后进行增强 逻辑: 使用自带的ProxyFactory生成代理类替换需要增强的类","categories":[],"tags":[{"name":"Spring","slug":"Spring","permalink":"https://gouguoqiang.github.io/tags/Spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gouguoqiang.github.io/tags/Mybatis/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gouguoqiang.github.io/tags/SpringBoot/"}]},{"title":"图灵商城项目2","slug":"tulingnote","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:46:42.511Z","comments":true,"path":"2022/09/01/tulingnote/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/tulingnote/","excerpt":"","text":"项目介绍前后端分离的网上商城后端微服务项目,在这个项目中 嗯，我做的是一个前后端分离的网上商城的微服务项目。我在我在本项目的工作主要是嗯对对实现一个商城的需求进行分析，就是你需要完成哪些功能，比如说你要实行商品的上架，然后你要一个管理员的界面，你必须就有普通用户的界面。管理员是来实现商品的上架呀，就是这是提供给这种工作人员使用的一个界面。嗯普通人员的话，需要去搜索商品，购买商品。购买商品，并且呃上作为商家也会有一些促销的手段，营销的手段来进行。提高交易额，促进消费，然后在这个项目中也会有一些，会有一些广告的。像广告，广告的模块。来来提高你的收入。嗯嗯，具体点来说的话，呃，使用了spring boot加呃，加s加s的。这基础框架，然后再使用了微服务的系列微服务，spring cloud微服务。嗯使用了服务的组件有优瑞卡注册注册中心新open原生调用。嗯，gateway网关等一系列的微服务系列，呃，微服务只是本科的项目的。就本科项目的一块儿内容吧，因为主要是为了熟悉恢复一个微服务项目需要怎么搭建呀，怎么去部署？包括你之间的调用，其实跟传统的项目区别也不是很大，只要你会用工具了之后。你只需要写一些配置，真正一些困难的地方是你去写一些具体的业务，就是你在分析好了你的需求之后，你的需求就是会一步步的，嗯。就是更加符合你你的实际，实际写代码。聊你要去设计库表啊之类的。 嗯，我做的是一个基于space比加mvc加实现的一个前后端分离的网上商城项目。嗯，晚上刷成微服务项使用使用了d收到视频cloud的各种组件，比如说因为他注册中心远程调用和大家都为网关等，嗯，也使用了一些中间键，比如说缓存呀，nx的。这些中间界比如来实现缓存NEX来进行负载均衡呀，等反向代理啊等，然后使用了也使用mk来进行异步操作，流量均分等。本是这样。 我做的是一个基于spring boot实现的网上商城项目。在linux做的系统环境下进行搭建。今天有x系统下进行垃圾使用了来进行缓存，使用了x来进行反向代理，负责均衡等，也有鱼。你学了x使用的是它里面会积一些路啊，使用了一些路啊，脚本等一些东西。完成的本地缓存，并且协同。是spring cloud的微服务各种主线来完成。没有服务的拆分呀，远程调用网关过滤之类的。嗯，采用来进行持久化。嗯的完成的功能呢主要就是一个商城的常见功能，比如说搜索呀，商品的上架，下架。就是有会有一个前前台的功能，会有一个提供给普通用户的一个界面，就是用户的登录啊。用户的灯注册，注册登录。给我搜索商品购买商品，下单，加入购物车的一系列基础的商城功能。也有一些高并发， 配置环境与启动项目Windows上传OSS镜像 导入镜像 创建实例 OSS与导入镜像地域要一致,否则会报invalid错 启动front项目 使用yarn即可 会报找不到Python路径错误 要提前安装Python2.7 (与项目的构建相关 我也不懂为啥) 然后设置Python路径全局配置 npm config set python “D:\\Python27\\python.exe” # 你安装的路径 启动后端项目 确认Maven settings 需要将pom里自带的设置Maven的信息注释掉 出现找不到等报错 可以去找到目标目录删了重新加载 本地域名解析配置 C:\\Windows\\System32\\drivers\\etc 127.0.0.1 tl.nacos.com tlshopdb.com tlshop.com 本机启动nacos 在黑窗体环境下切换目录到nacos&#x2F;bin下 startup.cmd -m standalone 配置数据库 1jdbc:mysql://tlshopdb.com:3306/micromall? root 123456 导入数据 SQL yog 新建同名数据库导入 执行SQL脚本 选择.sql文件执行 即可 速度贼快 Linux目的 学习docker指令 linux 实战,熟练掌握linux常用指令 docker部署各种中间件的一些配置 学会在linux跑起来一个项目使用docker 探明项目的功能 已配好docker环境 MySQL安装(已经安装好了) 下载MySQL5.7的docker镜像： docker pull mysql:5.7 使用如下命令启动MySQL服务： docker run -p 3306:3306 –name mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql \\ -e MYSQL_ROOT_PASSWORD&#x3D;123456 \\ -d mysql:5.7 docker run -p 3306:3306 –name mysql -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql -e MYSQL_ROOT_PASSWORD&#x3D;123456 -d mysql:5.7 docker run -p 3306:3306 –name mysql 参数说明 -p 3306:3306：将容器的3306端口映射到主机的3306端口 -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql：将配置文件夹挂在到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql：将日志文件夹挂载到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;：将数据文件夹挂载到主机 -e MYSQL_ROOT_PASSWORD&#x3D;root：初始化root用户的密码 进入运行MySQL的docker容器： docker exec -it mysql &#x2F;bin&#x2F;bash 使用MySQL命令打开客户端： mysql -uroot -proot –default-character-set&#x3D;utf8 创建mall数据库： create database mall character set utf8 安装上传下载插件，并将document&#x2F;sql&#x2F;mall.sql上传到Linux服务器上： yum -y install lrzsz 将mall.sql文件拷贝到mysql容器的&#x2F;目录下： docker cp &#x2F;mydata&#x2F;mall.sql mysql:&#x2F; 将sql文件导入到数据库： use mall; source &#x2F;mall.sql; 创建一个reader:123456帐号并修改权限，使得任何ip都能访问： grant all privileges on . to ‘reader’ @’%’ identified by ‘123456’; Redis安装 下载Redis5.0的docker镜像： docker pull redis:5 使用如下命令启动Redis服务： docker run -p 6379:6379 –name redis \\ -v &#x2F;mydata&#x2F;redis&#x2F;data:&#x2F;data \\ -d redis:5 redis-server –appendonly yes 进入Redis容器使用redis-cli命令进行连接： docker exec -it redis redis-cli ​ redis启动 redis-server &#x2F;usr&#x2F;local&#x2F;redis-5.0.2&#x2F;redis.conf redis-cli auth 123456 Nginx安装 下载Nginx1.10的docker镜像： docker pull nginx:1.10 先运行一次容器（为了拷贝配置文件）： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -d nginx:1.10 将容器内的配置文件拷贝到指定目录： docker container cp nginx:&#x2F;etc&#x2F;nginx &#x2F;mydata&#x2F;nginx&#x2F; 修改文件名称： mv nginx conf 终止并删除容器： docker stop nginx docker rm nginx 使用如下命令启动Nginx服务： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;conf:&#x2F;etc&#x2F;nginx \\ -d nginx:1.10 默认启动? 怎么查看启动的服务 RabbitMQ安装 下载rabbitmq3.7.15的docker镜像： docker pull rabbitmq:3.7.15 使用如下命令启动RabbitMQ服务： docker run -p 5672:5672 -p 15672:15672 –name rabbitmq -d rabbitmq:3.7.15 进入容器并开启管理功能： docker exec -it rabbitmq &#x2F;bin&#x2F;bash rabbitmq-plugins enable rabbitmq_management ​ 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;15672&#x2F;tcp –permanent firewall-cmd –reload 访问地址查看是否安装成功：http://192.168.3.101:15672 ​ 输入账号密码并登录：guest guest 创建帐号并设置其角色为管理员：mall mall ​ 创建一个新的虚拟host为：&#x2F;mall ​ 点击mall用户进入用户配置页面 ​ 给mall用户配置该虚拟host的权限 ​ Elasticsearch安装 下载Elasticsearch7.6.2的docker镜像： docker pull elasticsearch:7.6.2 修改虚拟内存区域大小，否则会因为过小而无法启动: sysctl -w vm.max_map_count&#x3D;262144 使用如下命令启动Elasticsearch服务： docker run -p 9200:9200 -p 9300:9300 –name elasticsearch \\ -e “discovery.type&#x3D;single-node” \\ -e “cluster.name&#x3D;elasticsearch” \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;plugins:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;plugins \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;data:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data \\ -d elasticsearch:7.6.2 启动时会发现&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data目录没有访问权限，只需要修改&#x2F;mydata&#x2F;elasticsearch&#x2F;data目录的权限，再重新启动即可； chmod 777 &#x2F;mydata&#x2F;elasticsearch&#x2F;data&#x2F; 安装中文分词器IKAnalyzer，并重新启动： docker exec -it elasticsearch &#x2F;bin&#x2F;bash #此命令需要在容器中运行 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip docker restart elasticsearch 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;9200&#x2F;tcp –permanent firewall-cmd –reload 访问会返回版本信息：http://192.168.3.101:9200 ​ Logstash安装 下载Logstash7.6.2的docker镜像： docker pull logstash:7.6.2 修改Logstash的配置文件logstash.conf中output节点下的Elasticsearch连接地址为es:9200，配置文件地址：&#x2F;document&#x2F;elk&#x2F;logstash.conf output { elasticsearch { hosts &#x3D;&gt; “es:9200” index &#x3D;&gt; “mall-%{type}-%{+YYYY.MM.dd}” } } 创建&#x2F;mydata&#x2F;logstash目录，并将Logstash的配置文件logstash.conf拷贝到该目录； mkdir &#x2F;mydata&#x2F;logstash 使用如下命令启动Logstash服务； docker run –name logstash -p 4560:4560 -p 4561:4561 -p 4562:4562 -p 4563:4563 \\ –link elasticsearch:es \\ -v &#x2F;mydata&#x2F;logstash&#x2F;logstash.conf:&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;pipeline&#x2F;logstash.conf \\ -d logstash:7.6.2 进入容器内部，安装json_lines插件。 logstash-plugin install logstash-codec-json_lines Kibana安装 下载Kibana7.6.2的docker镜像： docker pull kibana:7.6.2 使用如下命令启动Kibana服务： docker run –name kibana -p 5601:5601 \\ –link elasticsearch:es \\ -e “elasticsearch.hosts&#x3D;http://es:9200&quot; \\ -d kibana:7.6.2 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;5601&#x2F;tcp –permanent firewall-cmd –reload 访问地址进行测试：http://192.168.3.101:5601 ​ MongoDB安装 下载MongoDB4.2.5的docker镜像： docker pull mongo:4.2.5 使用docker命令启动： docker run -p 27017:27017 –name mongo \\ -v &#x2F;mydata&#x2F;mongo&#x2F;db:&#x2F;data&#x2F;db \\ -d mongo:4.2.5 Docker全部环境安装完成 所有下载镜像文件： REPOSITORY TAG IMAGE ID CREATED SIZE redis 5 071538dbbd71 2 weeks ago 98.3MB mongo 4.2.5 fddee5bccba3 3 months ago 388MB logstash 7.6.2 fa5b3b1e9757 4 months ago 813MB kibana 7.6.2 f70986bc5191 4 months ago 1.01GB elasticsearch 7.6.2 f29a1ee41030 4 months ago 791MB rabbitmq 3.7.15-management 6ffc11daa8d0 13 months ago 186MB mysql 5.7 7faa3c53e6d6 15 months ago 373MB registry 2 f32a97de94e1 17 months ago 25.8MB nginx 1.10 0346349a1a64 3 years ago 182MB java 8 d23bdf5b1b1b 3 years ago 643MB 所有运行在容器里面的应用： ​ SpringBoot应用部署构建所有Docker镜像并上传 修改项目根目录下的pom.xml中的docker.host属性： &lt;docker.host&gt;http://192.168.3.101:2375docker.host&gt; properties&gt; 如果项目根目录的pom.mxl中docker-maven-plugin的节点被注释掉了就打开注释，使项目在打包时直接构建Docker镜像； ​ 直接双击根项目mall的package命令可以一次性打包所有应用的Docker镜像； ​ REPOSITORY TAG IMAGE ID CREATED SIZE mall&#x2F;mall-portal 1.0-SNAPSHOT 70e0f76416a0 21 seconds ago 705MB mall&#x2F;mall-search 1.0-SNAPSHOT f3290bd1d0c7 41 seconds ago 725MB mall&#x2F;mall-admin 1.0-SNAPSHOT 26557b93a106 About a minute ago 705MB 部署mall-admin docker run -p 8080:8080 –name mall-admin \\ –link mysql:db \\ –link redis:redis \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;admin&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-admin:1.0-SNAPSHOT 注意：如果想使用Logstash收集日志的话，需要将应用容器连接到Logstsh，添加如下配置即可； –link logstash:logstash \\ 部署mall-search docker run -p 8081:8081 –name mall-search \\ –link elasticsearch:es \\ –link mysql:db \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;search&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-search:1.0-SNAPSHOT 部署mall-port docker run -p 8085:8085 –name mall-portal \\ –link mysql:db \\ –link redis:redis \\ –link mongo:mongo \\ –link rabbitmq:rabbit \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;portal&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-portal:1.0-SNAPSHOT 开启防火墙 firewall-cmd –zone&#x3D;public –add-port&#x3D;8080&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8081&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8085&#x2F;tcp –permanent firewall-cmd –reload 访问接口进行测试 mall-admin的api接口文档地址：http://192.168.3.101:8080/swagger-ui.html ​ mall-search的api接口文档地址：http://192.168.3.101:8081/swagger-ui.html ​ mall-portal的api接口文档地址：http://192.168.3.101:8085/swagger-ui.html ​ 总结psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD ​ docker run –name&#x3D;”容器新名字” 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p docker execdocker exec [OPTIONS] 容器名称 COMMAND [ARG…]OPTIONS说明： -d，以后台方式执行命令； -e，设置环境变量 -i，交互模式 -t，设置TTY -u，用户名或UID，例如myuser:myusergroup 通常COMMAND只能是一条语句，为了支持多个命令的执行，需要将多个命令连接起来交给Shell，docker exec命令的使用示例如下： sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject &amp;&amp; git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC} &amp;&amp; git checkout FETCH_HEAD”;sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject;git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC};git checkout FETCH_HEAD”; 注意：对于已经暂停或停止了的容器，无法执行docker exec命令，如下将抛出异常： docker pause myContainerdocker exec myContainer … options 作用 -d 在后台运行命令-i 即使没有附加也保持 STDIN 打开-t 设置TTY进入容器的 CLI 模式 -e 设置环境变量-w 需要执行命令的目录-u 指定访问容器的用户名备注：其实还有几个 options，但是目前还没用到，要用的时候再写吧 实际栗子执行 tomcat 容器的 startup.sh 脚本 docker exec -it tomcat7 startup.sh进入容器的 CLI 模式(最常用) docker exec -it tomcat7 bash执行普通命令 docker exec -it tomcat7 pwd 指定工作目录执行命令 docker exec -it -w &#x2F;usr tomcat7 pwd 以 root 用户身份进入容器(重点) docker exec -it -uroot jenkins1 bash linux入门：”&quot;的作用运行cellranger count，发现每行末尾有个\\，遂查了下\\的作用 123456cellranger count --id=XPBShm \\ --transcriptome=/home/rstudio/opt/refdata-gex-GRCh38-and-mm10-2020-A \\ --fastqs=/home/rstudio/data/rawx/xpbs \\ --sample=XPBS \\ --r1-length 26 \\ --r2-length 98 作用有2 1 作为转义符反斜线符号“ \\ ”在Bash中被解释为转义字符，用于去除一个单个字符的特殊意义，它保留了跟随在之后的字符的字面值，除了换行符（\\n,\\r）。 如果在反斜线之后一个换行字符立即出现，转义字符使行 得以继续，但是换行字符后必须紧跟命令，不能出现空格，遇到命令很长时使用反斜线很有效。 例一： 1234[linux@linux ~]$ echo $HOME/home/[linux@linux ~]$ echo \\$HOME$HOME 例子中，反斜线去除了“ $ ”字符的特殊意义，保留字面值，从而不输出home目录路径。 2. 作为换行符例二： 12345678910export PATH=\\/bin:\\/sbin:\\/usr/bin:\\/usr/sbin:\\/usr/local/bin:\\/apps/bin:\\/apps/tools:\\/apps/tslib/bin\\ 例子中，反斜线使行得以继续，命令可以正常输入。 例二（反） 12345678910export PATH=\\ /bin:\\ /sbin:\\ /usr/bin:\\ /usr/sbin:\\ /usr/local/bin:\\ /apps/bin:\\ /apps/tools:\\ /apps/tslib/bin\\ 例子中就会出现错误： &#x2F;bin:: bad variable name&#x2F;* &#x2F;bin：错误变量名 *&#x2F; 因为在”+换行符”之后必须紧跟命令，不能有空格。 笔记DevOps： DevOps的意思就是开发和运维不再是分开的两个团队，而是你中有我，我中有 你的一个团队。我们现在开发和运维已经是一个团队了，但是运维方面的知识和 经验还需要持续提高。 持续交付： 持续交付的意思就是在不影响用户使用服务的前提下频繁把新功能发布给用户使用，要做到 这点非常非常难。我们现在两周一个版本，每次上线之后都会给不同的用户造成不同程度的 影响。 容器化： 容器化的好处在于运维的时候不需要再关心每个服务所使用的技术栈了，每个服务都被无差 别地封装在容器里，可以被无差别地管理和维护，现在比较流行的工具是docker和k8s。 所以你也可以简单地把云原生理解为：云原生 &#x3D; 微服务 + DevOps + 持续交付 + 容器化 商品模块业务场景介绍： 商品模块业务详解 ​ ​ ​ 表的设计：打开游览器访问京东详细页问题： 商品这块的数据库如何更好的设计，商品详细页显示这么多信息，是一张表还是多张表更好了？ 这个问题到底是一张表还是多张表，我们判断依据是什么？我们判断商品详细页里面显示的这些信息他们的关系。通过他们的关系，我们才能知道到底是设计一张表还是多张表。 一张表： 如果是一张表存储所有数据的话，那么查询是非常方便的，这是其优点，但是你会发现存储的时候是不是很麻烦。不通类型不同大小不通商品等等都不一样，那这样的一张表设计起来实在是太复杂了。 多张表： 如果是多张表的话业务更加清晰，维护起来也更加方便，但是你会发现查询好像会非常的复杂，一个商品页面我们需要查很多的表和数据。 解决 我们正确的方式是根据不同的数据类型按不通的表进行存储 商品表的设计在设计这个表的时候，嗯，会画出它的实体关系图。嗯那你设计一张表的时候，你的查询十分方便，但是你的你设计这个表会很复杂，就是想要把各种各种情况，各种类型全部囊括进来，十分负责，所以选择按根据不同的类型。去跟显示不同的表进行存储。嗯商品有不同类型，就比如有吃的，穿的。还有其他用的。就可以根据分类来划分我们的礼品商品。嗯 需求分析为什么商品需要分类？ 我们知道商品是有不通类型的，比如有吃的 比如有穿的比如还有其他的用的。不通的商品用途不一样。我们一开始就可以按分类来进行划分我们的商品，这个就有点像我们去看论坛的分类是一样的。 第一个版本：商品+分类 ​ 问题：此时有什么问题？： 目前这个方案有什么问题了？我们慢慢发现一个问题，只有分类并不能适应所有的需求，比如nike鞋和nikeT恤，用户可能希望先看nike的所有商品，这个模型就不能满足。我们想在这个关系中，加入“品牌”概念 但是只有分类的时候并不能适应所有的需求，你只是比如说耐克鞋，耐克T恤，用户希望能先看耐克的所有商品，这个模型就不能满足，只要在加入品牌的概念。这样基本用户可以在通过分类或者品牌找到自己想要的商品。 第二个版本：商品+分类+品牌 ​ 这样基本用户可以在首页上通过分类或者品牌找到自己想要的商品，也可以直接查看热门的商品和新上架的商品。 问题：此时有什么问题？ 但是问题也来了，用户在进入分类后，展示在用户面前的是很多很多商品，用户希望再通过筛选查询出更接近他目标的商品？ 用户在进入分类之后展示在湖面前的是很多很多商品，用户希望通过筛选查询出更接近他目标的商品。其实就加入了属性。比如说图案呀！嗯，之类的。 ​ 加入属性： 于是优秀的产品设计师，设计出了类似这样的UI： ​ ​ 第三个版本：商品+分类+品牌+属性 用户可以通过这些筛选条件进一步缩小自己的目标范围，那么问题又来了，这样的产品需求排在程序员面前，怎么去实现它？经过分析，我们找出了一个方法，我们知道商品之间的属性可能存在着较大的差别，比如牛仔裤它有版型、腰型、裤长等属性；而电脑它有CPU、显卡等属性，各类商品的属性是不同的。再进一步想，休闲裤也版型、腰型、裤长等属性；台式电脑或者笔记本电脑都有CPU、显卡等属性。所以我们得出：一个分类对应若干属性，而一个属性，对应若干属性选项，而一个具体商品又对应若干属性选项（例如具体一条牛仔裤，他的裤长：7分，裤型：直筒）。有点绕，仔细品味一下。 通过分类与品牌查询到相关的商品，再根据属性嗯，在写。来进一步缩小范围，比如牛仔裤，它有版型，腰型，裤长等属性，而电脑有CPU，显卡的出现。一个分类对应若干属性来一个性对应若干属性选项。一个具体商品有对应活该属性决定。各分类对应若干属性，一个属性对应若干属性选项。一个具体商品有对应若干属性选项。月底一条牛仔裤，它的裤长裤型的。 ​ 从图上可以看出，分类和属性的关系（例如：“牛仔裤”分类下有裤型、裤长、版型等属性）、属性和属性选项的关系（例如：裤长属性有长款、九分裤、七分裤的选项）、商品和属性选项的关系（例如某条牛仔裤的裤长是7分裤）。至此，我们知道一个商品的分类、品牌以及它有什么属性和对应的属性值。那么通过筛选条件，自然就可以查询出指定的商品。这里特别说一句，价格也是属性，不要设想用商品表中的价格字段去做计算。这不利于查询也增加了复杂度，让商家编辑人员用属性来设置并保证他的正确性。 ​ ​ 这个页面展示商品的所有信息，按照之前的设计好像都可以满足。但是我们似乎感觉错过了什么，在图上右边我们发现该商品当前的颜色和尺寸，并且允许用户可以选择其他的颜色和尺寸。这给我们带来了疑惑，这里的“颜色”和“尺寸”是什么，一件商品的不同颜色不同尺寸是算一个商品还是多个商品。 ​ 为什么要加入规格： 第四个版本：商品+分类+品牌+属性+规格 经过思考后，我们发现我们混淆了两个概念——“商品”和“货品”。不同规格的货品作为独立的商品。比如一条裤子的有L尺寸、M尺寸、一个U盘有16G还是32G的，都是同样的货品，不同规格的商品。可以认为货品和商品是一对多的关系。弄清了这个概念，处理这个需求就容易多了，这里的“颜色”、“尺寸”我们就作为“规格”来处理，而红色、黑色；L号、M号我们视为规格的选项或者说规格值。一件货品对应若干规格，而具有某一规格值的货品就是商品。 spu：iphone12 sku：金色64 iphone12 ​ 好了，现在好像差不多了。基于这个模型可以满足基本的商品搜索、展示的需求。搜索引擎也可以根据这个模型数据生成对应的商品索引，达到准确搜索的目的。商品模块还会和其他模块一起协作，比如用户系统、订单系统、支付系统等。一般情况下我们会把商品业务独立出来做成“商品中心”的服务，集中处理商品查询、更新、发布等业务，支撑其他业务。 ​ \\ 嗯，总结商品表的设计，商品表的设计主要是根据呃，用户对商品进行搜索，并且商家对用户对商家对商品进行展示来设计的。首先你用户想搜索到商品它需要进行分类查询，就是会有衣服啊，电子商品啊这种。设计成一个分类，那么在分类之后你还嗯会显示态度很还是是太多，还是需要进行一些区分？嗯，就比如说你的需求是只查看特步的。鞋子啊，或者特步的特步鞋或者特步。特步的衣服，然后就是你需要加还有品牌的概念。然后用户再去根据分类还有品牌来缩小它的具体范围。但是这样还是有很多的，很多的区分就比如说你需要嗯，你是你需要看他的具体的属性就是如何一个牛仔裤它的裤长。裤长，腰宽这种属性，这个属性呢是跟它的分类挂钩的，比如说你的裤子它就是一个就能把它的属性选项给出来。然后 嗯，表的设计，表的设计主要是基于用户的搜索需求与商家的展示需求来进行设计。嗯比较重要的有五张表，然后其他还有一些。关联表啊，一些这种就这种关系表，嗯，比较重要的就是你首先要有一个商品表就是你对，但是你的商品会十分复杂，你就要把它拆分成分类，就是你一个啊，不对。商品货品 就是分类和品牌之后你可以添加，你可以根据商品的属性再进行搜索，就比如说你的CPU，CPU，CPU型号呀，机身内存呀这些对应，然后每一个，每一个这些属性要对应着每一个属性选项。嗯，属性之后呢你对应的属性选项就比如说16G呀，那机身内存16G这些。但是你在真正的我看看去 商品的搜索： ​ 搜索引擎elasticsearch 商品的展示： 三、商品模块展示技术难点商品详情页是展示商品详细信息的一个页面，承载在网站的大部分流量和订单的入口。京东商城目前有通用版、全球购、闪购、易车、惠买车、服装、拼购、今日抄底等许多套模板。各套模板的元数据是一样的，只是展示方式不一样。目前商品详情页个性化需求非常多，数据来源也是非常多的，而且许多基础服务做不了的都放我们这，因此我们需要一种架构能快速响应和优雅的解决这些需求问题。因此我们重新设计了商品详情页的架构，主要包括三部分：商品详情页系统、商品详情页统一服务系统和商品详情页动态服务系统；商品详情页系统负责静的部分，而统一服务负责动的部分，而动态服务负责给内网其他系统提供一些数据服务。 ​ 商品详情页前端结构 前端展示可以分为这么几个维度：商品维度(标题、图片、属性等)、主商品维度（商品介绍、规格参数）、分类维度、商家维度、店铺维度等；另外还有一些实时性要求比较高的如实时价格、实时促销、广告词、配送至、预售等是通过异步加载。 ​ ​ SPU： Standard Product Unit （标准化产品单元）,SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。 SKU： Stock keeping unit(库存量单位) SKU即库存进出计量的单位（买家购买、商家进货、供应商备货、工厂生产都是依据SKU进行的），在服装、鞋类商品中使用最多最普遍。 例如纺织品中一个SKU通常表示：规格、颜色、款式。SKU是物理上不可分割的最小存货单元。 单品页流量特点 热点少，各种爬虫、比价软件抓取。 静态处理 thymeleaf等模板引擎 架构方案的问题：问题一：我们知道数据新增分:增量和全量数据 如果后台的小二新增了很多的商品，那我们都要对这些商品进行静态化，但是现在有个问题。那这些数据如何同步了？这是一个新增商品同步的问题，那这个问题怎么解决比较好了？。 ​ 不同应用部署在不同服务器甚至在不同的机房不同的国家。 1、通过网络同步的方式 就是其中一台服务器静态化之后，然后把文件同步到其他应用服务器上去。比如我们的linux命令scp方式。这种方式虽然可行，但是我们发现问题还是蛮多的，有多少个节点就需要同步多少份，等于是商品的数量*服务器的应用数数。很显然这种办法不是最优的解决办法 如果上述办法无法解决，那我们就用另外的方案，同学们你们觉得还有其他的方案没有？ **2、定时任务:**可以在某个应用用一个定时任务，然后分别去执行数据库需要静态化的数据即可，可以解决上述1数据同步的问题，因为所有的任务都是在本机运行，就不需要数据同步了。但是也有一个问题。就是如何避免不通的机器跑的数据不要重复，也就是A和B定时任务都跑了一份商品。这个是这种方案需要解决的。（比较直观的就是上锁） 3、消息中间件：还有一种办法就是通过消息中间件来解决。订阅topic然后生成当前服务器静态化的页面。 问题二：我们的freemark它是数据要事先按我这个模板生产好的，那就是说一定你改了模板，如果要生效的话，需要重新在把数据取出来和我们这个模板进行匹配生产更多的的静态html文件。那这是一个比较大的问题 如果后台数据有变更呢?如何及时同步到其它服务端? 如果页面静态化了，我们搜索打开一个商品详细页，怎么知道要我需要的访问的静态页面？ 万一我们模板需要修改了怎么办？ 牵一发动全身。 ​ 3.1、压测测试 jmeter模板 ​ 1、换数据库 2、分库分表 3.2、后台 12345678910111213141516171819202122/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = portalProductDao.getProductInfo(id); if (null == productInfo) &#123; return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; return productInfo;&#125; 压测结果： 5000并发 ​ 后台优化：redis缓存： redis设置：RedisConifg》RedisOpsUtil 12345678910111213141516171819202122232425262728293031/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = null; //从缓存Redis里找 productInfo = redisOpsUtil.get(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, PmsProductParam.class); if(null!=productInfo)&#123; return productInfo; &#125; productInfo = portalProductDao.getProductInfo(id); if (null==productInfo) &#123; log.warn(&quot;没有查询到商品信息,id:&quot;+id); return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, productInfo, 3600, TimeUnit.SECONDS); return productInfo;&#125; 好处： 加入redis之后我们发现提高了可以把之前请求 数据库查询的商品都缓存到redis中，通过对redis的访问来减少对数据里的依赖，减少了依赖本质就是减少了磁盘IO。 问题：提高请求的吞吐量，除了减少磁盘IO，还有网络IO，我们可以发现，请求redis其实也会涉及到网络IO，我们所有的请求都要走xxx端口号。那有没有更好的优化思路了，来同学们你们鲜花在哪儿？ 读多写少有两种： 1、最终一致性方案： 设置超时时间来解决 ​ redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE+id,productInfo,360,TimeUnit.SECONDS); 2、实时一致性方案： ​ 课程讲到 交易canal binlog 两个问题(高并发)、压缩的问题》减少内存 现在有什么问题了？ 跟我们预期只set一次redis 是有出入，为何会这样子了？并发问题 当我第二次再去访问，此时此刻没有日志输出，说明全部走了缓存： 并发问题：并发编程》并发问题》锁的方式来实现 java并发 加锁方式（不适合》特殊》分布式） 分布式锁：redis、zookeeper QPS立马就提高了很多。 加入分布式锁: 1234567&lt;!--加入redisson--&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; setnx 缓存应用场景： 1、访问量大、QPS高、更新频率不是很高的业务 2、数据一致性要求不高 缓存和数据库双写一致性问题： 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。 答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。 我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 zk&gt;临时顺序节点》原子性 线程创建如果可以创建成功，是否第一个 拿到了锁 业务场景介绍： 熟悉秒杀系统的业务和技术核心点、以及流程等 正常电商流程: 秒杀场景演示： 完毕 活动和场次关系 秒杀活动表：sms_flash_promotion 秒杀场次表：sms_flash_promotion_session 场次商品关系表：sms_flash_promotion_product_relation 一个活动可以有多个场次，每个场次可以有多个商品进行秒-杀。 秒杀系统设计 分两部分内容；秒杀业务设计和秒杀技术实现。 3215186661 11秒杀链路优化三 图灵：楼兰 秒杀系统核心交易链路优化 三 今天要处理的问题：秒杀场景下如何进行限流。 今天要做的内容： 解决的问题：1、在秒杀页面，客户点击秒杀后，在前台弹出一个验证码，需要用户 输入验证码才能往后端发送请求，这样能够错开秒杀下单的时间。 2、通过验证码，对后台下单请求进行保护，防止刷单，即绕开前端，直接往后端发 送请求。 在秒杀页面开始秒杀后，客户点击秒杀按钮，要在前台弹出一个验证 码，需要用户输入验证码才能往后端发请求，这样能够错开秒杀下单时 间。 在我们的实现中，是要将memberId、producrId和验证码的值一起传入 后台，后台返回一个token。然后再根据这个tokne拼接一个后台秒杀地 址。这个token会存入到redis中。实际秒杀时，会增加一个判断，检测 这个token是不是在redis中存在。如果不存在，就是机器刷单 一、电商项目中秒杀的实现流程 1、在tmll-admin中添加秒杀活动，在秒杀活动中先设置活动的开始日期和结束日 期，然后添加商品。 这个秒杀活动信息会保存到mysql中。sms_flash_promotion_product_relation 表。 同时，添加商品后会将活动商品保存到ZK中。(路 径&#x2F;ZkLock&#x2F;load_db&#x2F;{productId})。然后，当访问到商城前端商品页时， http://lo calhost:8080&#x2F;#&#x2F;product&#x2F;{productId}，会检查Redis中的产品信息缓存。如果 Redis中没有产品信息，就会重建Redis缓存。key为product:detail:cache:{ProdId} 然后进入商城的单品页 http://localhost:8080/#/product/32 ， product.vue 那个”立即购买”的按钮就会变成”立即秒杀” 点击立即秒杀就会进入秒杀页。secKillDetail.vue 代码实现机制： 1、从Redis判断商品是否有秒杀活动。 一个商品要么就只能秒杀，要么就只能普通购买，这样是否合理？ 这就 是为什么要单独独立出一套秒杀服务集群。 2、发送后台请求申请验证码。后台返回验证码图片，并将验证码的计算结果保存 到Redis。 验证码的请求路径里header里的memeberId是怎么进去的。有什么用？ 生成验证码图片的这个请求要怎么防刷？ 3、保护后台请求接口。 输入验证码后，先验证输入的验证码结果，返回一个Token。这个Token会传 入到接下来的商品确认页面，同时会保存到Redis当中，表示当前用户有购买秒杀商 品的资格。有效期300秒，300秒内必须完成下单，否则就要重新申请秒杀资格。 在后续的下单过程中，需要传入这个Token才能正常下单。 验证码如果输入错误，是如何判断的？ 二、如何加强限流方案的安全性 了解整理流程后，要继续深入思考下我们这个限流方案的安全性。 1&gt; 针对验证码 针对验证码的安全性，可以加上之前的验证码内容。 1、我们做了这一套机制后，到底有多安全？ 下单请求依然是可以用机器人模拟 的。 用户ID是存在Cookie当中的，可以拿到。 图形验证码是随机的，那就总有可能产生容易被机器识别的验证码。 2、怎么加强验证码本身的安全性 这个问题也是必须要前后台配合来思考的，而不是单独靠前端或者后端能够解决 的。这个方案要如何设计？ 提高验证码安全性的措施：1、加干扰线或者干扰 点，2，将关键字符变形并且在图形上串到一起。3、增加更多的前端交互，行为 验证。 验证码的内容最好是一个比较复杂的题目，而不是简单的输入数字。这样可以有 效延长下单请求的时长，更好的分散请求峰值。 图形验证码可以篡改。可以用PostMan另外访问生成图形验证码的接口，这时 Redis里的值就被篡改了，不再是页面上看到的计算结果了。如何处理？1、增加 更多的判断因素，例如IP。2、前端签名，后端验证签名。 输入了验证码之后，存在Redis中的验证码要及时删除。同时生成一个Token， 代表当前用户有购买权限。这个Token有效期是非常短的。 针对验证机制的安全性，可以增加一些安全机制。 换一种验证码 我们动手来换一种复杂一点的验证码，HappyCaptcha 官网地址： https://gitee.c om&#x2F;ramostear&#x2F;Happy-Captcha 换的方式比较简单，首先在pom.xml中加入HappyCaptcha的依赖 com.ramostear Happy-Captcha 1.0.1 1 2 3 4 5 然后在OmsPortalOrderController中getVerifyCode方法，将生成验证码的部分修 改一下： 这样，前台的验证码就变成了一闪一闪的动画。并且是中文的加减法，更难破解。 可以看到整个HappyCaptcha的实现机制跟我们自己的实现机制是差不多的，也是 使用session来存储答案。 其他还有哪些更难以破解的验证码？ 2&gt; 针对下单请求 我们的实现机制是要求将token拼凑到请求路径上来。这跟把token作为参数传递有 什么区别？ 如果一个模拟程序需要使用机器来参与秒杀抢单，首先需要根据其他用户的请求来 分析获取下单路径。如果是同一个请求路径，只是带的参数不同，那机器完全可以 尝试用暴力破解的方式来尝试进行下单。如果碰巧传入了一个Redis中的token值， 那他就下单成功了。但是现在把参数隐藏到了请求路径当中，动态的请求路径对于 下单的机器来说，就比较难试探出请求的地址，这样就增加了他下单的难度。 三、电商整体的秒杀限流方案： try { &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;这个步骤就会完成生成图片并且往response发送的步骤。 HappyCaptcha.require(request,response).style(CaptchaStyle.ANIM) .type(CaptchaType.ARITHMETIC_ZH) .build().finish(); Object captcha &#x3D; request.getSession().getAttribute(“happycaptcha”); &#x2F;&#x2F;HappyCaptcha生成的验证码是String类型 int code &#x3D; Integer.parseInt(captcha.toString()); log.info(“验证码答案:{}”,captcha); redisOpsUtil.set(RedisKeyPrefixConst.MIAOSHA_VERIFY_CODE_PREFIX + memberId + “:” + productId ,code ,300 ,TimeUnit.SECONDS); &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码结束 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; return null; }catch(Exception e) { e.printStackTrace(); return CommonResult.failed(“秒杀失败”); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 我们天天都在说三高，高并发、高可用、高可扩展，那到底应该如何去落地一个三 高的设计方案？ 构建大并发、高性能、高可用系统中几种通用的优化思路，可以抽象总结为“4 要 1 不要”原则。也就是：数据要尽量少、请求数要尽量少、路径要尽量短、依赖要尽 量少，以及不要有单点。当然，这几点是你要努力的方向，具体操作时还是要密切 结合实际的场景和具体条件来进行。 针对秒杀这个场景，其实方案设计往往比技术细节更为重要。因为你可以想象，每 一个秒杀环节的经典问题，都意味着互联网的秒杀业务出现过大的问题，这都是实 打实买来的教训。发现了问题之后才会有针对性的方案设计。那现在，我们整体来 回顾下电商的秒杀限流方案。 错峰1：动静分离的本质是将包含浏览者信息的动态数据和不包含浏览者信息的静态 资源区分开。例如在商品单品页，商品信息是不包含浏览者信息的，这部分就可以 抽象出静态资源。而用户登录状态、cookie等这些动态数据也尽可能缓存起来，并 且使缓存能够离用户更近。 错峰2：秒杀答题的形式可以是多种多样的，目的是防止机器刷单，以及错开用户的 下单时长。在秒杀场景下，答题速度靠后的请求自然就没有库存了，也可以减少系 统的请求量。 错峰3：缓存的作用主要有两个，一是快速扣减库存，保护数据库流量，并且库存扣 减完成后，快速通知Nginx，屏蔽后续请求；二是提前识别热点数据，并且针对热 点数据提供优化处理。处理的方案主要是三个，一是优化，二是限制，三是隔离， 包括业务隔离、系统隔离、数据隔离。 错峰4：单独提供秒杀服务集群，有利于减少秒杀商品的超大流量对普通商品的性能 冲击，不要让1%的商品影响到另外的99%。 后台错峰：这一部分是我们实战课程的重点。之前monkey老师带大家在后端针对 秒杀场景做了非常多的设计与实战。我们这个图中每一个错峰点虽然在图上就是比 较简单的一个点，但是深入进去，每个地方要考虑的细节都还是非常多的，大家可 以回顾下之前的几节课，体会下如何在后端对秒杀服务做针对性的优化。 首先想到的是使用MQ进行削峰。但是实际上，后端需要考虑的三高问题也远不止 MQ削峰这一步。每一个环节都需要考虑后端组件是否能够承载得住。例如秒杀服务 集群，到底应该部署多大的集群？部署多少台机器呢？显然为了顶住秒杀的大流 量，秒杀集群就需要部署得非常大。但是，如果在大部分没有秒杀服务的时间内， 这个集群的资源就闲置得非常厉害。所以，虚拟化+云计算进行弹性部署也是非常重 要的。在我们的项目实战课后面就会由诸葛老师给大家带来k8s和云部署的实战课 程。 然后：在后端系统中，添加了Redis、MQ这样的一些中间产品。而这些产品集群本 身，也存在效率低下、服务崩溃的风险。这样也就给系统整体带来了更多的风险 点。那要怎么去屏蔽这些产品给系统带来的风险呢？大家可以思考一下，下一节课 将会由fox老师给大家进行系统降级方面的设计。 题外话 方案优先 &gt; 技术优先。学习技术的同时，都要增加对软件问题的思考，很多同学 技术学得很快，但是缺乏思考。秒杀这种超大并发场景下的限流问题，不是任何一 个技术或者任何一个步骤可以限制住的，需要一个完整全面的方案才能保证业务稳 定性。所以我们在开发过程中，不能只埋头于技术点，要站在更高的角度，整体来 理解解决方案，这样才能更深入的理解自己在做的事情，也才能真正来解决问题。 这才是高级程序员与普通程序员真正的区别。 例如针对前端验证问题，还有哪些优化方案？ 提前发Token。可以在秒杀前设置一个预约活动。 在活动中提前发放 token。例如一个秒杀活动有20W个商品，那就可以预先准备200W个 token。用户进行预约时，只发放200W个Token，其他人也能预约成 功，但是其实没有获得token，那后面的秒杀，直接通过这个token就可 以过滤掉一大部分人。相当于没有token的人都只预约了个寂寞。这也是 互联网常用的一个套路。 例如针对超卖问题，在之前的课程中，介绍了如何使用Redis分布式锁防 超卖。针对同一个商品ID，使用一把分布式锁，确实可以很快很方便的 处理超卖问题。但是如果同时进行秒杀的商品多了呢？像京东、淘宝一 场大型的秒杀活动，同时有成千上万个商品要进行秒杀，那就意味着同 一时间Redis上锁解锁的操作会要执行成千上万次，这对Redis的性能消 耗是相当巨大的，Redis就有可能升级成为新的性能瓶颈。这时该怎么 办？ 当然具体问题的解决方案从来不止一个，这里我们可以选择一种返璞归 真的方案，把秒杀超卖的问题从分布式降级到本地JVM中，来获取极限 性能。例如将秒杀服务接入配置中心，然后在秒杀服务开始前，由配置 中心给每个应用服务实例下发一个库存数量。然后每次下单，每个服务 器只管自己的库存数量，与其他应用服务器完全不进行库存同步，在各 自的内存里扣减库存，这样就不会有超卖的情况发生。减少了网络消 耗，性能也能够进一步提升。 这种方案可不可行呢？当然也会有一些问题需要去处理。有可能某给服 务器上的库存很快消耗完了，而其他的服务器上仍有库存。整个服务就 会表现为你抢不到商品，但是在你后面抢商品的人却能抢到商品。(你们 在参与秒杀时有没有过这样的经历？)但是这在秒杀这种场景下，完全是 可以接受的。另外，如果某一个应用服务器挂了，那给他分配的库存就 会丢失。这时候又要怎么办？其实也没必要再去设置什么复杂的逻辑， 大不了少卖一点出去。反正都是售罄了，全卖完了，和卖了99%，其实 没什么区别。这时只需要统计好订单的数量(可以通过MQ来统计，也可 以通过Redis统计)，等秒杀活动的30分钟等待支付期过去后，再将没卖 出去的库存重新丢回库存池，与没有付款而被取消的订单商品一起返场 售卖就可以了。这也是很多互联网公司目前采用的方案。 最后虽然我们是后台开发工程师，但是前端也必须要有所了解。今天我们关注的 这个问题，也不能只关注后端，需要前后端一起才能理解他的作用。","categories":[{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"jvm","slug":"9MicroService","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:43:44.438Z","comments":true,"path":"2022/09/01/9MicroService/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/9MicroService/","excerpt":"","text":"微服务架构1. 服务组件化 1. 服务间交互采用restful 风格 1. 去中心化 :每个服务有自己的私有数据库持久化系统 1. 自动化部署:把应用拆分为一个一个独立的单个服务,方便自动化部署,测试,运维 1.传统集中式架构 ​ 开发速度快 ​ 并发能力差 ​ 代码耦合度高,维护困难 ​ 容错率低 2.垂直拆分 ​ 一个数据库,多个系统模块 ​ 拆分实现了流量分担,解决并发 ​ 可针对不同模块进行优化 ​ 方便水平扩展(集群化),负载均衡,容错率高 ​ 系统间相互独立,会有很多重复开发,影响开发效率 3.分布式服务 ​ 垂直应用越来越多,应用交互不可避免 ​ 系统间相互调用,调高代码复用和效率 ​ 系统间耦合度高,调用关系错综复杂,难以维护 ​ 服务提供方一旦产生变更,所有消费方都需要变更 4.面向服务架构(SOA) ​ 中间增加一层ESB,用来连接各个服务结点,集成不同系统,不同协议的服务 ​ 做消息的转化解释和路由工作 ​ ESB产品实现复杂,服务粒度较大, ESB包含的功能如:负载均衡,流量控制 ​ 加密处理,服务监控,异常处理等等 ​ 集成整合所有服务,数据转换使运维,测试部署困难 ​ 所有服务通过一个通路通信,降低通信速度 ​ 通常松耦合 基于socket工作在会话层,自定义数据格式,速度快,效率高 5.微服务架构 ​ 基于TCP工作在应用层,规定了数据传输格式 ​ 服务粒度小,使用轻量级通信,通常是HTTP API ​ 各服务可使用不同的编程语言实现,以及不同的数据存储技术 ​ 服务间相互独立 ​ 并保持最低限度的集中式管理 ​ 网关给用户提供统一的方式接入微服务,在网关层处理所有的非业务功能,例如身份验证,监控,负载均衡,缓存,请求分片与管理,静态响应处理,服务端通过服务注册中心进行服务注册和管理 dubbo1-今日内容 分布式系统中的相关概念 dubbo 概述 dubbo快速入门 dubbo的高级特性 2-相关概念2.1-互联网项目架构-特点互联网项目架构-特点 用户多 流量大，并发高 海量数据 易受攻击 功能繁琐 变更快 传统项目和互联网项目的不同 用户体验： 美观、功能、速度、稳定性 衡量一个网站速度是否快: 打开一个新页面一瞬间完成;页面内跳转，-刹那间完成。 根据佛经《僧衹律》记载:一 刹那者为-念,二十念为-瞬,二十瞬为-弹 指，二十弹指为-罗预， 二十罗预为-须臾，一日一夜有三十须臾。 2.2-互联网项目架构-目标衡量网站的性能指标: **响应时间:**指执行一个请求从开始到最后收到响应数据所花费的总体时间。 **并发数:**指系统同时能处理的请求数量。 并发连接数: 指的是客户端向服务器发起请求，并建立了TCP连接。每秒钟服务器连接的总TCP数量 **请求数:**也称为QPS(Query Per Second)指每秒多少请求. **并发用户数:**单位时间内有多少用户 **吞吐量:**指单位时间内系统能处理的请求数量。 ●QPS: Query Per Second每秒查询数。 ●TPS: Transactions Per Second每秒事务数。 ●一个事务是指一 个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束 计时，以此来计算使用的时间和完成的事务个数。 ●一个页面的一次访问，只会形成一 个TPS; 但-次页面请求，可能产生多次对服务器的请求，就会有多个QPS QPS&gt;=并发连接数&gt;= TPS 大型互联网项目架构目标: ​ ●**高性能:提供快速的访问体验。​ ●高可用:**网站服务- 可以正常访问 2.3-集群和分布式集群和分布式，●集群:很多“人”一起，干一样的事。●一个业务模块，部署在多台服务器上。●分布式:很多”人”一起，干不样的事。这些不一样的事， 合起来是一件大事。 2.4-架构演进单体架构： 优点: 简单:开发部署都很方便，小型项目首选缺点:●项目启动慢●可靠性差 垂直架构：垂直架构是指将单体架构中的多个模块拆分为多个独立的项目。形成多个独立的单体架构。 单体架构存在的问题: 项目启动慢 可靠性差 可伸缩性差 扩展性和可维护性差 性能低 垂直架构存在的问题: 重复功能太多 分布式架构：是指在垂直架构的基础上,将公共业务模块抽取出来,作为独立的服务供其他调用者消费，以实现服务的共享和重用。底层通过RPC（远程过程调用实现） RPC: Remote Procedure Call远程过程调用。有非常多的协议和技术来都实现了RPC的过程。比如: HTTP REST风格，Java RMI规范、WebService SOAP协议Hession等等。垂直架构存在的问题: ●重复功能太多 分布式架构存在的问题:​ ●服务提供方- -旦产生变更,所有消费方都需要变更。 **SOA: (Service- Oriented Architecture,面向服务的架构)**：是一个组件模型,它将应用程序的不同功能单元(称为服务)进行拆分，并通过这些服务之间定义良好的接口和契约联系起来。 **ESB: (Enterparise Servce Bus)**：企业服务总线,服务中介。主要是提供了一一个服务于服务之间的交互。ESB包含的功能如:负载均衡，流量控制，加密处理，服务的监控，异常处理，监控告急等等。 微服务架构： ●微服务架构是在SOA上做的升华,微服务架构强调的一个重点是“业务需要彻底的组件化和服务化”，原有的单个业务系统会拆分为多个可以独立开发、设计、运行的小应用。这些小应用之间通过服务完成交互和集成。 ●微服务架构&#x3D; 80%的SOA服务架构思想+ 100%的组件化架构思想+ 80%的领域建模思想 特点:●服务实现组件化:开发者可以自由选择开发技术。也不需要协调其他团队●服务之间交互一 般使用REST API●去中心化:每个微服务有自己私有的数据库持久化业务数据●自动化部署:把应用拆分成为一 个-个独立的单个服务,方便自动化部署、测试、运维 3-dubbo 概述Dubbo概念 ●Dubbo是阿里巴巴公司开源的一个高性能、轻量级的Java RPC框架。●致力于提供高性能和透明化的RPC远程服务调用方案,以及SOA服务治理方案。●官网: htp:&#x2F;&#x2F;ubbo.apache.orgo 节点角色说明: .●Provider: 暴露服务的服务提供方●Container: 服务运行容器●Consumer: 调用远程服务的服务消费方●Registry: 服务注册与发现的注册中心●Monitor:统计服务的调用次数和调用时间的监控中心 4-dubbo快速入门4.1zookeeper安装安装步骤： 第一步：安装 jdk（略）第二步：把 zookeeper 的压缩包（zookeeper-3.4.6.tar.gz）上传到 linux 系统第三步：解压缩压缩包 1tar -zxvf zookeeper-3.4.6.tar.gz 第四步：进入zookeeper-3.4.6目录，创建data目录 1mkdir data 第五步：进入conf目录 ，把zoo_sample.cfg 改名为zoo.cfg 12cd confmv zoo_sample.cfg zoo.cfg 第六步：打开zoo.cfg文件, 修改data属性： 1dataDir=/root/zookeeper-3.4.6/data 进入Zookeeper的bin目录，启动服务命令 1./zkServer.sh start 停止服务命令 1./zkServer.sh stop 查看服务状态：standalone 单节点 1./zkServer.sh status 4.2spring和springmvc整合实施步骤： 1.创建服务提供者Provider模块2.创建服务消费者Consumer模块3.在服务提供者模块编写UserServicelmpl提供服务4.在服务消费者中的UserC ontroller远程调用5.UserServicelmpl提供的服务6.分别启动两个服务，测试 Dubbo作为一个RPC框架，其最核心的功能就是要实现跨网络的远程调用。本小节就是要创建两个应用，一个作为服务的提供方，一个作为服务的消费方。通过Dubbo来实现服务消费方远程调用服务提供方的方法。 1 服务提供方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_provider，在pom.xml文件中导入如下坐标 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jms&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- dubbo相关 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;dubbo&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javassist&lt;/groupId&gt; &lt;artifactId&gt;javassist&lt;/artifactId&gt; &lt;version&gt;3.12.1.GA&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.47&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.3.2&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;!-- 指定端口 --&gt; &lt;port&gt;8081&lt;/port&gt; &lt;!-- 请求路径 --&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; （2）配置web.xml文件 1234567891011121314&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext*.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;/web-app&gt; （3）创建服务接口 1234package com.itheima.service;public interface HelloService &#123; public String sayHello(String name);&#125; （4）创建服务实现类 注意：服务实现类上使用的Service注解是Dubbo提供的，用于对外发布服务 12345678910package com.itheima.service.impl;import com.alibaba.dubbo.config.annotation.Service;import com.itheima.service.HelloService;@Servicepublic class HelloServiceImpl implements HelloService &#123; public String sayHello(String name) &#123; return &quot;hello &quot; + name; &#125;&#125; tomcat7:run 2 服务消费方开发 开发步骤： （1）创建maven工程（打包方式为war）dubbodemo_consumer，pom.xml配置和上面服务提供者相同，只需要将Tomcat插件的端口号改为8082即可 （2）配置web.xml文件 1234567891011121314151617181920&lt;!DOCTYPE web-app PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot; &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot; &gt;&lt;web-app&gt; &lt;display-name&gt;Archetype Created Web Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 指定加载的配置文件 ，通过参数contextConfigLocation加载 --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-web.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; （3）将服务提供者工程中的HelloService接口复制到当前工程 （4）编写Controller 1234567891011121314151617181920package com.itheima.controller;import com.alibaba.dubbo.config.annotation.Reference;import com.itheima.service.HelloService;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController &#123; @Reference private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name)&#123; //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; &#125;&#125; 注意：Controller中注入HelloService使用的是Dubbo提供的@Reference注解 4.3服务提供者在dubbodemo_provider工程中src&#x2F;main&#x2F;resources下创建applicationContext-service.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo_provider&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 注册 协议和port 端口默认是20880 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20881&quot;&gt;&lt;/dubbo:protocol&gt; &lt;!-- 扫描指定包，加上@Service注解的类会被发布为服务 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.service.impl&quot; /&gt;&lt;/beans&gt; 4.4服务消费者在dubbodemo_consumer工程中src&#x2F;main&#x2F;resources下创建applicationContext-web.xml 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:p=&quot;http://www.springframework.org/schema/p&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:dubbo=&quot;http://code.alibabatech.com/schema/dubbo&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd&quot;&gt; &lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo-consumer&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 包扫描的方式 引用服务 扫描@Reference --&gt; &lt;dubbo:annotation package=&quot;com.itheima.controller&quot; /&gt;&lt;/beans&gt; 运行测试 tomcat7:run启动 在浏览器输入http://localhost:8082/demo/hello.do?name=Jack，查看浏览器输出结果 5-dubbo高级特性5.1dubbo-admin安装dubbo- admin ●dubbo-admin管理平台，是图形化的服务管理页面 ●从注册中心中获取到所有的提供者 &#x2F;消费者进行配置管理 ●路由规则、动态配置、服务降级、访问控制、权重调整、负载均衡等管理功能 ●dubbo- admin是一个前后端分离的项目。前端使用vue，后端使用springboot ●安装dubbo-admin其实就是部署该项目 具体安装参见：dubbo-admin.md 5.2-dubbo-admin使用具体安装参见：dubbo-admin.md 5.3序列化 dubbo 内部已经将序列化和反序列化的过程内部封装了 我们只需要在定义pojo类时实现seriali zable接口即可 一般会定义一 个公共的pojo模块,让生产者和消费者都依赖该模块。 User对象未实现seriali zable接口 错误信息： 解决办法： 1User implements Serializable 5.4地址缓存注册中心挂了，服务是否可以正常访问？ 可以，因为dubbo服务消费者在第一-次调用时，会将服务提供方地址缓存到本地，以后在调用则不会访问注册中心。 当服务提供者地址发生变化时，注册中心会通知服务消费者。 5.5 超时 服务消费者在调用服务提供者的时候发生了阻塞、等待的情形,这个时候,服务消费者会直等待下去。 在某个峰值时刻，大量的请求都在同时请求服务消费者,会造成线程的大量堆积，势必会造成雪崩。 dubbo利用超时机制来解决这个问题，设置-个超时时间, 在这个时间段内，无法完成服务访问,则自动断开连接。 使用timeout属性配置超时时间，默认值1000，单位毫秒 12//timeout 超时时间 单位毫秒 retries 重试次数@Service(timeout = 3000,retries=0) 5.6重试 设置了超时时间，在这个时间段内，无法完成服务访问,则自动断开连接。 如果出现网络抖动,则这一-次请求就会失败。 Dubbo提供重试机制来避免类似问题的发生。 通过retries属性来设置重试次数。默认为2次 12//timeout 超时时间 单位毫秒 retries 重试次数@Service(timeout = 3000,retries=0) 5.7多版本 **灰度发布:**当出现新功能时,会让一部分用户先使用新功能，用户反馈没问题时，再将所有用户迁移到新功能。 dubbo中使用version属性来设置和调用同一个接口的不同版本 生产者配置 12@Service(version=&quot;v2.0&quot;)public class UserServiceImp12 implements UserService &#123;...&#125; 消费者配置 12@Reference(version = &quot;v2.0&quot;)//远程注入private UserService userService; 5.8负载均衡负载均衡策略(4种) :**Random:**按权重随机，默认值。按权重设置随机概率。 RoundRobin: 按权重轮询。 LeastActive: 最少活跃调用数,相同活跃数的随机。 **ConsistentHash:**一 致性Hash,相同参数的请求总是发到同一提供者。 服务提供者配置 12@Service(weight = 100)public class UserServiceImp12 implements UserService &#123;...&#125; application.xml 配置parameter key 消费者配置 12345//@Reference(loadbalance = &quot;roundrobin&quot;)//@Reference(loadbalance = &quot;leastactive&quot;)//@Reference(loadbalance = &quot;consistenthash&quot;)@Reference(loadbalance = &quot;random&quot;)//默认 按权重随机private UserService userService; 5.9集群容错 集群容错模式: **Failover Cluster:**失败重试。默认值。当出现失败，重试其它服务器，默认重试2次，使用retries配置。一般用于读操作 **Failfast Cluster :**快速失败,发起-次调用，失败立即报错。通常用于写操作。 **Failsafe Cluster:**失败安全，出现异常时，直接忽略。返回一个空结果。 **Failback Cluster:**失败自动恢复,后台记录失败请求,定时重发。 **Forking Cluster :**并行调用多个服务器，只要一个成功即返回。 Broadcast Cluster: 广播调用所有提供者,逐个调用，任意一台报错则报错。 消费者配置 12@Reference(cluster = &quot;failover&quot;)//远程注入private UserService userService; 5.10服务降级服务降级：当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作 服务降级方式:mock&#x3D; force:return null：表示消费方对该服务的方法调用都直接返回null值,不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock&#x3D;fail:return null：表示消费方对该服务的方法调用在失败后，再返回null值,不抛异常。用来容忍不重要服务不稳定时对调用方的影响 消费方配置 123//远程注入@Reference(mock =“ force :return null&quot;)//不再调用userService的服务private UserService userService; 总结概述SOA,RPC框架 用Zookeeper作为注册中心 快速入门快速开始提供者 123456789&lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo_provider&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 注册 协议和port 端口默认是20880 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20881&quot;&gt;&lt;/dubbo:protocol&gt; &lt;!-- 扫描指定包，加上@Service注解的类会被发布为服务 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.service.impl&quot; /&gt; 消费者 12345&lt;dubbo:application name=&quot;dubbodemo-consumer&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 包扫描的方式 引用服务 扫描@Reference --&gt; &lt;dubbo:annotation package=&quot;com.itheima.controller&quot; /&gt; 超时,重试,负载均衡,灰度发布(多版本)@Service(timeout &#x3D; ,retries &#x3D; ,version&#x3D;””) 同spring @service一致采用dubbo包 用于服务发布 @Reference(loadbalance &#x3D; “”,version &#x3D; “”)) 类似Autowire 用于服务发现 &#x2F;&#x2F;@Reference(loadbalance &#x3D; “roundrobin”)&#x2F;&#x2F;@Reference(loadbalance &#x3D; “leastactive”)&#x2F;&#x2F;@Reference(loadbalance &#x3D; “consistenthash”)@Reference(loadbalance &#x3D; “random”)&#x2F;&#x2F;默认 按权重随机 @Service(timeout &#x3D; 3000,retries&#x3D;0) @Reference(version &#x3D; “v2.0”)&#x2F;&#x2F;远程注入 private UserService userService; 集群容错**Failover Cluster:**失败重试。默认值。当出现失败，重试其它服务器，默认重试2次，使用retries配置。一般用于读操作 **Failfast Cluster :**快速失败,发起-次调用，失败立即报错。通常用于写操作。 **Failsafe Cluster:**失败安全，出现异常时，直接忽略。返回一个空结果。 **Failback Cluster:**失败自动恢复,后台记录失败请求,定时重发。 **Forking Cluster :**并行调用多个服务器，只要一个成功即返回。 Broadcast Cluster: 广播调用所有提供者,逐个调用，任意一台报错则报错。 消费者配置 12@Reference(cluster = &quot;failover&quot;)//远程注入private UserService userService; 服务降级服务降级：当服务器压力剧增的情况下，根据实际业务情况及流量，对一些服务和页面有策略的不处理或换种简单的方式处理，从而释放服务器资源以保证核心交易正常运作或高效运作 服务降级方式:mock&#x3D; force:return null：表示消费方对该服务的方法调用都直接返回null值,不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 mock&#x3D;fail:return null：表示消费方对该服务的方法调用在失败后，再返回null值,不抛异常。用来容忍不重要服务不稳定时对调用方的影响 消费方配置 123//远程注入@Reference(mock =“ force :return null&quot;)//不再调用userService的服务private UserService userService; 分布式 很多人一起干不一样的事 合起来是一件大事 0-5 共六步 图片 todo 功能 一个RPC框架 远程过程调用,能够将单体上的应用的各个模块变成分布式,关键是call过程,通过zookeeper注册中心(服务治理) zookeepper 是一个树结构,结点可以存储信息 &#x2F; 根节点 分布式锁的实现 临时有序结点,选取顺序最小的 每个线程申请都会生成一结点, 集群治理 其他高级特性 心跳 负载均衡 容灾 服务注册&#x2F;发现 配置管理 服务提供者将服务信息注册到服务中心,消费者对生产者进行调用,会在本地存有地址和端口缓存,如果服务提供者的地址端口信息等进行更新,zookeepper会通知消费者更新缓存 1. (在下次调用时更新还是一更改就更新?思考:一更改就更新,会有缓存的嗅探()好处是只要缓存命中就能保证服务的可靠性,如果调用时更新,缓存的意义就不大了,) 还有 monitor (sentinel(或duboo-admin)对rpc进行监控) 特性 序列化 地址缓存 超时与重试 消费者的请求阻塞,等待的时候,dubbo设置了超时机制,无法完成服务访问则自动断开连接(基于TCP的连接?) 如果出现网络抖动,请求失败,通过retries设置重传次数 默认2 多版本 用version属性来调用同一个接口的不同版本 负载均衡选用哪个服务 random :按权重随机,每个默认都为100 roundRobin权重轮询 LeastActive最少活跃调用,相同活跃随机 ConsistentHash 一致性hash,相同参数的请求总是发到同一提供者 6)集群容错 实验1 通过Dubbo实现远程调用启动 zookeepper 生产者 编写service 在Impl上增加dubbo service注解 用于对外发布服务 123456@Servicepublic class HelloServiceImpl implements HelloService &#123; public String sayHello(String name) &#123; return &quot;hello &quot; + name; &#125;&#125; 12345678910&lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo_provider&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 注册 协议和port 端口默认是20880 --&gt; &lt;dubbo:protocol name=&quot;dubbo&quot; port=&quot;20881&quot;&gt;&lt;/dubbo:protocol&gt; &lt;!-- 扫描指定包，加上@Service注解的类会被发布为服务 --&gt; &lt;dubbo:annotation package=&quot;com.itheima.service.impl&quot; /&gt;&lt;/beans&gt; 消费者编写 controller 远程调用 service 1234567891011121314151617181920package com.itheima.controller;import com.alibaba.dubbo.config.annotation.Reference;import com.itheima.service.HelloService;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.ResponseBody;@Controller@RequestMapping(&quot;/demo&quot;)public class HelloController &#123; @Reference private HelloService helloService; @RequestMapping(&quot;/hello&quot;) @ResponseBody public String getName(String name)&#123; //远程调用 String result = helloService.sayHello(name); System.out.println(result); return result; &#125;&#125; 123456&lt;!-- 当前应用名称，用于注册中心计算应用间依赖关系，注意：消费者和提供者应用名不要一样 --&gt; &lt;dubbo:application name=&quot;dubbodemo-consumer&quot; /&gt; &lt;!-- 连接服务注册中心zookeeper ip为zookeeper所在服务器的ip地址--&gt; &lt;dubbo:registry address=&quot;zookeeper://192.168.134.129:2181&quot;/&gt; &lt;!-- 包扫描的方式 引用服务 扫描@Reference --&gt; &lt;dubbo:annotation package=&quot;com.itheima.controller&quot; /&gt; Zookeeper1)初识 Zookeeper1.1)Zookeeper概念•Zookeeper 是 Apache Hadoop 项目下的一个子项目，是一个树形目录服务。 •Zookeeper 翻译过来就是 动物园管理员，他是用来管 Hadoop（大象）、Hive(蜜蜂)、Pig(小 猪)的管理员。简称zk •Zookeeper 是一个分布式的、开源的分布式应用程序的协调服务。 •Zookeeper 提供的主要功能包括： •配置管理 •分布式锁 •集群管理 2)ZooKeeper 安装与配置2.1) 下载安装2.1.1、环境准备ZooKeeper服务器是用Java创建的，它运行在JVM之上。需要安装JDK 7或更高版本。 2.1.2、上传将下载的ZooKeeper放到&#x2F;opt&#x2F;ZooKeeper目录下 12345678#上传zookeeper alt+pput f:/setup/apache-zookeeper-3.5.6-bin.tar.gz#打开 opt目录cd /opt#创建zooKeeper目录mkdir zooKeeper#将zookeeper安装包移动到 /opt/zooKeepermv apache-zookeeper-3.5.6-bin.tar.gz /opt/zookeeper/ 2.1.3、解压将tar包解压到&#x2F;opt&#x2F;zookeeper目录下 1tar -zxvf apache-ZooKeeper-3.5.6-bin.tar.gz 2.2) 配置启动2.2.1、配置zoo.cfg进入到conf目录拷贝一个zoo_sample.cfg并完成配置 1234#进入到conf目录cd /opt/zooKeeper/apache-zooKeeper-3.5.6-bin/conf/#拷贝cp zoo_sample.cfg zoo.cfg 修改zoo.cfg 123456#打开目录cd /opt/zooKeeper/#创建zooKeeper存储目录mkdir zkdata#修改zoo.cfgvim /opt/zooKeeper/apache-zooKeeper-3.5.6-bin/conf/zoo.cfg 修改存储目录：dataDir&#x3D;&#x2F;opt&#x2F;zookeeper&#x2F;zkdata 2.2.2、启动ZooKeeper123cd /opt/zooKeeper/apache-zooKeeper-3.5.6-bin/bin/#启动 ./zkServer.sh start 看到上图表示ZooKeeper成功启动 3、查看ZooKeeper状态 1./zkServer.sh status zookeeper启动成功。standalone代表zk没有搭建集群，现在是单节点 zookeeper没有启动 3)ZooKeeper 命令操作3.1)Zookeeper命令操作数据模型•ZooKeeper 是一个树形目录服务,其数据模型和Unix的文件系统目录树很类似，拥有一个层次化结构。 •这里面的每一个节点都被称为： ZNode，每个节点上都会保存自己的数据和节点信息。 • 节点可以拥有子节点，同时也允许少量（1MB）数据存储在该节点之下。 •节点可以分为四大类： •PERSISTENT 持久化节点 •EPHEMERAL 临时节点 ：-e •PERSISTENT_SEQUENTIAL 持久化顺序节点 ：-s •EPHEMERAL_SEQUENTIAL 临时顺序节点 ：-es 3.2)Zookeeper命令操作服务端命令•启动 ZooKeeper 服务: .&#x2F;zkServer.sh start •查看 ZooKeeper 服务状态: .&#x2F;zkServer.sh status •停止 ZooKeeper 服务: .&#x2F;zkServer.sh stop •重启 ZooKeeper 服务: .&#x2F;zkServer.sh restart 3.3)Zookeeper客户端常用命令•连接ZooKeeper服务端 1./zkCli.sh –server ip:port •断开连接 1quit •查看命令帮助 1help •显示指定目录下节点 1ls 目录 •创建节点 1create /节点path value •获取节点值 1get /节点path •设置节点值 1set /节点path value •删除单个节点 1delete /节点path •删除带有子节点的节点 1deleteall /节点path 3.4)客户端命令-创建临时有序节点•创建临时节点 1create -e /节点path value •创建顺序节点 1create -s /节点path value •查询节点详细信息 1ls –s /节点path •czxid：节点被创建的事务ID •ctime: 创建时间 •mzxid: 最后一次被更新的事务ID •mtime: 修改时间 •pzxid：子节点列表最后一次被更新的事务ID •cversion：子节点的版本号 •dataversion：数据版本号 •aclversion：权限版本号 •ephemeralOwner：用于临时节点，代表临时节点的事务ID，如果为持久节点则为0 •dataLength：节点存储的数据的长度 •numChildren：当前节点的子节点个数 4)ZooKeeper JavaAPI 操作4.1)urator介绍•Curator 是 Apache ZooKeeper 的Java客户端库。 •常见的ZooKeeper Java API ： •原生Java API •ZkClient •Curator •Curator 项目的目标是简化 ZooKeeper 客户端的使用。 •Curator 最初是 Netfix 研发的,后来捐献了 Apache 基金会,目前是 Apache 的顶级项目。 •官网：http://curator.apache.org/ 4.2)JavaAPI操作建立连接1，搭建项目 创建项目curator-zk 引入pom和日志文件 资料文件夹下pom.xml和log4j.properties 2、创建测试类，使用curator连接zookeeper 12345678910111213141516@Beforepublic void testConnect() &#123; //重试策略 RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 10); //2.第二种方式 //CuratorFrameworkFactory.builder(); client = CuratorFrameworkFactory.builder() .connectString(&quot;192.168.200.130:2181&quot;) .sessionTimeoutMs(60 * 1000) .connectionTimeoutMs(15 * 1000) .retryPolicy(retryPolicy) .namespace(&quot;itheima&quot;) .build(); //开启连接 client.start();&#125; 4.3)Zookeeper JavaAPI操作-创建节点1234567891011121314/*** 创建节点：create 持久 临时 顺序 数据* 1. 基本创建 ：create().forPath(&quot;&quot;)* 2. 创建节点 带有数据:create().forPath(&quot;&quot;,data)* 3. 设置节点的类型：create().withMode().forPath(&quot;&quot;,data)* 4. 创建多级节点 /app1/p1 ：create().creatingParentsIfNeeded().forPath(&quot;&quot;,data)*/@Testpublic void testCreate() throws Exception &#123; //2. 创建节点 带有数据 //如果创建节点，没有指定数据，则默认将当前客户端的ip作为数据存储 String path = client.create().forPath(&quot;/app2&quot;, &quot;hehe&quot;.getBytes()); System.out.println(path);&#125; 123456789101112131415161718192021@Testpublic void testCreate2() throws Exception &#123; //1. 基本创建 //如果创建节点，没有指定数据，则默认将当前客户端的ip作为数据存储 String path = client.create().forPath(&quot;/app1&quot;); System.out.println(path);&#125;@Testpublic void testCreate3() throws Exception &#123; //3. 设置节点的类型 //默认类型：持久化 String path = client.create().withMode(CreateMode.EPHEMERAL).forPath(&quot;/app3&quot;); System.out.println(path);&#125;@Testpublic void testCreate4() throws Exception &#123; //4. 创建多级节点 /app1/p1 //creatingParentsIfNeeded():如果父节点不存在，则创建父节点 String path = client.create().creatingParentsIfNeeded().forPath(&quot;/app4/p1&quot;); System.out.println(path);&#125; 4.4)ZookeeperJavaAPI操作-查询节点123456789101112/*** 查询节点：* 1. 查询数据：get: getData().forPath()* 2. 查询子节点： ls: getChildren().forPath()* 3. 查询节点状态信息：ls -s:getData().storingStatIn(状态对象).forPath()*/@Testpublic void testGet1() throws Exception &#123; //1. 查询数据：get byte[] data = client.getData().forPath(&quot;/app1&quot;); System.out.println(new String(data));&#125; 1234567891011121314@Testpublic void testGet2() throws Exception &#123; // 2. 查询子节点： ls List&lt;String&gt; path = client.getChildren().forPath(&quot;/&quot;); System.out.println(path);&#125;@Testpublic void testGet3() throws Exception &#123; Stat status = new Stat(); System.out.println(status); //3. 查询节点状态信息：ls -s client.getData().storingStatIn(status).forPath(&quot;/app1&quot;); System.out.println(status);&#125; 4.5)Zookeeper JavaAPI操作-修改节点123456789101112/*** 修改数据* 1. 基本修改数据：setData().forPath()* 2. 根据版本修改: setData().withVersion().forPath()* * version 是通过查询出来的。目的就是为了让其他客户端或者线程不干扰我。** @throws Exception*/@Testpublic void testSet() throws Exception &#123; client.setData().forPath(&quot;/app1&quot;, &quot;itcast&quot;.getBytes());&#125; 123456789@Testpublic void testSetForVersion() throws Exception &#123; Stat status = new Stat(); //3. 查询节点状态信息：ls -s client.getData().storingStatIn(status).forPath(&quot;/app1&quot;); int version = status.getVersion();//查询出来的 3 System.out.println(version); client.setData().withVersion(version).forPath(&quot;/app1&quot;, &quot;hehe&quot;.getBytes());&#125; 4.6)Zookeeper JavaAPI操作-删除节点123456789101112131415161718/*** 删除节点： delete deleteall* 1. 删除单个节点:delete().forPath(&quot;/app1&quot;);* 2. 删除带有子节点的节点:delete().deletingChildrenIfNeeded().forPath(&quot;/app1&quot;);* 3. 必须成功的删除:为了防止网络抖动。本质就是重试。 client.delete().guaranteed().forPath(&quot;/app2&quot;);* 4. 回调：inBackground* @throws Exception*/@Testpublic void testDelete() throws Exception &#123; // 1. 删除单个节点 client.delete().forPath(&quot;/app1&quot;);&#125;@Testpublic void testDelete2() throws Exception &#123; //2. 删除带有子节点的节点 client.delete().deletingChildrenIfNeeded().forPath(&quot;/app4&quot;);&#125; 12345678910111213141516@Testpublic void testDelete3() throws Exception &#123; //3. 必须成功的删除 client.delete().guaranteed().forPath(&quot;/app2&quot;);&#125;@Testpublic void testDelete4() throws Exception &#123; //4. 回调 client.delete().guaranteed().inBackground(new BackgroundCallback()&#123; @Override public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println(&quot;我被删除了~&quot;); System.out.println(event); &#125; &#125;).forPath(&quot;/app1&quot;);&#125; 4.7)Zookeeper JavaAPI操作-Watch监听概述•ZooKeeper 允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。 •ZooKeeper 中引入了Watcher机制来实现了发布&#x2F;订阅功能能，能够让多个订阅者同时监听某一个对象，当一个对象自身状态变化时，会通知所有订阅者。 •ZooKeeper 原生支持通过注册Watcher来进行事件监听，但是其使用并不是特别方便 ​ 需要开发人员自己反复注册Watcher，比较繁琐。 •Curator引入了 Cache 来实现对 ZooKeeper 服务端事件的监听。 •ZooKeeper提供了三种Watcher： •NodeCache : 只是监听某一个特定的节点 •PathChildrenCache : 监控一个ZNode的子节点. •TreeCache : 可以监控整个树上的所有节点，类似于PathChildrenCache和NodeCache的组合 4.8Zookeeper JavaAPI操作-Watch监听-NodeCache12345678910111213141516171819202122/*** 演示 NodeCache：给指定一个节点注册监听器*/@Testpublic void testNodeCache() throws Exception &#123; //1. 创建NodeCache对象 final NodeCache nodeCache = new NodeCache(client,&quot;/app1&quot;); //2. 注册监听 nodeCache.getListenable().addListener(new NodeCacheListener() &#123; @Override public void nodeChanged() throws Exception &#123; System.out.println(&quot;节点变化了~&quot;); //获取修改节点后的数据 byte[] data = nodeCache.getCurrentData().getData(); System.out.println(new String(data)); &#125; &#125;); //3. 开启监听.如果设置为true，则开启监听是，加载缓冲数据 nodeCache.start(true); while (true)&#123; &#125;&#125; 4.9)Zookeeper JavaAPI操作-Watch监听-PathChildrenCache12345678910111213141516171819202122232425@Testpublic void testPathChildrenCache() throws Exception &#123; //1.创建监听对象 PathChildrenCache pathChildrenCache = new PathChildrenCache(client,&quot;/app2&quot;,true); //2. 绑定监听器 pathChildrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; @Override public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; System.out.println(&quot;子节点变化了~&quot;); System.out.println(event); //监听子节点的数据变更，并且拿到变更后的数据 //1.获取类型 PathChildrenCacheEvent.Type type = event.getType(); //2.判断类型是否是update if(type.equals(PathChildrenCacheEvent.Type.CHILD_UPDATED))&#123; System.out.println(&quot;数据变了！！！&quot;); byte[] data = event.getData().getData(); System.out.println(new String(data)); &#125; &#125; &#125;); //3. 开启 pathChildrenCache.start(); while (true)&#123; &#125;&#125; 4.10)Zookeeper JavaAPI操作-Watch监听-TreeCache1234567891011121314151617181920/*** 演示 TreeCache：监听某个节点自己和所有子节点们*/@Testpublic void testTreeCache() throws Exception &#123; //1. 创建监听器 TreeCache treeCache = new TreeCache(client,&quot;/app2&quot;); //2. 注册监听 treeCache.getListenable().addListener(new TreeCacheListener() &#123; @Override public void childEvent(CuratorFramework client, TreeCacheEvent event) throws Exception &#123; System.out.println(&quot;节点变化了&quot;); System.out.println(event); &#125; &#125;); //3. 开启 treeCache.start(); while (true)&#123; &#125;&#125; 4.11)Zookeeper分布式锁-概念•在我们进行单机应用开发，涉及并发同步的时候，我们往往采用synchronized或者Lock的方式来解决多线程间的代码同步问题，这时多线程的运行都是在同一个JVM之下，没有任何问题。 •但当我们的应用是分布式集群工作的情况下，属于多JVM下的工作环境，跨JVM之间已经无法通过多线程的锁解决同步问题。 •那么就需要一种更加高级的锁机制，来处理种跨机器的进程之间的数据同步问题——这就是分布式锁。 4.12)Zookeeper 分布式锁-zookeeper分布式锁原理•核心思想：当客户端要获取锁，则创建节点，使用完锁，则删除该节点。 1.客户端获取锁时，在lock节点下创建临时顺序节点。 2.然后获取lock下面的所有子节点，客户端获取到所有的子节点之后，如果发现自己创建的子节点序号最小，那么就认为该客户端获取到了锁。使用完锁后，将该节点删除。 3.如果发现自己创建的节点并非lock所有子节点中最小的，说明自己还没有获取到锁，此时客户端需要找到比自己小的那个节点，同时对其注册事件监听器，监听删除事件。 4.如果发现比自己小的那个节点被删除，则客户端的 ​ Watcher会收到相应通知，此时再次判断自己创建的节点 ​ 是否是lock子节点中序号最小的，如果是则获取到了锁， ​ 如果不是则重复以上步骤继续获取到比自己小的一个节点 ​ 并注册监听。 4.13)Zookeeper 分布式锁-模拟12306售票案例Curator实现分布式锁API 在Curator中有五种锁方案： InterProcessSemaphoreMutex：分布式排它锁（非可重入锁） InterProcessMutex：分布式可重入排它锁 InterProcessReadWriteLock：分布式读写锁 InterProcessMultiLock：将多个锁作为单个实体管理的容器 InterProcessSemaphoreV2：共享信号量 1,创建线程进行加锁设置 12345678910111213141516171819202122232425262728public class Ticket12306 implements Runnable&#123; private int tickets = 10;//数据库的票数 private InterProcessMutex lock ; @Override public void run() &#123; while(true)&#123; //获取锁 try &#123; lock.acquire(3, TimeUnit.SECONDS); if(tickets &gt; 0)&#123; System.out.println(Thread.currentThread()+&quot;:&quot;+tickets); Thread.sleep(100); tickets--; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //释放锁 try &#123; lock.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 2,创建连接，并且初始化锁 123456789101112131415public Ticket12306()&#123; //重试策略 RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 10); //2.第二种方式 //CuratorFrameworkFactory.builder(); CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;192.168.149.135:2181&quot;) .sessionTimeoutMs(60 * 1000) .connectionTimeoutMs(15 * 1000) .retryPolicy(retryPolicy) .build(); //开启连接 client.start(); lock = new InterProcessMutex(client,&quot;/lock&quot;);&#125; 3,运行多个线程进行测试 12345678910public class LockTest &#123; public static void main(String[] args) &#123; Ticket12306 ticket12306 = new Ticket12306(); //创建客户端 Thread t1 = new Thread(ticket12306,&quot;携程&quot;); Thread t2 = new Thread(ticket12306,&quot;飞猪&quot;); t1.start(); t2.start(); &#125;&#125; 5)ZooKeeper 集群搭建5.1)Zookeeper集群介绍Leader选举： •Serverid：服务器ID 比如有三台服务器，编号分别是1,2,3。 编号越大在选择算法中的权重越大。 •Zxid：数据ID 服务器中存放的最大数据ID.值越大说明数据 越新，在选举算法中数据越新权重越大。 •在Leader选举的过程中，如果某台ZooKeeper ​ 获得了超过半数的选票， ​ 则此ZooKeeper就可以成为Leader了。 5.2)搭建要求真实的集群是需要部署在不同的服务器上的，但是在我们测试时同时启动很多个虚拟机内存会吃不消，所以我们通常会搭建伪集群，也就是把所有的服务都搭建在一台虚拟机上，用端口进行区分。 我们这里要求搭建一个三个节点的Zookeeper集群（伪集群）。 5.3)准备工作重新部署一台虚拟机作为我们搭建集群的测试服务器。 （1）安装JDK 【此步骤省略】。 （2）Zookeeper压缩包上传到服务器（3）将Zookeeper解压 ，建立&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster目录，将解压后的Zookeeper复制到以下三个目录 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-1 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-2 &#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-3 1234[root@localhost ~]# mkdir /usr/local/zookeeper-cluster[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-1[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-2[root@localhost ~]# cp -r apache-zookeeper-3.5.6-bin /usr/local/zookeeper-cluster/zookeeper-3 （4）创建data目录 ，并且将 conf下zoo_sample.cfg 文件改名为 zoo.cfg 1234567mkdir /usr/local/zookeeper-cluster/zookeeper-1/datamkdir /usr/local/zookeeper-cluster/zookeeper-2/datamkdir /usr/local/zookeeper-cluster/zookeeper-3/datamv /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgmv /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgmv /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo_sample.cfg /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfg （5） 配置每一个Zookeeper 的dataDir 和 clientPort 分别为2181 2182 2183 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-1&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgclientPort=2181dataDir=/usr/local/zookeeper-cluster/zookeeper-1/data 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-2&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgclientPort=2182dataDir=/usr/local/zookeeper-cluster/zookeeper-2/data 修改&#x2F;usr&#x2F;local&#x2F;zookeeper-cluster&#x2F;zookeeper-3&#x2F;conf&#x2F;zoo.cfg 1234vim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfgclientPort=2183dataDir=/usr/local/zookeeper-cluster/zookeeper-3/data 5.4)配置集群（1）在每个zookeeper的 data 目录下创建一个 myid 文件，内容分别是1、2、3 。这个文件就是记录每个服务器的ID 123echo 1 &gt;/usr/local/zookeeper-cluster/zookeeper-1/data/myidecho 2 &gt;/usr/local/zookeeper-cluster/zookeeper-2/data/myidecho 3 &gt;/usr/local/zookeeper-cluster/zookeeper-3/data/myid （2）在每一个zookeeper 的 zoo.cfg配置客户端访问端口（clientPort）和集群服务器IP列表。 集群服务器IP列表如下 1234567vim /usr/local/zookeeper-cluster/zookeeper-1/conf/zoo.cfgvim /usr/local/zookeeper-cluster/zookeeper-2/conf/zoo.cfgvim /usr/local/zookeeper-cluster/zookeeper-3/conf/zoo.cfgserver.1=192.168.149.135:2881:3881server.2=192.168.149.135:2882:3882server.3=192.168.149.135:2883:3883 解释：server.服务器ID&#x3D;服务器IP地址：服务器之间通信端口：服务器之间投票选举端口 5.5)启动集群启动集群就是分别启动每个实例。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start 启动后我们查询一下每个实例的运行状态 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 先查询第一个服务 Mode为follower表示是跟随者（从） 再查询第二个服务Mod 为leader表示是领导者（主） 查询第三个为跟随者（从） 5.6)模拟集群异常（1）首先我们先测试如果是从服务器挂掉，会怎么样 把3号服务器停掉，观察1号和2号，发现状态并没有变化 1234/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status 由此得出结论，3个节点的集群，从服务器挂掉，集群正常 （2）我们再把1号服务器（从服务器）也停掉，查看2号（主服务器）的状态，发现已经停止运行了。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status 由此得出结论，3个节点的集群，2个从服务器都挂掉，主服务器也无法运行。因为可运行的机器没有超过集群总数量的半数。 （3）我们再次把1号服务器启动起来，发现2号服务器又开始正常工作了。而且依然是领导者。 123/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status （4）我们把3号服务器也启动起来，把2号服务器停掉,停掉后观察1号和3号的状态。 12345/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh stop/usr/local/zookeeper-cluster/zookeeper-1/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 发现新的leader产生了~ 由此我们得出结论，当集群中的主服务器挂了，集群中的其他服务器会自动进行选举状态，然后产生新得leader （5）我们再次测试，当我们把2号服务器重新启动起来启动后，会发生什么？2号服务器会再次成为新的领导吗？我们看结果 1234/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh start/usr/local/zookeeper-cluster/zookeeper-2/bin/zkServer.sh status/usr/local/zookeeper-cluster/zookeeper-3/bin/zkServer.sh status 我们会发现，2号服务器启动后依然是跟随者（从服务器），3号服务器依然是领导者（主服务器），没有撼动3号服务器的领导地位。 由此我们得出结论，当领导者产生后，再次有新服务器加入集群，不会影响到现任领导者。 6)Zookeeper 核心理论Zookeepe集群角色 在ZooKeeper集群服中务中有三个角色： •Leader 领导者 ： ​ 1. 处理事务请求 ​ 2. 集群内部各服务器的调度者 •Follower 跟随者 ： ​ 1. 处理客户端非事务请求，转发事务请求给Leader服务器 ​ 2. 参与Leader选举投票 •Observer 观察者： 1. 处理客户端非事务请求，转发事务请求给Leader服务器 Spring Cloud高级特性 Spring提供了一个RestTemplate模板工具类，对基于Http的客户端进行了封装，并且实现了对象与json的序列化和 反序列化，非常方便。RestTemplate并没有限定Http的客户端类型，而是进行了抽象，目前常用的3种都有支持： HttpClient OkHttp JDK原生的URLConnection（默认的） RestTemplate的使用1234567import org.springframework.web.client.RestTemplate;//注册bean//使用方法 发动请求,接收响应,并且帮我们对响应结果进行反序列化String url = &quot;http://localhost/user/8&quot;;User user = restTemplate.getForObject(url, User.class);System.out.println(user); Spring Cloud 实践1 编写消费者和生产者(无注册中心)生产者:常规后端 ​ spring boot应用, entity,service实现增删改查业务,controller接收请求调用业务进行处理,返回给请求者 消费者:简单controller使用restT请求生产者 网页访问消费者,消费者通过resttemplate(硬编码)请求生产者 存在的问题 url硬编码 如果变更,得不到通知地址将失效,不清楚服务状态,服务宕机也不知道 (采用注册中心对服务进行治理 ​ 自动注册和发现 ​ 服务注册到注册中心(服务类型,联系方式) ​ 消费者向服务中心订阅服务,选择服务类型,注册中心自动安排一个符合需求的服务 ​ 状态监管 “心跳(续约)机制” 定期通过http向Eureka刷新自己的状态 ​ 动态路由 一台生产者不具备高可用(集群解决,如何实现负载均衡) 容灾问题 服务如何实现统一配置 实践2 搭建Eureka-server工程Eureka注册中心EurekaServer主要功能:检查那些没有定期发送心跳的,记录服务, 自我保护机制(todo) EurekaClient主要功能: 从Server拉取服务列表,基于负载均衡算法对远程服务进行调用,定时续约, 设置剔除时间(?) 搭建 添加依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; 添加Eureka的服务注解 @EnableEurekaServer 开启服务发现功能 编写配置 服务名,服务端口号 eureka客户端配置 service-url: defaultZone register-with-eureka: 是否注册 默认true fetch-registry: 是否拉取服务 默认true 123456789101112131415server: port: 10086spring: application: name: eureka-servereureka: client: service-url: # eureka 服务地址，如果是集群的话；需要指定其它集群eureka地址 defaultZone: http://127.0.0.1:10086/eureka # 不注册自己 register-with-eureka: false # 不拉取服务 fetch-registry: false 启动测试 依赖是~client 客户端的注解是@EnableDiscoveryClient 客户端常用配置: 续约间隔 服务失效时间 获取服务地址列表间隔时间 Eureka配置123456789101112131415import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;//在启动类添加 @EnableEurekaServer*** //编写配置文件 //网页端口 //应用名称 eureka-server(作为服务的id表示 serviceID) eureka:client:service-url: # EurekaServer的地址，现在是自己的地址，如果是集群，需要写其它Server的地址。defaultZone: http://127.0.0.1:10086/eurekaregister-with-eureka: false # 不注册自己fetch-registry: false #不拉取启动服务 服务注册生产者添加eureka依赖 123456启动类添加@EnabeDiscoveryClient开启Eureka客户端功能 //修改配置 eureka:client:service-url:defaultZone: http://localhost:10086/eureka 服务发现消费者 添加依赖 修改启动类和配置(同生产者) 修改处理器 123456789String url = &quot;http://localhost:9091/user/&quot; + id;//获取eureka中注册的user-service实例列表List&lt;ServiceInstance&gt; serviceInstanceList =discoveryClient.getInstances(&quot;user-service&quot;);ServiceInstance serviceInstance = serviceInstanceList.get(0);url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort()+ &quot;/user/&quot; + id;return restTemplate.getForObject(url, User.class); 服务者从注册中心获取服务列表,从而得知每个服务方的信息, Eureka可以是集群,形成高可用 多个之间互相注册服务,有注册时,会同步到每个节点 三个的集群注册规则 ​ 10086要注册到10087和10088上 ​ 10087要注册到10086和10088上 ​ 10088要注册到10086和10087上 实践3 搭建Eureka集群多机集群 只需要区分IP 端口号可相同 在本机模拟 在启动时修改VM options defaultzone复制个项目 修改端口号启动多个服务 客户端注册 添加注册(???不应该还是注册到一个中心,中心间相互数据同步吗) 1234eureka:client:service-url: # EurekaServer地址,多个地址以&#x27;,&#x27;隔开defaultZone: http://127.0.0.1:10086/eureka,http://127.0.0.1:10087/eureka 生产者会向Eureka服务发起一个Rest请求,并携带自己的元数据信息,Eureka 将信息保存到双层map 1Map&lt;name(服务ID),Map&lt;localhost:user-service:8081(实例ID), &gt; &gt; 默认注册使用主机名或localHost 如果想用ip注册 12345//生产者中eureka:instance:ip-address: 127.0.0.1 # ip地址prefer-ip-address: true # 更倾向于使用ip，而不是host名 服务续约 1234eureka:instance:lease-expiration-duration-in-seconds: 90 lease-renewal-interval-in-seconds: 30 服务失效 默认 90s 认为服务宕机 从服务列表中移除 心跳 30s一次 获取服务列表 当服务消费者启动时，会检测 eureka.client.fetch-registry&#x3D;true 参数的值，如果为true，则会从Eureka Server服务的列表拉取只读备份，然后缓存在本地。并且 每隔30秒 会重新拉取并更新数据。可以在 consumer-demo 项目中通过下面的参数来修改： 123eureka:client:registry-fetch-interval-seconds: 30 失效剔除和自我保护 服务下线 当服务进行正常关闭操作时，它会触发一个服务下线的REST请求给Eureka Server，告诉服务注册中心：“我要下线 了”。服务中心接受到请求之后，将该服务置为下线状态 失效剔除 有时我们的服务可能由于内存溢出或网络故障等原因使得服务不能正常的工作，而服务注册中心并未收到“服务下 线”的请求。相对于服务提供者的“服务续约”操作，服务注册中心在启动时会创建一个定时任务，默认每隔一段时间 （默认为60秒）将当前清单中超时（默认为90秒）没有续约的服务剔除，这个操作被称为失效剔除。 可以通过 eureka.server.eviction-interval-timer-in-ms 参数对其进行修改，单位是毫秒。 自我保护 当服务未按时进行心跳续约时，Eureka会统计服务实例最近15分钟心跳续约的 比例是否低于了85%。在生产环境下，因为网络延迟等原因，心跳失败实例的比例很有可能超标，但是此时就把服务 剔除列表并不妥当，因为服务可能没有宕机。Eureka在这段时间内不会剔除任何服务实例，直到网络恢复正常。生 产环境下这很有效，保证了大多数服务依然可用，不过也有可能获取到失败的服务实例，因此服务调用者必须做好服 务的失败容错。 可以通过下面的配置来关停自我保护： 123eureka:server:enable-self-preservation: false # 关闭自我保护模式（缺省为打开） 实践4 负载均衡Ribbon在实际环境中往往会开启很多个 user-service的集群,此时discoveryClient 获取的服务列表有多个,到底该访问哪一个呢 ​ 负载均衡算法就是在多个实例列表中进行选择 Eureka中集成了 ribbon 简单修改代码即可(提供例如轮询(默认),随机等) 1.首先 配置两个 user-service实例 启动两个,动态设置启动端口号 2.在RestTemplate的bean方法上添加 @LoadBalanced注解 3.不再手动获取IP和端口 而是直接通过服务名称调用 123456@GetMapping(&quot;&#123;id&#125;&quot;)public User queryById(@PathVariable(&quot;id&quot;) Long id)&#123;String url = &quot;http://user-service/user/&quot; + id;User user = restTemplate.getForObject(url, User.class);return user;&#125; 默认轮询,boot提供修改负载均衡配置入口 在消费者中加 1234user-service:ribbon:NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 实验5 熔断器Hystrix 服务降级延迟和容错库,用于隔离访问远程访问,第三方库,防止出现级联失败 雪崩问题:微服务间一个请求可能需要调用多个微服务接口才能实现 如果某个服务异常,请求不会得到响应,则tomcat的这个线程不会释放,于是越来越多的用户到来,越来越多的线程阻塞,导致服务器资源耗尽,其他所有服务都不可用,形成雪崩效应,一个微服务的阻塞导致整个服务的瘫痪 熔断器Hystrix的解决手段: 线程隔离 为每个依赖服务调用分配一个小的线程池 如果线程池已满调用将被立即拒绝,默认不采用排队,加速失败判定时间 服务降级 如果线程池满,或请求超时会进行服务降级:优先保证核心服务,非核心不可用或弱可用 服务熔断 断开服务,防止整个系统被拖垮 1.添加依赖 2.启动类开启熔断 @EnableCircuitBreaker 123456组合注解@SpringCloudApplication @SpringBootApplication @EnableDiscoveryClient @EnableCircuitBreaker 3.编写降级逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041package com.itheima.consumer.controller;import com.itheima.consumer.pojo.User;import com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import org.springframework.web.client.RestTemplate;import java.util.List;@RestController@RequestMapping(&quot;/consumer&quot;)@Slf4jpublic class ConsumerController &#123;@Autowiredprivate RestTemplate restTemplate;@Autowiredprivate DiscoveryClient discoveryClient;@GetMapping(&quot;&#123;id&#125;&quot;)@HystrixCommand(fallbackMethod = &quot;queryByIdFallback&quot;)public String queryById(@PathVariable Long id)&#123;String url = &quot;http://localhost:9091/user/&quot; + id;//获取eureka中注册的user-service实例列表/*List&lt;ServiceInstance&gt; serviceInstanceList =discoveryClient.getInstances(&quot;user-service&quot;);ServiceInstance serviceInstance = serviceInstanceList.get(0);url = &quot;http://&quot; + serviceInstance.getHost() + &quot;:&quot; + serviceInstance.getPort()+ &quot;/user/&quot; + id;*/url = &quot;http://user-service/user/&quot; + id;return restTemplate.getForObject(url, String.class);&#125;public String queryByIdFallback(Long id)&#123;log.error(&quot;查询用户信息失败。id：&#123;&#125;&quot;, id);return &quot;对不起，网络太拥挤了！&quot;; //相同的参数列表和返回值声明&#125;&#125; 不写方法 默认fallbcak 实践6 服务熔断熔断器三个状态 关闭 所有请求正常访问 打开 所有请求都会被降级 ,请求次数不低于20次 失败比例 到达一半 触发熔断 半开 熔断器打开后会计时5s转为此状态(半开),释放部分请求,若都是健康的则关闭,否则保持打开,再次进行5s计时 源码阅读 配置 todo 实践7 Feigh(伪装)把rest的请求进行隐藏,伪装成类似springMVC的controller一样 不用自己拼接url参数等等,一切交给Feign 1234567891011package com.itheima.consumer.client;import com.itheima.consumer.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(&quot;user-service&quot;)public interface UserClient &#123;@GetMapping(&quot;/user/&#123;id&#125;&quot;)User queryById(@PathVariable(&quot;id&quot;) Long id);&#125; 首先这是一个接口 feign会通过动态代理,帮我们生成实现类,跟mybatis的mapper很像 编写新的控制器类 1234567891011121314151617181920package com.itheima.consumer.controller;import com.itheima.consumer.client.UserClient;import com.itheima.consumer.pojo.User;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(&quot;/cf&quot;)public class ConsumerFeignController &#123;@Autowiredprivate UserClient userClient;@GetMapping(&quot;/&#123;id&#125;&quot;)public User queryById(@PathVariable Long id)&#123;return userClient.queryById(id);&#125;&#125; 在启动类开启feign功能 @EnableFeignClients 可以开启请求压缩 gzip减少通信过程中的性能损耗 123456feign:compression:request:enabled: true # 开启请求压缩response:enabled: true # 开启响应压缩 1234567feign:compression:request:enabled: true # 开启请求压缩mime-types: text/html,application/xml,application/json # 设置压缩的数据类型min-request-size: 2048 # 设置触发压缩的大小下限//默认 日志级别(了解)123logging:level:com.itheima: debug 12345678910111213141516package com.itheima.consumer.config;import feign.Logger;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FeignConfig &#123;@BeanLogger.Level feignLoggerLevel()&#123;//记录所有请求和响应的明细，包括头信息、请求体、元数据return Logger.Level.FULL; /*NONE：不记录任何日志信息，这是默认值。BASIC：仅记录请求的方法，URL以及响应状态码和执行时间HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。*/&#125;&#125; 观察日志 实践8 Gateway替代 netflix zuul核心是一系列的过滤器,通过这些过滤器可以将客户端发送的请求转发(路由)到对应微服务,是加在整个微服务最前沿的防火墙和代理器,隐藏微服务结点IP端口信息,从而加强安全保护,本身也是一个微服务,需要注册到中心 核心功能: ​ 路由:路由信息的组成:由一个ID,一个目的URL,一组断言工厂,一组Filter组成 如果路由断言为真,说明请求URL和配置路由匹配 ​ 断言: 网关中的断言函数输入类型是Spring5.0中的 ServerWebExchange,断言函数允许开发者去定义匹配来自于Http request中的任何信息比如请求头和参数 ​ 过滤器: 标准 spring WebFilter 网关中有两种filter 分别是 网关过滤器和 全局过滤器,过滤器将会对请求和响应进行修改处理 快速入门 创建项目(模块Maven) 1.引入依赖123456789101112131415161718192021222324&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;&lt;parent&gt;&lt;artifactId&gt;heima-springcloud&lt;/artifactId&gt;&lt;groupId&gt;com.itheima&lt;/groupId&gt;&lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;groupId&gt;com.itheima&lt;/groupId&gt;&lt;artifactId&gt;heima-gateway&lt;/artifactId&gt;&lt;dependencies&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;/dependencies&gt;&lt;/project&gt; 2.编写启动类123456789101112package com.itheima.gateway;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;@SpringBootApplication@EnableDiscoveryClientpublic class GatewayApplication &#123;public static void main(String[] args) &#123;SpringApplication.run(GatewayApplication.class, args);&#125;&#125; 3.编写配置1234567891011server:port: 10010spring:application:name: api-gatewayeureka:client:service-url:defaultZone: http://127.0.0.1:10086/eurekainstance:prefer-ip-address: true 4.编写路由规则需要用网关来代理 User-service 看下当前 状态 修改网关配置 123456789101112131415161718192021server:port: 10010spring:application:name: api-gatewaycloud:gateway:routes:# 路由id，可以随意写- id: user-service-route# 代理的服务地址uri: http://127.0.0.1:9091# 路由断言，可以配置映射路径predicates:- Path=/user/** #将符合Path规则的一切请求都代理到uri参数指定的地址eureka:client:service-url:defaultZone: http://127.0.0.1:10086/eurekainstance:prefer-ip-address: true 5.启动测试 spring cloud config通过修改git中的配置文件实现所有微服务的配置文件的修改 添加配置中心依赖 配置 1234567891011121314server: port: 12000spring: application: name: config-server cloud: config: server: git: uri: https://gitee.com/goheima/heima-config.giteureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 修改service 删除application 创建 bootstrap.yml 12345678910111213141516171819spring: cloud: config: # 要与仓库中的配置文件的application保持一致 name: user # 要与仓库中的配置文件的profile保持一致 profile: dev # 要与仓库中的配置文件所属的版本（分支）一样 label: master discovery: # 使用配置中心 enabled: true # 配置中心服务名 service-id: config-servereureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka BusSpring Cloud Bus作用：将git仓库的配置文件更新，在不重启系统的情况下实现及时同步到各个微服务 Spring Cloud AlibabaSpring Cloud Alibaba 官方文档note 弹簧靴 &#x3D;&#x3D; spring boot 春云 &#x3D;&#x3D; spring cloud 概述Spring Cloud Alibaba 提供分布式应用开发的一站式解决方案。它包含开发分布式应用程序所需的所有组件，使您可以轻松地使用 Spring Cloud 开发应用程序。 使用 Spring Cloud Alibaba，您只需要添加一些注解和少量配置，即可将 Spring Cloud 应用连接到阿里巴巴的分布式解决方案，并通过阿里巴巴中间件构建分布式应用系统。 特征春云 流量控制和服务降级： Alibaba Sentinel的流量控制、断路和系统自适应保护 服务注册与发现：实例可以注册到阿里巴巴的 Nacos，客户端可以通过 Spring 管理的 bean 发现实例。通过 Spring Cloud Netflix 支持客户端负载均衡器 Ribbon 分布式配置：使用阿里巴巴Nacos作为数据存储 事件驱动：构建与Spring Cloud Stream RocketMQ Binder连接的高度可扩展的事件驱动微服务 Message Bus : 用 Spring Cloud Bus RocketMQ 链接分布式系统的节点 Distributed Transaction ：支持Seata高性能、易用的分布式事务解决方案 Dubbo RPC ：通过Apache Dubbo RPC扩展Spring Cloud service-to-service调用的通信协议 弹簧靴所有的 Spring Boot Starter 都在阿里云 Spring Boot Project中维护。 阿里云对象存储服务Spring Boot Starter 阿里云短信服务Spring Boot Starter 阿里云 Redis的 Spring Boot Starter 阿里云RDS MySQL的Spring Boot Starter 阿里云 SchedulerX的 Spring Boot Starter 入门最简单的入门方法是包含 Spring Cloud BOM，然后添加spring-cloud-alibaba-dependencies到应用程序的类路径中。如果您不想包含所有 Spring Cloud Alibaba 功能，您可以为您想要的功能添加单独的启动器。 spring-cloud-alibaba-dependenciespom中的依赖： 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;&#123;project-version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;复制 如果你想为阿里云服务使用 Spring Boot Starters，你应该将 Aliyun Spring Boot BOM 添加到你的 pom.xml 中： 1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;aliyun-spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;&#123;project-version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; nacossentinelseataNginx","categories":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://gouguoqiang.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]},{"title":"业务分析","slug":"畅购商城/0业务","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:05:14.211Z","comments":true,"path":"2022/09/01/畅购商城/0业务/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/0%E4%B8%9A%E5%8A%A1/","excerpt":"","text":"","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"mysql优化实践","slug":"畅购商城/mysql优化","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:03:44.436Z","comments":true,"path":"2022/09/01/畅购商城/mysql优化/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/mysql%E4%BC%98%E5%8C%96/","excerpt":"","text":"","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"图灵商城项目1","slug":"tuling","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:46:44.477Z","comments":true,"path":"2022/09/01/tuling/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/tuling/","excerpt":"","text":"项目介绍前后端分离的网上商城后端微服务项目,在这个项目中 嗯，我做的是一个前后端分离的网上商城的微服务项目。我在我在本项目的工作主要是嗯对对实现一个商城的需求进行分析，就是你需要完成哪些功能，比如说你要实行商品的上架，然后你要一个管理员的界面，你必须就有普通用户的界面。管理员是来实现商品的上架呀，就是这是提供给这种工作人员使用的一个界面。嗯普通人员的话，需要去搜索商品，购买商品。购买商品，并且呃上作为商家也会有一些促销的手段，营销的手段来进行。提高交易额，促进消费，然后在这个项目中也会有一些，会有一些广告的。像广告，广告的模块。来来提高你的收入。嗯嗯，具体点来说的话，呃，使用了spring boot加呃，加s加s的。这基础框架，然后再使用了微服务的系列微服务，spring cloud微服务。嗯使用了服务的组件有优瑞卡注册注册中心新open原生调用。嗯，gateway网关等一系列的微服务系列，呃，微服务只是本科的项目的。就本科项目的一块儿内容吧，因为主要是为了熟悉恢复一个微服务项目需要怎么搭建呀，怎么去部署？包括你之间的调用，其实跟传统的项目区别也不是很大，只要你会用工具了之后。你只需要写一些配置，真正一些困难的地方是你去写一些具体的业务，就是你在分析好了你的需求之后，你的需求就是会一步步的，嗯。就是更加符合你你的实际，实际写代码。聊你要去设计库表啊之类的。 嗯，我做的是一个前后端分离的网上商城的微服务项目。 嗯，我做的是一个基于space比加mvc加实现的一个前后端分离的网上商城项目。嗯，晚上刷成微服务项使用使用了d收到视频cloud的各种组件，比如说因为他注册中心远程调用和大家都为网关等，嗯，也使用了一些中间键，比如说缓存呀，nx的。这些中间界比如来实现缓存NEX来进行负载均衡呀，等反向代理啊等，然后使用了也使用mk来进行异步操作，流量均分等。本是这样。 我做的是一个基于spring boot实现的网上商城项目。在linux做的系统环境下进行搭建。今天有x系统下进行垃圾使用了来进行缓存，使用了x来进行反向代理，负责均衡等，也有鱼。你学了x使用的是它里面会积一些路啊，使用了一些路啊，脚本等一些东西。完成的本地缓存，并且协同。是spring cloud的微服务各种主线来完成。没有服务的拆分呀，远程调用网关过滤之类的。嗯，采用来进行持久化。嗯的完成的功能呢主要就是一个商城的常见功能，比如说搜索呀，商品的上架，下架。就是有会有一个前前台的功能，会有一个提供给普通用户的一个界面，就是用户的登录啊。用户的灯注册，注册登录。给我搜索商品购买商品，下单，加入购物车的一系列基础的商城功能。也有一些高并发， 配置环境与启动项目Windows上传OSS镜像 导入镜像 创建实例 OSS与导入镜像地域要一致,否则会报invalid错 启动front项目 使用yarn即可 会报找不到Python路径错误 要提前安装Python2.7 (与项目的构建相关 我也不懂为啥) 然后设置Python路径全局配置 npm config set python “D:\\Python27\\python.exe” # 你安装的路径 启动后端项目 确认Maven settings 需要将pom里自带的设置Maven的信息注释掉 出现找不到等报错 可以去找到目标目录删了重新加载 本地域名解析配置 C:\\Windows\\System32\\drivers\\etc 127.0.0.1 tl.nacos.com tlshopdb.com tlshop.com 本机启动nacos 在黑窗体环境下切换目录到nacos&#x2F;bin下 startup.cmd -m standalone 配置数据库 1jdbc:mysql://tlshopdb.com:3306/micromall? root 123456 导入数据 SQL yog 新建同名数据库导入 执行SQL脚本 选择.sql文件执行 即可 速度贼快 Linux目的 学习docker指令 linux 实战,熟练掌握linux常用指令 docker部署各种中间件的一些配置 学会在linux跑起来一个项目使用docker 探明项目的功能 已配好docker环境 MySQL安装(已经安装好了) 下载MySQL5.7的docker镜像： docker pull mysql:5.7 使用如下命令启动MySQL服务： docker run -p 3306:3306 –name mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql \\ -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql \\ -e MYSQL_ROOT_PASSWORD&#x3D;123456 \\ -d mysql:5.7 docker run -p 3306:3306 –name mysql -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql -e MYSQL_ROOT_PASSWORD&#x3D;123456 -d mysql:5.7 docker run -p 3306:3306 –name mysql 参数说明 -p 3306:3306：将容器的3306端口映射到主机的3306端口 -v &#x2F;mydata&#x2F;mysql&#x2F;conf:&#x2F;etc&#x2F;mysql：将配置文件夹挂在到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;log:&#x2F;var&#x2F;log&#x2F;mysql：将日志文件夹挂载到主机 -v &#x2F;mydata&#x2F;mysql&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql&#x2F;：将数据文件夹挂载到主机 -e MYSQL_ROOT_PASSWORD&#x3D;root：初始化root用户的密码 进入运行MySQL的docker容器： docker exec -it mysql &#x2F;bin&#x2F;bash 使用MySQL命令打开客户端： mysql -uroot -proot –default-character-set&#x3D;utf8 创建mall数据库： create database mall character set utf8 安装上传下载插件，并将document&#x2F;sql&#x2F;mall.sql上传到Linux服务器上： yum -y install lrzsz 将mall.sql文件拷贝到mysql容器的&#x2F;目录下： docker cp &#x2F;mydata&#x2F;mall.sql mysql:&#x2F; 将sql文件导入到数据库： use mall; source &#x2F;mall.sql; 创建一个reader:123456帐号并修改权限，使得任何ip都能访问： grant all privileges on . to ‘reader’ @’%’ identified by ‘123456’; Redis安装 下载Redis5.0的docker镜像： docker pull redis:5 使用如下命令启动Redis服务： docker run -p 6379:6379 –name redis \\ -v &#x2F;mydata&#x2F;redis&#x2F;data:&#x2F;data \\ -d redis:5 redis-server –appendonly yes 进入Redis容器使用redis-cli命令进行连接： docker exec -it redis redis-cli ​ redis启动 redis-server &#x2F;usr&#x2F;local&#x2F;redis-5.0.2&#x2F;redis.conf redis-cli auth 123456 Nginx安装 下载Nginx1.10的docker镜像： docker pull nginx:1.10 先运行一次容器（为了拷贝配置文件）： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -d nginx:1.10 将容器内的配置文件拷贝到指定目录： docker container cp nginx:&#x2F;etc&#x2F;nginx &#x2F;mydata&#x2F;nginx&#x2F; 修改文件名称： mv nginx conf 终止并删除容器： docker stop nginx docker rm nginx 使用如下命令启动Nginx服务： docker run -p 80:80 –name nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;html:&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html \\ -v &#x2F;mydata&#x2F;nginx&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx \\ -v &#x2F;mydata&#x2F;nginx&#x2F;conf:&#x2F;etc&#x2F;nginx \\ -d nginx:1.10 默认启动? 怎么查看启动的服务 RabbitMQ安装 下载rabbitmq3.7.15的docker镜像： docker pull rabbitmq:3.7.15 使用如下命令启动RabbitMQ服务： docker run -p 5672:5672 -p 15672:15672 –name rabbitmq -d rabbitmq:3.7.15 进入容器并开启管理功能： docker exec -it rabbitmq &#x2F;bin&#x2F;bash rabbitmq-plugins enable rabbitmq_management ​ 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;15672&#x2F;tcp –permanent firewall-cmd –reload 访问地址查看是否安装成功：http://192.168.3.101:15672 ​ 输入账号密码并登录：guest guest 创建帐号并设置其角色为管理员：mall mall ​ 创建一个新的虚拟host为：&#x2F;mall ​ 点击mall用户进入用户配置页面 ​ 给mall用户配置该虚拟host的权限 ​ Elasticsearch安装 下载Elasticsearch7.6.2的docker镜像： docker pull elasticsearch:7.6.2 修改虚拟内存区域大小，否则会因为过小而无法启动: sysctl -w vm.max_map_count&#x3D;262144 使用如下命令启动Elasticsearch服务： docker run -p 9200:9200 -p 9300:9300 –name elasticsearch \\ -e “discovery.type&#x3D;single-node” \\ -e “cluster.name&#x3D;elasticsearch” \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;plugins:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;plugins \\ -v &#x2F;mydata&#x2F;elasticsearch&#x2F;data:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data \\ -d elasticsearch:7.6.2 启动时会发现&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data目录没有访问权限，只需要修改&#x2F;mydata&#x2F;elasticsearch&#x2F;data目录的权限，再重新启动即可； chmod 777 &#x2F;mydata&#x2F;elasticsearch&#x2F;data&#x2F; 安装中文分词器IKAnalyzer，并重新启动： docker exec -it elasticsearch &#x2F;bin&#x2F;bash #此命令需要在容器中运行 elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.6.2/elasticsearch-analysis-ik-7.6.2.zip docker restart elasticsearch 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;9200&#x2F;tcp –permanent firewall-cmd –reload 访问会返回版本信息：http://192.168.3.101:9200 ​ Logstash安装 下载Logstash7.6.2的docker镜像： docker pull logstash:7.6.2 修改Logstash的配置文件logstash.conf中output节点下的Elasticsearch连接地址为es:9200，配置文件地址：&#x2F;document&#x2F;elk&#x2F;logstash.conf output { elasticsearch { hosts &#x3D;&gt; “es:9200” index &#x3D;&gt; “mall-%{type}-%{+YYYY.MM.dd}” } } 创建&#x2F;mydata&#x2F;logstash目录，并将Logstash的配置文件logstash.conf拷贝到该目录； mkdir &#x2F;mydata&#x2F;logstash 使用如下命令启动Logstash服务； docker run –name logstash -p 4560:4560 -p 4561:4561 -p 4562:4562 -p 4563:4563 \\ –link elasticsearch:es \\ -v &#x2F;mydata&#x2F;logstash&#x2F;logstash.conf:&#x2F;usr&#x2F;share&#x2F;logstash&#x2F;pipeline&#x2F;logstash.conf \\ -d logstash:7.6.2 进入容器内部，安装json_lines插件。 logstash-plugin install logstash-codec-json_lines Kibana安装 下载Kibana7.6.2的docker镜像： docker pull kibana:7.6.2 使用如下命令启动Kibana服务： docker run –name kibana -p 5601:5601 \\ –link elasticsearch:es \\ -e “elasticsearch.hosts&#x3D;http://es:9200&quot; \\ -d kibana:7.6.2 开启防火墙： firewall-cmd –zone&#x3D;public –add-port&#x3D;5601&#x2F;tcp –permanent firewall-cmd –reload 访问地址进行测试：http://192.168.3.101:5601 ​ MongoDB安装 下载MongoDB4.2.5的docker镜像： docker pull mongo:4.2.5 使用docker命令启动： docker run -p 27017:27017 –name mongo \\ -v &#x2F;mydata&#x2F;mongo&#x2F;db:&#x2F;data&#x2F;db \\ -d mongo:4.2.5 Docker全部环境安装完成 所有下载镜像文件： REPOSITORY TAG IMAGE ID CREATED SIZE redis 5 071538dbbd71 2 weeks ago 98.3MB mongo 4.2.5 fddee5bccba3 3 months ago 388MB logstash 7.6.2 fa5b3b1e9757 4 months ago 813MB kibana 7.6.2 f70986bc5191 4 months ago 1.01GB elasticsearch 7.6.2 f29a1ee41030 4 months ago 791MB rabbitmq 3.7.15-management 6ffc11daa8d0 13 months ago 186MB mysql 5.7 7faa3c53e6d6 15 months ago 373MB registry 2 f32a97de94e1 17 months ago 25.8MB nginx 1.10 0346349a1a64 3 years ago 182MB java 8 d23bdf5b1b1b 3 years ago 643MB 所有运行在容器里面的应用： ​ SpringBoot应用部署构建所有Docker镜像并上传 修改项目根目录下的pom.xml中的docker.host属性： &lt;docker.host&gt;http://192.168.3.101:2375docker.host&gt; properties&gt; 如果项目根目录的pom.mxl中docker-maven-plugin的节点被注释掉了就打开注释，使项目在打包时直接构建Docker镜像； ​ 直接双击根项目mall的package命令可以一次性打包所有应用的Docker镜像； ​ REPOSITORY TAG IMAGE ID CREATED SIZE mall&#x2F;mall-portal 1.0-SNAPSHOT 70e0f76416a0 21 seconds ago 705MB mall&#x2F;mall-search 1.0-SNAPSHOT f3290bd1d0c7 41 seconds ago 725MB mall&#x2F;mall-admin 1.0-SNAPSHOT 26557b93a106 About a minute ago 705MB 部署mall-admin docker run -p 8080:8080 –name mall-admin \\ –link mysql:db \\ –link redis:redis \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;admin&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-admin:1.0-SNAPSHOT 注意：如果想使用Logstash收集日志的话，需要将应用容器连接到Logstsh，添加如下配置即可； –link logstash:logstash \\ 部署mall-search docker run -p 8081:8081 –name mall-search \\ –link elasticsearch:es \\ –link mysql:db \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;search&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-search:1.0-SNAPSHOT 部署mall-port docker run -p 8085:8085 –name mall-portal \\ –link mysql:db \\ –link redis:redis \\ –link mongo:mongo \\ –link rabbitmq:rabbit \\ -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime \\ -v &#x2F;mydata&#x2F;app&#x2F;portal&#x2F;logs:&#x2F;var&#x2F;logs \\ -d mall&#x2F;mall-portal:1.0-SNAPSHOT 开启防火墙 firewall-cmd –zone&#x3D;public –add-port&#x3D;8080&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8081&#x2F;tcp –permanent firewall-cmd –zone&#x3D;public –add-port&#x3D;8085&#x2F;tcp –permanent firewall-cmd –reload 访问接口进行测试 mall-admin的api接口文档地址：http://192.168.3.101:8080/swagger-ui.html ​ mall-search的api接口文档地址：http://192.168.3.101:8081/swagger-ui.html ​ mall-portal的api接口文档地址：http://192.168.3.101:8085/swagger-ui.html ​ 总结psLinux中的ps命令是Process Status的缩写。ps命令用来列出系统中当前运行的那些进程。 ps命令列出的是当前那些进程的快照，就是执行ps命令的那个时刻的那些进程，如果想要动态的显示进程信息，就可以使用top命令。要对进程进行监测和控制，首先必须要了解当前进程的情况，也就是需要查看当前进程，而 ps 命令就是最基本同时也是非常强大的进程查看命令。使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等。总之大部分信息都是可以通过执行该命令得到的。 ps 为我们提供了进程的一次性的查看，它所提供的查看结果并不动态连续的；如果想对进程时间监控，应该用 top 工具。 linux上进程有5种状态 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码 D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 语法 ps [option] 命令参数 a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的行程 -C&lt;命令&gt; 列出指定命令的状况 –lines&lt;行数&gt; 每页显示的行数 –width&lt;字符数&gt; 每页显示的字符数 –help 显示帮助信息 –version 显示版本显示 部分使用实例 ps -A 显示所有进程信息 ps -u root 显示指定用户信息 ps -ef 显示所有进程信息，连同命令行 ps -ef | grep ssh 查找特定进程 ps -l 将目前属于您自己这次登入的 PID 与相关信息列示出来 ps aux 列出目前所有的正在内存当中的程序 ps -axjf 列出类似程序树的程序显示 ps aux | egrep ‘(cron|syslog)’ 找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps -aux | more 可以用 | 管道和 more 连接起来分页查看 ps -aux &gt; ps001.txt 把所有进程显示出来，并输出到ps001.txt文件 ps -o pid,ppid,pgrp,session,tpgid,comm 输出指定的字段 F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) STIME 启动时间 TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts&#x2F;0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 ps -efUID PID PPID C STIME TTY TIME CMD ​ docker run –name&#x3D;”容器新名字” 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即启动交互式容器(前台有伪终端，等待交互)； -P: 随机端口映射，大写P -p: 指定端口映射，小写p docker execdocker exec [OPTIONS] 容器名称 COMMAND [ARG…]OPTIONS说明： -d，以后台方式执行命令； -e，设置环境变量 -i，交互模式 -t，设置TTY -u，用户名或UID，例如myuser:myusergroup 通常COMMAND只能是一条语句，为了支持多个命令的执行，需要将多个命令连接起来交给Shell，docker exec命令的使用示例如下： sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject &amp;&amp; git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC} &amp;&amp; git checkout FETCH_HEAD”;sudo docker exec myContainer bash -c “cd &#x2F;home&#x2F;myuser&#x2F;myproject;git fetch ssh:&#x2F;&#x2F;gerrit_server:29418&#x2F;myparent&#x2F;myproject ${GERRIT_REFSPEC};git checkout FETCH_HEAD”; 注意：对于已经暂停或停止了的容器，无法执行docker exec命令，如下将抛出异常： docker pause myContainerdocker exec myContainer … options 作用 -d 在后台运行命令-i 即使没有附加也保持 STDIN 打开-t 设置TTY进入容器的 CLI 模式 -e 设置环境变量-w 需要执行命令的目录-u 指定访问容器的用户名备注：其实还有几个 options，但是目前还没用到，要用的时候再写吧 实际栗子执行 tomcat 容器的 startup.sh 脚本 docker exec -it tomcat7 startup.sh进入容器的 CLI 模式(最常用) docker exec -it tomcat7 bash执行普通命令 docker exec -it tomcat7 pwd 指定工作目录执行命令 docker exec -it -w &#x2F;usr tomcat7 pwd 以 root 用户身份进入容器(重点) docker exec -it -uroot jenkins1 bash linux入门：”&quot;的作用运行cellranger count，发现每行末尾有个\\，遂查了下\\的作用 123456cellranger count --id=XPBShm \\ --transcriptome=/home/rstudio/opt/refdata-gex-GRCh38-and-mm10-2020-A \\ --fastqs=/home/rstudio/data/rawx/xpbs \\ --sample=XPBS \\ --r1-length 26 \\ --r2-length 98 作用有2 1 作为转义符反斜线符号“ \\ ”在Bash中被解释为转义字符，用于去除一个单个字符的特殊意义，它保留了跟随在之后的字符的字面值，除了换行符（\\n,\\r）。 如果在反斜线之后一个换行字符立即出现，转义字符使行 得以继续，但是换行字符后必须紧跟命令，不能出现空格，遇到命令很长时使用反斜线很有效。 例一： 1234[linux@linux ~]$ echo $HOME/home/[linux@linux ~]$ echo \\$HOME$HOME 例子中，反斜线去除了“ $ ”字符的特殊意义，保留字面值，从而不输出home目录路径。 2. 作为换行符例二： 12345678910export PATH=\\/bin:\\/sbin:\\/usr/bin:\\/usr/sbin:\\/usr/local/bin:\\/apps/bin:\\/apps/tools:\\/apps/tslib/bin\\ 例子中，反斜线使行得以继续，命令可以正常输入。 例二（反） 12345678910export PATH=\\ /bin:\\ /sbin:\\ /usr/bin:\\ /usr/sbin:\\ /usr/local/bin:\\ /apps/bin:\\ /apps/tools:\\ /apps/tslib/bin\\ 例子中就会出现错误： &#x2F;bin:: bad variable name&#x2F;* &#x2F;bin：错误变量名 *&#x2F; 因为在”+换行符”之后必须紧跟命令，不能有空格。 笔记DevOps： DevOps的意思就是开发和运维不再是分开的两个团队，而是你中有我，我中有 你的一个团队。我们现在开发和运维已经是一个团队了，但是运维方面的知识和 经验还需要持续提高。 持续交付： 持续交付的意思就是在不影响用户使用服务的前提下频繁把新功能发布给用户使用，要做到 这点非常非常难。我们现在两周一个版本，每次上线之后都会给不同的用户造成不同程度的 影响。 容器化： 容器化的好处在于运维的时候不需要再关心每个服务所使用的技术栈了，每个服务都被无差 别地封装在容器里，可以被无差别地管理和维护，现在比较流行的工具是docker和k8s。 所以你也可以简单地把云原生理解为：云原生 &#x3D; 微服务 + DevOps + 持续交付 + 容器化 商品模块业务场景介绍： 商品模块业务详解 ​ ​ ​ 表的设计：打开游览器访问京东详细页问题： 商品这块的数据库如何更好的设计，商品详细页显示这么多信息，是一张表还是多张表更好了？ 这个问题到底是一张表还是多张表，我们判断依据是什么？我们判断商品详细页里面显示的这些信息他们的关系。通过他们的关系，我们才能知道到底是设计一张表还是多张表。 一张表： 如果是一张表存储所有数据的话，那么查询是非常方便的，这是其优点，但是你会发现存储的时候是不是很麻烦。不通类型不同大小不通商品等等都不一样，那这样的一张表设计起来实在是太复杂了。 多张表： 如果是多张表的话业务更加清晰，维护起来也更加方便，但是你会发现查询好像会非常的复杂，一个商品页面我们需要查很多的表和数据。 解决 我们正确的方式是根据不同的数据类型按不通的表进行存储 商品表的设计在设计这个表的时候，嗯，会画出它的实体关系图。嗯那你设计一张表的时候，你的查询十分方便，但是你的你设计这个表会很复杂，就是想要把各种各种情况，各种类型全部囊括进来，十分负责，所以选择按根据不同的类型。去跟显示不同的表进行存储。嗯商品有不同类型，就比如有吃的，穿的。还有其他用的。就可以根据分类来划分我们的礼品商品。嗯 需求分析为什么商品需要分类？ 我们知道商品是有不通类型的，比如有吃的 比如有穿的比如还有其他的用的。不通的商品用途不一样。我们一开始就可以按分类来进行划分我们的商品，这个就有点像我们去看论坛的分类是一样的。 第一个版本：商品+分类 ​ 问题：此时有什么问题？： 目前这个方案有什么问题了？我们慢慢发现一个问题，只有分类并不能适应所有的需求，比如nike鞋和nikeT恤，用户可能希望先看nike的所有商品，这个模型就不能满足。我们想在这个关系中，加入“品牌”概念 但是只有分类的时候并不能适应所有的需求，你只是比如说耐克鞋，耐克T恤，用户希望能先看耐克的所有商品，这个模型就不能满足，只要在加入品牌的概念。这样基本用户可以在通过分类或者品牌找到自己想要的商品。 第二个版本：商品+分类+品牌 ​ 这样基本用户可以在首页上通过分类或者品牌找到自己想要的商品，也可以直接查看热门的商品和新上架的商品。 问题：此时有什么问题？ 但是问题也来了，用户在进入分类后，展示在用户面前的是很多很多商品，用户希望再通过筛选查询出更接近他目标的商品？ 用户在进入分类之后展示在湖面前的是很多很多商品，用户希望通过筛选查询出更接近他目标的商品。其实就加入了属性。比如说图案呀！嗯，之类的。 ​ 加入属性： 于是优秀的产品设计师，设计出了类似这样的UI： ​ ​ 第三个版本：商品+分类+品牌+属性 用户可以通过这些筛选条件进一步缩小自己的目标范围，那么问题又来了，这样的产品需求排在程序员面前，怎么去实现它？经过分析，我们找出了一个方法，我们知道商品之间的属性可能存在着较大的差别，比如牛仔裤它有版型、腰型、裤长等属性；而电脑它有CPU、显卡等属性，各类商品的属性是不同的。再进一步想，休闲裤也版型、腰型、裤长等属性；台式电脑或者笔记本电脑都有CPU、显卡等属性。所以我们得出：一个分类对应若干属性，而一个属性，对应若干属性选项，而一个具体商品又对应若干属性选项（例如具体一条牛仔裤，他的裤长：7分，裤型：直筒）。有点绕，仔细品味一下。 通过分类与品牌查询到相关的商品，再根据属性嗯，在写。来进一步缩小范围，比如牛仔裤，它有版型，腰型，裤长等属性，而电脑有CPU，显卡的出现。一个分类对应若干属性来一个性对应若干属性选项。一个具体商品有对应活该属性决定。各分类对应若干属性，一个属性对应若干属性选项。一个具体商品有对应若干属性选项。月底一条牛仔裤，它的裤长裤型的。 ​ 从图上可以看出，分类和属性的关系（例如：“牛仔裤”分类下有裤型、裤长、版型等属性）、属性和属性选项的关系（例如：裤长属性有长款、九分裤、七分裤的选项）、商品和属性选项的关系（例如某条牛仔裤的裤长是7分裤）。至此，我们知道一个商品的分类、品牌以及它有什么属性和对应的属性值。那么通过筛选条件，自然就可以查询出指定的商品。这里特别说一句，价格也是属性，不要设想用商品表中的价格字段去做计算。这不利于查询也增加了复杂度，让商家编辑人员用属性来设置并保证他的正确性。 ​ ​ 这个页面展示商品的所有信息，按照之前的设计好像都可以满足。但是我们似乎感觉错过了什么，在图上右边我们发现该商品当前的颜色和尺寸，并且允许用户可以选择其他的颜色和尺寸。这给我们带来了疑惑，这里的“颜色”和“尺寸”是什么，一件商品的不同颜色不同尺寸是算一个商品还是多个商品。 ​ 为什么要加入规格： 第四个版本：商品+分类+品牌+属性+规格 经过思考后，我们发现我们混淆了两个概念——“商品”和“货品”。不同规格的货品作为独立的商品。比如一条裤子的有L尺寸、M尺寸、一个U盘有16G还是32G的，都是同样的货品，不同规格的商品。可以认为货品和商品是一对多的关系。弄清了这个概念，处理这个需求就容易多了，这里的“颜色”、“尺寸”我们就作为“规格”来处理，而红色、黑色；L号、M号我们视为规格的选项或者说规格值。一件货品对应若干规格，而具有某一规格值的货品就是商品。 spu：iphone12 sku：金色64 iphone12 ​ 好了，现在好像差不多了。基于这个模型可以满足基本的商品搜索、展示的需求。搜索引擎也可以根据这个模型数据生成对应的商品索引，达到准确搜索的目的。商品模块还会和其他模块一起协作，比如用户系统、订单系统、支付系统等。一般情况下我们会把商品业务独立出来做成“商品中心”的服务，集中处理商品查询、更新、发布等业务，支撑其他业务。 ​ \\ 嗯，总结商品表的设计，商品表的设计主要是根据呃，用户对商品进行搜索，并且商家对用户对商家对商品进行展示来设计的。首先你用户想搜索到商品它需要进行分类查询，就是会有衣服啊，电子商品啊这种。设计成一个分类，那么在分类之后你还嗯会显示态度很还是是太多，还是需要进行一些区分？嗯，就比如说你的需求是只查看特步的。鞋子啊，或者特步的特步鞋或者特步。特步的衣服，然后就是你需要加还有品牌的概念。然后用户再去根据分类还有品牌来缩小它的具体范围。但是这样还是有很多的，很多的区分就比如说你需要嗯，你是你需要看他的具体的属性就是如何一个牛仔裤它的裤长。裤长，腰宽这种属性，这个属性呢是跟它的分类挂钩的，比如说你的裤子它就是一个就能把它的属性选项给出来。然后 嗯，表的设计，表的设计主要是基于用户的搜索需求与商家的展示需求来进行设计。嗯比较重要的有五张表，然后其他还有一些。关联表啊，一些这种就这种关系表，嗯，比较重要的就是你首先要有一个商品表就是你对，但是你的商品会十分复杂，你就要把它拆分成分类，就是你一个啊，不对。商品货品 就是分类和品牌之后你可以添加，你可以根据商品的属性再进行搜索，就比如说你的CPU，CPU，CPU型号呀，机身内存呀这些对应，然后每一个，每一个这些属性要对应着每一个属性选项。嗯，属性之后呢你对应的属性选项就比如说16G呀，那机身内存16G这些。但是你在真正的我看看去 商品的搜索： ​ 搜索引擎elasticsearch 商品的展示： 三、商品模块展示技术难点商品详情页是展示商品详细信息的一个页面，承载在网站的大部分流量和订单的入口。京东商城目前有通用版、全球购、闪购、易车、惠买车、服装、拼购、今日抄底等许多套模板。各套模板的元数据是一样的，只是展示方式不一样。目前商品详情页个性化需求非常多，数据来源也是非常多的，而且许多基础服务做不了的都放我们这，因此我们需要一种架构能快速响应和优雅的解决这些需求问题。因此我们重新设计了商品详情页的架构，主要包括三部分：商品详情页系统、商品详情页统一服务系统和商品详情页动态服务系统；商品详情页系统负责静的部分，而统一服务负责动的部分，而动态服务负责给内网其他系统提供一些数据服务。 ​ 商品详情页前端结构 前端展示可以分为这么几个维度：商品维度(标题、图片、属性等)、主商品维度（商品介绍、规格参数）、分类维度、商家维度、店铺维度等；另外还有一些实时性要求比较高的如实时价格、实时促销、广告词、配送至、预售等是通过异步加载。 ​ ​ SPU： Standard Product Unit （标准化产品单元）,SPU是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息的集合，该集合描述了一个产品的特性。 SKU： Stock keeping unit(库存量单位) SKU即库存进出计量的单位（买家购买、商家进货、供应商备货、工厂生产都是依据SKU进行的），在服装、鞋类商品中使用最多最普遍。 例如纺织品中一个SKU通常表示：规格、颜色、款式。SKU是物理上不可分割的最小存货单元。 单品页流量特点 热点少，各种爬虫、比价软件抓取。 静态处理 thymeleaf等模板引擎 架构方案的问题：问题一：我们知道数据新增分:增量和全量数据 如果后台的小二新增了很多的商品，那我们都要对这些商品进行静态化，但是现在有个问题。那这些数据如何同步了？这是一个新增商品同步的问题，那这个问题怎么解决比较好了？。 ​ 不同应用部署在不同服务器甚至在不同的机房不同的国家。 1、通过网络同步的方式 就是其中一台服务器静态化之后，然后把文件同步到其他应用服务器上去。比如我们的linux命令scp方式。这种方式虽然可行，但是我们发现问题还是蛮多的，有多少个节点就需要同步多少份，等于是商品的数量*服务器的应用数数。很显然这种办法不是最优的解决办法 如果上述办法无法解决，那我们就用另外的方案，同学们你们觉得还有其他的方案没有？ **2、定时任务:**可以在某个应用用一个定时任务，然后分别去执行数据库需要静态化的数据即可，可以解决上述1数据同步的问题，因为所有的任务都是在本机运行，就不需要数据同步了。但是也有一个问题。就是如何避免不通的机器跑的数据不要重复，也就是A和B定时任务都跑了一份商品。这个是这种方案需要解决的。（比较直观的就是上锁） 3、消息中间件：还有一种办法就是通过消息中间件来解决。订阅topic然后生成当前服务器静态化的页面。 问题二：我们的freemark它是数据要事先按我这个模板生产好的，那就是说一定你改了模板，如果要生效的话，需要重新在把数据取出来和我们这个模板进行匹配生产更多的的静态html文件。那这是一个比较大的问题 如果后台数据有变更呢?如何及时同步到其它服务端? 如果页面静态化了，我们搜索打开一个商品详细页，怎么知道要我需要的访问的静态页面？ 万一我们模板需要修改了怎么办？ 牵一发动全身。 ​ 3.1、压测测试 jmeter模板 ​ 1、换数据库 2、分库分表 3.2、后台 12345678910111213141516171819202122/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = portalProductDao.getProductInfo(id); if (null == productInfo) &#123; return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; return productInfo;&#125; 压测结果： 5000并发 ​ 后台优化：redis缓存： redis设置：RedisConifg》RedisOpsUtil 12345678910111213141516171819202122232425262728293031/** * 获取商品详情信息 * * @param id 产品ID */public PmsProductParam getProductInfo(Long id) &#123; PmsProductParam productInfo = null; //从缓存Redis里找 productInfo = redisOpsUtil.get(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, PmsProductParam.class); if(null!=productInfo)&#123; return productInfo; &#125; productInfo = portalProductDao.getProductInfo(id); if (null==productInfo) &#123; log.warn(&quot;没有查询到商品信息,id:&quot;+id); return null; &#125; FlashPromotionParam promotion = flashPromotionProductDao.getFlashPromotion(id); if (!ObjectUtils.isEmpty(promotion)) &#123; productInfo.setFlashPromotionCount(promotion.getRelation().get(0).getFlashPromotionCount()); productInfo.setFlashPromotionLimit(promotion.getRelation().get(0).getFlashPromotionLimit()); productInfo.setFlashPromotionPrice(promotion.getRelation().get(0).getFlashPromotionPrice()); productInfo.setFlashPromotionRelationId(promotion.getRelation().get(0).getId()); productInfo.setFlashPromotionEndDate(promotion.getEndDate()); productInfo.setFlashPromotionStartDate(promotion.getStartDate()); productInfo.setFlashPromotionStatus(promotion.getStatus()); &#125; redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE + id, productInfo, 3600, TimeUnit.SECONDS); return productInfo;&#125; 好处： 加入redis之后我们发现提高了可以把之前请求 数据库查询的商品都缓存到redis中，通过对redis的访问来减少对数据里的依赖，减少了依赖本质就是减少了磁盘IO。 问题：提高请求的吞吐量，除了减少磁盘IO，还有网络IO，我们可以发现，请求redis其实也会涉及到网络IO，我们所有的请求都要走xxx端口号。那有没有更好的优化思路了，来同学们你们鲜花在哪儿？ 读多写少有两种： 1、最终一致性方案： 设置超时时间来解决 ​ redisOpsUtil.set(RedisKeyPrefixConst.PRODUCT_DETAIL_CACHE+id,productInfo,360,TimeUnit.SECONDS); 2、实时一致性方案： ​ 课程讲到 交易canal binlog 两个问题(高并发)、压缩的问题》减少内存 现在有什么问题了？ 跟我们预期只set一次redis 是有出入，为何会这样子了？并发问题 当我第二次再去访问，此时此刻没有日志输出，说明全部走了缓存： 并发问题：并发编程》并发问题》锁的方式来实现 java并发 加锁方式（不适合》特殊》分布式） 分布式锁：redis、zookeeper QPS立马就提高了很多。 加入分布式锁: 1234567&lt;!--加入redisson--&gt;&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt;&lt;/dependency&gt; setnx 缓存应用场景： 1、访问量大、QPS高、更新频率不是很高的业务 2、数据一致性要求不高 缓存和数据库双写一致性问题： 一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。 答这个问题，先明白一个前提。就是如果对数据有强一致性要求，不能放缓存。 我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说降低不一致发生的概率，无法完全避免。因此，有强一致性要求的数据，不能放缓存。 zk&gt;临时顺序节点》原子性 线程创建如果可以创建成功，是否第一个 拿到了锁 业务场景介绍： 熟悉秒杀系统的业务和技术核心点、以及流程等 正常电商流程: 秒杀场景演示： 完毕 活动和场次关系 秒杀活动表：sms_flash_promotion 秒杀场次表：sms_flash_promotion_session 场次商品关系表：sms_flash_promotion_product_relation 一个活动可以有多个场次，每个场次可以有多个商品进行秒-杀。 秒杀系统设计 分两部分内容；秒杀业务设计和秒杀技术实现。 3215186661 11秒杀链路优化三 图灵：楼兰 秒杀系统核心交易链路优化 三 今天要处理的问题：秒杀场景下如何进行限流。 今天要做的内容： 解决的问题：1、在秒杀页面，客户点击秒杀后，在前台弹出一个验证码，需要用户 输入验证码才能往后端发送请求，这样能够错开秒杀下单的时间。 2、通过验证码，对后台下单请求进行保护，防止刷单，即绕开前端，直接往后端发 送请求。 在秒杀页面开始秒杀后，客户点击秒杀按钮，要在前台弹出一个验证 码，需要用户输入验证码才能往后端发请求，这样能够错开秒杀下单时 间。 在我们的实现中，是要将memberId、producrId和验证码的值一起传入 后台，后台返回一个token。然后再根据这个tokne拼接一个后台秒杀地 址。这个token会存入到redis中。实际秒杀时，会增加一个判断，检测 这个token是不是在redis中存在。如果不存在，就是机器刷单 一、电商项目中秒杀的实现流程 1、在tmll-admin中添加秒杀活动，在秒杀活动中先设置活动的开始日期和结束日 期，然后添加商品。 这个秒杀活动信息会保存到mysql中。sms_flash_promotion_product_relation 表。 同时，添加商品后会将活动商品保存到ZK中。(路 径&#x2F;ZkLock&#x2F;load_db&#x2F;{productId})。然后，当访问到商城前端商品页时， http://lo calhost:8080&#x2F;#&#x2F;product&#x2F;{productId}，会检查Redis中的产品信息缓存。如果 Redis中没有产品信息，就会重建Redis缓存。key为product:detail:cache:{ProdId} 然后进入商城的单品页 http://localhost:8080/#/product/32 ， product.vue 那个”立即购买”的按钮就会变成”立即秒杀” 点击立即秒杀就会进入秒杀页。secKillDetail.vue 代码实现机制： 1、从Redis判断商品是否有秒杀活动。 一个商品要么就只能秒杀，要么就只能普通购买，这样是否合理？ 这就 是为什么要单独独立出一套秒杀服务集群。 2、发送后台请求申请验证码。后台返回验证码图片，并将验证码的计算结果保存 到Redis。 验证码的请求路径里header里的memeberId是怎么进去的。有什么用？ 生成验证码图片的这个请求要怎么防刷？ 3、保护后台请求接口。 输入验证码后，先验证输入的验证码结果，返回一个Token。这个Token会传 入到接下来的商品确认页面，同时会保存到Redis当中，表示当前用户有购买秒杀商 品的资格。有效期300秒，300秒内必须完成下单，否则就要重新申请秒杀资格。 在后续的下单过程中，需要传入这个Token才能正常下单。 验证码如果输入错误，是如何判断的？ 二、如何加强限流方案的安全性 了解整理流程后，要继续深入思考下我们这个限流方案的安全性。 1&gt; 针对验证码 针对验证码的安全性，可以加上之前的验证码内容。 1、我们做了这一套机制后，到底有多安全？ 下单请求依然是可以用机器人模拟 的。 用户ID是存在Cookie当中的，可以拿到。 图形验证码是随机的，那就总有可能产生容易被机器识别的验证码。 2、怎么加强验证码本身的安全性 这个问题也是必须要前后台配合来思考的，而不是单独靠前端或者后端能够解决 的。这个方案要如何设计？ 提高验证码安全性的措施：1、加干扰线或者干扰 点，2，将关键字符变形并且在图形上串到一起。3、增加更多的前端交互，行为 验证。 验证码的内容最好是一个比较复杂的题目，而不是简单的输入数字。这样可以有 效延长下单请求的时长，更好的分散请求峰值。 图形验证码可以篡改。可以用PostMan另外访问生成图形验证码的接口，这时 Redis里的值就被篡改了，不再是页面上看到的计算结果了。如何处理？1、增加 更多的判断因素，例如IP。2、前端签名，后端验证签名。 输入了验证码之后，存在Redis中的验证码要及时删除。同时生成一个Token， 代表当前用户有购买权限。这个Token有效期是非常短的。 针对验证机制的安全性，可以增加一些安全机制。 换一种验证码 我们动手来换一种复杂一点的验证码，HappyCaptcha 官网地址： https://gitee.c om&#x2F;ramostear&#x2F;Happy-Captcha 换的方式比较简单，首先在pom.xml中加入HappyCaptcha的依赖 com.ramostear Happy-Captcha 1.0.1 1 2 3 4 5 然后在OmsPortalOrderController中getVerifyCode方法，将生成验证码的部分修 改一下： 这样，前台的验证码就变成了一闪一闪的动画。并且是中文的加减法，更难破解。 可以看到整个HappyCaptcha的实现机制跟我们自己的实现机制是差不多的，也是 使用session来存储答案。 其他还有哪些更难以破解的验证码？ 2&gt; 针对下单请求 我们的实现机制是要求将token拼凑到请求路径上来。这跟把token作为参数传递有 什么区别？ 如果一个模拟程序需要使用机器来参与秒杀抢单，首先需要根据其他用户的请求来 分析获取下单路径。如果是同一个请求路径，只是带的参数不同，那机器完全可以 尝试用暴力破解的方式来尝试进行下单。如果碰巧传入了一个Redis中的token值， 那他就下单成功了。但是现在把参数隐藏到了请求路径当中，动态的请求路径对于 下单的机器来说，就比较难试探出请求的地址，这样就增加了他下单的难度。 三、电商整体的秒杀限流方案： try { &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x2F;&#x2F;这个步骤就会完成生成图片并且往response发送的步骤。 HappyCaptcha.require(request,response).style(CaptchaStyle.ANIM) .type(CaptchaType.ARITHMETIC_ZH) .build().finish(); Object captcha &#x3D; request.getSession().getAttribute(“happycaptcha”); &#x2F;&#x2F;HappyCaptcha生成的验证码是String类型 int code &#x3D; Integer.parseInt(captcha.toString()); log.info(“验证码答案:{}”,captcha); redisOpsUtil.set(RedisKeyPrefixConst.MIAOSHA_VERIFY_CODE_PREFIX + memberId + “:” + productId ,code ,300 ,TimeUnit.SECONDS); &#x2F;&#x2F;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; HappyCaptcha验证码结束 &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; return null; }catch(Exception e) { e.printStackTrace(); return CommonResult.failed(“秒杀失败”); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 我们天天都在说三高，高并发、高可用、高可扩展，那到底应该如何去落地一个三 高的设计方案？ 构建大并发、高性能、高可用系统中几种通用的优化思路，可以抽象总结为“4 要 1 不要”原则。也就是：数据要尽量少、请求数要尽量少、路径要尽量短、依赖要尽 量少，以及不要有单点。当然，这几点是你要努力的方向，具体操作时还是要密切 结合实际的场景和具体条件来进行。 针对秒杀这个场景，其实方案设计往往比技术细节更为重要。因为你可以想象，每 一个秒杀环节的经典问题，都意味着互联网的秒杀业务出现过大的问题，这都是实 打实买来的教训。发现了问题之后才会有针对性的方案设计。那现在，我们整体来 回顾下电商的秒杀限流方案。 错峰1：动静分离的本质是将包含浏览者信息的动态数据和不包含浏览者信息的静态 资源区分开。例如在商品单品页，商品信息是不包含浏览者信息的，这部分就可以 抽象出静态资源。而用户登录状态、cookie等这些动态数据也尽可能缓存起来，并 且使缓存能够离用户更近。 错峰2：秒杀答题的形式可以是多种多样的，目的是防止机器刷单，以及错开用户的 下单时长。在秒杀场景下，答题速度靠后的请求自然就没有库存了，也可以减少系 统的请求量。 错峰3：缓存的作用主要有两个，一是快速扣减库存，保护数据库流量，并且库存扣 减完成后，快速通知Nginx，屏蔽后续请求；二是提前识别热点数据，并且针对热 点数据提供优化处理。处理的方案主要是三个，一是优化，二是限制，三是隔离， 包括业务隔离、系统隔离、数据隔离。 错峰4：单独提供秒杀服务集群，有利于减少秒杀商品的超大流量对普通商品的性能 冲击，不要让1%的商品影响到另外的99%。 后台错峰：这一部分是我们实战课程的重点。之前monkey老师带大家在后端针对 秒杀场景做了非常多的设计与实战。我们这个图中每一个错峰点虽然在图上就是比 较简单的一个点，但是深入进去，每个地方要考虑的细节都还是非常多的，大家可 以回顾下之前的几节课，体会下如何在后端对秒杀服务做针对性的优化。 首先想到的是使用MQ进行削峰。但是实际上，后端需要考虑的三高问题也远不止 MQ削峰这一步。每一个环节都需要考虑后端组件是否能够承载得住。例如秒杀服务 集群，到底应该部署多大的集群？部署多少台机器呢？显然为了顶住秒杀的大流 量，秒杀集群就需要部署得非常大。但是，如果在大部分没有秒杀服务的时间内， 这个集群的资源就闲置得非常厉害。所以，虚拟化+云计算进行弹性部署也是非常重 要的。在我们的项目实战课后面就会由诸葛老师给大家带来k8s和云部署的实战课 程。 然后：在后端系统中，添加了Redis、MQ这样的一些中间产品。而这些产品集群本 身，也存在效率低下、服务崩溃的风险。这样也就给系统整体带来了更多的风险 点。那要怎么去屏蔽这些产品给系统带来的风险呢？大家可以思考一下，下一节课 将会由fox老师给大家进行系统降级方面的设计。 题外话 方案优先 &gt; 技术优先。学习技术的同时，都要增加对软件问题的思考，很多同学 技术学得很快，但是缺乏思考。秒杀这种超大并发场景下的限流问题，不是任何一 个技术或者任何一个步骤可以限制住的，需要一个完整全面的方案才能保证业务稳 定性。所以我们在开发过程中，不能只埋头于技术点，要站在更高的角度，整体来 理解解决方案，这样才能更深入的理解自己在做的事情，也才能真正来解决问题。 这才是高级程序员与普通程序员真正的区别。 例如针对前端验证问题，还有哪些优化方案？ 提前发Token。可以在秒杀前设置一个预约活动。 在活动中提前发放 token。例如一个秒杀活动有20W个商品，那就可以预先准备200W个 token。用户进行预约时，只发放200W个Token，其他人也能预约成 功，但是其实没有获得token，那后面的秒杀，直接通过这个token就可 以过滤掉一大部分人。相当于没有token的人都只预约了个寂寞。这也是 互联网常用的一个套路。 例如针对超卖问题，在之前的课程中，介绍了如何使用Redis分布式锁防 超卖。针对同一个商品ID，使用一把分布式锁，确实可以很快很方便的 处理超卖问题。但是如果同时进行秒杀的商品多了呢？像京东、淘宝一 场大型的秒杀活动，同时有成千上万个商品要进行秒杀，那就意味着同 一时间Redis上锁解锁的操作会要执行成千上万次，这对Redis的性能消 耗是相当巨大的，Redis就有可能升级成为新的性能瓶颈。这时该怎么 办？ 当然具体问题的解决方案从来不止一个，这里我们可以选择一种返璞归 真的方案，把秒杀超卖的问题从分布式降级到本地JVM中，来获取极限 性能。例如将秒杀服务接入配置中心，然后在秒杀服务开始前，由配置 中心给每个应用服务实例下发一个库存数量。然后每次下单，每个服务 器只管自己的库存数量，与其他应用服务器完全不进行库存同步，在各 自的内存里扣减库存，这样就不会有超卖的情况发生。减少了网络消 耗，性能也能够进一步提升。 这种方案可不可行呢？当然也会有一些问题需要去处理。有可能某给服 务器上的库存很快消耗完了，而其他的服务器上仍有库存。整个服务就 会表现为你抢不到商品，但是在你后面抢商品的人却能抢到商品。(你们 在参与秒杀时有没有过这样的经历？)但是这在秒杀这种场景下，完全是 可以接受的。另外，如果某一个应用服务器挂了，那给他分配的库存就 会丢失。这时候又要怎么办？其实也没必要再去设置什么复杂的逻辑， 大不了少卖一点出去。反正都是售罄了，全卖完了，和卖了99%，其实 没什么区别。这时只需要统计好订单的数量(可以通过MQ来统计，也可 以通过Redis统计)，等秒杀活动的30分钟等待支付期过去后，再将没卖 出去的库存重新丢回库存池，与没有付款而被取消的订单商品一起返场 售卖就可以了。这也是很多互联网公司目前采用的方案。 最后虽然我们是后台开发工程师，但是前端也必须要有所了解。今天我们关注的 这个问题，也不能只关注后端，需要前后端一起才能理解他的作用。","categories":[{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"网关模块","slug":"畅购商城/gateway","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:03:11.863Z","comments":true,"path":"2022/09/01/畅购商城/gateway/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/gateway/","excerpt":"","text":"概述 谓词 &#x3D;&#x3D; 过滤条件 该项目提供了一个库，用于在 Spring WebFlux 之上构建 API 网关。Spring Cloud Gateway 旨在提供一种简单而有效的方式来路由到 API，并为它们提供横切关注点，例如：安全性、监控&#x2F;指标和弹性。 特征Spring Cloud Gateway 特点： 基于 Spring Framework 5、Project Reactor 和 Spring Boot 2.0 构建 能够匹配任何请求属性的路由。 谓词和过滤器特定于路由。 断路器集成。 Spring Cloud Discovery客户端集成 易于编写谓词和过滤器 请求速率限制 路径重写 入门12345678910111213141516171819202122232425@SpringBootApplicationpublic class DemogatewayApplication &#123; @Bean public RouteLocator customRouteLocator(RouteLocatorBuilder builder) &#123; return builder.routes() .route(&quot;path_route&quot;, r -&gt; r.path(&quot;/get&quot;) .uri(&quot;http://httpbin.org&quot;)) .route(&quot;host_route&quot;, r -&gt; r.host(&quot;*.myhost.org&quot;) .uri(&quot;http://httpbin.org&quot;)) .route(&quot;rewrite_route&quot;, r -&gt; r.host(&quot;*.rewrite.org&quot;) .filters(f -&gt; f.rewritePath(&quot;/foo/(?&lt;segment&gt;.*)&quot;, &quot;/$&#123;segment&#125;&quot;)) .uri(&quot;http://httpbin.org&quot;)) .route(&quot;hystrix_route&quot;, r -&gt; r.host(&quot;*.hystrix.org&quot;) .filters(f -&gt; f.hystrix(c -&gt; c.setName(&quot;slowcmd&quot;))) .uri(&quot;http://httpbin.org&quot;)) .route(&quot;hystrix_fallback_route&quot;, r -&gt; r.host(&quot;*.hystrixfallback.org&quot;) .filters(f -&gt; f.hystrix(c -&gt; c.setName(&quot;slowcmd&quot;).setFallbackUri(&quot;forward:/hystrixfallback&quot;))) .uri(&quot;http://httpbin.org&quot;)) .route(&quot;limit_route&quot;, r -&gt; r .host(&quot;*.limited.org&quot;).and().path(&quot;/anything/**&quot;) .filters(f -&gt; f.requestRateLimiter(c -&gt; c.setRateLimiter(redisRateLimiter()))) .uri(&quot;http://httpbin.org&quot;)) .build(); &#125;&#125; Learn3. 它是如何工作的下图提供了 Spring Cloud Gateway 如何工作的高级概述： 客户端向 Spring Cloud Gateway 发出请求。如果网关处理程序映射确定请求与路由匹配，则将其发送到网关 Web 处理程序。此处理程序通过特定于请求的过滤器链运行请求。过滤器用虚线划分的原因是过滤器可以在发送代理请求之前和之后运行逻辑。执行所有“预”过滤器逻辑。然后发出代理请求。发出代理请求后，将运行“发布”过滤器逻辑。 在没有端口的路由中定义的 URI 分别获得 HTTP 和 HTTPS URI 的默认端口值 80 和 443。 Route：网关的基本构建块。它由 ID、目标 URI、谓词集合和过滤器集合定义。如果聚合谓词为真，则匹配路由。 谓词：这是一个Java 8 函数谓词。输入类型是Spring FrameworkServerWebExchange。这使您可以匹配来自 HTTP 请求的任何内容，例如标头或参数。 过滤器GatewayFilter：这些是使用特定工厂构建的实例。在这里，您可以在发送下游请求之前或之后修改请求和响应。 4. 配置路由谓词工厂和网关过滤工厂有两种配置谓词和过滤器的方法：快捷方式和完全扩展的参数。下面的大多数示例都使用快捷方式。 名称和参数名称将code在每个部分的第一句或第二句中列出。参数通常按快捷方式配置所需的顺序列出。 4.1。快捷方式配置快捷方式配置由过滤器名称识别，后跟等号 ( =)，后跟以逗号 ( ,) 分隔的参数值。 应用程序.yml 12345678spring: cloud: gateway: routes: - id: after_route uri: https://example.org predicates: - Cookie=mycookie,mycookievalue 前面的示例Cookie使用两个参数定义了路由谓词工厂，cookie 名称mycookie和要匹配的值mycookievalue。 4.2. 完全扩展的参数完全扩展的参数看起来更像是带有名称&#x2F;值对的标准 yaml 配置。通常，会有一把name钥匙和一把args钥匙。键是用于配置谓词或过滤器的args键值对映射。 应用程序.yml 1234567891011spring: cloud: gateway: routes: - id: after_route uri: https://example.org predicates: - name: Cookie args: name: mycookie regexp: mycookievalue Cookie这是上面显示的谓词的快捷配置的完整配置。 5.路由谓词工厂Spring Cloud Gateway 将路由匹配为 Spring WebFluxHandlerMapping基础架构的一部分。Spring Cloud Gateway 包含许多内置的路由谓词工厂。所有这些谓词都匹配 HTTP 请求的不同属性。您可以将多个路由谓词工厂与逻辑and语句结合起来。 5.1。After Route 谓词工厂After路由谓词工厂采用一个参数 a （datetime它是一个 java ZonedDateTime）。此谓词匹配在指定日期时间之后发生的请求。以下示例配置了一个 after 路由谓词： 示例 1.application.yml 12345678spring: cloud: gateway: routes: - id: after_route uri: https://example.org predicates: - After=2017-01-20T17:42:47.789-07:00[America/Denver] 此路线匹配 2017 年 1 月 20 日 17:42 Mountain Time（丹佛）之后提出的任何请求。 5.3. 路线间谓词工厂Between路由谓词工厂有两个参数，它们datetime1是datetime2 javaZonedDateTime对象。此谓词匹配发生在 afterdatetime1和 before的请求datetime2。datetime2参数必须在之后datetime1。以下示例配置了一个 between 路由谓词： 示例 3.application.yml 12345678spring: cloud: gateway: routes: - id: between_route uri: https://example.org predicates: - Between=2017-01-20T17:42:47.789-07:00[America/Denver], 2017-01-21T17:42:47.789-07:00[America/Denver] 此路线匹配 2017 年 1 月 20 日 17:42 山区时间（丹佛）和 2017 年 1 月 21 日 17:42 山区时间（丹佛）之前提出的任何请求。这对于维护窗口可能很有用。 5.4. Cookie 路由谓词工厂Cookie路由谓词工厂有两个参数，cookie和namea regexp（这是一个 Java 正则表达式）。此谓词匹配具有给定名称且其值与正则表达式匹配的 cookie。以下示例配置 cookie 路由谓词工厂： 示例 4.application.yml 12345678spring: cloud: gateway: routes: - id: cookie_route uri: https://example.org predicates: - Cookie=chocolate, ch.p 此路由匹配具有名为chocolate其值与ch.p正则表达式匹配的 cookie 的请求。 5.5. 标头路由谓词工厂Header路由谓词工厂有两个参数，the和headera regexp（这是一个 Java 正则表达式）。此谓词与具有给定名称且值与正则表达式匹配的标头匹配。以下示例配置了一个标头路由谓词： 示例 5.application.yml 12345678spring: cloud: gateway: routes: - id: header_route uri: https://example.org predicates: - Header=X-Request-Id, \\d+ 如果请求具有一个名为X-Request-Id其值与\\d+正则表达式匹配的标头（即，它具有一个或多个数字的值），则此路由匹配。 5.6. 主机路由谓词工厂路由谓词工厂采用Host一个参数：主机名列表patterns。该模式是一种 Ant 风格的模式，.以分隔符为分隔符。此谓词匹配Host与模式匹配的标头。以下示例配置主机路由谓词： 示例 6.application.yml 12345678spring: cloud: gateway: routes: - id: host_route uri: https://example.org predicates: - Host=**.somehost.org,**.anotherhost.org 还支持URI 模板变量（例如&#123;sub&#125;.myhost.org）。 如果请求具有Host值为www.somehost.orgorbeta.somehost.org或的标头，则此路由匹配www.anotherhost.org。 此谓词将 URI 模板变量（例如sub，在前面的示例中定义）提取为名称和值的映射，并将其放置在 中，ServerWebExchange.getAttributes()其中的键定义为ServerWebExchangeUtils.URI_TEMPLATE_VARIABLES_ATTRIBUTE。然后这些值可供GatewayFilter工厂使用 5.7. 方法路由谓词工厂MethodRoute Predicate Factory 接受一个参数，该methods参数是一个或多个参数：要匹配的 HTTP 方法。以下示例配置方法路由谓词： 示例 7.application.yml 12345678spring: cloud: gateway: routes: - id: method_route uri: https://example.org predicates: - Method=GET,POST GET如果请求方法是 a或 a ，则此路由匹配POST。 5.8. 路径路由谓词工厂PathRoute Predicate Factory 有两个参数：一个 Spring 列表PathMatcher patterns和一个名为matchTrailingSlash（默认为）的可选标志true。以下示例配置路径路由谓词： 示例 8.application.yml 12345678spring: cloud: gateway: routes: - id: path_route uri: https://example.org predicates: - Path=/red/&#123;segment&#125;,/blue/&#123;segment&#125; 如果请求路径是，则此路由匹配，例如：/red/1or/red/1/或/red/blueor /blue/green。 如果matchTrailingSlash设置为false，则请求路径/red/1/将不匹配。 此谓词将 URI 模板变量（例如segment，在前面的示例中定义）提取为名称和值的映射，并将其放置在 中，ServerWebExchange.getAttributes()其中的键定义为ServerWebExchangeUtils.URI_TEMPLATE_VARIABLES_ATTRIBUTE。然后这些值可供GatewayFilter工厂使用 可以使用一种实用方法（称为get）来更轻松地访问这些变量。以下示例显示了如何使用该get方法： 123Map&lt;String, String&gt; uriVariables = ServerWebExchangeUtils.getPathPredicateVariables(exchange);String segment = uriVariables.get(&quot;segment&quot;); 5.9. 查询路由谓词工厂Query路由谓词工厂有两个参数：一个必需的param和一个可选的regexp（它是一个 Java 正则表达式）。以下示例配置查询路由谓词： 示例 9.application.yml 12345678spring: cloud: gateway: routes: - id: query_route uri: https://example.org predicates: - Query=green 如果请求包含green查询参数，则前面的路由匹配。 应用程序.yml 12345678spring: cloud: gateway: routes: - id: query_route uri: https://example.org predicates: - Query=red, gree. 如果请求包含red其值与正则gree.表达式匹配的查询参数，则前面的路由匹配，因此green并且greet会匹配。 5.10。RemoteAddr 路由谓词工厂路由谓词工厂采用的RemoteAddr列表（最小大小为 1）sources，它们是 CIDR 表示法（IPv4 或 IPv6）字符串，例如192.168.0.1/16（其中192.168.0.1是 IP 地址和16子网掩码）。以下示例配置 RemoteAddr 路由谓词： 示例 10.application.yml 12345678spring: cloud: gateway: routes: - id: remoteaddr_route uri: https://example.org predicates: - RemoteAddr=192.168.1.1/24 5.11。权重路线谓词工厂Weight路由谓词工厂有两个参数：和group（weight一个 int）。权重是按组计算的。以下示例配置权重路由谓词： 示例 12.application.yml 123456789101112spring: cloud: gateway: routes: - id: weight_high uri: https://weighthigh.org predicates: - Weight=group1, 8 - id: weight_low uri: https://weightlow.org predicates: - Weight=group1, 2 该路由会将约 80% 的流量转发到weighthigh.org，将约 20% 的流量转发到weightlow.org 5.12。XForwarded 远程地址路由谓词工厂路由谓词工厂采用的XForwarded Remote Addr列表（最小大小为 1）sources，它们是 CIDR 表示法（IPv4 或 IPv6）字符串，例如192.168.0.1/16（其中192.168.0.1是 IP 地址和16子网掩码）。 此路由谓词允许根据X-Forwarded-ForHTTP 标头过滤请求。 这可以与反向代理一起使用，例如负载平衡器或 Web 应用程序防火墙，其中仅当请求来自这些反向代理使用的受信任的 IP 地址列表时才允许请求。 以下示例配置 XForwardedRemoteAddr 路由谓词： 示例 13.application.yml 12345678spring: cloud: gateway: routes: - id: xforwarded_remoteaddr_route uri: https://example.org predicates: - XForwardedRemoteAddr=192.168.1.1/24 X-Forwarded-For如果标头包含例如，则此路由匹配192.168.1.10。","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"Nginx模块","slug":"畅购商城/nginx","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:04:09.863Z","comments":true,"path":"2022/09/01/畅购商城/nginx/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/nginx/","excerpt":"","text":"5.2 nginx的限流nginx提供两种限流的方式： 一是控制速率 二是控制并发连接数 5.2.1 控制速率控制速率的方式之一就是采用漏桶算法。 (1)漏桶算法实现控制速率限流 漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率.示意图如下: (2)nginx的配置 配置示意图如下： 超出则返回503 我当前无法响应你 binary_remote_addr 是一种key，表示基于 remote_addr(客户端IP) 来做限流，binary_ 的目的是压缩内存占用量。zone：定义共享内存区来存储访问信息， contentRateLimit:10m 表示一个大小为10M，名字为contentRateLimit的内存区域。1M能存储16000 IP地址的访问信息，10M可以存储16W IP地址访问信息。rate 用于设置最大访问速率，rate&#x3D;10r&#x2F;s 表示每秒最多处理10个请求。Nginx 实际上以毫秒为粒度来跟踪请求信息，因此 10r&#x2F;s 实际上是限制：每100毫秒处理一个请求。这意味着，自上一个请求处理完后，若后续100毫秒内又有请求到达，将拒绝处理该请求.我们这里设置成2 方便测试。 修改&#x2F;usr&#x2F;local&#x2F;openresty&#x2F;nginx&#x2F;conf&#x2F;nginx.conf: 12345678910111213141516171819202122232425262728293031323334353637383940user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; #使用限流配置 limit_req zone=contentRateLimit; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试： 重新加载配置文件 123cd /usr/local/openresty/nginx/sbin./nginx -s reload 访问页面：http://192.168.211.132/read_content?id=1 ,连续刷新会直接报错。 (3)处理突发流量 上面例子限制 2r&#x2F;s，如果有时正常流量突然增大，超出的请求将被拒绝，无法处理突发流量，可以结合 burst 参数使用来解决该问题。 例如，如下配置表示： 上图代码如下： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; burst 译为突发、爆发，表示在超过设定的处理速率后能额外处理的请求数,当 rate&#x3D;10r&#x2F;s 时，将1s拆成10份，即每100ms可处理1个请求。 此处，**burst&#x3D;4 **，若同时有4个请求到达，Nginx 会处理第一个请求，剩余3个请求将放入队列，然后每隔500ms从队列中获取一个请求进行处理。若请求数大于4，将拒绝处理多余的请求，直接返回503. 不过，单独使用 burst 参数并不实用。假设 burst&#x3D;50 ，rate依然为10r&#x2F;s，排队中的50个请求虽然每100ms会处理一个，但第50个请求却需要等待 50 * 100ms即 5s，这么长的处理时间自然难以接受。 因此，burst 往往结合 nodelay 一起使用。 例如：如下配置： 1234567891011server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125;&#125; limit_req zone&#x3D;one burst&#x3D;5 nodelay;第一个参数：zone&#x3D;contentRateLimit 设置使用哪个配置区域来做限制，与上面limit_req_zone 里的name对应。 第二个参数：burst&#x3D;5，重点说明一下这个配置，burst爆发的意思，这个配置的意思是设置一个大小为5的缓冲区当有大量请求(爆发)过来时，超过了访问频次限制的请求可以先放到这个缓冲区内。 第三个参数：nodelay，如果设置，超过访问频次而且缓冲区也满了的时候就会直接返回503，如果没有设置，则所有请求会等待排队。实例二 burst缓存处理 我们看到，我们短时间内发送了大量请求，Nginx按照毫秒级精度统计，超出限制的请求直接拒绝。这在实际场景中未免过于苛刻，真实网络环境中请求到来不是匀速的，很可能有请求“突发”的情况，也就是“一股子一股子”的。Nginx考虑到了这种情况，可以通过burst关键字开启对突发请求的缓存处理，而不是直接拒绝。 来看我们的配置： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4; }}我们加入了burst&#x3D;4，意思是每个key(此处是每个IP)最多允许4个突发请求的到来。如果单个IP在10ms内发送6个请求，结果会怎样呢？ 相比实例一成功数增加了4个，这个我们设置的burst数目是一致的。具体处理流程是：1个请求被立即处理，4个请求被放到burst队列里，另外一个请求被拒绝。通过burst参数，我们使得Nginx限流具备了缓存处理突发流量的能力。 但是请注意：burst的作用是让多余的请求可以先放到队列里，慢慢处理。如果不加nodelay参数，队列里的请求不会立即处理，而是按照rate设置的速度，以毫秒级精确的速度慢慢处理。 实例三 nodelay降低排队时间 实例二中我们看到，通过设置burst参数，我们可以允许Nginx缓存处理一定程度的突发，多余的请求可以先放到队列里，慢慢处理，这起到了平滑流量的作用。但是如果队列设置的比较大，请求排队的时间就会比较长，用户角度看来就是RT变长了，这对用户很不友好。 有什么解决办法呢？nodelay参数允许请求在排队的时候就立即被处理，也就是说只要请求能够进入burst队列，就会立即被后台worker处理，请注意，这意味着burst设置了nodelay时，系统瞬间的QPS可能会超过rate设置的阈值。nodelay参数要跟burst一起使用才有作用。 延续实例二的配置，我们加入nodelay选项： limit_req_zone $binary_remote_addr zone&#x3D;mylimit:10m rate&#x3D;2r&#x2F;s;server { location &#x2F; { limit_req zone&#x3D;mylimit burst&#x3D;4 nodelay; }}单个IP 10ms内并发发送6个请求，结果如下： 跟实例二相比，请求成功率没变化，但是总体耗时变短了。这怎么解释呢？实例二中，有4个请求被放到burst队列当中，工作进程每隔500ms(rate&#x3D;2r&#x2F;s)取一个请求进行处理，最后一个请求要排队2s才会被处理；实例三中，请求放入队列跟实例二是一样的，但不同的是，队列中的请求同时具有了被处理的资格，所以实例三中的5个请求可以说是同时开始被处理的，花费时间自然变短了。 但是请注意，虽然设置burst和nodelay能够降低突发请求的处理时间，但是长期来看并不会提高吞吐量的上限，长期吞吐量的上限是由rate决定的，因为nodelay只能保证burst的请求被立即处理，但Nginx会限制队列元素释放的速度，就像是限制了令牌桶中令牌产生的速度。 看到这里你可能会问，加入了nodelay参数之后的限速算法，到底算是哪一个“桶”，是漏桶算法还是令牌桶算法？当然还算是漏桶算法。考虑一种情况，令牌桶算法的token为耗尽时会怎么做呢？由于它有一个请求队列，所以会把接下来的请求缓存下来，缓存多少受限于队列大小。但此时缓存这些请求还有意义吗？如果server已经过载，缓存队列越来越长，RT越来越高，即使过了很久请求被处理了，对用户来说也没什么价值了。所以当token不够用时，最明智的做法就是直接拒绝用户的请求，这就成了漏桶算法 令牌桶算法: 2.5.2 令牌桶算法令牌桶算法是比较常见的限流算法之一，大概描述如下：1）所有的请求在处理之前都需要拿到一个可用的令牌才会被处理；2）根据限流大小，设置按照一定的速率往桶里添加令牌；3）桶设置最大的放置令牌限制，当桶满时、新添加的令牌就被丢弃或者拒绝；4）请求达到后首先要获取令牌桶中的令牌，拿着令牌才可以进行其他的业务逻辑，处理完业务逻辑之后，将令牌直接删除；5）令牌桶有最低限额，当桶中的令牌达到最低限额的时候，请求处理完之后将不会删除令牌，以此保证足够的限流 如下图： 这个算法的实现，有很多技术，Guaua是其中之一，redis客户端也有其实现。 123456789101112routes: - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/brand** filters: - StripPrefix=1 - name: RequestRateLimiter #请求数限流 名字不能随便写 ，使用默认的facatory args: key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 redis-rate-limiter.replenishRate是您希望允许用户每秒执行多少请求，而不会丢弃任何请求。这是令牌桶填充的速率 redis-rate-limiter.burstCapacity是指令牌桶的容量，允许在一秒钟内完成的最大请求数,将此值设置为零将阻止所有请求。 超过也许会返回429错误码 too many request 如上表示： 平均每秒允许不超过2个请求，突发不超过4个请求，并且处理突发4个请求的时候，没有延迟，等到完成之后，按照正常的速率处理。 如上两种配置结合就达到了速率稳定，但突然流量也能正常处理的效果。完整配置代码如下： 123456789101112131415161718192021222324252627282930313233343536373839user root root;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 测试：如下图 在1秒钟之内可以刷新4次，正常处理。 但是超过之后，连续刷新5次，抛出异常。 5.2.2 控制并发量（连接数）ngx_http_limit_conn_module 提供了限制连接数的能力。主要是利用limit_conn_zone和limit_conn两个指令。 利用连接数限制 某一个用户的ip连接的数量来控制流量。 注意：并非所有连接都被计算在内 只有当服务器正在处理请求并且已经读取了整个请求头时，才会计算有效连接。此处忽略测试。 配置语法： 123Syntax: limit_conn zone number;Default: —;Context: http, server, location; (1)配置限制固定连接数 如下，配置如下： limit_conn_zone $binary_remote_addr zone&#x3D;addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10M limit_conn addr 2; 表示 同一个地址只允许连接2次。 上图配置如下： 12345678910111213141516171819202122232425262728293031323334353637383940http &#123; include mime.types; default_type application/octet-stream; #cache lua_shared_dict dis_cache 128m; #限流设置 limit_req_zone $binary_remote_addr zone=contentRateLimit:10m rate=2r/s; #根据IP地址来限制，存储内存大小10M limit_conn_zone $binary_remote_addr zone=addr:1m; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; #所有以brand开始的请求，访问本地changgou-service-goods微服务 location /brand &#123; limit_conn addr 2; proxy_pass http://192.168.211.1:18081; &#125; location /update_content &#123; content_by_lua_file /root/lua/update_content.lua; &#125; location /read_content &#123; limit_req zone=contentRateLimit burst=4 nodelay; content_by_lua_file /root/lua/read_content.lua; &#125; &#125;&#125; 表示： 123limit_conn_zone $binary_remote_addr zone=addr:10m; 表示限制根据用户的IP地址来显示，设置存储地址为的内存大小10Mlimit_conn addr 2; 表示 同一个地址只允许连接2次。 测试： 此时开3个线程，测试的时候会发生异常，开2个就不会有异常 (2)限制每个客户端IP与服务器的连接数，同时限制与虚拟服务器的连接总数。(了解) 如下配置： 12345678910111213limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m; server &#123; listen 80; server_name localhost; charset utf-8; location / &#123; limit_conn perip 10;#单个客户端ip与服务器的连接数． limit_conn perserver 100; ＃限制与服务器的总连接数 root html; index index.html index.htm; &#125;&#125;","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"},{"name":"Nginx","slug":"Nginx","permalink":"https://gouguoqiang.github.io/tags/Nginx/"}]},{"title":"jvm","slug":"7JVM","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:41:41.043Z","comments":true,"path":"2022/09/01/7JVM/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/7JVM/","excerpt":"","text":"第一章 字节码篇分析一个对象的创建字节码D:\\IDEA\\projects\\Pinduo&gt;javap -verbose -p out&#x2F;production&#x2F;Pinduo&#x2F;P1&#x2F;djp1&#x2F;Main.class 123456789101112131415//// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//package P1.djp1;public class Main &#123; public Main() &#123; &#125; public static void main(String[] args) &#123; new Object(); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869D:\\IDEA\\projects\\Pinduo&gt;javap -verbose -p out/production/Pinduo/P1/djp1/Main.classClassfile /D:/IDEA/projects/Pinduo/out/production/Pinduo/P1/djp1/Main.class Last modified 2022年9月23日; size 418 bytes MD5 checksum 339314e49862fe3b31f3c66703efb5ca Compiled from &quot;Main.java&quot;public class P1.djp1.Main minor version: 0 major version: 52 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #3 // P1/djp1/Main super_class: #2 // java/lang/Object interfaces: 0, fields: 0, methods: 2, attributes: 1Constant pool: #1 = Methodref #2.#19 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Class #20 // java/lang/Object #3 = Class #21 // P1/djp1/Main #4 = Utf8 &lt;init&gt; #5 = Utf8 ()V #6 = Utf8 Code #7 = Utf8 LineNumberTable #8 = Utf8 LocalVariableTable #9 = Utf8 this #10 = Utf8 LP1/djp1/Main; #11 = Utf8 main #12 = Utf8 ([Ljava/lang/String;)V #13 = Utf8 args #14 = Utf8 [Ljava/lang/String; #15 = Utf8 o #16 = Utf8 Ljava/lang/Object; #17 = Utf8 SourceFile #18 = Utf8 Main.java #19 = NameAndType #4:#5 // &quot;&lt;init&gt;&quot;:()V #20 = Utf8 java/lang/Object #21 = Utf8 P1/djp1/Main&#123; public P1.djp1.Main(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 7: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this LP1/djp1/Main; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: new #2 // class java/lang/Object 3: dup 4: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: return LineNumberTable: line 11: 0 line 13: 8 LocalVariableTable: Start Length Slot Name Signature 0 9 0 args [Ljava/lang/String; 8 1 1 o Ljava/lang/Object;&#125;SourceFile: &quot;Main.java&quot; 123456789101112131415161718D:\\IDEA\\projects\\Pinduo&gt;javap -c out/production/Pinduo/P1/djp1/Main.classCompiled from &quot;Main.java&quot;public class P1.djp1.Main &#123; public P1.djp1.Main(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return public static void main(java.lang.String[]); Code: 0: new #2 // class java/lang/Object 3: dup 4: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 7: astore_1 8: return&#125; NEW ：如果找不到Class对象，则进行类加载。加载成功后，则在堆中分配内存，从Object 开始到本类路径上的所有属性值都要分配内存。分配完毕之后，进行零值初始化。在分配过程中，注意引用是占据存储空间的，它是一个变量，占用4个字节。这个指令完毕后，将指向实例对象的引用变量压入虚拟机栈顶。DUP ：在栈顶复制该引用变量，这时的栈顶有两个指向堆内实例对象的引用变量。如果 方法有参数，还需要把参数压人操作栈中。两个引用变量的目的不同，其中压至底下的引用用于赋值，或者保存到局部变量表，另一个栈顶的引用变量作为句柄调用相关方法。INVOKESPECIAL ：调用对象实例方法，通过栈顶的引用变量调用＜init&gt; 方法。 第二章 类的加载1. 概述注意：方法区只有HotSpot虚拟机有，J9，JRockit都没有 如果自己想手写一个Java虚拟机的话，主要考虑哪些结构呢？ 类加载器 执行引擎 Bootstrap -&gt; Ext -&gt; App(也叫系统) 组合关系 但是 有一个GetParent方法 URLcl -&gt; Ext -&gt; App 1.1 类加载器子系统类加载器子系统作用： ClassLoader只负责class文件的加载，至于它是否可以运行，则由Execution Engine决定。 加载的类信息存放于一块称为方法区的内存空间。除了类的信息外，方法区中还会存放运行时常量池信息，可能还包括字符串字面量和数字常量（这部分常量信息是Class文件中常量池部分的内存映射） 1.2 类加载器ClassLoader角色 class file存在于本地硬盘上，可以理解为设计师画在纸上的模板，而最终这个模板在执行的时候是要加载到JVM当中来根据这个文件实例化出n个一模一样的实例。 class file加载到JVM中，被称为DNA元数据模板，放在方法区。 在.class文件–&gt;JVM–&gt;最终成为元数据模板，此过程就要一个运输工具（类装载器Class Loader），扮演一个快递员的角色。 2. 类加载过程(生命周期)面试题 Java 类加载过程?（苏宁） 描述一下 JVM 加载 Class 文件的原理机制?（国美） JVM底层怎么加载class文件的？（蚂蚁金服） 类加载过程 （蚂蚁金服） Java 类加载过程? （百度） 描述一下 JVM 加载 Class 文件的原理机制? （蚂蚁金服） Java类加载过程 （美团） 描述一下JVM加载class文件的原理机制 （美团） 什么是类的加载？ （京东） 讲一下JVM加载一个类的过程 （京东） 概述12345678JAVApublic class HelloLoader &#123; public static void main(String[] args) &#123; System.out.println(&quot;谢谢ClassLoader加载我....&quot;); System.out.println(&quot;你的大恩大德，我下辈子再报！&quot;); &#125;&#125; 它加载(与对象创建区分)是怎么样的呢? 执行 main() 方法（静态方法）就需要先加载main方法所在类 HelloLoader 加载成功，则进行链接、初始化等操作。完成后调用 HelloLoader 类中的静态方法 main 加载失败则抛出异常 &#96;&#96;&#96;javaclass A {static { System.out.println(“1”);}A() { System.out.println(“a”);}}class B extends A {static { System.out.println(“2”);}B() { System.out.println(“b”);}}public class Main { public static void main(String[] args) &#123; B b = new B(); A a = new A(); // 12aba &#125; } 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#### 2.1. 加载阶段**加载：**1. 通过一个类的全限定名获取定义此类的二进制字节流2. 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构3. **在内存中生成一个代表这个类的java.lang.Class对象**，作为方法区这个类的各种数据的访问入口**加载class文件的方式：**1. 从本地系统中直接加载2. 通过网络获取，典型场景：Web Applet3. 从zip压缩包中读取，成为日后jar、war格式的基础4. 运行时计算生成，使用最多的是：动态代理技术5. 由其他文件生成，典型场景：JSP应用从专有数据库中提取.class文件，比较少见6. 从加密文件中获取，典型的防Class文件被反编译的保护措施**数组类的加载**创建数组类的情况稍微有些特殊，因为数组类本身并不是由类加载器负责创建，而是由JVM在运行时根据需要而直接创建的，但数组的元素类型仍然需要依靠类加载器去创建。创建数组类（下述简称A）的过程：1. 如果数组的元素类型是引用类型，那么就遵循定义的加载过程递归加载和创建数组A的元素类型；2. JVM使用指定的元素类型和数组维度来创建新的数组类。3. 如果数组的元素类型是引用类型，数组类的可访问性就由元素类型的可访问性决定。否则数组类的可访问性将被缺省定义为public。int[] arrString[] arrObject[] arr#### 2.2. 链接阶段链接分为三个子阶段：验证 -&gt; 准备 -&gt; 解析##### 验证(Verify)1. 目的在于确保Class文件的字节流中包含信息符合当前虚拟机要求，保证被加载类的正确性，不会危害虚拟机自身安全2. 主要包括四种验证，文件格式验证，元数据验证，字节码验证，符号引用验证。链接过程之验证阶段(Verification)当类加载到系统后，就开始链接操作，验证是链接操作的第一步。它的目的是保证加载的字节码是合法、合理并符合规范的。验证的步骤比较复杂，实际要验证的项目也很繁多，大体上Java虚拟机需要做以下检査，如图所示。整体说明：验证的内容则涵盖了类数据信息的格式验证、语义检查、字节码验证，以及符号引用验证等。- 其中格式验证会和装载阶段一起执行。验证通过之后，类加载器才会成功将类的二进制数据信息加载到方法区中。- 格式验证之外的验证操作将会在方法区中进行。**举例**使用 BinaryViewer软件查看字节码文件，其开头均为 CAFE BABE ，如果出现不合法的字节码文件，那么将会验证不通过。##### 准备(Prepare)1. 为类变量（static变量）分配内存并且设置该类变量的默认初始值，即零值2. 这里不包含用final修饰的static，因为final在编译的时候就会分配好了默认值，准备阶段会显式初始化3. 注意：这里不会为实例变量分配初始化，类变量会分配在方法区中，而实例变量是会随着对象一起分配到Java堆中链接过程之准备阶段(Preparation)简言之，为类的静态变量分配内存，并将其初始化为默认值。在这个阶段，虚拟机就会为这个类分配相应的内存空间，并设置默认初始值。Java虚拟机为各类型变量默认的初始值如表所示。注意：Java并不支持boolean类型，对于boolean类型，内部实现是int,由于int的默认值是0,故对应的，boolean的默认值就是false。**举例**代码：变量a在准备阶段会赋初始值，但不是1，而是0，在初始化阶段会被赋值为 1```javaJAVApublic class HelloApp &#123; private static int a = 1;//prepare：a = 0 ---&gt; initial : a = 1 public static void main(String[] args) &#123; System.out.println(a); &#125;&#125; 解析(Resolve) 将常量池内的符号引用转换为直接引用的过程 事实上，解析操作往往会伴随着JVM在执行完初始化之后再执行 符号引用就是一组符号来描述所引用的目标。符号引用的字面量形式明确定义在《java虚拟机规范》的class文件格式中。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型等。对应常量池中的CONSTANT Class info、CONSTANT Fieldref info、CONSTANT Methodref info等 符号引用 反编译 class 文件后可以查看符号引用，下面带# 的就是符号引用 链接过程之解析阶段(Resolution) 简言之，将类、接口、字段和方法的符号引用转为直接引用。 1.具体描述: 符号引用就是一些字面量的引用，和虚拟机的内部数据结构和和内存布局无关。比较容易理解的就是在Class类文件中，通过常量池进行了大量的符号引用。但是在程序实际运行时，只有符号引用是不够的，比如当如下println()方法被调用时，系统需要明确知道该方法的位置。 举例：输出操作System.out.println()对应的字节码： invokevirtual #24 以方法为例，Java虚拟机为每个类都准备了一张方法表，将其所有的方法都列在表中，当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法。通过解析操作，符号引用就可以转变为目标方法在类中方法表中的位置，从而使得方法被成功调用。 2.小结： 所谓解析就是将符号引用转为直接引用，也就是得到类、字段、方法在内存中的指针或者偏移量。因此，可以说，如果直接引用存在，那么可以肯定系统中存在该类、方法或者字段。但只存在符号引用，不能确定系统中一定存在该结构。 不过Java虚拟机规范并没有明确要求解析阶段一定要按照顺序执行。在HotSpot VM中，加载、验证、准备和初始化会按照顺序有条不紊地执行，但链接阶段中的解析操作往往会伴随着JVM在执行完初始化之后再执行。 2.3. 初始化阶段类的初始化时机 创建类的实例 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（比如：Class.forName(“com.atguigu.Test”)） 初始化一个类的子类 Java虚拟机启动时被标明为启动类的类 JDK7开始提供的动态语言支持：java.lang.invoke.MethodHandle实例的解析结果REF_getStatic、REF putStatic、REF_invokeStatic句柄对应的类没有初始化，则初始化 除了以上七种情况，其他使用Java类的方式都被看作是对类的被动使用，都不会导致类的初始化，即不会执行初始化阶段（不会调用 clinit() 方法和 init() 方法） clinit() 初始化阶段就是执行类构造器方法&lt;clinit&gt;()的过程 此方法不需定义，是javac编译器自动收集类中的所有类变量的赋值动作和静态代码块中的语句合并而来。也就是说，当我们代码中包含static变量的时候，就会有clinit方法 &lt;clinit&gt;()方法中的指令按语句在源文件中出现的顺序执行 &lt;clinit&gt;()不同于类的构造器。（关联：构造器是虚拟机视角下的&lt;init&gt;()） 若该类具有父类，JVM会保证子类的&lt;clinit&gt;()执行前，父类的&lt;clinit&gt;()已经执行完毕 虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁 IDEA 中安装 JClassLib Bytecode viewer 插件，可以很方便的看字节码。安装过程可以自行百度 Java编译器并不会为所有的类都产生clinit()初始化方法。哪些类在编译为字节码后，字节码文件中将不会包含()方法？ 一个类中并没有声明任何的类变量，也没有静态代码块时 一个类中声明类变量，但是没有明确使用类变量的初始化语句以及静态代码块来执行初始化操作时 一个类中包含static final修饰的基本数据类型的字段，这些类字段初始化语句采用编译时常量表达式 查看下面这个代码的字节码，可以发现有一个&lt;clinit&gt;()方法。 123456789101112131415161718192021222324JAVApublic class ClassInitTest &#123; private static int num = 1; static&#123; num = 2; number = 20; System.out.println(num); //System.out.println(number);//报错：非法的前向引用。即在定义之前使用 &#125; /** * 1、linking之prepare: number = 0 --&gt; initial: 20 --&gt; 10 * 2、这里因为静态代码块出现在声明变量语句前面，所以之前被准备阶段为0的number变量会 * 首先被初始化为20，再接着被初始化成10（这也是面试时常考的问题哦） * */ private static int number = 10; public static void main(String[] args) &#123; System.out.println(ClassInitTest.num);//2 System.out.println(ClassInitTest.number);//10 &#125;&#125; 虚拟机必须保证一个类的&lt;clinit&gt;()方法在多线程下被同步加锁 类的初始化情况：主动使用vs被动使用Java程序对类的使用分为两种：主动使用 和 被动使用。 主动使用的说明：Class只有在必须要首次使用的时候才会被装载，Java虚拟机不会无条件地装载Class类型。Java虚拟机规定，一个类或接口在初次使用前，必须要进行初始化。这里指的“使用”，是指主动使用。 主动使用只有下列几种情况：（即：如果出现如下的情况，则会对类进行初始化操作。而初始化操作之前的加载、验证、准备已经完成。） 当创建一个类的实例时，比如使用new关键字，或者通过反射、克隆、反序列化。 当调用类的静态方法时，即当使用了字节码invokestatic指令。 当使用类、接口的静态字段时(final修饰特殊考虑)，比如，使用getstatic或者putstatic指令。 当使用java.lang.reflect包中的方法反射类的方法时。比如：Class.forName(“com.atguigu.java.Test”) todo 当初始化子类时，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，该接口要在其之前被初始化。 todo 当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。 当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。（涉及解析REF_getStatic、REF_putStatic、REF_invokeStatic方法句柄对应的类） todo 被动使用的情况除了以上的情况属于主动使用，其他的情况均属于被动使用。被动使用不会引起类的初始化。 也就是说：并不是在代码中出现的类，就一定会被加载或者初始化。如果不符合主动使用的条件，类就不会初始化。 当访问一个静态字段时，只有真正声明这个字段的类才会被初始化。 当通过子类引用父类的静态变量，不会导致子类初始化 通过数组定义类引用，不会触发此类的初始化 引用常量不会触发此类或接口的初始化。因为常量在链接阶段就已经被显式赋值了。 调用ClassLoader类的loadClass()方法加载一个类，并不是对类的主动使用，不会导致类的初始化。 被动的使用，意味着不需要执行初始化环节，意味着没有()的调用。 2.4. 类的使用任何一个类型在使用之前都必须经历过完整的加载、链接和初始化3个类加载步骤。一旦一个类型成功经历过这3个步骤之后，便“万事俱备，只欠东风”，就等着开发者使用了。 开发人员可以在程序中访问和调用它的静态类成员信息（比如：静态字段、静态方法），或者使用new关键字为其创建对象实例。 2.5. 类的卸载任何一个类型在使用之前都必须经历过完整的加载、链接和初始化3个类加载步骤。一旦一个类型成功经历过这3个步骤之后，便“万事俱备，只欠东风”，就等着开发者使用了。 开发人员可以在程序中访问和调用它的静态类成员信息（比如：静态字段、静态方法），或者使用new关键字为其创建对象实例。 当Sample类被加载、链接和初始化后，它的生命周期就开始了。当代表Sample类的Class对象不再被引用，即不可触及时，Class对象就会结束生命周期，Sample类在方法区内的数据也会被卸载，从而结束Sample类的生命周期。(因为双向引用所以除非类加载器被卸载) 类的卸载 (1) 启动类加载器加载的类型在整个运行期间是不可能被卸载的(jvm和jls规范) (2) 被系统类加载器和扩展类加载器加载的类型在运行期间不太可能被卸载，因为系统类加载器实例或者扩展类的实例基本上在整个运行期间总能直接或者间接的访问的到，其达到unreachable的可能性极小。 (3) 被开发者自定义的类加载器实例加载的类型只有在很简单的上下文环境中才能被卸载，而且一般还要借助于强制调用虚拟机的垃圾收集功能才可以做到。可以预想，稍微复杂点的应用场景中(比如：很多时候用户在开发自定义类加载器实例的时候采用缓存的策略以提高系统性能)，被加载的类型在运行期间也是几乎不太可能被卸载的(至少卸载的时间是不确定的)。 综合以上三点，一个已经加载的类型被卸载的几率很小至少被卸载的时间是不确定的。同时我们可以看的出来，开发者在开发代码时候，不应该对虚拟机的类型卸载做任何假设的前提下，来实现系统中的特定功能。 方法区的垃圾收集主要回收两部分内容：常量池中废弃的常量和不再使用的类型。 HotSpot虚拟机对常量池的回收策略是很明确的，只要常量池中的常量没有被任何地方引用，就可以被回收。 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收。也就是Java堆中不存在该类及其任何派生子类的实例。 加载该类的类加载器已经被回收。这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的。 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。 2.6. 总结1. 加载:将字节码静态数据装载到JVM中成为运行时数据 2. 链接阶段 1. 验证 1. 验证一些信息 2. 准备 1. 为类的静态变量分配内存，并将其初始化为默认值,final static 在编译时就初始化默认值了 在准备阶段直接赋予真正的值 3. 解析 1. 将符号引用转化为直接引用 3. 初始化 4. clinit()初始化.class 2. init()只是new时进行调用的方法与类加载无关 3. 类的加载器面试题 什么是类加载器，类加载器有哪些?（苏宁） 简单说说你了解的类加载器（拼多多） 类加载器都有哪些？（百度） 类加载器有哪些？ （腾讯） 什么是类加载器，类加载器有哪些？（字节跳动） 1. 类的加载分类：显式加载 vs 隐式加载class文件的显式加载与隐式加载的方式是指JVM加载class文件到内存的方式。 显式加载：指的是在代码中通过调用ClassLoader加载class对象，如直接使用Class.forName(name)或this.getClass().getClassLoader().loadClass()加载class对象。 隐式加载：则是不直接在代码中调用ClassLoader的方法加载class对象，而是通过虚拟机自动加载到内存中，如在加载某个类的class文件时，该类的class文件中引用了另外一个类的对象，此时额外引用的类将通过JVM自动加载到内存中。 在日常开发以上两种方式一般会混合使用。 2. 类加载机制的必要性一般情况下，Java开发人员并不需要在程序中显式地使用类加载器，但是了解类加载器的加载机制却显得至关重要。从以下几个方面说： 避免在开发中遇到 java.lang.ClassNotFoundException异常或java.lang.NoClassDefFoundError异常时，手足无措。只有了解类加载器的加载机制才能够在出现异常的时候快速地根据错误异常日志定位问题和解决问题 需要支持类的动态加载或需要对编译后的字节码文件进行加解密操作时，就需要与类加载器打交道了。 开发人员可以在程序中编写自定义类加载器来重新定义类的加载规则，以便实现一些自定义的处理逻辑。 3. 加载的类是唯一的吗?1.何为类的唯一性？ 对于任意一个类，都需要由加载它的类加载器和这个类本身一同确认其在Java虚拟机中的唯一性。每一个类加载器，都拥有一个独立的类名称空间：比较两个类是否相等，只有在这两个类是由同一个类加载器加载的前提下才有意义。否则，即使这两个类源自同一个Class文件，被同一个虚拟机加载，只要加载他们的类加载器不同，那这两个类就必定不相等。 2.命名空间 每个类加载器都有自己的命名空间，命名空间由该加载器及所有的父加载器所加载的类组成 在同一命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类 在不同的命名空间中，有可能会出现类的完整名字（包括类的包名）相同的两个类 在大型应用中，我们往往借助这一特性，来运行同一个类的不同版本。 4. 类加载的特性通常类加载机制有三个基本特征： 双亲委派模型。但不是所有类加载都遵守这个模型，有的时候，启动类加载器所加载的类型，是可能要加载用户代码的，比如JDK内部的ServiceProvider&#x2F;ServiceLoader机制，用户可以在标准API框架上，提供自己的实现，JDK也需要提供些默认的参考实现。例如，Java 中JNDI、JDBC、文件系统、Cipher等很多方面，都是利用的这种机制，这种情况就不会用双亲委派模型去加载，而是利用所谓的上下文加载器。 todo 可见性。子类加载器可以访问父加载器加载的类型，但是反过来是不允许的。不然，因为缺少必要的隔离，我们就没有办法利用类加载器去实现容器的逻辑。 todo 单一性。由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，就不会在子加载器中重复加载。但是注意，类加载器“邻居”间，同一类型仍然可以被加载多次，因为互相并不可见。 todo 5. 类加载器的分类说明JVM支持两种类型的类加载器，分别为引导类加载器（Bootstrap ClassLoader）和自定义类加载器（User-Defined ClassLoader）。 从概念上来讲，自定义类加载器一般指的是程序中由开发人员自定义的一类类加载器，但是Java虚拟机规范却没有这么定义，而是将所有派生于抽象类ClassLoader的类加载器都划分为自定义类加载器。 子父类的关系 除了顶层的启动类加载器外，其余的类加载器都应当有自己的“父类”加载器。 不同类加载器看似是继承（Inheritance）关系，实际上是包含关系。在下层加载器中，包含着上层加载器的引用。 启动类加载器（引导类加载器，Bootstrap ClassLoader） 这个类加载使用C&#x2F;C++语言实现的，嵌套在JVM内部。 它用来加载Java的核心库（JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;rt.jar或sun.boot.class.path路径下的内容）。用于提供JVM自身需要的类。 并不继承自java.lang.ClassLoader，没有父加载器。 出于安全考虑，Bootstrap启动类加载器只加载包名为java、javax、sun等开头的类 加载扩展类和应用程序类加载器，并指定为他们的父类加载器。 扩展类加载器（Extension ClassLoader） Java语言编写，由sun.misc.Launcher$ExtClassLoader实现。 继承于ClassLoader类 父类加载器为启动类加载器 从java.ext.dirs系统属性所指定的目录中加载类库，或从JDK的安装目录的jre&#x2F;lib&#x2F;ext子目录下加载类库。如果用户创建的JAR放在此目录下，也会自动由扩展类加载器加载。 应用程序类加载器（系统类加载器，AppClassLoader） java语言编写，由sun.misc.Launcher$AppClassLoader实现 继承于ClassLoader类 父类加载器为扩展类加载器 它负责加载环境变量classpath或系统属性 java.class.path 指定路径下的类库 应用程序中的类加载器默认是系统类加载器。 它是用户自定义类加载器的默认父加载器 通过ClassLoader的getSystemClassLoader()方法可以获取到该类加载器 用户自定义类加载器 在Java的日常应用程序开发中，类的加载几乎是由上述3种类加载器相互配合执行的。在必要时，我们还可以自定义类加载器，来定制类的加载方式。 体现Java语言强大生命力和巨大魅力的关键因素之一便是,Java开发者可以自定义类加载器来实现类库的动态加载，加载源可以是本地的JAR包，也可以是网络上的远程资源。 通过类加载器可以实现非常绝妙的插件机制，这方面的实际应用案例举不胜举。例如，著名的OSGI组件框架，再如Eclipse的插件机制。类加载器为应用程序提供了一种动态增加新功能的机制，这种机制无须重新打包发布应用程序就能实现。 同时，自定义加载器能够实现应用隔离，例如 Tomcat，Spring等中间件和组件框架都在内部实现了自定义的加载器，并通过自定义加载器隔离不同的组件模块。这种机制比C&#x2F;C++程序要好太多，想不修改C&#x2F;C++程序就能为其新增功能，几乎是不可能的，仅仅一个兼容性便能阻挡住所有美好的设想。 所有用户自定义类加载器通常需要继承于抽象类java.lang.ClassLoader。 待复习tomcat机制 4. cl源码剖析及相关机制第三章 运行时数据区 面试题 说一说JVM的内存结构是什么样子的,每个区域放什么，各有什么特点？（快手、搜狐） JVM的内存结构，及各个结构的内容。（vivo） 详细介绍一下内存结构（墨迹天气） JVM内存模型有哪些？（龙湖地产） Java虚拟机中内存划分为哪些区域（高德地图） JVM内存模型（中国计算机研究院、亚信） JVM内存结构（花旗银行） JVM 内存分哪几个区，每个区的作用是什么?（唯品会） 详解JVM内存模型（360） JVM有那些组成，堆，栈各放了什么东西？（搜狐、万达集团） JVM的内存模型，线程独有的放在哪里？哪些是线程共享的？哪些是线程独占的？（万达集团） 讲一下为什么JVM要分为堆、方法区等？原理是什么？（小米、搜狐） JVM的内存模型，线程独有的放在哪里？哪些是线程共享的？哪些是线程独占的？（菜鸟） 简单说一下JVM内存结构（浪潮） 说一下JVM内存模型吧，有哪些区？分别干什么的？ (百度) JVM的内存结构划分是什么样子的？ (支付宝) JVM 内存分哪几个区，每个区的作用是什么? (蚂蚁金服) Java虚拟机内存模型能说说吗？ (蚂蚁金服) JVM内存分布&#x2F;内存结构？ (蚂蚁金服) 讲讲JVM分区 (携程) 讲一下JVM内存布局 (滴滴) Java的内存分区 (字节跳动) 讲讲JVM运行时数据库区 (字节跳动) JVM内存模型以及分区，需要详细到每个区放什么。 (天猫) JVM 内存分哪几个区，每个区的作用是什么? (拼多多) JVM的内存布局以及垃圾回收原理及过程讲一下 (京东) 1. 程序计数寄存器（Program Counter Register）每个线程都有它自己的程序计数器，是线程私有的，生命周期与线程的生命周期保持一致。它是唯一一个在Java 虚拟机规范中没有规定任何OutOtMemoryError 情况的区域。 使用PC寄存器存储字节码指令地址有什么用呢？ （为什么使用PC寄存器记录当前线程的执行地址呢？）线程切换 因为CPU需要不停的切换各个线程，这时候切换回来以后，就得知道接着从哪开始继续执行。 JVM的字节码解释器就需要通过改变PC寄存器的值来明确下一条应该执行什么样的字节码指令。 PC寄存器为什么会被设定为线程私有？ 我们都知道所谓的多线程在一个特定的时间段内只会执行其中某一个线程的方法，CPU会不停地做任务切换，这样必然导致经常中断或恢复，如何保证分毫无差呢？为了能够准确地记录各个线程正在执行的当前字节码指令地址，最好的办法自然是为每一个线程都分配一个PC寄存器，这样一来各个线程之间便可以进行独立计算，从而不会出现相互干扰的情况。 2. JVM栈概述不存在GC ; 存在OOM StackOverFlowError？OutOfMemoryError？ Java 虚拟机规范允许Java栈的大小是动态的或者是固定不变的。 如果采用固定大小的Java虚拟机栈，那每一个线程的Java虚拟机栈容量可以在线程创建的时候独立选定。如果线程请求分配的栈容量超过Java虚拟机栈允许的最大容量，Java虚拟机将会抛出一个 StackOverflowError 异常。 如果Java虚拟机栈可以动态扩展，并且在尝试扩展的时候无法申请到足够的内存，或者在创建新的线程时没有足够的内存去创建对应的虚拟机栈，那Java虚拟机将会抛出—个 OutOfMemoryError 异常。 如何设置栈内存的大小？ -Xss size (即：-XX:ThreadStackSize) 一般默认为512k-1024k，取决于操作系统。 栈的大小直接决定了函数调用的最大可达深度。 12345678910public class Main &#123; private static int count = 1; public static void main(String[] args) &#123; System.out.println(count++); main(args); &#125;&#125;SOF OOM: 死循环创建线程 方法和栈桢之间存在怎样的关系？ 在这个线程上正在执行的每个方法都各自对应一个栈帧（Stack Frame）。 栈帧是一个内存区块，是一个数据集，维系着方法执行过程中的各种数据信息。 栈帧内部结构0. 概述每个栈帧中存储着： 局部变量表（Local Variables） 操作数栈（Operand Stack）（或表达式栈） 动态链接(Dynamic Linking) （或指向运行时常量池的方法引用） 方法返回地址（Return Address）（或方法正常退出或者异常退出的定义） 一些附加信息 1. 局部变量表局部变量表（local variables) 局部变量表也被称之为局部变量数组或本地变量表 定义为一个数字数组，主要用于存储方法参数和定义在方法体内的局部变量，这些数据类型包括各类基本数据类型(8种)、对象引用（reference），以及returnAddress类型。 局部变量表所需的容量大小是在编译期确定下来的，并保存在方法的Code属性的maximum local variables数据项中。在方法运行期间是不会改变局部变量表的大小的。 方法嵌套调用的次数由栈的大小决定。一般来说，栈越大，方法嵌套调用次数越多。对一个函数而言，它的参数和局部变量越多，使得局部变量表膨胀，它的栈帧就越大，以满足方法调用所需传递的信息增大的需求。进而函数调用就会占用更多的栈空间，导致其嵌套调用次数就会减少。 局部变量表中的变量只在当前方法调用中有效。在方法执行时，虚拟机通过使用局部变量表完成参数值到参数变量列表的传递过程。当方法调用结束后，随着方法栈帧的销毁，局部变量表也会随之销毁。 slot 参数值的存放总是在局部变量数组的index为0开始，到数组长度-1的索引结束。 局部变量表，最基本的存储单元是Slot（变量槽） 在局部变量表里，32位以内的类型只占用一个slot（包括returnAddress类型），64位的类型（long和double)占用两个slot。 byte 、short 、char 在存储前被转换为int，boolean 也被转换为int，0 表示false ，非0 表示true。 long 和double 则占据两个Slot。 JVM会为局部变量表中的每一个Slot都分配一个访问索引，通过这个索引即可成功访问到局部变量表中指定的局部变量值 当一个实例方法被调用的时候，它的方法参数和方法体内部定义的局部变量将会按照顺序被复制到局部变量表中的每一个Slot上 如果需要访问局部变量表中一个64bit的局部变量值时，只需要使用前一个索引即可。(比如：访问long或double类型变量） 如果当前帧是由构造方法或者实例方法创建的，那么该对象引用this将会存放在index为0的slot处，其余的参数按照参数表顺序继续排列。 栈帧中的局部变量表中的槽位是可以重用的，如果一个局部变量过了其作用域，那么在其作用域之后申明的新的局部变量就很有可能会复用过期局部变量的槽位，从而达到节省资源的目的。 参数表分配完毕之后，再根据方法体内定义的变量的顺序和作用域分配。 我们知道类变量表有两次初始化的机会，第一次是在“准备阶段”，执行系统初始化，对类变量设置零值，另一次则是在“初始化”阶段，赋予程序员在代码中定义的初始值。 和类变量初始化不同的是，局部变量表不存在系统初始化的过程，这意味着一旦定义了局部变量则必须人为的初始化，否则无法使用。 public void test() {int i; System.out.println(i); } 这样的代码是错误的，没有赋值不能够使用。 局部变量表中的变量也是重要的垃圾回收根节点，只要被局部变量表中直接或间接引用的对象都不会被回收。 2. 操作数栈操作数栈（Operand Stack） 我们说Java虚拟机的解释引擎是基于栈的执行引擎，其中的栈指的就是操作数栈。 每一个独立的栈帧中除了包含局部变量表以外，还包含一个后进先出（Last-In-First-Out）的操作数栈，也可以称之为表达式栈（Expression Stack）。 操作数栈就是JVM执行引擎的一个工作区，当一个方法刚开始执行的时候，一个新的栈帧也会随之被创建出来，这个方法的操作数栈是空的。 每一个操作数栈都会拥有一个明确的栈深度用于存储数值，其所需的最大深度在编译期就定义好了，保存在方法的Code属性中，为max_stack的值。 栈中的任何一个元素都是可以任意的Java数据类型。 32bit的类型占用一个栈单位深度 64bit的类型占用两个栈单位深度 操作数栈，在方法执行过程中，根据字节码指令，并非采用访问索引的方式来进行数据访问的，而是只能通过标准的入栈（push）和出栈（pop）操作，往栈中写入数据或提取数据来完成一次数据访问。 某些字节码指令将值压入操作数栈，其余的字节码指令将操作数取出栈。使用它们后再把结果压入栈。比如：执行复制、交换、求和等操作 如果被调用的方法带有返回值的话，其返回值将会被压入当前栈帧的操作数栈中，并更新PC寄存器中下一条需要执行的字节码指令。 栈顶缓存技术 前面提过，基于栈式架构的虚拟机所使用的零地址指令更加紧凑，但完成一项操作的时候必然需要使用更多的入栈和出栈指令，这同时也就意味着将需要更多的指令分派（instruction dispatch）次数和内存读&#x2F;写次数。 由于操作数是存储在内存中的，因此频繁地执行内存读&#x2F;写操作必然会影响执行速度。为了解决这个问题，HotSpot JVM的设计者们提出了栈顶缓存（ToS，Top-of-Stack Cashing）技术，将栈顶元素全部缓存在物理CPU的寄存器中，以此降低对内存的读&#x2F;写次数，提升执行引擎的执行效率。 3. 动态链接动态链接（或指向运行时常量池的方法引用） 每一个栈帧内部都包含一个指向运行时常量池中该栈帧所属方法的引用。包含这个引用的目的就是为了支持当前方法的代码能够实现动态链接（Dynamic Linking）。比如：invokedynamic指令 在Java源文件被编译到字节码文件中时，所有的变量和方法引用都作为符号引用（Symbolic Reference）保存在class文件的常量池里。比如：描述一个方法调用了另外的其他方法时，就是通过常量池中指向方法的符号引用来表示的，那么动态链接的作用就是为了将这些符号引用转换为调用方法的直接引用。 public void testGetSum(){**int i &#x3D; getSum();int j &#x3D; 10;} 4. 方法返回地址 存放调用该方法的pc寄存器的值。 一个方法的结束，有两种方式： 正常执行完成 出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的pc计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。而通过异常退出的，返回地址是要通过异常表来确定，栈帧中一般不会保存这部分信息。 5. 一些附加信息栈帧中还允许携带与Java虚拟机实现相关的一些附加信息。例如，对程序调试提供支持的信息。 面试扩展问题问题一：栈溢出的情况? 栈溢出:StackOverflowError; 举个简单的例子:在main方法中调用main方法,就会不断压栈执行,直到栈溢出; 栈的大小可以是固定大小的,也可以是动态变化（动态扩展）的。 如果是固定的,可以通过-Xss设置栈的大小; 如果是动态变化的,当栈大小到达了整个内存空间不足了,就是抛出OutOfMemory异常(java.lang.OutOfMemoryError) 问题二：调整栈大小,就能保证不出现溢出吗? 不能。因为调整栈大小,只会减少出现溢出的可能,栈大小不是可以无限扩大的,所以不能保证不出现溢出 问题三：分配的栈内存越大越好吗? 不是,因为增加栈大小，会造成每个线程的栈都变的很大,使得一定的栈空间下,能创建的线程数量会变小 问题四：垃圾回收是否会涉及到虚拟机栈? 不会;垃圾回收只会涉及到方法区和堆中,方法区和堆也会存在溢出的可能; 程序计数器,只记录运行下一行的地址,不存在溢出和垃圾回收; 虚拟机栈和本地方法栈,都是只涉及压栈和出栈,可能存在栈溢出,不存在垃圾回收。 问题五：方法中定义的局部变量是否线程安全? 具体问题具体分析,见分析代码: 1234567891011121314151617181920212223242526272829303132333435363738394041424344/**方法中定义的局部变量是否线程安全? 具体问题具体分析 * @author shkstart * @create 15:53 */public class LocalVariableThreadSafe &#123;//s1的声明方式是线程安全的,因为线程私有，在线程内创建的s1 ，不会被其它线程调用public static void method1() &#123;//StringBuilder:线程不安全StringBuilder s1 = new StringBuilder(); s1.append(&quot;a&quot;); s1.append(&quot;b&quot;);//...&#125;//stringBuilder的操作过程：是线程不安全的， // 因为stringBuilder是外面传进来的，有可能被多个线程调用public static void method2(StringBuilder stringBuilder) &#123; stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);//...&#125;//stringBuilder的操作：是线程不安全的；因为返回了一个stringBuilder， // stringBuilder有可能被其他线程共享public static StringBuilder method3() &#123; StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);return stringBuilder; &#125;//stringBuilder的操作：是线程安全的；因为返回了一个stringBuilder.toString()相当于new了一个String， // 所以stringBuilder没有被其他线程共享的可能public static String method4() &#123; StringBuilder stringBuilder = new StringBuilder(); stringBuilder.append(&quot;a&quot;); stringBuilder.append(&quot;b&quot;);return stringBuilder.toString();/** * 结论：如果局部变量在内部产生并在内部消亡的，那就是线程安全的 */&#125;&#125; 3. 本地方法接口与本地方法栈Java使用起来非常方便，然而有些层次的任务用Java实现起来不容易，或者我们对程序的效率很在意时，问题就来了。 与Java环境外交互： 有时Java应用需要与Java外面的环境交互，这是本地方法存在的主要原因。你可以想想Java需要与一些底层系统，如操作系统或某些硬件交换信息时的情况。本地方法正是这样一种交流机制：它为我们提供了一个非常简洁的接口，而且我们无需去了解Java应用之外的繁琐的细节。 与操作系统交互： JVM支持着Java语言本身和运行时库，它是Java程序赖以生存的平台，它由一个解释器（解释字节码）和一些连接到本地代码的库组成。然而不管怎样，它毕竟不是一个完整的系统，它经常依赖于一些底层系统的支持。这些底层系统常常是强大的操作系统。通过使用本地方法，我们得以用Java实现了jre的与底层系统的交互，甚至JVM的一些部分就是用C写的。还有，如果我们要使用一些Java语言本身没有提供封装的操作系统的特性时，我们也需要使用本地方法。 Sun’s Java Sun的解释器是用C实现的，这使得它能像一些普通的C一样与外部交互。jre大部分是用Java实现的，它也通过一些本地方法与外界交互。例如：类java.lang.Thread 的 setPriority()方法是用Java实现的，但是它实现调用的是该类里的本地方法setPriority0()。这个本地方法是用C实现的，并被植入JVM内部，在Windows 95的平台上，这个本地方法最终将调用Win32 SetPriority() API。这是一个本地方法的具体实现由JVM直接提供，更多的情况是本地方法由外部的动态链接库（external dynamic link library）提供，然后被JVM调用。 4. 堆概述 《Java虚拟机规范》中对Java堆的描述是：所有的对象实例以及数组都应当在运行时分配在堆上。（The heap is the run-time data area from which memory for all class instances and arrays is allocated ) 数组和对象可能永远不会存储在栈上，因为栈帧中保存引用，这个引用指向对象或者数组在堆中的位置。 我要说的是：“几乎”所有的对象实例都在这里分配内存。——从实际使用角度看的。 所有的线程共享Java堆，在这里还可以划分线程私有的缓冲区（Thread Local Allocation Buffer, TLAB)。 堆的内部结构 几乎所有的Java对象都是在Eden区被new出来的。 绝大部分的Java对象的销毁都在新生代进行了。 IBM 公司的专门研究表明，新生代中 80% 的对象都是“朝生夕死”的。 如何设置堆内存大小？如何设置新生代与老年代比例？ 下面这参数开发中一般不会调： 配置新生代与老年代在堆结构的占比。 默认**-XX:NewRatio&#x3D;2**，表示新生代占1，老年代占2，新生代占整个堆的1&#x2F;3 可以修改-XX:NewRatio&#x3D;4，表示新生代占1，老年代占4，新生代占整个堆的1&#x2F;5 可以使用选项”**-Xmn**”设置新生代最大内存大小 这个参数一般使用默认值就可以了。 如何设置Eden、幸存者区比例？ 在HotSpot中，Eden空间和另外两个Survivor空间缺省所占的比例是8:1:1 当然开发人员可以通过选项“**-XX:SurvivorRatio”调整这个空间比例。比如-XX:SurvivorRatio&#x3D;8** OOM举例​ 参数设置小结 什么是空间分配担保策略？（渣打银行） 什么是空间分配担保策略？（顺丰） 什么是空间分配担保策略？（腾讯、百度） 1234567891011121314151617/** * 测试堆空间常用的jvm参数： * -XX:+PrintFlagsInitial : 查看所有的参数的默认初始值 * -XX:+PrintFlagsFinal ：查看所有的参数的最终值（可能会存在修改，不再是初始值） * 具体查看某个参数的指令： jps：查看当前运行中的进程 * jinfo -flag SurvivorRatio 进程id * * -Xms：初始堆空间内存 （默认为物理内存的1/64） * -Xmx：最大堆空间内存（默认为物理内存的1/4） * -Xmn：设置新生代的大小。(初始值及最大值) * -XX:NewRatio：配置新生代与老年代在堆结构的占比 * -XX:SurvivorRatio：设置新生代中Eden和S0/S1空间的比例 * -XX:MaxTenuringThreshold：设置新生代垃圾的最大年龄 * -XX:+PrintGCDetails：输出详细的GC处理日志 * 打印gc简要信息：① -XX:+PrintGC ② -verbose:gc * -XX:HandlePromotionFailure：是否设置空间分配担保 */ 对象分配1.new的对象先放伊甸园区。此区有大小限制。 2.当伊甸园的空间填满时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收(Minor GC&#x2F;YGC)，将伊甸园区中的不再被其他对象所引用的对象进行销毁。再加载新的对象放到伊甸园区 3.然后将伊甸园中的剩余对象移动到幸存者0区。 4**.如果再次触发垃圾回收，此时上次幸存下来的放到幸存者0区的，如果没有回收，就会放到幸存者1区**。 5.如果再次经历垃圾回收，此时会重新放回幸存者0区，接着再去幸存者1区。 6.啥时候能去养老区呢？可以设置次数。默认是15次。 可以设置参数：**-XX:MaxTenuringThreshold&#x3D;** 设置对象晋升老年代的年龄阈值。 7.在养老区，相对悠闲。当养老区内存不足时，再次触发GC：Major GC，进行养老区的内存清理。 8.若养老区执行了Major GC之后发现依然无法进行对象的保存，就会产生OOM异常 空间分配担保1、在发生Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总空间。 如果大于，则此次Minor GC是安全的 如果小于，则虚拟机会查看**-XX:HandlePromotionFailure**设置值是否允担保失败。 如果HandlePromotionFailure&#x3D;true，那么会继续检查 老年代最大可用连续空间是否大于历次晋升到老年代的对象的平均大小 。 如果大于，则尝试进行一次Minor GC，但这次Minor GC依然是有风险的； 如果小于，则进行一次Full GC。 如果HandlePromotionFailure&#x3D;false，则进行一次Full GC。 5. 方法区1. 栈、堆、方法区的关系 2. 什么是TLAB？ 从内存模型而不是垃圾收集的角度，对Eden区域继续进行划分，JVM为每个线程分配了一个私有缓存区域，它包含在Eden空间内。 据我所知所有OpenJDK衍生出来的JVM都提供了TLAB的设计。 3. 方法区在哪里《Java虚拟机规范》中明确说明: “尽管所有的方法区在逻辑上是属于堆的一部分，但一些简单的实现可能不会选择去进行垃圾收集或者进行压缩。” 但对于HotSpotJVM而言，方法区还有一个别名叫做Non-Heap(非堆)，目的就是要和堆分开。 所以，方法区看作是一块独立于Java 堆的内存空间。 4. 方法区的理解方法区（Method Area）与Java堆一样，是各个线程共享的内存区域。方法区在JVM启动的时候被创建，并且它的实际的物理内存空间中和Java堆区一样都可以是不连续的。方法区的大小，跟堆空间一样，可以选择固定大小或者可扩展。方法区的大小决定了系统可以保存多少个类，如果系统定义了太多的类，导致方法区溢出，虚拟机同样会抛出内存溢出错误：java.lang.OutOfMemoryError: PermGen space 或者 java.lang.OutOfMemoryError: Metaspace加载大量的第三方的jar包；Tomcat部署的工程过多（30-50个）；大量动态的生成反射类关闭JVM就会释放这个区域的内存。 5. 方法区的演进到了JDK 8，终于完全废弃了永久代的概念，改用与JRockit、J9一样在本地内存中实现的元空间（Metaspace）来代替￼元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代最大的区别在于：元空间不在虚拟机设置的内存中，而是使用本地内存。永久代、元空间二者并不只是名字变了，内部结构也调整了。根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OOM异常。 6. 设置方法区内存的大小 方法区的大小不必是固定的，jvm可以根据应用的需要动态调整。 jdk7及以前： 通过-XX:PermSize来设置永久代初始分配空间。默认值是20.75M -XX:MaxPermSize来设定永久代最大可分配空间。32位机器默认是64M，64位机器模式是82M 当JVM加载的类信息容量超过了这个值，会报异常OutOfMemoryError:PermGen space 。 jdk8及以后： 元数据区大小可以使用参数-XX:MetaspaceSize和-XX:MaxMetaspaceSize指定,替代上述原有的两个参数。 默认值依赖于平台。windows下，-XX:MetaspaceSize是21M，-XX:MaxMetaspaceSize 的值是-1，即没有限制。 与永久代不同，如果不指定大小，默认情况下，虚拟机会耗尽所有的可用系统内存。如果元数据区发生溢出，虚拟机一样会抛出异常OutOfMemoryError: Metaspace -XX:MetaspaceSize：设置初始的元空间大小。对于一个64位的服务器端JVM来说，其默认的-XX:MetaspaceSize值为21MB。这就是初始的高水位线，一旦触及这个水位线，Full GC将会被触发并卸载没用的类（即这些类对应的类加载器不再存活），然后这个高水位线将会重置。新的高水位线的值取决于GC后释放了多少元空间。如果释放的空间不足，那么在不超过MaxMetaspaceSize时，适当提高该值。如果释放空间过多，则适当降低该值。 如果初始化的高水位线设置过低，上述高水位线调整情况会发生很多次。通过垃圾回收器的日志可以观察到Full GC多次调用。为了避免频繁地GC ，建议将-XX:MetaspaceSize设置为一个相对较高的值。 在JDK8 及以上版本中，设定MaxPermSize 参数， JVM在启动时并不会报错，但是会提示： Java HotSpot 64Bit Server VM warning: ignoring option MaxPermSize&#x3D;2560m; support was removed in 8.0 。 第四章 对象的内存布局1. 对象的实例化1.1 有几种创建对象的方式 实操 todo new 变形1: Xxx静态方法的new 变形2: XxxBuilder&#x2F;XxxFactory的静态方法 Class的newInstance() Constructor的newInstance() clone() 反序列化 第三发库Objenesis,利用了asm字节码技术,动态生成Constructor 典型用途： 需要在不调用构造函数的情况下实例化对象是一项相当特殊的任务，但是在某些情况下这是有用的： 序列化，远程调用和持久化-对象需要被实例化并恢复到特定的状态，而不需要调用代码 代理、 AOP 库和 mock 对象-类可以被子类继承而子类不用担心父类的构造器 容器框架-对象可以以非标准的方式动态地实例化 Spring中的封装的元数据读取器等就是利用了asm,需要注意的是，SimpleMetadataReader去解析类时，使用的ASM技术。 并不是等Java类加载到JVM在解析,而是直接读取字节码文件 为什么要使用ASM技术，Spring启动的时候需要去扫描，如果指定的包路径比较宽泛，那么扫描的类是非常多的，那如果在Spring启动时就把这些类全部加载进JVM了，这样不太好，所以使用了ASM技术。 1.2 创建对象的过程1.2.1 从字节码角度看待对象创建过程 : 见字节码篇1.2.2 从执行步骤角度分析 判断对象对应的类是否加载,链接,初始化 虚拟机遇到一条new指令，首先去检查这个指令的参数能否在Metaspace的常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已经被加载、解析和初始化。（即判断类元信息是否存在）。 如果没有，那么在双亲委派模式下，使用当前类加载器以ClassLoader+包名+类名为Key进行查找对应的.class 文件。 如果没有找到文件，则抛出ClassNotFoundException 异常。 如果找到，则进行类加载，并生成对应的Class类对象。 为对象分配内存 指针碰撞 如果内存规整，使用指针碰撞 如果内存是规整的，那么虚拟机将采用的是指针碰撞法（Bump The Pointer）来为对象分配内存。意思是所有用过的内存在一边，空闲的内存在另外一边，中间放着一个指针作为分界点的指示器，分配内存就仅仅是把指针向空闲那边挪动一段与对象大小相等的距离罢了。 如果垃圾收集器选择的是Serial、ParNew这种基于压缩算法的，虚拟机采用这种分配方式。 一般使用带有compact（整理）过程的收集器时，使用指针碰撞。 空闲列表 如果内存不规整，虚拟机需要维护一个列表，使用空闲列表分配 如果内存不是规整的，已使用的内存和未使用的内存相互交错，那么虚拟机将采用的是空闲列表法来为对象分配内存。意思是虚拟机维护了一个列表，记录上哪些内存块是可用的，再分配的时候从列表中找到一块足够大的空间划分给对象实例，并更新列表上的内容。这种分配方式称为“空闲列表（Free List）”。 处理并发安全问题 在分配内存空间时，另外一个问题是及时保证new对象时候的线程安全性：创建对象是非常频繁的操作，虚拟机需要解决并发问题。 虚拟机采用了两种方式解决并发问题: CAS ( Compare And Swap ）失败重试、区域加锁：保证指针更新操作的原子性; TLAB 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存，称为本地线程分配缓冲区，（TLAB ，Thread Local Allocation Buffer）虚拟机是否使用TLAB，可以通过-XX:+&#x2F;-UseTLAB参数来设定。 初始化分配到的空间 内存分配结束，虚拟机将分配到的内存空间都初始化为零值（不包括对象头）。这一步保证了对象的实例字段在Java代码中可以不用赋初始值就可以直接使用，程序能访问到这些字段的数据类型所对应的零值。 设置对象的对象头 将对象的所属类（即类的元数据信息）、对象的HashCode和对象的GC信息、锁信息等数据存储在对象的对象头中。这个过程的具体设置方式取决于JVM实现。 执行init方法进行初始化 在Java程序的视角看来，初始化才正式开始。初始化成员变量，执行实例化代码块，调用类的构造方法，并把堆内对象的首地址赋值给引用变量。 因此一般来说（由字节码中是否跟随有invokespecial指令所决定），new指令之后会接着就是执行方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全创建出来。 2. 对象的内存布局 对象头 对象头：它主要包括两部分。 一个是对象自身的运行时元数据(mark word)。 哈希值(hashcode)：对象在堆空间中都有一个首地址值，栈空间的引用根据这个地址指向堆中的对象，这就是哈希值起的作用 GC分代年龄：对象首先是在Eden中创建的，在经过多次GC后，如果没有被进行回收，就会在survivor中来回移动，其对应的年龄计数器会发生变化，达到阈值后会进入养老区 锁状态标志，在同步中判断该对象是否是锁 线程持有的锁 线程偏向ID 偏向时间戳 另一个是类型指针，指向元数据区的类元数据InstanceKlass，确定该对象所属的类型 此外，如果对象是一个数组，对象头中还必须有一块用于记录数组的长度的数据。 因为正常对象元数据就知道对象的确切大小。所以数组必须得知道长度。 实例数据 作用： 它是对象真正存储的有效信息，包括程序代码中定义的各种类型的字段（包括从父类继承下来的和本身拥有的字段）。 这里需要遵循的一些规则： 相同宽度的字段总是被分配在一起 父类中定义的变量会出现在子类之前（因为父类的加载是优先于子类加载的） 如果CompactFields参数为true(默认为true)：子类的窄变量可能插入到父类变量的空隙 对齐填充 对齐填充：不是必须的，也没特别含义，仅仅起到占位符的作用 3. 对象的访问定位 句柄访问 实现：堆需要划分出一块内存来做句柄池，reference中存储对象的句柄池地址，句柄中包含对象实例与类型数据各自具体的地址信息。 好处：reference中存储稳定句柄地址，对象被移动（垃圾收集时移动对象很普遍）时只会改变句柄中实例数据指针，reference本身不需要被修改。 直接使用指针访问 实现：reference中存储的就是对象的地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 好处：速度更快，java中对象访问频繁，每次访问都节省了一次指针定位的时间开销。 HotSpot这里主要使用第2种方式：直接指针访问 JVM可以通过对象引用准确定位到Java堆区中的instanceOopDesc对象，这样既可成功访问到对象的实例信息，当需要访问目标对象的具体类型时，JVM则会通过存储在instanceOopDesc中的元数据指针定位到存储在方法区中的instanceKlass对象上。 第五章 执行引擎篇第六章 垃圾回收篇从次数上讲：频繁收集Young区较少收集Old区基本不动Perm区（或元空间） 1.垃圾回收算法1.1标记算法1.1.1引用计数法原理： 对于一个对象A，只要有任何一个对象引用了A ，则A 的引用计数器就加1，当引用失效时，引用计数器就减1。只要对象A 的引用计数器的值为0，即表示对象A不可能再被使用，可进行回收。 优点：实现简单，垃圾对象便于辨识；判定效率高，回收没有延迟性。 缺点： 缺点1：它需要单独的字段存储计数器，这样的做法增加了存储空间的开销。 缺点2：每次赋值都需要更新计数器，伴随着加法和减法操作，这增加了时间开销。 缺点3：引用计数器有一个严重的问题，即无法处理循环引用的情况。这是一条致命缺陷，导致在Java 的垃圾回收器中没有使用这类算法。 引用计数算法，是很多语言的资源回收选择，例如因人工智能而更加火热的Python，它更是同时支持引用计数和垃圾收集机制。 具体哪种最优是要看场景的，业界有大规模实践中仅保留引用计数机制，以提高吞吐量的尝试。 Java并没有选择引用计数，是因为其存在一个基本的难题，也就是很难处理循环引用关系。 Python如何解决循环引用？ 手动解除：很好理解，就是在合适的时机，解除引用关系。 使用弱引用weakref， weakref是Python提供的标准库，旨在解决循环引用。 1.1.2可达性分析算法原理： 其原理简单来说，就是将对象及其引用关系看作一个图，选定活动的对象作为 GC Roots，然后跟踪引用链条，如果一个对象和GC Roots之间不可达，也就是不存在引用链条，那么即可认为是可回收对象。 优点： 实现简单，执行高效 ，有效的解决循环引用的问题，防止内存泄漏。 GC root在Java 语言中， GC Roots 包括以下几类元素： 虚拟机栈中引用的对象 比如：各个线程被调用的方法中使用到的参数、局部变量等。 本地方法栈内JNI(通常说的本地方法)引用的对象 类静态属性引用的对象 比如：Java类的引用类型静态变量 方法区中常量引用的对象 比如：字符串常量池（String Table）里的引用 所有被同步锁synchronized持有的对象 Java虚拟机内部的引用。 基本数据类型对应的Class对象，一些常驻的异常对象（如：NullPointerException、OutOfMemoryError），系统类加载器。 反映java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合。比如：分代收集和局部回收（Partial GC）。 不太了解细节 如果只针对Java堆中的某一块区域进行垃圾回收（比如：典型的只针对新生代），必须考虑到内存区域是虚拟机自己的实现细节，更不是孤立封闭的，这个区域的对象完全有可能被其他区域的对象所引用，这时候就需要一并将关联的区域对象也加入GC Roots集合中去考虑，才能保证可达性分析的准确性。 小技巧： 由于Root 采用栈方式存放变量和指针，所以如果一个指针，它保存了堆内存里面的对象，但是自己又不存放在堆内存里面，那它就是一个Root 。 STW 如果要使用可达性分析算法来判断内存是否可回收，那么分析工作必须在一个能保障一致性的快照中进行。这点不满足的话分析结果的准确性就无法保证。 这点也是导致GC进行时必须“Stop The World”的一个重要原因。 即使是号称（几乎）不会发生停顿的CMS收集器中，枚举根节点时也是必须要停顿的。 1.2.清除算法标记清除标记-清除（Mark - Sweep）算法 背景： 标记 - 清除算法（ Mark-Sweep ）是一种非常基础和常见的垃圾收集算法，该算法被J.McCarthy等人在1960年提出并并应用于Lisp语言。 执行过程： 当堆中的有效内存空间（available memory）被耗尽的时候，就会停止整个程序（也被称为stop the world），然后进行两项工作，第一项则是标记，第二项则是清除。 标记：Collector从引用根节点开始遍历，标记所有被引用的对象。一般是在对象的Header中记录为可达对象。 清除：Collector对堆内存从头到尾进行线性的遍历，如果发现某个对象在其Header中没有标记为可达对象，则将其回收。 （很多书、视频讲错了！说是标记的垃圾对象。这里要注意了！） 缺点： 1、效率比较低：递归与全堆对象遍历两次 2、在进行GC的时候，需要停止整个应用程序，导致用户体验差 3、这种方式清理出来的空闲内存是不连续的，产生内存碎片。 注意：何为清除? 这里所谓的清除并不是真的置空，而是把需要清除的对象地址保存在空闲的地址列表里。下次有新对象需要加载时，判断垃圾的位置空间是否够，如果够，就存放。 复制算法核心思想： 将活着的内存空间分为两块，每次只使用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后清除正在使用的内存块中的所有对象，交换两个内存的角色，最后完成垃圾回收。 优点： 没有标记和清除过程，实现简单，运行高效 复制过去以后保证空间的连续性，不会出现“碎片”问题。 缺点： 此算法的缺点也是很明显的，就是需要两倍的内存空间。 对于G1这种分拆成为大量region的GC，复制而不是移动，意味着GC需要维护region之间对象引用关系，不管是内存占用或者时间开销也不小。 特别的： 如果系统中的存活对象很多，复制算法不会很理想。因为复制算法需要复制的存活对象数量并不会太大,或者说非常低才行。 应用场景： 在新生代，对常规应用的垃圾回收，一次通常可以回收70%-99%的内存空间。回收性价比很高。所以现在的商业虚拟机都是用这种收集算法回收新生代。 比如：IBM 公司的专门研究表明，新生代中 80% 的对象都是“朝生夕死”的。 标记压缩标记-压缩（或标记-整理、Mark - Compact）算法 背景： 复制算法的高效性是建立在存活对象少、垃圾对象多的前提下的。这种情况在新生代经常发生，但是在老年代，更常见的情况是大部分对象都是存活对象。如果依然使用复制算法，由于存活对象较多，复制的成本也将很高。因此，基于老年代垃圾回收的特性，需要使用其他的算法。 标记－清除算法的确可以应用在老年代中，但是该算法不仅执行效率低下，而且在执行完内存回收后还会产生内存碎片，所以JVM 的设计者需要在此基础之上进行改进。标记 - 压缩（Mark - Compact）算法由此诞生。 执行过程： 第一阶段和标记-清除算法一样，从根节点开始标记所有被引用对象 第二阶段将所有的存活对象压缩到内存的一端，按顺序排放。 之后， 清理边界外所有的空间。 标记-压缩算法的最终效果等同于标记-清除算法执行完成后，再进行一次内存碎片整理，因此，也可以把它称为标记-清除-压缩(Mark-Sweep-Compact)算法。 二者的本质差异在于标记-清除算法是一种非移动式的回收算法，标记-压缩是移动式的。是否移动回收后的存活对象是一项优缺点并存的风险决策。 可以看到，标记的存活对象将会被整理，按照内存地址依次排列，而未被标记的内存会被清理掉。如此一来，当我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可，这比维护一个空闲列表显然少了许多开销。 指针碰撞（Bump the Pointer） 如果内存空间以规整和有序的方式分布，即已用和未用的内存都各自一边，彼此之间维系着一个记录下一次分配起始点的标记指针，当为新对象分配内存时，只需要通过修改指针的偏移量将新对象分配在第一个空闲内存位置上，这种分配方式就叫做指针碰撞（Bump the Pointer）。 优点：（此算法消除了“标记-清除”和“复制”两个算法的弊端。） 消除了标记&#x2F;清除算法当中，内存区域分散的缺点，我们需要给新对象分配内存时，JVM只需要持有一个内存的起始地址即可。 消除了复制算法当中，内存减半的高额代价。 缺点： 从效率上来说，标记-压缩算法要低于复制算法。 效率不高，不仅要标记所有存活对象，还要整理所有存活对象的引用地址。 对于老年代每次都有大量对象存活的区域来说，极为负重。 移动对象的同时，如果对象被其他对象引用，则还需要调整引用的地址。 移动过程中，需要全程暂停用户应用程序。即：STW 分代收集算法 三种算法的对比： 效率上来说，复制算法是当之无愧的老大，但是却浪费了太多内存。 而为了尽量兼顾上面提到的三个指标，标记-整理算法相对来说更平滑一些，但是效率上不尽如人意，它比复制算法多了一个标记的阶段，比标记-清除多了一个整理内存的阶段。 分代收集算法，是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的收集方式，以便提高回收效率。一般是把Java堆分为新生代和老年代，这样就可以根据各个年代的特点使用不同的回收算法，以提高垃圾回收的效率。 在Java程序运行的过程中，会产生大量的对象，其中有些对象是与业务信息相关，比如Http请求中的Session对象、线程、Socket连接，这类对象跟业务直接挂钩，因此生命周期比较长。但是还有一些对象，主要是程序运行过程中生成的临时变量，这些对象生命周期会比较短，比如：String对象，由于其不变类的特性，系统会产生大量的这些对象，有些对象甚至只用一次即可回收。 目前几乎所有的GC都是采用分代收集（Generational Collecting）算法执行垃圾回收的。 在HotSpot中，基于分代的概念，GC所使用的内存回收算法必须结合年轻代和老年代各自的特点。 年轻代(Young Gen) 年轻代特点：区域相对老年代较小，对象生命周期短、存活率低，回收频繁。 这种情况复制算法的回收整理，速度是最快的。复制算法的效率只和当前存活对象大小有关，因此很适用于年轻代的回收。而复制算法内存利用率不高的问题，通过hotspot中的两个survivor的设计得到缓解。 老年代(Tenured Gen) 老年代特点：区域较大，对象生命周期长、存活率高，回收不及年轻代频繁。 这种情况存在大量存活率高的对象，复制算法明显变得不合适。一般是由标记-清除或者是标记-清除与标记-整理的混合实现。 Mark阶段的开销与存活对象的数量成正比。 Sweep阶段的开销与所管理区域的大小成正相关。 Compact阶段的开销与存活对象的数据成正比。 以HotSpot中的CMS回收器为例，CMS是基于Mark-Sweep实现的，对于对象的回收效率很高。而对于碎片问题，CMS采用基于Mark-Compact算法的Serial Old回收器作为补偿措施：当内存回收不佳（碎片导致的Concurrent Mode Failure时），将采用Serial Old执行Full GC以达到对老年代内存的整理。 分代的思想被现有的虚拟机广泛使用。几乎所有的垃圾回收器都区分新生代和老年代。 增量收集算法上述现有的算法，在垃圾回收过程中，应用软件将处于一种Stop the World 的状态。在Stop the World 状态下，应用程序所有的线程都会挂起，暂停一切正常的工作，等待垃圾回收的完成。如果垃圾回收时间过长，应用程序会被挂起很久，将严重影响用户体验或者系统的稳定性。为了解决这个问题，即对实时垃圾收集算法的研究直接导致了增量收集（Incremental Collecting）算法的诞生。 基本思想 如果一次性将所有的垃圾进行处理，需要造成系统长时间的停顿，那么就可以让垃圾收集线程和应用程序线程交替执行。每次，垃圾收集线程只收集一小片区域的内存空间，接着切换到应用程序线程。依次反复，直到垃圾收集完成。 总的来说，增量收集算法的基础仍是传统的标记-清除和复制算法。增量收集算法通过对线程间冲突的妥善处理，允许垃圾收集线程以分阶段的方式完成标记、清理或复制工作。 缺点： 使用这种方式，由于在垃圾回收过程中，间断性地还执行了应用程序代码，所以能减少系统的停顿时间。但是，因为线程切换和上下文转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量的下降。 分区算法分区算法：—G1 GC使用的算法 分代算法将按照对象的生命周期长短划分成两个部分，分区算法将整个堆空间划分成连续的不同小区间。 每一个小区间都独立使用，独立回收。这种算法的好处是可以控制一次回收多少个小区间。 一般来说，在相同条件下，堆空间越大，一次GC时所需要的时间就越长，有关GC产生的停顿也越长。为了更好地控制GC产生的停顿时间，将一块大的内存区域分割成多个小块，根据目标的停顿时间，每次合理地回收若干个小区间，而不是整个堆空间，从而减少一次GC所产生的停顿。 2.相关概念System.gc()System.gc()和Runtime.getRunTime().gc()会做什么事情？ (字节跳动) 在默认情况下，通过System.gc()或者Runtime.getRuntime().gc()的调用，会显式触发Full GC，同时对老年代和新生代进行回收，尝试释放被丢弃对象占用的内存。 然而System.gc()调用附带一个免责声明，无法保证对垃圾收集器的调用。 JVM实现者可以通过System.gc()调用来决定JVM的GC行为。而一般情况下，垃圾回收应该是自动进行的，无须手动触发，否则就太过于麻烦了。在一些特殊情况下，如我们正在编写一个性能基准，我们可以在运行之间调用System.gc()。 finalize()方法详解finalize()方法详解，前言，finalize()是Object的protected方法，子类可以覆盖该方法以实现资源清理工作，GC在回收对象之前调用该方法。 finalize的作用 (1)finalize()与C++中的析构函数不是对应的。C++中的析构函数调用的时机是确定的（对象离开作用域或delete掉），但Java中的finalize的调用具有不确定性 (2)不建议用finalize方法完成“非内存资源”的清理工作，但建议用于：① **清理本地对象(通过JNI创建的对象)**；② 作为确保某些非内存资源(如Socket、文件等)释放的一个补充：在finalize方法中显式调用其他资源释放方法。 内存泄漏与内存溢出内存溢出 内存溢出相对于内存泄漏来说，尽管更容易被理解，但是同样的，内存溢出也是引发程序崩溃的罪魁祸首之一。 由于GC一直在发展，所有一般情况下，除非应用程序占用的内存增长速度非常快，造成垃圾回收已经跟不上内存消耗的速度，否则不太容易出现OOM的情况。 大多数情况下，GC会进行各种年龄段的垃圾回收，实在不行了就放大招，来一次独占式的Full GC操作，这时候会回收大量的内存，供应用程序继续使用。 javadoc中对OutOfMemoryError的解释是，没有空闲内存，并且垃圾收集器也无法提供更多内存。 OOM之前必回调用GC? 这里面隐含着一层意思是，在抛出OutOfMemoryError之前，通常垃圾收集器会被触发，尽其所能去清理出空间。 例如：在引用机制分析中，涉及到JVM会去尝试回收软引用指向的对象等。 在java.nio.BIts.reserveMemory()方法中，我们能清楚的看到，System.gc()会被调用，以清理空间。 当然，也不是在任何情况下垃圾收集器都会被触发的 比如，我们去分配一个超大对象，类似一个超大数组超过堆的最大值，JVM可以判断出垃圾收集并不能解决这个问题，所以直接抛出OutOfMemoryError。 内存泄漏 何为内存泄漏（memory leak） 可达性分析算法来判断对象是否是不再使用的对象，本质都是判断一个对象是否还被引用。那么对于这种情况下，由于代码的实现不同就会出现很多种内存泄漏问题（让JVM误以为此对象还在引用中，无法回收，造成内存泄漏）。 是否还被使用？ 是 是否还被需要？ 否 内存泄漏（memory leak）的理解 严格来说，只有对象不会再被程序用到了，但是GC又不能回收他们的情况，才叫内存泄漏。 但实际情况很多时候一些不太好的实践（或疏忽）会导致对象的生命周期变得很长甚至导致OOM，也可以叫做宽泛意义上的“内存泄漏”。 对象 X 引用对象 Y，X 的生命周期比 Y 的生命周期长； 那么当Y生命周期结束的时候，X依然引用着Y，这时候，垃圾回收期是不会回收对象Y的； 如果对象X还引用着生命周期比较短的A、B、C，对象A又引用着对象 a、b、c，这样就可能造成大量无用的对象不能被回收，进而占据了内存资源，造成内存泄漏，直到内存溢出。 内存泄漏与内存溢出的关系： 内存泄漏（memory leak ） 申请了内存用完了不释放，比如一共有 1024M 的内存，分配了 512M 的内存一直不回收，那么可以用的内存只有 512M 了，仿佛泄露掉了一部分； 通俗一点讲的话，内存泄漏就是【占着茅坑不拉shi】。 内存溢出（out of memory） 申请内存时，没有足够的内存可以使用； 通俗一点儿讲，一个厕所就三个坑，有两个站着茅坑不走的（内存泄漏），剩下最后一个坑，厕所表示接待压力很大，这时候一下子来了两个人，坑位（内存）就不够了，内存泄漏变成内存溢出了。 可见，内存泄漏和内存溢出的关系：内存泄漏的增多，最终会导致内存溢出。 泄漏的分类 经常发生：发生内存泄露的代码会被多次执行，每次执行，泄露一块内存； 偶然发生：在某些特定情况下才会发生； 一次性：发生内存泄露的方法只会执行一次； 隐式泄漏：一直占着内存不释放，直到执行结束；严格的说这个不算内存泄漏，因为最终释放掉了，但是如果执行时间特别长，也可能会导致内存耗尽。 Java中内存泄漏的8种情况 静态集合类，如HashMap、LinkedList等等。如果这些容器为静态的，那么它们的生命周期与JVM程序一致，则容器中的对象在程序结束之前将不能被释放，从而造成内存泄漏。简单而言，长生命周期的对象持有短生命周期对象的引用，尽管短生命周期的对象不再使用，但是因为长生命周期对象持有它的引用而导致不能被回收。 12345678public class MemoryLeak &#123;static List *list* = new ArrayList();public void oomTests() &#123; Object obj = new Object();//局部变量*list*.add(obj); &#125;&#125; 单例模式 单例模式，和静态集合导致内存泄露的原因类似，因为单例的静态特性，它的生命周期和 JVM 的生命周期一样长，所以如果单例对象如果持有外部对象的引用，那么这个外部对象也不会被回收，那么就会造成内存泄漏。 内部类持有外部类 内部类持有外部类，如果一个外部类的实例对象的方法返回了一个内部类的实例对象。 这个内部类对象被长期引用了，即使那个外部类实例对象不再被使用，但由于内部类持有外部类的实例对象，这个外部类对象将不会被垃圾回收，这也会造成内存泄漏。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class HandlerDemoActivity extends Activity implements OnClickListener &#123; private static final int MESSAGE_INCRESE = 0; private static final int MESSAGE_DECRESE = 1; private TextView tv_demo_number; private Button btn_demo_increase; private Button btn_demo_decrease; private Button btn_demo_pause; private Handler handler = new Handler()&#123; //回调方法 public void handleMessage(android.os.Message msg) &#123; String strNum = tv_demo_number.getText().toString(); //转换为整型数据,获取当前显示的数值 int num = Integer.parseInt(strNum); switch(msg.what)&#123; case MESSAGE_INCRESE: num++; tv_demo_number.setText(num + &quot;&quot;); if(num == 20)&#123; Toast.makeText(HandlerDemoActivity.this, &quot;已达到最大值&quot;, 0).show(); btn_demo_pause.setEnabled(false); return; &#125; //发送延迟的+1的消息 sendEmptyMessageDelayed(MESSAGE_INCRESE, 300);//指的是延迟处理，而不是延迟发送 break; case MESSAGE_DECRESE: num--; tv_demo_number.setText(num + &quot;&quot;); if(num == 0)&#123; Toast.makeText(HandlerDemoActivity.this, &quot;已达到最小值&quot;, 0).show(); btn_demo_pause.setEnabled(false); return; &#125; //发送延迟的-1的消息 sendEmptyMessageDelayed(MESSAGE_DECRESE, 300);//指的是延迟处理，而不是延迟发送 break; &#125; &#125; &#125;; @Override protected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_handler_demo); init(); &#125; private void init() &#123; tv_demo_number = (TextView) findViewById(R.id.tv_demo_number); btn_demo_increase = (Button) findViewById(R.id.btn_demo_increase); btn_demo_decrease = (Button) findViewById(R.id.btn_demo_decrease); btn_demo_pause = (Button) findViewById(R.id.btn_demo_pause); btn_demo_increase.setOnClickListener(this); btn_demo_decrease.setOnClickListener(this); btn_demo_pause.setOnClickListener(this); &#125; @Override public void onClick(View v) &#123; .... &#125;&#125; 各种连接，如数据库连接、网络连接和IO连接等 变量不合理的作用域 改变哈希值 缓存泄漏 监听器和回调 STW Stop-the-World ，简称STW，指的是GC事件发生过程中，会产生应用程序的停顿。停顿产生时整个应用程序线程都会被暂停，没有任何响应，有点像卡死的感觉，这个停顿称为STW。 可达性分析算法中枚举根节点（GC Roots）会导致所有Java执行线程停顿。 分析工作必须在一个能确保一致性的快照中进行 一致性指整个分析期间整个执行系统看起来像被冻结在某个时间点上 如果出现分析过程中对象引用关系还在不断变化，则分析结果的准确性无法保证 被STW中断的应用程序线程会在完成GC之后恢复，频繁中断会让用户感觉像是网速不快造成电影卡带一样，所以我们需要减少STW的发生。 STW事件和采用哪款GC无关，所有的GC都有这个事件。 哪怕是G1也不能完全避免Stop-the-world 情况发生，只能说垃圾回收器越来越优秀，回收效率越来越高，尽可能地缩短了暂停时间。 STW是JVM在后台自动发起和自动完成的。在用户不可见的情况下，把用户正常的工作线程全部停掉。 开发中不要用System.gc();会导致Stop-the-world的发生。 垃圾回收的并行与并发并发(Concurrent) 在操作系统中，是指一个时间段中有几个程序都处于已启动运行到运行完毕之间，且这几个程序都是在同一个处理器上运行。 并发不是真正意义上的“同时进行”，只是CPU把一个时间段划分成几个时间片段(时间区间)，然后在这几个时间区间之间来回切换，由于CPU处理的速度非常快，只要时间间隔处理得当，即可让用户感觉是多个应用程序同时在进行。 并行(Parallel) 当系统有一个以上CPU时，当一个CPU执行一个进程时，另一个CPU可以执行另一个进程，两个进程互不抢占CPU资源，可以同时进行，我们称之为并行(Parallel)。 其实决定并行的因素不是CPU的数量，而是CPU的核心数量，比如一个CPU多个核也可以并行。 适合科学计算，后台处理等弱交互场景 二者对比： 并发，指的是多个事情，在同一时间段内同时发生了。 并行，指的是多个事情，在同一时间点上同时发生了。 并发的多个任务之间是互相抢占资源的。 并行的多个任务之间是不互相抢占资源的。 只有在多CPU或者一个CPU多核的情况中，才会发生并行。 否则，看似同时发生的事情，其实都是并发执行的。 并发和并行，在谈论垃圾收集器的上下文语境中，它们可以解释如下： 并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍处于等待状态。 如ParNew、Parallel Scavenge、Parallel Old； 串行（Serial） 相较于并行的概念，单线程执行。 如果内存不够，则程序暂停，启动JVM垃圾回收器进行垃圾回收。回收完，再启动程序的线程。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），垃圾回收线程在执行时不会停顿用户程序的运行。 用户程序在继续运行，而垃圾收集程序线程运行于另一个CPU上； 如：CMS、G1 安全点与安全区域安全点(Safepoint) 程序执行时并非在所有地方都能停顿下来开始 GC，只有在特定的位置才能停顿下来开始GC，这些位置称为“安全点（Safepoint）”。 Safe Point的选择很重要，如果太少可能导致GC等待的时间太长，如果太频繁可能导致运行时的性能问题。大部分指令的执行时间都非常短暂，通常会根据“是否具有让程序长时间执行的特征”为标准。比如：选择一些执行时间较长的指令作为Safe Point，如方法调用、循环跳转和异常跳转等。 如何在GC发生时，检查所有线程都跑到最近的安全点停顿下来呢？ 抢先式中断：（目前没有虚拟机采用了） 首先中断所有线程。如果还有线程不在安全点，就恢复线程，让线程跑到安全点。 主动式中断： 设置一个中断标志，各个线程运行到Safe Point的时候主动轮询这个标志，如果中断标志为真，则将自己进行中断挂起。 安全区域(Safe Region) Safepoint 机制保证了程序执行时，在不太长的时间内就会遇到可进入 GC 的 Safepoint 。但是，程序“不执行”的时候呢？例如线程处于 Sleep 状态或 Blocked 状态，这时候线程无法响应 JVM 的中断请求，“走”到安全点去中断挂起，JVM 也不太可能等待线程被唤醒。对于这种情况，就需要安全区域（Safe Region）来解决。 安全区域是指在一段代码片段中，对象的引用关系不会发生变化，在这个区域中的任何位置开始GC都是安全的。我们也可以把 Safe Region 看做是被扩展了的 Safepoint。 实际执行时： 1、当线程运行到Safe Region的代码时，首先标识已经进入了Safe Region，如果这段时间内发生GC，JVM会忽略标识为Safe Region状态的线程； 2、当线程即将离开Safe Region时，会检查JVM是否已经完成GC，如果完成了，则继续运行，否则线程必须等待直到收到可以安全离开Safe Region的信号为止； 5种引用强引用（Strong Reference）——不回收在Java程序中，最常见的引用类型是强引用（普通系统99%以上都是强引用），也就是我们最常见的普通对象引用，也是默认的引用类型。 当在Java语言中使用new操作符创建一个新的对象，并将其赋值给一个变量的时候，这个变量就成为指向该对象的一个强引用。 强引用的对象是可触及的，垃圾收集器就永远不会回收掉被引用的对象。 对于一个普通的对象，如果没有其他的引用关系，只要超过了引用的作用域或者显式地将相应（强）引用赋值为null，就是可以当做垃圾被收集了，当然具体回收时机还是要看垃圾收集策略。 相对的， 软引用、弱引用和虚引用的对象是软可触及、弱可触及和虚可触及的，在一定条件下，都是可以被回收的。所以，强引用是造成Java内存泄漏的主要原因之一。 强引用例子： StringBuffer str &#x3D; new StringBuffer (“Hello,尚硅谷”); 局部变量str指向StringBuffer实例所在堆空间，通过str可以操作该实例， 那么str就是StringBuffer实例的强引用 对应内存结构： 此时，如果再运行一个赋值语句： StringBuffer str1 &#x3D; str; 对应内存结构： 本例中的两个引用，都是强引用，强引用具备以下特点： 强引用可以直接访问目标对象。 强引用所指向的对象在任何时候都不会被系统回收，虚拟机宁愿抛出OOM异常，也不会回收强引用所指向对象。 强引用可能导致内存泄漏。 软引用（Soft Reference）— 内存不足即回收软引用是用来描述一些还有用，但非必需的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。 软引用通常用来实现内存敏感的缓存。比如：高速缓存就有用到软引用。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 垃圾回收器在某个时刻决定回收软可达的对象的时候，会清理软引用，并可选地把引用存放到一个引用队列（Reference Queue）。 类似弱引用，只不过Java虚拟机会尽量让软引用的存活时间长一些，迫不得已才清理。 在JDK 1.2版之后提供了java.lang.ref.SoftReference类来实现软引用。 Object obj &#x3D; new Object(); &#x2F;&#x2F;声明强引用 SoftReference sf &#x3D; new SoftReference(obj); obj &#x3D; null; &#x2F;&#x2F;销毁强引用 弱引用（Weak Reference）—发现即回收弱引用也是用来描述那些非必需对象，只被弱引用关联的对象只能生存到下一次垃圾收集发生为止。在系统GC时，只要发现弱引用，不管系统堆空间使用是否充足，都会回收掉只被弱引用关联的对象。 但是，由于垃圾回收器的线程通常优先级很低，因此, 并不一定能很快地发现持有弱引用的对象。在这种情况下，弱引用对象可以存在较长的时间。 弱引用和软引用一样，在构造弱引用时，也可以指定一个引用队列，当弱引用对象被回收时，就会加入指定的引用队列，通过这个队列可以跟踪对象的回收情况。 弱引用非常适合来保存那些可有可无的缓存数据。如果这么做，当系统内存不足时，这些缓存数据会被回收，不会导致内存溢出。而当内存资源充足时，这些缓存数据又可以存在相当长的时间，从而起到加速系统的作用。 在JDK 1.2版之后提供了java.lang.ref.WeakReference类来实现弱引用。 Object obj &#x3D; new Object(); &#x2F;&#x2F;声明强引用 WeakReference wr &#x3D; new WeakReference(obj); obj &#x3D; null; &#x2F;&#x2F;销毁强引用 弱引用对象与软引用对象的最大不同就在于，当GC在进行回收时，需要通过算法检查是否回收软引用对象，而对于弱引用对象，GC总是进行回收。弱引用对象更容易、更快被GC回收。 虚引用（Phantom Reference）—对象回收跟踪也称为“幽灵引用”或者“幻影引用”，是所有引用类型中最弱的一个。 一个对象是否有虚引用的存在，完全不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它和没有引用几乎是一样的，随时都可能被垃圾回收器回收。 它不能单独使用，也无法通过虚引用来获取被引用的对象。当试图通过虚引用的get()方法取得对象时，总是null。 为一个对象设置虚引用关联的唯一目的在于跟踪垃圾回收过程。比如：能在这个对象被收集器回收时收到一个系统通知。 虚引用必须和引用队列一起使用。虚引用在创建时必须提供一个引用队列作为参数。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象后，将这个虚引用加入引用队列，以通知应用程序对象的回收情况。 由于虚引用可以跟踪对象的回收时间，因此，也可以将一些资源释放操作放置在虚引用中执行和记录。 在JDK 1.2版之后提供了PhantomReference类来实现虚引用。 Object obj &#x3D; new Object();ReferenceQueue phantomQueue &#x3D; new ReferenceQueue(); PhantomReference pf &#x3D; new PhantomReference(obj, phantomQueue); obj &#x3D; null; 终结器引用(Final reference) 不太懂 它用以实现对象的finalize()方法，也可以称为终结器引用。 无需手动编码，其内部配合引用队列使用。 在GC时，终结器引用入队。由Finalizer线程通过终结器引用找到被引用对象并调用它的finalize()方法，第二次GC时才能回收被引用对象。 3.垃圾回收器GC分类 按线程数分，可以分为串行垃圾回收器和并行垃圾回收器。 串行回收指的是在同一时间段内只允许有一个CPU用于执行垃圾回收操作，此时工作线程被暂停，直至垃圾收集工作结束。 在诸如单CPU处理器或者较小的应用内存等硬件平台不是特别优越的场合，串行回收器的性能表现可以超过并行回收器和并发回收器。所以，串行回收默认被应用在客户端的Client模式下的JVM中 在并发能力比较强的CPU上，并行回收器产生的停顿时间要短于串行回收器。 和串行回收相反，并行收集可以运用多个CPU同时执行垃圾回收，因此提升了应用的吞吐量，不过并行回收仍然与串行回收一样，采用独占式，使用了“Stop-the-world”机制。 按照工作模式分，可以分为并发式垃圾回收器和独占式垃圾回收器。 并发式垃圾回收器与应用程序线程交替工作，以尽可能减少应用程序的停顿时间。 独占式垃圾回收器(Stop the world)一旦运行，就停止应用程序中的所有用户线程，直到垃圾回收过程完全结束。 按碎片处理方式分，可分为压缩式垃圾回收器和非压缩式垃圾回收器。 压缩式垃圾回收器会在回收完成后，对存活对象进行压缩整理，消除回收后的碎片。 再分配对象空间使用：指针碰撞 非压缩式的垃圾回收器不进行这步操作。 再分配对象空间使用：空闲列表 按工作的内存区间分，又可分为年轻代垃圾回收器和老年代垃圾回收器。 GC评估指标评估GC的性能指标：吞吐量(throughput) 吞吐量就是CPU用于运行用户代码的时间与CPU总消耗时间的比值，即吞吐量 &#x3D; 运行用户代码时间 &#x2F;（运行用户代码时间 + 垃圾收集时间）。 比如：虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 这种情况下，应用程序能容忍较高的暂停时间。因此，高吞吐量的应用程序有更长的时间基准，快速响应是不必考虑的。 吞吐量优先，意味着在单位时间内，STW的时间最短：0.2 + 0.2 &#x3D; 0.4 评估GC的性能指标：暂停时间(pause time) “暂停时间”是指一个时间段内应用程序线程暂停，让GC线程执行的状态 例如，GC期间100毫秒的暂停时间意味着在这100毫秒期间内没有应用程序线程是活动的。 暂停时间优先，意味着尽可能让单次STW的时间最短：0.1 + 0.1 + 0.1 + 0.1 + 0.1 &#x3D; 0.5 评估GC的性能指标：吞吐量vs暂停时间 高吞吐量较好因为这会让应用程序的最终用户感觉只有应用程序线程在做“生产性”工作。直觉上，吞吐量越高程序运行越快。 低暂停时间（低延迟）较好因为从最终用户的角度来看不管是GC还是其他原因导致一个应用被挂起始终是不好的。这取决于应用程序的类型，有时候甚至短暂的200毫秒暂停都可能打断终端用户体验。因此，具有低的较大暂停时间是非常重要的，特别是对于一个交互式应用程序。 不幸的是”高吞吐量”和”低暂停时间”是一对相互竞争的目标（矛盾）。 因为如果选择以吞吐量优先，那么必然需要降低内存回收的执行频率，但是这样会导致GC需要更长的暂停时间来执行内存回收。 相反的，如果选择以低延迟优先为原则，那么为了降低每次执行内存回收时的暂停时间，也只能频繁地执行内存回收，但这又引起了年轻代内存的缩减和导致程序吞吐量的下降。 在设计（或使用）GC算法时，我们必须确定我们的目标：一个GC算法只可能针对两个目标之一（即只专注于较大吞吐量或最小暂停时间），或尝试找到一个二者的折衷。 现在JVM调优标准：在最大吞吐量优先的情况下，降低停顿时间。 垃圾回收器都有哪些？GC发展史有了虚拟机，就一定需要收集垃圾的机制，这就是Garbage Collection，对应的产品我们称为Garbage Collector。 1999年随JDK1.3.1一起来的是串行方式的Serial GC ，它是第一款GC。ParNew垃圾收集器是Serial收集器的多线程版本 2002年2月26日，Parallel GC 和Concurrent Mark Sweep GC跟随JDK1.4.2一起发布 Parallel GC在JDK6之后成为HotSpot默认GC。 2012年，在JDK1.7u4版本中，G1可用。-XX:+UseG1GC 2017年，JDK9中G1变成默认的垃圾收集器，以替代CMS。 2018年3月，JDK 10中G1垃圾回收器的并行完整垃圾回收，实现并行性来改善最坏情况下的延迟。 2018年9月，JDK11发布。引入Epsilon 垃圾回收器，又被称为”No-Op（无操作）”回收器。同时，引入ZGC：可伸缩的低延迟垃圾回收器(Experimental)。 2019年3月，JDK12发布。增强G1，自动返回未用堆内存给操作系统。同时，引入Shenandoah GC：低停顿时间的GC(Experimental)。 2019年9月，JDK13发布。增强ZGC，自动返回未用堆内存给操作系统。 2020年3月，JDK14发布。删除CMS垃圾回收器。扩展ZGC在macOS和Windows上的应用 查看默认 垃圾回收期 -XX:+PrintCommandLineFlags：查看命令行相关参数（包含使用的垃圾收集器） 使用命令行指令：jinfo –flag 相关垃圾回收器参数 进程ID Serial GC：串行回收 Serial收集器是最基本、历史最悠久的垃圾收集器了。JDK1.3之前回收新生代唯一的选择。 Serial收集器作为HotSpot中Client模式下的默认新生代垃圾收集器。 Serial 收集器采用复制算法、串行回收和”Stop-the-World”机制的方式执行内存回收。 除了年轻代之外，Serial收集器还提供用于执行老年代垃圾收集的Serial Old收集器。Serial Old 收集器同样也采用了串行回收和”Stop the World”机制，只不过内存回收算法使用的是标记-压缩算法。 Serial Old是运行在Client模式下默认的老年代的垃圾回收器 Serial Old在Server模式下主要有两个用途：① 与新生代的Parallel Scavenge配合使用 ② 作为老年代CMS收集器的后备垃圾收集方案 这个收集器是一个单线程的收集器，但它的“单线程”的意义并不仅仅说明它只会使用一个 CPU 或一条收集线程去完成垃圾收集工作，更重要的是在它进行垃圾收集时，必须暂停其他所有的工作线程，直到它收集结束（Stop The World）。 优势：简单而高效（与其他收集器的单线程比），对于限定单个 CPU 的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。 运行在Client模式下的虚拟机是个不错的选择。 在用户的桌面应用场景中，可用内存一般不大（几十MB至一两百MB），可以在较短时间内完成垃圾收集（几十ms至一百多ms）,只要不频繁发生，使用串行回收器是可以接受的 在HotSpot虚拟机中，使用 -XX:+UseSerialGC 参数可以指定年轻代和老年代都使用串行收集器。 等价于 新生代用Serial GC，且老年代用Serial Old GC 总 结： 这种垃圾收集器大家了解，现在已经不用串行的了。而且在限定单核cpu才可以用。现在都不是单核的了。 对于交互较强的应用而言，这种垃圾收集器是不能接受的。一般在Java web应用程序中是不会采用串行垃圾收集器的。 ParNew GC：并行回收 如果说Serial GC是年轻代中的单线程垃圾收集器，那么ParNew收集器则是Serial收集器的多线程版本。 Par是Parallel的缩写，New：只能处理的是新生代 ParNew 收集器除了采用并行回收的方式执行内存回收外，两款垃圾收集器之间几乎没有任何区别。ParNew收集器在年轻代中同样也是采用复制算法、”Stop-the-World”机制。 ParNew 是很多JVM运行在Server模式下新生代的默认垃圾收集器。 对于新生代，回收次数频繁，使用并行方式高效。 对于老年代，回收次数少，使用串行方式节省资源。（CPU并行需要切换线程，串行可以省去切换线程的资源） 由于ParNew收集器是基于并行回收，那么是否可以断定ParNew收集器的回收效率在任何场景下都会比Serial收集器更高效？ ParNew 收集器运行在多CPU的环境下，由于可以充分利用多CPU、多核心等物理硬件资源优势，可以更快速地完成垃圾收集，提升程序的吞吐量。 但是在单个CPU的环境下，ParNew收集器不比Serial 收集器更高效。虽然Serial收集器是基于串行回收，但是由于CPU不需要频繁地做任务切换，因此可以有效避免多线程交互过程中产生的一些额外开销。 因为除Serial外，目前只有ParNew GC能与CMS收集器配合工作 在程序中，开发人员可以通过选项”-XX:+UseParNewGC”手动指定使用ParNew收集器执行内存回收任务。它表示年轻代使用并行收集器，不影响老年代。 -XX:ParallelGCThreads 限制线程数量，默认开启和CPU数据相同的线程数。 Parallel GC：吞吐量优先 HotSpot的年轻代中除了拥有ParNew收集器是基于并行回收的以外，Parallel Scavenge收集器同样也采用了复制算法、并行回收和”Stop the World”机制。 那么Parallel收集器的出现是否多此一举？ 和ParNew收集器不同，Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput），它也被称为吞吐量优先的垃圾收集器。 自适应调节策略也是Parallel Scavenge与ParNew一个重要区别。 高吞吐量则可以高效率地利用 CPU 时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。因此，常见在服务器环境中使用。例如，那些执行批量处理、订单处理、工资支付、科学计算的应用程序。 Parallel 收集器在JDK1.6时提供了用于执行老年代垃圾收集的Parallel Old收集器，用来代替老年代的Serial Old收集器。 Parallel Old收集器采用了标记-压缩算法，但同样也是基于并行回收和”Stop-the-World”机制。 在程序吞吐量优先的应用场景中， Parallel 收集器和Parallel Old收集器的组合，在Server模式下的内存回收性能很不错。 在Java8中，默认是此垃圾收集器。 参数配置： -XX:+UseParallelGC 手动指定年轻代使用Parallel并行收集器执行内存回收任务。 -XX:+UseParallelOldGC 手动指定老年代都是使用并行回收收集器。 分别适用于新生代和老年代。默认jdk8是开启的。 上面两个参数，默认开启一个，另一个也会被开启。（互相激活） -XX:ParallelGCThreads 设置年轻代并行收集器的线程数。一般地，最好与CPU数量相等，以避免过多的线程数影响垃圾收集性能。 在默认情况下，当CPU 数量小于8个， ParallelGCThreads 的值等于CPU 数量。 当CPU数量大于8个，ParallelGCThreads 的值等于3+[5*CPU_Count]&#x2F;8] 。 -XX:MaxGCPauseMillis 设置垃圾收集器最大停顿时间(即STW的时间)。单位是毫秒。 为了尽可能地把停顿时间控制在MaxGCPauseMills以内，收集器在工作时会调整Java堆大小或者其他一些参数。 对于用户来讲，停顿时间越短体验越好。但是在服务器端，我们注重高并发，整体的吞吐量。所以服务器端适合Parallel，进行控制。 该参数使用需谨慎。 -XX:GCTimeRatio垃圾收集时间占总时间的比例（&#x3D; 1 &#x2F; (N + 1))。用于衡量吞吐量的大小。 取值范围（0,100）。默认值99，也就是垃圾回收时间不超过1%。 与前一个-XX:MaxGCPauseMillis参数有一定矛盾性。暂停时间越长，Radio参数就容易超过设定的比例。 -XX:+UseAdaptiveSizePolicy 设置Parallel Scavenge收集器具有自适应调节策略 在这种模式下，年轻代的大小、Eden和Survivor的比例、晋升老年代的对象年龄等参数会被自动调整，已达到在堆大小、吞吐量和停顿时间之间的平衡点。 在手动调优比较困难的场合，可以直接使用这种自适应的方式，仅指定虚拟机的最大堆、目标的吞吐量（GCTimeRatio）和停顿时间（MaxGCPauseMills），让虚拟机自己完成调优工作。 CMS：低延迟 在 JDK 1.5 时期，HotSpot 推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS (Concurrent-Mark-Sweep)收集器，这款收集器是HotSpot虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提升用户体验。 目前很大一部分的Java应用集中在互联网站或者B&#x2F;S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS收集器就非常符合这类应用的需求。 CMS的垃圾收集算法采用标记-清除算法，并且也会”Stop-the-world” 不幸的是，CMS 作为老年代的收集器，却无法与 JDK 1.4.0 中已经存在的新生代收集器Parallel Scavenge 配合工作，所以在JDK 1.5中使用CMS来收集老年代的时候，新生代只能选择ParNew或者Serial收集器中的一个。 在G1出现之前，CMS使用还是非常广泛的。一直到今天，仍然有很多系统使用CMS GC。 初始标记（STW）：暂时时间非常短，标记与GC Roots直接关联的对象。 并发标记（最耗时）：从GC Roots开始遍历整个对象图的过程。不会停顿用户线程 重新标记：（STW）：修复并发标记环节，因为用户线程的执行，导致数据的不一致性问题 并发清理（最耗时） CMS整个过程比之前的收集器要复杂,整个过程分为4个主要阶段，即初始标记阶段、并发标记阶段、重新标记阶段和并发清除阶段。 初始标记（Initial-Mark）阶段：在这个阶段中，程序中所有的工作线程都将会因为“Stop-the-World”机制而出现短暂的暂停，这个阶段的主要任务仅仅只是标记出GC Roots能直接关联到的对象。一旦标记完成之后就会恢复之前被暂停的所有应用线程。由于直接关联对象比较小，所以这里的速度非常快。 并发标记（Concurrent-Mark）阶段：从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。 重新标记（Remark）阶段：由于在并发标记阶段中，程序的工作线程会和垃圾收集线程同时运行或者交叉运行，因此为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录（比如：由不可达变为可达对象的数据），这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短。 并发清除（Concurrent-Sweep）阶段：此阶段清理删除掉标记阶段判断的已经死亡的对象，释放内存空间。由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的 尽管CMS收集器采用的是并发回收（非独占式），但是在其初始化标记和再次标记这两个阶段中仍然需要执行“Stop-the-World”机制暂停程序中的工作线程，不过暂停时间并不会太长，因此可以说明目前所有的垃圾收集器都做不到完全不需要“Stop-the-World”，只是尽可能地缩短暂停时间。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 另外，由于在垃圾收集阶段用户线程没有中断，所以在CMS回收过程中，还应该确保应用程序用户线程有足够的内存可用。因此，CMS收集器不能像其他收集器那样等到老年代几乎完全被填满了再进行收集，而是当堆内存使用率达到某一阈值时，便开始进行回收，以确保应用程序在CMS工作过程中依然有足够的空间支持应用程序运行。要是CMS运行期间预留的内存无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时虚拟机将启动后备预案：临时启用 Serial Old 收集器来重新进行老年代的垃圾收集，这样停顿时间就很长了。 CMS收集器的垃圾收集算法采用的是标记—清除算法，这意味着每次执行完内存回收后，由于被执行内存回收的无用对象所占用的内存空间极有可能是不连续的一些内存块，不可避免地将会产生一些内存碎片。那么CMS在为新对象分配内存空间时，将无法使用指针碰撞（Bump the Pointer）技术，而只能够选择空闲列表（Free List）执行内存分配。 有人会觉得既然Mark Sweep会造成内存碎片,那么为什么不把算法换成Mark Compact呢? 答案其实很简答，因为当并发清除的时候，用Compact整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提的它运行的资源不受影响嘛。Mark-Compact更适合“Stop the World”这种场景下使用 参数 -XX:+UseConcMarkSweepGC 手动指定使用CMS 收集器执行内存回收任务。 开启该参数后会自动将-XX:+UseParNewGC打开。即：ParNew(Young区用)+CMS(Old区用)+Serial Old的组合。 -XX:CMSlnitiatingOccupanyFraction 设置堆内存使用率的阈值，一旦达到该阈值，便开始进行回收。 JDK5及以前版本的默认值为68,即当老年代的空间使用率达到68%时，会执行一次CMS 回收。JDK6及以上版本默认值为92% 如果内存增长缓慢，则可以设置一个稍大的值，大的阈值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阈值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC 的执行次数。 -XX:+UseCMSCompactAtFullCollection 用于指定在执行完Full GC后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX:CMSFullGCsBeforeCompaction设置在执行多少次Full GC后对内存空间进行压缩整理。 -XX:ParallelCMSThreads 设置CMS的线程数量。 CMS 默认启动的线程数是（ParallelGCThreads+3)&#x2F;4，ParallelGCThreads 是年轻代并行收集器的线程数。当CPU 资源比较紧张时，受到CMS收集器线程的影响，应用程序的性能在垃圾回收阶段可能会非常糟糕。 CMS的优点： 并发收集 低延迟 CMS的弊端： 1）会产生内存碎片，导致并发清除后，用户线程可用的空间不足。在无法分配大对象的情况下，不得不提前触发Full GC。 2）CMS收集器对CPU资源非常敏感。在并发阶段，它虽然不会导致用户停顿，但是会因为占用了一部分线程而导致应用程序变慢，总吞吐量会降低。 3）CMS收集器无法处理浮动垃圾。可能出现“Concurrent Mode Failure”失败而导致另一次 Full GC 的产生。在并发标记阶段由于程序的工作线程和垃圾收集线程是同时运行或者交叉运行的，那么在并发标记阶段如果产生新的垃圾对象，CMS将无法对这些垃圾对象进行标记，最终会导致这些新产生的垃圾对象没有被及时回收，从而只能在下一次执行GC时释放这些之前未被回收的内存空间。 小结： HotSpot有这么多的垃圾回收器，那么如果有人问，Serial GC、Parallel GC、Concurrent Mark Sweep GC这三个GC有什么不同呢？ 请记住以下口令： 如果你想要最小化地使用内存和并行开销，请选Serial GC； 如果你想要最大化应用程序的吞吐量，请选Parallel GC； 如果你想要最小化GC的中断或停顿时间，请选CMS GC。 JDK9新特性：CMS被标记为Deprecate了(JEP291) 如果对JDK 9及以上版本的HotSpot虚拟机使用参数-XX：+UseConcMarkSweepGC来开启CMS收集器的话，用户会收到一个警告信息，提示CMS未来将会被废弃。 JDK14新特性：删除CMS垃圾回收器(JEP363) 移除了CMS垃圾收集器，如果在JDK14中使用-XX:+UseConcMarkSweepGC的话，JVM不会报错，只是给出一个warning信息，但是不会exit。JVM会自动回退以默认GC方式启动JVM G1 GC：区域化分代式既然我们已经有了前面几个强大的GC，为什么还要发布Garbage First（G1）GC？ 原因就在于应用程序所应对的业务越来越庞大、复杂，用户越来越多，没有GC就不能保证应用程序正常进行，而经常造成STW的GC又跟不上实际的需求，所以才会不断地尝试对GC进行优化。G1（Garbage-First）垃圾回收器是在Java7 update 4之后引入的一个新的垃圾回收器，是当今收集器技术发展的最前沿成果之一。 与此同时，为了适应现在不断扩大的内存和不断增加的处理器数量，进一步降低暂停时间（pause time），同时兼顾良好的吞吐量。 官方给G1设定的目标是在延迟可控的情况下获得尽可能高的吞吐量，所以才担当起“全功能收集器”的重任与期望。 为什么名字叫做Garbage First（G1）呢？ 因为G1是一个并行回收器，它把堆内存分割为很多不相关的区域（Region）（物理上不连续的）。使用不同的Region来表示Eden、幸存者0区，幸存者1区，老年代等。 G1 GC有计划地避免在整个Java 堆中进行全区域的垃圾收集。G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。 由于这种方式的侧重点在于回收垃圾最大量的区间（Region），所以我们给G1一个名字：垃圾优先（Garbage First）。 G1（Garbage-First）是一款面向服务端应用的垃圾收集器，主要针对配备多核CPU及大容量内存的机器，以极高概率满足GC停顿时间的同时，还兼具高吞吐量的性能特征。 在JDK1.7版本正式启用，移除了Experimental的标识，是JDK 9以后的默认垃圾回收器，取代了CMS 回收器以及Parallel + Parallel Old组合。被Oracle官方称为“全功能的垃圾收集器”。 与此同时，CMS已经在JDK 9中被标记为废弃（deprecated）。在jdk8中还不是默认的垃圾回收器，需要使用-XX:+UseG1GC来启用。 G1（Garbage-First）是一款面向服务端应用的垃圾收集器，兼顾吞吐量和停顿时间的GC实现。 在JDK1.7版本正式启用，是JDK 9以后的默认GC选项，取代了CMS 回收器。 与其他 GC 收集器相比，G1使用了全新的分区算法，其特点如下所示： 并行与并发 并行性：G1在回收期间，可以有多个GC线程同时工作，有效利用多核计算能力。此时用户线程STW 并发性：G1拥有与应用程序交替执行的能力，部分工作可以和应用程序同时执行，因此，一般来说，不会在整个回收阶段发生完全阻塞应用程序的情况 分代收集 从分代上看，G1依然属于分代型垃圾回收器，它会区分年轻代和老年代，年轻代依然有Eden区和Survivor区。但从堆的结构上看，它不要求整个Eden区、年轻代或者老年代都是连续的，也不再坚持固定大小和固定数量。 将堆空间分为若干个区域（Region）,这些区域中包含了逻辑上的年轻代和老年代。 和之前的各类回收器不同，它同时兼顾年轻代和老年代。对比其他回收器，或者工作在年轻代，或者工作在老年代； 空间整合 CMS：“标记-清除”算法、内存碎片、若干次GC后进行一次碎片整理 G1将内存划分为一个个的region。内存的回收是以region作为基本单位的。Region之间是复制算法，但整体上实际可看作是标记-压缩（Mark-Compact）算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。尤其是当Java堆非常大的时候，G1的优势更加明显。 可预测的停顿时间模型（即：软实时soft real-time） 这是 G1 相对于 CMS 的另一大优势，G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 由于分区的原因，G1可以只选取部分区域进行内存回收，这样缩小了回收的范围，因此对于全局停顿情况的发生也能得到较好的控制。 G1 跟踪各个 Region 里面的垃圾堆积的价值大小（回收所获得的空间大小以及回收所需时间的经验值），在后台维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的Region。保证了 G1 收集器在有限的时间内可以获取尽可能高的收集效率。 相比于CMS GC，G1未必能做到CMS在最好情况下的延时停顿，但是最差情况要好很多。 缺点： 相较于CMS，G1还不具备全方位、压倒性优势。比如在用户程序运行过程中，G1无论是为了垃圾收集产生的内存占用（Footprint）还是程序运行时的额外执行负载（Overload）都要比CMS要高。 从经验上来说，在小内存应用上CMS的表现大概率会优于G1，而G1在大内存应用上则发挥其优势。平衡点在6-8GB之间。 参数 -XX：+UseG1GC 手动指定使用G1收集器执行内存回收任务。 -XX:G1HeapRegionSize 设置每个Region的大小。值是2的幂，范围是1MB到32MB之间，目标是根据最小的Java堆大小划分出约2048个区域。默认是堆内存的1&#x2F;2000。 -XX:MaxGCPauseMillis 设置期望达到的最大GC停顿时间指标(JVM会尽力实现，但不保证达到)。默认值是200ms -XX:ParallelGCThread 设置STW时GC线程数的值。最多设置为8 -XX:ConcGCThreads 设置并发标记的线程数。将n设置为并行垃圾回收线程数(ParallelGCThreads)的1&#x2F;4左右。 -XX:InitiatingHeapOccupancyPercent 设置触发并发GC周期的Java堆占用率阈值。超过此值，就触发GC。默认值是45。 G1的设计原则就是简化JVM性能调优，开发人员只需要简单的三步即可完成调优： 第一步：开启G1垃圾收集器 第二步：设置堆的最大内存 第三步：设置最大的停顿时间 G1中提供了三种垃圾回收模式：YoungGC、Mixed GC和Full GC，在不同的条件下被触发。 适用场景 面向服务端应用，针对具有大内存、多处理器的机器。(在普通大小的堆里表现并不惊喜) 最主要的应用是需要低GC延迟，并具有大堆的应用程序提供解决方案； 如：在堆大小约6GB或更大时，可预测的暂停时间可以低于0.5秒；（G1通过每次只清理一部分而不是全部的Region的增量式清理来保证每次GC停顿时间不会过长）。 用来替换掉JDK1.5中的CMS收集器； 在下面的情况时，使用G1可能比CMS好： ① 超过50％的Java堆被活动数据占用； ② 对象分配频率或年代提升频率变化很大； ③ GC停顿时间过长（长于0.5至1秒）。 HotSpot 垃圾收集器里，除了G1以外，其他的垃圾收集器使用内置的JVM线程执行GC的多线程操作，而G1 GC可以采用应用线程承担后台运行的GC工作，即当JVM的GC线程处理速度慢时，系统会调用应用程序线程帮助加速垃圾回收过程。 web应用，java进程最大堆4G,每分钟1500个请求，45s年轻代的垃圾回收。 31小时使用率达到了45%，则开发并发标记，进行混合回收。 分区Region：化整为零 使用 G1 收集器时，它将整个Java堆划分成约2048个大小相同的独立Region块，每个Region块大小根据堆空间的实际大小而定，整体被控制在1MB到32MB之间，且为2的N次幂，即1MB,2MB,4MB,8MB,16MB,32MB。可以通过-XX:G1HeapRegionSize设定。所有的Region大小相同，且在JVM生命周期内不会被改变。 虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔离的了，它们都是一部分Region（不需要连续）的集合。通过Region的动态分配方式实现逻辑上的连续。 一个 region 有可能属于 Eden，Survivor 或者 Old&#x2F;Tenured 内存区域。但是一个region只可能属于一个角色。图中的 E 表示该region属于Eden内存区域，S表示属于Survivor内存区域，O表示属于Old内存区域。图中空白的表示未使用的内存空间。 G1 垃圾收集器还增加了一种新的内存区域，叫做 Humongous 内存区域，如图中的 H 块。主要用于存储大对象，如果超过1.5个region，就放到H。 设置H的原因： 对于堆中的大对象，默认直接会被分配到老年代，但是如果它是一个短期存在的大对象，就会对垃圾收集器造成负面影响。为了解决这个问题，G1划分了一个Humongous区，它用来专门存放大对象。如果一个H区装不下一个大对象，那么G1会寻找连续的H区来存储。为了能找到连续的H区，有时候不得不启动Full GC。G1的大多数行为都把H区作为老年代的一部分来看待。 Bump – the – pointer 即：指针碰撞 TLAB 垃圾回收过程G1 GC的垃圾回收过程主要包括如下三个环节： 年轻代GC （Young GC） 老年代并发标记过程 （Concurrent Marking） 混合回收（Mixed GC） （如果需要，单线程、独占式、高强度的Full GC还是继续存在的。它针对GC的评估失败提供了一种失败保护机制，即强力回收。） 顺时针，young gc -&gt; young gc + concurrent mark-&gt; Mixed GC顺序，进行垃圾回收。 应用程序分配内存，当年轻代的Eden区用尽时开始年轻代回收过程；G1的年轻代收集阶段是一个并行的独占式收集器。在年轻代回收期，G1 GC暂停所有应用程序线程，启动多线程执行年轻代回收。然后从年轻代区间移动存活对象到Survivor区间或者老年区间，也有可能是两个区间都会涉及。 当堆内存使用达到一定值（默认45%）时，开始老年代并发标记过程。 标记完成马上开始混合回收过程。对于一个混合回收期，G1 GC从老年区间移动存活对象到空闲区间，这些空闲区间也就成为了老年代的一部分。和年轻代不同，老年代的G1回收器和其他GC不同，G1的老年代回收器不需要整个老年代被回收，一次只需要扫描&#x2F;回收一小部分老年代的Region就可以了。同时，这个老年代Region是和年轻代一起被回收的。 举个例子：一个Web服务器，Java进程最大堆内存为4G，每分钟响应1500个请求，每45秒钟会新分配大约2G的内存。G1会每45秒钟进行一次年轻代回收，每31个小时整个堆的使用率会达到45%，会开始老年代并发标记过程，标记完成后开始四到五次的混合回收。 G1回收器垃圾回收过程: Remembered Set 一个对象被不同区域引用的问题 一个Region不可能是孤立的，一个Region中的对象可能被其他任意Region中对象引用，判断对象存活时，是否需要扫描整个Java堆才能保证准确？ 在其他的分代收集器，也存在这样的问题（而G1更突出） 回收新生代也不得不同时扫描老年代？ 这样的话会降低Minor GC的效率； 解决方法： 无论G1还是其他分代收集器，JVM都是使用Remembered Set来避免全局扫描： 每个Region都有一个对应的Remembered Set； 每次Reference类型数据写操作时，都会产生一个Write Barrier暂时中断操作； 然后检查将要写入的引用指向的对象是否和该Reference类型数据在不同的Region（其他收集器：检查老年代对象是否引用了新生代对象）； 如果不同，通过CardTable把相关引用信息记录到引用指向对象的所在Region对应的Remembered Set中； 当进行垃圾收集时，在GC根节点的枚举范围加入Remembered Set；就可以保证不进行全局扫描，也不会有遗漏。 G1回收过程一：年轻代GC JVM启动时，G1先准备好Eden区，程序在运行过程中不断创建对象到Eden区，当Eden空间耗尽时，G1会启动一次年轻代垃圾回收过程。 年轻代垃圾回收只会回收Eden区和Survivor区。 YGC时，首先G1停止应用程序的执行（Stop-The-World），G1创建回收集（Collection Set），回收集是指需要被回收的内存分段的集合，年轻代回收过程的回收集包含年轻代Eden区和Survivor区所有的内存分段。 然后开始如下回收过程： 第一阶段，扫描根。 根是指static变量指向的对象，正在执行的方法调用链条上的局部变量等。根引用连同RSet记录的外部引用作为扫描存活对象的入口。 第二阶段，更新RSet。 处理dirty card queue(见备注)中的card，更新RSet。此阶段完成后，RSet可以准确的反映老年代对所在的内存分段中对象的引用。 第三阶段，处理RSet。 识别被老年代对象指向的Eden中的对象，这些被指向的Eden中的对象被认为是存活的对象。 第四阶段，复制对象。 此阶段，对象树被遍历，Eden区内存段中存活的对象会被复制到Survivor区中空的内存分段，Survivor区内存段中存活的对象如果年龄未达阈值，年龄会加1，达到阀值会被会被复制到Old区中空的内存分段。如果Survivor空间不够，Eden空间的部分数据会直接晋升到老年代空间。 第五阶段，处理引用。 处理Soft，Weak，Phantom，Final，JNI Weak 等引用。最终Eden空间的数据为空，GC停止工作，而目标内存中的对象都是连续存储的，没有碎片，所以复制过程可以达到内存整理的效果，减少碎片。 G1回收过程二：并发标记过程 \\1. 初始标记阶段：标记从根节点直接可达的对象。这个阶段是STW的，并且会触发一次年轻代GC。 \\2. 根区域扫描（Root Region Scanning）：G1 GC扫描Survivor区直接可达的老年代区域对象，并标记被引用的对象。这一过程必须在young GC之前完成。 3.并发标记(Concurrent Marking)：在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那这个区域会被立即回收。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 \\4. 再次标记(Remark)： 由于应用程序持续进行，需要修正上一次的标记结果。是STW的。G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 \\5. 独占清理(cleanup,STW)：计算各个区域的存活对象和GC回收比例，并进行排序，识别可以混合回收的区域。为下阶段做铺垫。是STW的。 这个阶段并不会实际上去做垃圾的收集 \\6. 并发清理阶段：识别并清理完全空闲的区域。 G1回收过程三：混合回收 当越来越多的对象晋升到老年代old region时，为了避免堆内存被耗尽，虚拟机会触发一个混合的垃圾收集器，即Mixed GC，该算法并不是一个Old GC，除了回收整个Young Region，还会回收一部分的Old Region。这里需要注意：是一部分老年代，而不是全部老年代。可以选择哪些Old Region进行收集，从而可以对垃圾回收的耗时时间进行控制。也要注意的是Mixed GC并不是Full GC。 并发标记结束以后，老年代中百分百为垃圾的内存region被回收了，部分为垃圾的内存region被计算了出来。默认情况下，这些老年代的内存分段会分8次（可以通过-XX:G1MixedGCCountTarget设置）被回收。 混合回收的回收集（Collection Set）包括八分之一的老年代内存分段，Eden区内存分段，Survivor区内存分段。混合回收的算法和年轻代回收的算法完全一样，只是回收集多了老年代的内存分段。具体过程请参考上面的年轻代回收过程。 由于老年代中的内存分段默认分8次回收，G1会优先回收垃圾多的内存分段。垃圾占内存分段比例越高的，越会被先回收。并且有一个阈值会决定内存分段是否被回收，-XX:G1MixedGCLiveThresholdPercent，默认为65%，意思是垃圾占内存分段比例要达到65%才会被回收。如果垃圾占比太低，意味着存活的对象占比高，在复制的时候会花费更多的时间。 混合回收并不一定要进行8次。有一个阈值-XX:G1HeapWastePercent，默认值为10%，意思是允许整个堆内存中有10%的空间被浪费，意味着如果发现可以回收的垃圾占堆内存的比例低于10%，则不再进行混合回收。因为GC会花费很多的时间但是回收到的内存却很少。 G1回收可选的过程四：Full GC G1的初衷就是要避免Full GC的出现。但是如果上述方式不能正常工作，G1会停止应用程序的执行（Stop-The-World），使用单线程的内存回收算法进行垃圾回收，性能会非常差，应用程序停顿时间会很长。 要避免Full GC的发生，一旦发生需要进行调整。什么时候会发生Full GC呢？比如堆内存太小，当G1在复制存活对象的时候没有空的内存分段可用，则会回退到full gc，这种情况可以通过增大内存解决。 导致G1Full GC的原因可能有两个： Evacuation（回收阶段）的时候没有足够的to-space来存放晋升的对象； 并发处理过程完成之前空间耗尽。 G1回收器优化建议 年轻代大小 避免使用-Xmn或-XX:NewRatio等相关选项显式设置年轻代大小 固定年轻代的大小会覆盖暂停时间目标 暂停时间目标不要太过严苛 G1 GC的吞吐量目标是90%的应用程序时间和10%的垃圾回收时间 评估G1 GC的吞吐量时，暂停时间目标不要太严苛。目标太过严苛表示你愿意承受更多的垃圾回收开销，而这些会直接影响到吞吐量。 补充： 从Oracle官方透露出来的信息可获知，回收阶段（Evacuation）其实本也有想过设计成与用户程序一起并发执行，但这件事情做起来比较复杂，考虑到G1只是回收一部分Region，停顿时间是用户可控制的，所以并不迫切去实现，而选择把这个特性放到了G1之后出现的低延迟垃圾收集器（即ZGC）中。另外，还考虑到G1不是仅仅面向低延迟，停顿用户线程能够最大幅度提高垃圾收集效率，为了保证吞吐量所以才选择了完全暂停用户线程的实现方案。 截止JDK 1.8，一共有7款不同的垃圾收集器。每一款不同的垃圾收集器都有不同的特点，在具体使用的时候，需要根据具体的情况选用不同的垃圾收集器。 Java垃圾收集器的配置对于JVM优化来说是一个很重要的选择，选择合适的垃圾收集器可以让JVM的性能有一个很大的提升。 怎么选择垃圾收集器？ 优先调整堆的大小让JVM自适应完成。 如果内存小于100M，使用串行收集器 如果是单核、单机程序，并且没有停顿时间的要求，串行收集器 如果是多CPU、需要高吞吐量、允许停顿时间超过1秒，选择并行收集器 如果是多CPU、追求低停顿时间，需快速响应（比如延迟不能超过1秒，如互联网应用），使用并发收集器 官方推荐G1，性能高。现在互联网的项目，基本都是使用G1。 最后需要明确两个观点： 没有最好的收集器，更没有万能的收集； 调优永远是针对特定场景、特定需求，不存在一劳永逸的收集器 GC新发展 Epsilon GC （http://openjdk.java.net/jeps/318），只做内存分配，不做垃圾回收的GC，对于运行完就退出的程序非常适合。称为无操作的垃圾收集器。 Open JDK12的Shenandoah GC：低停顿时间的GC（实验性） Shenandoah，无疑是众多GC中最孤独的一个。是第一款不由Oracle公司团队领导开发的HotSpot垃圾收集器。不可避免的受到官方的排挤。比如号称OpenJDK和OracleJDK没有区别的Oracle公司仍拒绝在OracleJDK12中支持Shenandoah。 Shenandoah垃圾回收器最初由RedHat进行的一项垃圾收集器研究项目Pauseless GC的实现，旨在针对JVM上的内存回收实现低停顿的需求。在2014年贡献给OpenJDK。 Red Hat研发Shenandoah团队对外宣称，Shenandoah垃圾回收器的暂停时间与堆大小无关，这意味着无论将堆设置为200 MB还是200GB，99.9%的目标都可以把垃圾收集的停顿时间限制在十毫秒以内。不过实际使用性能将取决于实际工作堆的大小和工作负载。 Shenandoah开发团队在实际应用中的测试数据 这是RedHat在2016年发表的论文数据，测试内容是使用ES对200GB的维基百科数据进行索引。从结果看： 停顿时间比其他几款收集器确实有了质的飞跃，但也未实现最大停顿时间控制在十毫秒以内的目标。 而吞吐量方面出现了明显的下降，总运行时间是所有测试收集器里最长的。 总结： Shenandoah GC的弱项：高运行负担下的吞吐量下降。 Shenandoah GC的强项：低延迟时间。 Shenandoah GC的工作过程大致分为九个阶段，这里就不再赘述。在之前Java12新特性视频里有过介绍。 令人震惊、革命性的ZGC ZGC与Shenandoah目标高度相似，在尽可能对吞吐量影响不大的前提下，实现在任意堆内存大小下都可以把垃圾收集的停顿时间限制在十毫秒以内的低延迟。 ZGC：是一款基于Region内存布局的，（暂时）不设分代的，使用了读屏障、染色指针和内存多重映射等技术来实现可并发的标记-压缩算法的，以低延迟为首要目标的一款垃圾收集器。 ZGC的工作过程可以分为4个阶段：并发标记-并发预备重分配-并发重分配-并发重映射等。 ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际时间是非常少的。 测试数据： 在ZGC的强项停顿时间测试上，它毫不留情的将Parallel、G1拉开了两个数量级的差距。无论平均停顿、95%停顿、99%停顿、99.9%停顿，还是最大停顿时间，ZGC都能毫不费劲控制在10毫秒以内。 虽然ZGC还在试验状态，没有完成所有特性，但此时性能已经相当亮眼，用“令人震惊、革命性”来形容，不为过。 未来将在服务端、大内存、低延迟应用的首选垃圾收集器。 jdk14新特性 JEP 364：ZGC应用在macOS上 JEP 365：ZGC应用在Windows上 JDK14之前，ZGC仅Linux才支持。 尽管许多使用ZGC的用户都使用类Linux的环境，但在Windows和macOS上，人们也需要ZGC进行开发部署和测试。许多桌面应用也可以从ZGC中受益。因此，ZGC特性被移植到了Windows和macOS上。 现在mac或Windows上也能使用ZGC了，示例如下： -XX:+UnlockExperimentalVMOptions -XX:+UseZGC AliGC是阿里巴巴JVM团队基于G1算法， 面向大堆(LargeHeap)应用场景。 指定场景下的对比： 当然，其他厂商也提供了各种独具一格的GC实现，例如比较有名的低延迟GC，Zing（https://www.infoq.com/articles/azul_gc_in_detail），有兴趣可以参考提供的链接。 各GC使用场景如何选择？GC新发展4.分析日志JVM面试Java内存区域说一下 JVM 的主要组成部分及其作用？ JVM包含两个子系统和两个组件，两个子系统为Class loader(类装载)、Execution engine(执行引擎)；两个组件为Runtime data area(运行时数据区)、Native Interface(本地接口)。 Class loader(类装载)：根据给定的全限定名类名(如：java.lang.Object)来装载class文件到运行时数据区中的方法区。 Execution engine（执行引擎）：执行字节码中的指令。 Native Interface(本地接口)：与native libraries交互，是与其它编程语言交互的接口。 Runtime data area(运行时数据区域)：这就是我们常说的JVM的内存。 作用 ：首先通过编译器把 Java 代码转换成字节码，类加载器（ClassLoader）再把字节码加载到内存中，将其放在运行时数据区（Runtime data area）的方法区内，而字节码文件只是 JVM 的一套指令集规范，并不能直接交给底层操作系统去执行，因此需要特定的解释器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。 @$说一下 JVM 运行时数据区Java 虚拟机在执行 Java 程序的过程中会把它所管理的内存区域划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁的时间，有些区域随着虚拟机进程的启动而存在，有些区域则是依赖线程的启动和结束而建立和销毁。Java 虚拟机所管理的内存被划分为如下几个区域： 不同虚拟机的运行时数据区可能略微有所不同，但都会遵从 Java 虚拟机规范， Java 虚拟机规范规定的区域分为以下 5 个部分： Java 堆（Java Heap）：Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存； 方法区（Method Area）：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。 Java 虚拟机栈（Java Virtual Machine Stacks）：用于存储局部变量表、操作数栈、动态链接、方法出口等信息； 本地方法栈（Native Method Stack）：与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的； 程序计数器（Program Counter Register）：当前线程所执行的字节码的行号指示器，字节码解释器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成； 深拷贝和浅拷贝浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址 深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存， 使用深拷贝的情况下，不会出现浅拷贝时释放同一个内存的错误。 说一下堆栈的区别？ 堆 栈 物理地址 物理地址分配是不连续的，因此性能慢些。在GC的时候需要考虑到不连续的分配，所以有各种垃圾回收算法。 栈使用的是数据结构中的栈，具有先进后出的规则，物理地址分配是连续的，因此性能快 分配内存时机 堆因为是不连续的，所以分配的内存是在运行期确认的，因此大小不固定。一般堆大小远远大于栈。 栈是连续的，所以分配的内存大小要在编译期就确认，大小是固定的。 存放的内容 堆存放的是对象的实例和数组。此区域更关注的是数据的存储 栈存放：局部变量，操作数栈，返回结果。此区域更关注的是程序方法的执行。 程序的可见度 堆对于整个应用程序都是共享的、可见的。 栈对当前线程是可见的，是线程私有。他的生命周期和线程相同。 HotSpot虚拟机对象探秘@$对象的创建说到对象的创建，首先让我们看看 Java 中提供的几种对象创建方式： Header 解释 使用new关键字 调用了构造函数 使用Class的newInstance方法 调用了构造函数 使用Constructor类的newInstance方法 调用了构造函数 使用clone方法 没有调用构造函数 使用反序列化 没有调用构造函数 下面是对象创建的主要流程： 虚拟机遇到一条new指令时，先检查常量池是否已经加载相应的类，如果没有，必须先执行相应的类加载。类加载通过后，接下来分配内存。若Java堆中内存是绝对规整的，使用“指针碰撞“方式分配内存；如果不是规整的，就从空闲列表中分配，叫做”空闲列表“方式。划分内存时还需要考虑一个问题-并发，也有两种方式：CAS同步处理，或者本地线程分配缓冲(Thread Local Allocation Buffer, TLAB)。然后将分配到的内存空间都初始化为零值，接着是做一些必要的对象设置(元信息、哈希码…)，最后执行&lt;init&gt;方法。 为对象分配内存类加载完成后，接着会在Java堆中划分一块内存分配给对象。内存分配根据Java堆是否规整，有两种方式： 指针碰撞：如果Java堆的内存是规整，即所有用过的内存放在一边，而空闲的的放在另一边。分配内存时将位于中间的指针指示器向空闲的内存移动一段与对象大小相等的距离，这样便完成分配内存工作。 空闲列表：如果Java堆的内存不是规整的，则需要由虚拟机维护一个列表来记录哪些内存是可用的，这样在分配的时候可以从列表中查询到足够大的内存分配给对象，并在分配后更新列表记录。 选择哪种分配方式是由 Java 堆是否规整来决定的，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定。 处理并发安全问题在虚拟机中对象的创建是一个非常频繁的行为，哪怕只是修改一个指针所指向的位置，在并发情况下也是不安全的，可能出现正在给对象 A 分配内存，指针还没来得及修改，对象 B 又同时使用了原来的指针来分配内存的情况。解决这个问题有两种方案： 对内存分配的动作进行同步处理（采用 CAS + 失败重试来保障更新操作的原子性）； 把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在 Java 堆中预先分配一小块内存，称为本地线程分配缓冲（Thread Local Allocation Buffer, TLAB）。哪个线程要分配内存，就在哪个线程的 TLAB 上分配。只有 TLAB 用完并分配新的 TLAB 时，才需要同步锁。通过-XX:+UseTLAB参数来设定虚拟机是否使用TLAB。 对象的访问定位Java程序需要通过 JVM 栈上的引用访问堆中的具体对象。对象的访问方式取决于 JVM 虚拟机的实现。目前主流的访问方式有 句柄 和 直接指针 两种方式。 指针： 一种内存地址，代表一个对象在内存中的地址。 句柄： 可以理解为指向指针的指针，维护着对象的指针。句柄不直接指向对象，而是指向对象的指针（句柄不发生变化，指向固定内存地址），再由对象的指针指向对象的真实内存地址。 句柄访问Java堆中划分出一块内存来作为句柄池，引用中存储对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息，具体构造如下图所示： 优势：引用中存储的是稳定的句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而引用本身不需要修改。 直接指针如果使用直接指针访问，引用 中存储的直接就是对象地址。 优势：速度更快，节省了一次指针定位的时间开销。由于对象的访问在Java中非常频繁，因此这类开销积少成多后也是非常可观的执行成本。HotSpot 中采用的就是这种方式。 内存溢出异常@$Java会存在内存泄漏吗？请简单描述内存泄漏是指不再被使用的对象或者变量一直存在于内存中。理论上来说，Java是有GC垃圾回收机制的，也就是说，不再被使用的对象，会被GC自动回收掉，自动从内存中清除。 但是，即使这样，Java也还是存在着内存泄漏的情况，Java导致内存泄露的原因很明确：长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是Java中内存泄露的发生场景。 垃圾收集器简述Java垃圾回收机制，GC是什么？垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？Java垃圾回收机制：GC 是垃圾收集的意思（Gabage Collection），在java中，程序员是不需要显示的去释放一个对象的内存的，而是由虚拟机自行执行，这就是垃圾回收机制，垃圾回收机制有效的防止了内存泄露 垃圾回收器的基本原理：在JVM中，有一个垃圾回收线程，它是低优先级的，在正常情况下是不会执行的，只有在虚拟机空闲或者当前堆内存不足时，才会触发执行，扫描那些没有被引用的对象，并将它们添加到要回收的集合中，进行回收。 垃圾回收器可以马上回收内存吗？：程序员不能实时的对某个对象或所有对象调用垃圾回收器进行垃圾回收。但可以手动执行System.gc()主动通知虚拟机进行垃圾回收，但是Java语言规范并不能保证GC一定会执行。 Java 中都有哪些引用类型？强引用、软引用、弱引用、幻象引用有什么区别？具体使用场景是什么？在Java语言中，除了基本数据类型外，其他的都是指向各类对象的对象引用，根据其生命周期的长短，将引用分为4类。 不同的引用类型，主要体现的是对象不同的可达性状态和对垃圾收集的影响。 强引用：最常见的普通对象引用，通过关键字new创建的对象所关联的引用就是强引用，发生 gc 的时候不会被回收。 软引用：软引用的生命周期比强引用短一些。有用但不是必须的对象，在发生内存溢出之前会被回收。应用场景：软引用通常用来实现内存敏感的缓存。如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 弱引用：弱引用的生命周期比软引用短。有用但不是必须的对象，在下一次GC时会被回收。应用场景：弱应用同样可用于内存敏感的缓存。 虚引用（幽灵引用&#x2F;幻象引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用。应用场景：虚引用的用途是在这个对象被 gc 时返回一个系统通知。 怎么判断对象是否可以被回收？在Java中，对象什么时候可以被垃圾回收垃圾收集器在做垃圾回收的时候，首先需要判定的就是哪些内存是需要被回收的，哪些对象是「存活」的，是不可以被回收的；哪些对象已经「死掉」了，需要被回收。 一般有两种方法来判断： 引用计数器法：为每个对象创建一个引用计数器，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题； 可达性分析算法：当一个对象到GC Roots不可达时，在下一个垃圾回收周期中尝试回收该对象。定义一系列的 GC ROOT 为起点。从起点开始向下开始搜索，搜索走过的路径称为引用链。当一个对象到 GC ROOT没有任何引用链相连的话，则对象可以判定是可以被回收的。 可达性分析算法详答 当不能从GC Root寻找一条路径到达该对象时，将进行第一次标记。 第一次标记后检查对象是否重写了finalize() 和是否已经被调用了finalize()方法。若没有重写finalize()方法或已经被调用，则进行回收。 在已经重写finalize()方法且未调用的情况下，将对象加入一个F-Queue 的队列中，稍后进行第二次检查 在第二次标记之前，对象如果执行finalize()方法并完成自救，对象则不会被回收。否则完成第二次标记，进行回收。值得注意的是finalize()方法并不可靠。 虚拟机默认采用的是可达性分析算法。 可以作为 GC ROOT 的对象包括： 栈中引用的对象； 静态变量、常量引用的对象； 本地方法栈 native 方法引用的对象。 JVM中的永久代中会发生垃圾回收吗垃圾回收一般不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。通过查看垃圾收集器的输出信息，就会发现永久代也是被回收的。所以正确的设置永久代大小可以有效避免Full GC。 Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区，现在大多数的类元数据分配在本地化内存中。 @$说一下 JVM 有哪些垃圾回收算法？ 标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。 复制算法：按照容量划分两个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的那块内存空间清理掉。缺点：内存使用率不高，只有原来的一半。 标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后清除掉端边界以外的内存。 分代收集算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代采用复制算法，老年代采用标记整理算法。 标记-清除算法标记无用对象，然后进行清除回收。 标记-清除算法（Mark-Sweep）是一种常见的基础垃圾收集算法，它将垃圾收集分为两个阶段： 标记阶段：标记出可以回收的对象。 清除阶段：回收被标记的对象所占用的空间。 标记-清除算法之所以是基础的，是因为后面讲到的垃圾收集算法都是在此算法的基础上进行改进的。 优点：实现简单，不需要对象进行移动。 缺点：由于标记的过程需要遍历所有的 GC ROOT，清除的过程也要遍历堆中所有的对象，标记、清除过程效率低，产生大量不连续的内存碎片，提高了垃圾回收的频率。 标记-清除算法的执行的过程如下图所示 复制算法为了解决标记-清除算法的效率不高的问题，产生了复制算法。把内存空间划为两个相等的区域，每次只使用其中一个区域。垃圾收集时，遍历当前使用的区域，把存活对象复制到另外一个区域中，最后将已使用的内存空间一次清理掉。 优点：按顺序分配内存即可，实现简单、运行高效，不用考虑内存碎片。 缺点：可用的内存大小缩小为原来的一半，对象存活率高时会频繁进行复制。 复制算法的执行过程如下图所示 标记-整理算法在新生代中可以使用复制算法，但是在老年代就不能选择复制算法了，因为老年代的对象存活率会较高，这样会有较多的复制操作，导致效率变低。标记-清除算法可以应用在老年代中，但是它效率不高，在内存回收后容易产生大量内存碎片。因此就出现了一种标记-整理算法（Mark-Compact）算法，与标记-清除算法不同的是，在标记可回收的对象后将所有存活的对象压缩到内存的一端，使他们紧凑的排列在一起，然后对端边界以外的内存进行回收。回收后，已用和未用的内存都各自一边。 优点：解决了标记-清除算法存在的内存碎片问题。 缺点：需要进行局部对象移动，一定程度上降低了效率。 标记-整理算法的执行过程如下图所示 分代收集算法当前商业虚拟机都采用分代收集的垃圾收集算法。分代收集算法，顾名思义是根据对象的存活周期将内存划分为几块。一般包括新生代、老年代，新生代采用复制算法，老年代采用标记-整理算法，注：Java8中已经移除了永久代，添加了元数据区。如图所示： 垃圾收集算法小结 @$说一下 JVM 有哪些垃圾回收器？如果说垃圾收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。下图展示了7种作用于不同分代的收集器，其中用于回收新生代的收集器包括Serial、ParNew、Parallel Scavenge，回收老年代的收集器包括Serial Old、Parallel Old、CMS，还有用于回收整个Java堆的G1收集器。不同收集器之间的连线表示它们可以搭配使用。 Serial收集器(复制算法)：新生代单线程收集器，标记和清理都是单线程，优点是简单高效； ParNew收集器 (复制算法)：新生代并行收集器，实际上是Serial收集器的多线程版本，在多核CPU环境下有着比Serial更好的表现； Parallel Scavenge收集器 (复制算法)：新生代并行收集器，追求高吞吐量，高效利用 CPU。吞吐量 &#x3D; 用户线程时间&#x2F;(用户线程时间+GC线程时间)，高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，适合后台应用等对交互相应要求不高的场景； Serial Old收集器 (标记-整理算法)：老年代单线程收集器，Serial收集器的老年代版本； Parallel Old收集器 (标记-整理算法)：老年代并行收集器，吞吐量优先，Parallel Scavenge收集器的老年代版本； CMS(Concurrent Mark Sweep)收集器(标记-清除算法)：老年代并行收集器，追求最短GC回收停顿时间，具有高并发、低停顿的特点； G1(Garbage First)收集器 (标记-整理算法)：Java堆并行收集器，G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。此外，G1收集器不同于之前的收集器的一个重要特点是：G1回收的范围是整个Java堆(包括新生代，老年代)，而前六种收集器回收的范围仅限于新生代或老年代。 新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？ 新生代回收器：Serial、ParNew、Parallel Scavenge 老年代回收器：Serial Old、Parallel Old、CMS 整堆回收器：G1 新生代垃圾回收器一般采用的是复制算法 老年代垃圾回收器一般采用的是标记-整理的算法 详细介绍一下 CMS 垃圾回收器？CMS 是英文 Concurrent Mark-Sweep 的简称，是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。 在启动 JVM 的参数加上“-XX:+UseConcMarkSweepGC”来指定使用 CMS 垃圾回收器。 CMS 使用的是标记-清除的算法实现的，所以在 gc 的时候会产生大量的内存碎片，当剩余内存不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时 CMS 会采用 Serial Old 回收器进行垃圾清除，此时的性能将会被降低。 CMS 收集器是以获取最短停顿时间为目标的收集器。相对于其他的收集器 STW 的时间更短暂，可以并行收集是它的特点，同时它基于标记-清除算法。整个 GC 过程分为4步： 初始标记：标记 GC ROOT 能关联到的对象，需要 STW； 并发标记：从 GCRoots 的直接关联对象开始遍历整个对象图的过程，不需要 STW； 重新标记：为了修正并发标记期间，因用户程序继续运作而导致标记产生改变的标记，需要 STW； 并发清除：清理删除掉标记阶段判断的已经死亡的对象，不需要 STW。 从整个过程来看，并发标记和并发清除的耗时最长，但是不需要停止用户线程。而初始标记和重新标记的耗时较短，但是需要停止用户线程。总体而言，整个过程造成的停顿时间较短，大部分时候是可以和用户线程一起工作的。 G1垃圾回收器的原理了解吗？ G1 作为 JDK9 之后的服务端默认收集器，不再区分年轻代和老年代进行垃圾回收。 把内存划分为多个 Region，每个 Region 的大小可以通过 -XX:G1HeapRegionSize 设置，大小为1~32M。 对于大对象的存储则衍生出 Humongous 的概念。超过 Region 大小一半的对象会被认为是大对象，而超过整个 Region 大小的对象被认为是超级大对象，将会被存储在连续的 N 个 Humongous Region 中。 G1 在进行回收的时候会在后台维护一个优先级列表，每次根据用户设定允许的收集停顿时间优先回收收益最大的 Region。 G1 的回收过程分为以下四个步骤： 初始标记：标记 GC ROOT 能关联到的对象，需要 STW； 并发标记：从 GCRoots 的直接关联对象开始遍历整个对象图的过程，扫描完成后还会重新处理并发标记过程中产生变动的对象； 最终标记：短暂暂停用户线程，再处理一次，需要 STW； 筛选回收：更新 Region 的统计数据，对每个 Region 的回收价值和成本排序，根据用户设置的停顿时间制定回收计划。再把Region 中存活对象复制到空的 Region，同时清理旧的 Region。需要 STW。 总的来说除了并发标记之外，其他几个过程也还是需要短暂的 STW。G1 的目标是在停顿和延迟可控的情况下尽可能提高吞吐量。 简述分代垃圾回收器是怎么工作的？根据对象的存活周期将堆内存划分：老年代和新生代，新生代默认的空间占比总空间的 1&#x2F;3，老年代的默认占比是 2&#x2F;3。 新生代使用的是复制算法，新生代里有 3 个分区：Eden、From Survivor、To Survivor，它们的默认占比是 8:1:1，它的执行流程如下： 把 Eden + From Survivor 存活的对象放入 To Survivor 区； 清空 Eden 和 From Survivor 分区； From Survivor 和 To Survivor 分区交换，From Survivor 变 To Survivor，To Survivor 变 From Survivor。 每次在 From Survivor 到 To Survivor 移动存活的对象，年龄就 +1，当年龄到达 15（默认配置是 15）时，升级为老年代。大对象会直接进入老年代。 老年代当空间占用到达某个值之后就会触发全局垃圾回收，一般使用标记整理算法。 以上这些循环往复就构成了整个分代垃圾回收的整体执行流程。 内存分配策略@$简述java内存分配与回收策略以及Minor GC和Major GC所谓自动内存管理，最终要解决的也就是内存分配和内存回收两个问题。前面我们介绍了内存回收，这里我们再来聊聊内存分配。 对象的内存分配通常是在 Java 堆上分配（随着虚拟机优化技术的诞生，某些场景下也会在栈上分配，后面会详细介绍），对象主要分配在新生代的 Eden 区，如果启动了本地线程缓冲，则线程优先在 TLAB 上分配。少数情况下也会直接在老年代上分配。总的来说分配规则不是百分百固定的，其细节取决于哪一种垃圾收集器组合以及虚拟机相关参数有关，但是虚拟机对于内存的分配还是会遵循以下几种「普世」规则： 对象优先在 Eden 区分配多数情况，对象都在新生代 Eden 区分配。当 Eden 区没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC。如果本次 GC 后还是没有足够的空间，则将启用分配担保机制在老年代中分配内存。 这里我们提到 Minor GC，如果你仔细观察过 GC 日常，通常我们还能从日志中发现 Major GC&#x2F;Full GC。 Minor GC 是指发生在新生代的 GC，因为 Java 对象大多都是朝生夕死，所以 Minor GC 非常频繁，一般回收速度也非常快； Major GC&#x2F;Full GC 是指发生在老年代的 GC，出现了 Major GC 通常会伴随至少一次 Minor GC。Major GC 的速度通常会比 Minor GC 慢 10 倍以上。 大对象直接进入老年代所谓大对象是指需要大量连续内存空间的对象，频繁出现大对象是致命的，会导致在内存还有不少空间的情况下提前触发 GC，以获取足够的连续空间来安置新对象。 新生代使用的是复制算法，如果大对象直接在新生代分配，就会导致 Eden 区和两个 Survivor 区之间发生大量的内存复制，因此对于大对象都会直接在老年代进行分配。 长期存活对象将进入老年代虚拟机采用分代收集的思想来管理内存，会给每个对象定义了一个对象年龄的计数器，对象在 Eden 区出生，经过一次 Minor GC 对象年龄就会加 1，当年龄达到一定程度（默认 15） 就会被晋升到老年代，也就是长期存活对象将进入老年代。 虚拟机类加载机制简述Java类加载机制？类加载机制的原理Java中的所有类，都需要由类加载器装载到JVM中才能运行，同时对数据进行验证，准备，解析和初始化，最终形成可以被虚拟机直接使用的类型。 类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的需求，像是反射，就需要显式的加载所需要的类。 类装载方式，有两种 ： 隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中 显式装载， 通过class.forname()等方法，显式加载需要的类 Java中类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载，这是为了节省内存开销。 什么是类加载器，类加载器有哪些?类加载器负责将字节码文件的类加载到虚拟机内存中。 主要有一下四种类加载器: 启动类加载器（Bootstrap ClassLoader）：用来加载Java核心类库，无法被Java程序直接引用。即用来加载JAVA_HOME&#x2F;jre&#x2F;lib目录中的，或者被 -Xbootclasspath 参数所指定的路径中并且被虚拟机识别的类库； 扩展类加载器（Extension ClassLoader）：用来加载 Java 的扩展库。负责加载JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;ext目录或-Djava.ext.dir系统变量指定的路径中的所有类库； 应用程序类加载器（Application ClassLoader）：用来加载用户类路径（classpath）上的指定类库，我们可以直接使用这个类加载器。一般情况，如果我们没有自定义类加载器，默认就是用这个加载器。 用户自定义类加载器：通过继承 java.lang.ClassLoader类的方式实现用户自定义类加载器。 说一下类装载的执行过程？ 类装载分为以下 5 个步骤： 加载：根据查找路径找到相应的 class 文件然后导入； 验证：检查加载的 class 文件的正确性； 准备：给类中的静态变量分配内存空间； 解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用可以理解为一个标识，而直接引用直接指向内存中的地址； 初始化：对静态变量和静态代码块执行初始化工作。 @$什么是双亲委派模型？双亲委派模型工作流程是怎样的？双亲委派模型的好处是什么？在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。 双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去加载，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载器无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载此类。 自下而上检查类是否已经被加载，自上而下尝试加载类 双亲委派模型工作流程： 当Application ClassLoader 收到一个类加载请求时，他首先不会自己去尝试加载这个类，而是将这个请求委派给父类加载器Extension ClassLoader去完成。 当Extension ClassLoader收到一个类加载请求时，他首先也不会自己去尝试加载这个类，而是将请求委派给父类加载器Bootstrap ClassLoader去完成。 Bootstrap ClassLoader尝试加载此类，如果Bootstrap ClassLoader加载失败，就会让Extension ClassLoader尝试加载。 Extension ClassLoader尝试加载此类，如果Extension ClassLoader也加载失败，就会让Application ClassLoader尝试加载。 Application ClassLoader尝试加载此类，如果Application ClassLoader也加载失败，就会让自定义加载器尝试加载。 如果均加载失败，就会抛出ClassNotFoundException异常。 双亲委派模型的好处：保证核心类库不被覆盖。如果没有使用双亲委派模型，由各个类加载器自行加载的话，如果用户自己编写了一个称为java.lang.Object的类，并放在程序的ClassPath中，那系统将会出现多个不同的Object类， Java类型体系中最基础的行为就无法保证，应用程序也将会变得一片混乱。 JVM调优说一下 JVM 调优的工具？JDK 自带了很多监控工具，都位于 JDK 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款可视化监控工具。 jconsole：JDK 自带的可视化管理工具，用于对 JVM 中的内存、线程和类等进行监控，对垃圾回收算法有很详细的跟踪，功能简单； jvisualvm：JDK 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等，功能强大。 常用的故障检测，监视，修理工具 工具名称 主要作用 jps (JVM Process Status Tool) 显示系统中所有的虚拟机进程 jstat (JVM Statistics Monitoring Tool) 收集虚拟机各方面的运行数据 jinfo (Configuration Info for Java) 显示虚拟机配置信息 jmap (Memory Map for Java) 生成虚拟机的内存转储快照 jhat (JVM Heap Dump Browser) 分析堆内存转储快照，不推荐使用，消耗资源而且慢 jstack (Stack Trace for Java） 显示线程堆栈快照 谈谈你的GC调优思路?谈到调优，这一定是针对特定场景、特定目的的事情， 对于 GC 调优来说，首先就需要清楚调优的目标是什么？从性能的角度看，通常关注三个方面，内存占用（footprint）、延时（latency）和吞吐量（throughput） 基本的调优思路可以总结为： 理解应用需求和问题，确定调优目标。假设，我们开发了一个应用服务，但发现偶尔会出现性能抖动，出现较长的服务停顿。评估用户可接受的响应时间和业务量，将目标简化为，希望 GC 暂停尽量控制在 200ms 以内，并且保证一定标准的吞吐量。 掌握 JVM 和 GC 的状态，定位具体问题，确定是否有 GC 调优的必要。具体有很多方法，比如，通过 jstat 等工具查看 GC 等相关状态，可以开启 GC 日志，或者是利用操作系统提供的诊断工具等。例如，通过追踪 GC 日志，就可以查找是不是 GC 在特定时间发生了长时间的暂停，进而导致了应用响应不及时。 接着需要思考选择的 GC 类型是否符合我们的应用特征，具体问题表现在哪里。是 Minor GC 过长，还是 Mixed GC 等出现异常停顿情况；如果不是，考虑切换到什么类型，如 CMS 和 G1 都是更侧重于低延迟的 GC 选项。 通过分析确定具体调整的参数或者软硬件配置。 验证是否达到调优目标，如果达到目标，即可以考虑结束调优；否则，重复进行分析、调整、验证。 @$常用的 JVM 调优的参数都有哪些？1、性能调优要做到有的放矢，根据实际业务系统的特点，以一定时间的JVM日志记录为依据，进行有针对性的调整、比较和观察。 2、性能调优是个无止境的过程，要综合权衡调优成本和更换硬件成本的大小，使用最经济的手段达到最好的效果。 3、性能调优不仅仅包括JVM的调优，还有服务器硬件配置、操作系统参数、中间件线程池、数据库连接池、数据库本身参数以及具体的数据库表、索引、分区等的调整和优化。 4、通过特定工具检查代码中存在的性能问题并加以修正是一种比较经济快捷的调优方法。 常用的 JVM 调优的参数 -Xms2g：初始化堆大小为 2g； -Xmx2g：堆最大内存为 2g； -Xmn1g：新生代内存大小为1g；-XX:NewSize 新生代大小，-XX:MaxNewSize 新生代最大值，-Xmn 则是相当于同时配置 -XX:NewSize 和 -XX:MaxNewSize 为一样的值； -XX:NewRatio&#x3D;2：设置新生代的和老年代的内存比例为 1:2，即新生代占堆内存的1&#x2F;3，老年代占堆内存的2&#x2F;3； -XX:SurvivorRatio&#x3D;8：设置新生代 Eden 和 两个Survivor 比例为 8:1:1； –XX:+UseParNewGC：对新生代使用并行垃圾回收器。 -XX:+UseParallelOldGC：对老年代并行垃圾回收器。 -XX:+UseConcMarkSweepGC：以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。 -XX:+PrintGC：开启打印 gc 信息； -XX:+PrintGCDetails：打印 gc 详细信息。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://gouguoqiang.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"}]},{"title":"商品搜索模块","slug":"畅购商城/商品搜索","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:01:26.777Z","comments":true,"path":"2022/09/01/畅购商城/商品搜索/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E5%95%86%E5%93%81%E6%90%9C%E7%B4%A2/","excerpt":"","text":"ES+IK分词器+Kibana环境配置 4. 数据导入ES4.1 SpringData Elasticsearch介绍4.1.1 SpringData介绍Spring Data是一个用于简化数据库访问，并支持云服务的开源框架。其主要目标是使得对数据的访问变得方便快捷，并支持map-reduce框架和云计算数据服务。 Spring Data可以极大的简化JPA的写法，可以在几乎不用写实现的情况下，实现对数据的访问和操作。除了CRUD外，还包括如分页、排序等一些常用的功能。 Spring Data的官网：http://projects.spring.io/spring-data/ 4.1.2 SpringData ES介绍Spring Data ElasticSearch 基于 spring data API 简化 elasticSearch操作，将原始操作elasticSearch的客户端API 进行封装 。Spring Data为Elasticsearch项目提供集成搜索引擎。Spring Data Elasticsearch POJO的关键功能区域为中心的模型与Elastichsearch交互文档和轻松地编写一个存储库数据访问层。 官方网站：http://projects.spring.io/spring-data-elasticsearch/ 4.3 数据导入现在需要将数据从数据库中查询出来，然后将数据导入到ES中。 数据导入流程如下： 1231.请求search服务,调用数据导入地址2.根据注册中心中的注册的goods服务的地址，使用Feign方式查询所有已经审核的Sku3.使用SpringData Es将查询到的Sku集合导入到ES中 1.实现过程 创建SKUjavaBean SkuInfo 2.创建feign 查询sku 数据量大建议分页查询 3.调用 feign查询 sku集合 转换为 实体集合 4.导入到ES control -&gt; service -&gt;dao(继承ElasticsearchRepository) 4.3.1 文档映射Bean创建搜索商品的时候，会根据如下属性搜索数据,并且不是所有的属性都需要分词搜索，我们创建JavaBean，将JavaBean数据存入到ES中要以搜索条件和搜索展示结果为依据，部分关键搜索条件分析如下： 123451.可能会根据商品名称搜素，而且可以搜索商品名称中的任意一个词语，所以需要分词2.可能会根据商品分类搜索，商品分类不需要分词3.可能会根据商品品牌搜索，商品品牌不需要分词4.可能会根据商品商家搜索，商品商家不需要分词5.可能根据规格进行搜索，规格时一个键值对结构，用Map 根据上面的分析，我们可以在changgou-service-search-api工程中创建com.changgou.search.pojo.SkuInfo，如下 索引（index） ElasticSearch存储数据的地方，可以理解成关系型数据库中的数据库概念。 映射（mapping） mapping定义了每个字段的类型、字段所使用的分词器等。相当于关系型数据库中的表结构。 文档（document） Elasticsearch中的最小数据单元，常以json格式显示。一个document相当于关系型数据库中的一行数据。 倒排索引 一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，对应一个包含它的文档id列表。 类型（type） 一种type就像一类表。如用户表、角色表等。在Elasticsearch7.X默认type为_doc - ES 5.x中一个index可以有多种type。 - ES 6.x中一个index只能有一种type。 - ES 7.x以后，将逐步移除type这个概念，现在的操作已经不再使用，默认_doc 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Document(indexName = &quot;skuinfo&quot;,type = &quot;docs&quot;) //索引库 类似scheme public class SkuInfo implements Serializable &#123; //商品id，同时也是商品编号 @Id private Long id; //SKU名称 /** * FieldType.Text 类型 txt 支持分词 * 创建索引的分词器 * index = true 添加数据是否分词 默认分 * store = false 是否存储 * 不写field 也会自动 只是 */ @Field(type = FieldType.Text, analyzer = &quot;ik_smart&quot;) private String name; //商品价格，单位为：元 @Field(type = FieldType.Double) private Long price; //库存数量 private Integer num; //商品图片 private String image; //商品状态，1-正常，2-下架，3-删除 private String status; //创建时间 private Date createTime; //更新时间 private Date updateTime; //是否默认 private String isDefault; //SPUID private Long spuId; //类目ID private Long categoryId; //类目名称 @Field(type = FieldType.Keyword) private String categoryName; //品牌名称 // keyword 不分词 分类,品牌等 不需要分词 @Field(type = FieldType.Keyword) private String brandName; //规格 private String spec; //规格参数 private Map&lt;String,Object&gt; specMap; //...略 &#125; 4.3.2 搜索审核通过Sku修改changgou-service-goods微服务，添加搜索审核通过的Sku，供search微服务调用。下面都是针对goods微服务的操作。 修改SkuService接口，添加根据状态查询Sku方法，代码如下：","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"商品发布模块","slug":"畅购商城/商品发布","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:01:09.768Z","comments":true,"path":"2022/09/01/畅购商城/商品发布/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E5%95%86%E5%93%81%E5%8F%91%E5%B8%83/","excerpt":"","text":"1 商品表设计tb_sku 库存单位 123456789101112131415161718192021222324252627DROP TABLE IF EXISTS `tb_sku`;CREATE TABLE `tb_sku` ( `id` varchar(20) NOT NULL COMMENT &#x27;商品id&#x27;, `sn` varchar(100) NOT NULL COMMENT &#x27;商品条码&#x27;, `name` varchar(200) NOT NULL COMMENT &#x27;SKU名称&#x27;, `price` int(20) NOT NULL COMMENT &#x27;价格（分）&#x27;, `num` int(10) NOT NULL COMMENT &#x27;库存数量&#x27;, `alert_num` int(11) DEFAULT NULL COMMENT &#x27;库存预警数量&#x27;, `image` varchar(200) DEFAULT NULL COMMENT &#x27;商品图片&#x27;, `images` varchar(2000) DEFAULT NULL COMMENT &#x27;商品图片列表&#x27;, `weight` int(11) DEFAULT NULL COMMENT &#x27;重量（克）&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `update_time` datetime DEFAULT NULL COMMENT &#x27;更新时间&#x27;, `spu_id` varchar(20) DEFAULT NULL COMMENT &#x27;SPUID&#x27;, `category_id` int(10) DEFAULT NULL COMMENT &#x27;类目ID&#x27;, `category_name` varchar(200) DEFAULT NULL COMMENT &#x27;类目名称&#x27;, `brand_name` varchar(100) DEFAULT NULL COMMENT &#x27;品牌名称&#x27;, `spec` varchar(200) DEFAULT NULL COMMENT &#x27;规格&#x27;, `sale_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;销量&#x27;, `comment_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;评论数&#x27;, `status` char(1) DEFAULT &#x27;1&#x27; COMMENT &#x27;商品状态 1-正常，2-下架，3-删除&#x27;, `version` int(255) DEFAULT &#x27;1&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `cid` (`category_id`) USING BTREE, KEY `status` (`status`) USING BTREE, KEY `updated` (`update_time`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC COMMENT=&#x27;商品表&#x27;; PRIMARY KEY (id) USING BTREE,KEY cid (category_id) USING BTREE,KEY status (status) USING BTREE,KEY updated (update_time) USING BTREE 123456789INSERT INTO `tb_sku` VALUES (&#x27;100000003145&#x27;, &#x27;&#x27;, &#x27;vivo X23 8GB+128GB 幻夜蓝 水滴屏全面屏 游戏手机 移动联通电信全网通4G手机&#x27;, &#x27;95900&#x27;, &#x27;9980&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/4612/28/6223/298257/5ba22d66Ef665222f/d97ed0b25cbe8c6e.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/4612/28/6223/298257/5ba22d66Ef665222f/d97ed0b25cbe8c6e.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;889527500&#x27;, &#x27;0&#x27;, &#x27;手机&#x27;, &#x27;vivo&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;红色\\&#x27;, \\&#x27;版本\\&#x27;: \\&#x27;8GB+128GB\\&#x27;&#125;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000004580&#x27;, &#x27;&#x27;, &#x27;薇妮(viney)女士单肩包 时尚牛皮女包百搭斜挎包女士手提大包(经典黑)&#x27;, &#x27;87900&#x27;, &#x27;10000&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t5590/64/5811657380/234462/5398e856/5965e173N34179777.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t5590/64/5811657380/234462/5398e856/5965e173N34179777.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;451277100&#x27;, &#x27;0&#x27;, &#x27;真皮包&#x27;, &#x27;viney&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;黑色\\&#x27;&#125;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000006163&#x27;, &#x27;&#x27;, &#x27;巴布豆(BOBDOG)柔薄悦动婴儿拉拉裤XXL码80片(15kg以上)&#x27;, &#x27;1&#x27;, &#x27;9995&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t23998/350/2363990466/222391/a6e9581d/5b7cba5bN0c18fb4f.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t23998/350/2363990466/222391/a6e9581d/5b7cba5bN0c18fb4f.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;10000000616300&#x27;, &#x27;757&#x27;, &#x27;拉拉裤&#x27;, &#x27;巴布豆&#x27;, &#x27;&#123;&#125;&#x27;, &#x27;5&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000011113&#x27;, &#x27;&#x27;, &#x27;莎米特SUMMIT拉杆箱22英寸PC材质万向轮旅行箱行李箱PC154T4A可扩容 法拉利红&#x27;, &#x27;78900&#x27;, &#x27;10000&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/25363/12/2929/274060/5c21df3aE1789bda7/030af31afd116ae0.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/25363/12/2929/274060/5c21df3aE1789bda7/030af31afd116ae0.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;595502400&#x27;, &#x27;0&#x27;, &#x27;拉杆箱&#x27;, &#x27;莎米特&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;红色\\&#x27;&#125;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000011127&#x27;, &#x27;&#x27;, &#x27;莎米特SUMMIT拉杆箱22英寸PC材质万向轮旅行箱行李箱PC154T4A可扩容 米白&#x27;, &#x27;26600&#x27;, &#x27;10000&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/25363/12/2929/274060/5c21df3aE1789bda7/030af31afd116ae0.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t1/25363/12/2929/274060/5c21df3aE1789bda7/030af31afd116ae0.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;595502400&#x27;, &#x27;0&#x27;, &#x27;拉杆箱&#x27;, &#x27;莎米特&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;米色\\&#x27;&#125;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000015158&#x27;, &#x27;&#x27;, &#x27;华为 HUAWEI 麦芒7 6G+64G 魅海蓝 全网通 前置智慧双摄 移动联通电信4G手机 双卡双待&#x27;, &#x27;40800&#x27;, &#x27;9996&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t22642/312/2563982615/103706/1398b13d/5b865bb3N0409f0d0.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t22642/312/2563982615/103706/1398b13d/5b865bb3N0409f0d0.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;10000001516600&#x27;, &#x27;0&#x27;, &#x27;手机&#x27;, &#x27;华为&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;蓝色\\&#x27;&#125;&#x27;, &#x27;4&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000015166&#x27;, &#x27;&#x27;, &#x27;华为 HUAWEI 麦芒7 6G+64G 亮黑色 全网通 前置智慧双摄 移动联通电信4G手机 双卡双待&#x27;, &#x27;89200&#x27;, &#x27;9930&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t22642/312/2563982615/103706/1398b13d/5b865bb3N0409f0d0.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t22642/312/2563982615/103706/1398b13d/5b865bb3N0409f0d0.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;10000001516600&#x27;, &#x27;0&#x27;, &#x27;手机&#x27;, &#x27;华为&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;黑色\\&#x27;&#125;&#x27;, &#x27;70&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;);INSERT INTO `tb_sku` VALUES (&#x27;100000022652&#x27;, &#x27;&#x27;, &#x27;vivo Y81s 刘海全面屏 3GB+32GB 香槟金 移动联通电信4G手机&#x27;, &#x27;11700&#x27;, &#x27;10000&#x27;, &#x27;100&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t20707/78/2349564629/130172/50a245d8/5b8e00e2Nf0bcd624.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s720x720_jfs/t20707/78/2349564629/130172/50a245d8/5b8e00e2Nf0bcd624.jpg!q70.jpg.webp&#x27;, &#x27;10&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;2019-05-01 00:00:00&#x27;, &#x27;10000015453200&#x27;, &#x27;0&#x27;, &#x27;手机&#x27;, &#x27;vivo&#x27;, &#x27;&#123;\\&#x27;颜色\\&#x27;: \\&#x27;槟色\\&#x27;&#125;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;); tb_spu描述单位 抽取公共部分 12345678910111213141516171819202122232425262728293031323334DROP TABLE IF EXISTS `tb_spu`;CREATE TABLE `tb_spu` ( `id` varchar(20) NOT NULL COMMENT &#x27;主键&#x27;, `sn` varchar(60) DEFAULT NULL COMMENT &#x27;货号&#x27;, `name` varchar(100) DEFAULT NULL COMMENT &#x27;SPU名&#x27;, `caption` varchar(100) DEFAULT NULL COMMENT &#x27;副标题&#x27;, `brand_id` int(11) DEFAULT NULL COMMENT &#x27;品牌ID&#x27;, `category1_id` int(20) DEFAULT NULL COMMENT &#x27;一级分类&#x27;, `category2_id` int(10) DEFAULT NULL COMMENT &#x27;二级分类&#x27;, `category3_id` int(10) DEFAULT NULL COMMENT &#x27;三级分类&#x27;, `template_id` int(20) DEFAULT NULL COMMENT &#x27;模板ID&#x27;, `freight_id` int(11) DEFAULT NULL COMMENT &#x27;运费模板id&#x27;, `image` varchar(200) DEFAULT NULL COMMENT &#x27;图片&#x27;, `images` varchar(2000) DEFAULT NULL COMMENT &#x27;图片列表&#x27;, `sale_service` varchar(50) DEFAULT NULL COMMENT &#x27;售后服务&#x27;, `introduction` text COMMENT &#x27;介绍&#x27;, `spec_items` varchar(3000) DEFAULT NULL COMMENT &#x27;规格列表&#x27;, `para_items` varchar(3000) DEFAULT NULL COMMENT &#x27;参数列表&#x27;, `sale_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;销量&#x27;, `comment_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;评论数&#x27;, `is_marketable` char(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;是否上架&#x27;, `is_enable_spec` char(1) DEFAULT &#x27;1&#x27; COMMENT &#x27;是否启用规格&#x27;, `is_delete` char(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;是否删除&#x27;, `status` char(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;审核状态&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_spuINSERT INTO `tb_spu` VALUES (&#x27;10000004750400&#x27;, &#x27;&#x27;, &#x27;荣耀8X Max 7.12英寸90%屏占比珍珠屏 4GB+64GB &#123;颜色&#125; 移动联通电信4G全面屏手机 双卡双待&#x27;, &#x27;领券立减100，成交价1399！新一代7.12寸珍珠屏，霸屏实力！荣耀爆品特惠，选品质，购荣耀~&#x27;, &#x27;8557&#x27;, &#x27;104&#x27;, &#x27;105&#x27;, &#x27;106&#x27;, &#x27;36&#x27;, &#x27;0&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t25582/259/1942499054/80811/1fd3432/5bc06426Nc4199ba0.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t25582/259/1942499054/80811/1fd3432/5bc06426Nc4199ba0.jpg!q70.jpg.webp&#x27;, &#x27;&#x27;, &#x27;&lt;img src=\\&#x27;https://img12.360buyimg.com/cms/jfs/t26248/81/2416355183/299770/5d354ebf/5bffa1c7N5e588b49.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img11.360buyimg.com/cms/jfs/t26368/105/2634117826/306450/4a278811/5c05f55aN6f1521c7.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img11.360buyimg.com/cms/jfs/t27580/253/2608871630/243514/53aae46a/5c05fa3aN5574819c.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img14.360buyimg.com/cms/jfs/t1/611/9/3439/204134/5b987c44E9614209c/bab048696d141007.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img13.360buyimg.com/cms/jfs/t27280/201/859395522/70630/b1b66e3c/5bbb5457N166fef73.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img10.360buyimg.com/cms/jfs/t1/1634/7/3042/286285/5b987cc1E9b3c0040/d45a568dd4fa52ec.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img20.360buyimg.com/cms/jfs/t1/5906/1/3091/264855/5b987d67E1ec35afe/7ff5c484bbc73937.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img12.360buyimg.com/cms/jfs/t1/941/25/3432/168955/5b987f0cE3ba19c00/d551f0e7effcbd71.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img11.360buyimg.com/cms/jfs/t27145/272/856658324/234362/f4c0d5cc/5bbb5499Nd3d7aafe.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/cms/jfs/t1/3803/30/3248/158413/5b987f59E8654de9b/76c35d3cad064367.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img10.360buyimg.com/cms/jfs/t1/1998/15/3086/114365/5b987f67E8c492b49/232370aa0c7f34ff.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img20.360buyimg.com/cms/jfs/t1/664/10/3361/170257/5b987f73E37197068/8f3051154f48322b.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img20.360buyimg.com/cms/jfs/t1/676/33/3376/126696/5b987f7fEca5a15dd/0e8a677638ab0bb2.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img14.360buyimg.com/cms/jfs/t1/975/37/3498/130931/5b987f8aEc2eb9bbf/a2502b9e4bdbe1ab.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img11.360buyimg.com/cms/jfs/t1/3441/37/3145/113463/5b987f9eE89c54589/b7a6efb51c405596.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img12.360buyimg.com/cms/jfs/t23881/192/1197260064/302565/49e6a227/5b548434Ndee3ec95.jpg\\&#x27;&gt;&#x27;, &#x27;&#123;\\&quot;选择套餐\\&quot;:[\\&quot;官方标配\\&quot;],\\&quot;颜色\\&quot;:[\\&quot;红色\\&quot;,\\&quot;黑色\\&quot;,\\&quot;蓝色\\&quot;],\\&quot;版本\\&quot;:[\\&quot;4GB+64GB\\&quot;,\\&quot;6GB+64GB\\&quot;,\\&quot;4GB+128GB\\&quot;]&#125;&#x27;, &#x27;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;, &#x27;0&#x27;, &#x27;1&#x27;);INSERT INTO `tb_spu` VALUES (&#x27;10000006409700&#x27;, &#x27;&#x27;, &#x27;娅丽达 YERAD 2018秋季新款直筒牛仔裤女韩版显瘦大码女长裤H&#123;尺码&#125;27 蓝色 30&#x27;, &#x27;&#x27;, &#x27;19042&#x27;, &#x27;439&#x27;, &#x27;440&#x27;, &#x27;457&#x27;, &#x27;39&#x27;, &#x27;0&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t23563/144/2485673269/339199/5b91f716/5b811a36N7a85c666.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t23563/144/2485673269/339199/5b91f716/5b811a36N7a85c666.jpg!q70.jpg.webp&#x27;, &#x27;&#x27;, &#x27;&lt;img src=\\&#x27;https://img30.360buyimg.com/popWaterMark/jfs/t22963/199/1302260221/181653/16dddd1f/INSERT INTO `tb_spu` VALUES (&#x27;10000018202600&#x27;, &#x27;&#x27;, &#x27;美旅AmericanTourister拉杆箱IP款 几米卡通儿童女登机箱可爱男孩万向轮行李箱 18英寸TH7密码锁&#123;颜色&#125;&#x27;, &#x27;此商品将于2019-01-04 10点结束闪购特卖，美旅卡米龙闪购&#x27;, &#x27;22364&#x27;, &#x27;717&#x27;, &#x27;757&#x27;, &#x27;758&#x27;, &#x27;41&#x27;, &#x27;0&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t1/3244/7/107/193372/5b8f899eE80e4dc95/467ec01e89847c8e.jpg!q70.jpg.webp&#x27;, &#x27;https://m.360buyimg.com/mobilecms/s450x450_jfs/t1/3244/7/107/193372/5b8f899eE80e4dc95/467ec01e89847c8e.jpg!q70.jpg.webp&#x27;, &#x27;&#x27;, &#x27;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/1628/6/6126/291117/5ba1be82E039ff8cd/9fe70718f2031c87.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/1820/20/6090/304524/5ba1be82E6abc24ad/d20bc6c8f020a418.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/2227/27/6131/298380/5ba1be82Ece429584/b6a829fd6dc2b4bb.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/4767/9/6216/265371/5ba1be82Ebbc8227f/3adf17452bc96f48.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/2436/23/6038/263321/5ba1be82E165b84aa/e6df296ad65d9caf.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/5324/5/6106/258305/5ba1be82Eb9c95c37/bfde6db1eee7f4a9.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/5719/7/6127/281131/5ba1be83Ee63af717/f398bbcde3a4052e.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/1230/3/6095/244985/5ba1be83E5b527886/b3096d780644d68c.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/4362/31/6116/289360/5ba1be83Ee06612fa/6bf3d9e12db05c44.jpg\\&#x27;&gt;&lt;br/&gt;&lt;img src=\\&#x27;https://img30.360buyimg.com/sku/jfs/t1/519/28/6448/277838/5ba1be83Edcf9f4ad/c1ecafcd163a0f0a.jpg\\&#x27;&gt;&#x27;, &#x27;&#123;\\&quot;颜色\\&quot;:[\\&quot;深色\\&quot;,\\&quot;绿色\\&quot;,\\&quot;蓝色\\&quot;,\\&quot;粉色\\&quot;]&#125;&#x27;, &#x27;&#x27;, &#x27;0&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;, &#x27;0&#x27;, &#x27;1&#x27;); Mysql不按自增插入的优化 brand_id int(11) DEFAULT NULL COMMENT ‘品牌ID’, category1_id int(20) DEFAULT NULL COMMENT ‘一级分类’, category2_id int(10) DEFAULT NULL COMMENT ‘二级分类’, category3_id int(10) DEFAULT NULL COMMENT ‘三级分类’, template_id int(20) DEFAULT NULL COMMENT ‘模板ID’, freight_id int(11) DEFAULT NULL COMMENT ‘运费模板id’, 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152DROP TABLE IF EXISTS `tb_template`;CREATE TABLE `tb_template` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;模板名称&#x27;, `spec_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;规格数量&#x27;, `para_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;参数数量&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=44 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_template-- ----------------------------INSERT INTO `tb_template` VALUES (&#x27;42&#x27;, &#x27;手机&#x27;, null, null);INSERT INTO `tb_template` VALUES (&#x27;43&#x27;, &#x27;电视&#x27;, null, null);-- ------------------------------ Table structure for undo_log-- ----------------------------DROP TABLE IF EXISTS `undo_log`;CREATE TABLE `undo_log` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `branch_id` bigint(20) NOT NULL, `xid` varchar(100) NOT NULL, `rollback_info` longblob NOT NULL, `log_status` int(11) NOT NULL, `log_created` datetime DEFAULT NULL, `log_modified` datetime DEFAULT NULL, `ext` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE, KEY `idx_unionkey` (`xid`,`branch_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;DROP TABLE IF EXISTS `tb_album`;CREATE TABLE `tb_album` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;编号&#x27;, `title` varchar(100) DEFAULT NULL COMMENT &#x27;相册名称&#x27;, `image` varchar(100) DEFAULT NULL COMMENT &#x27;相册封面&#x27;, `image_items` text COMMENT &#x27;图片列表&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_album-- ----------------------------INSERT INTO `tb_album` VALUES (&#x27;2&#x27;, &#x27;s12&#x27;, &#x27;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrtmAf_tXAAALP8HQLWA987.jpg&#x27;, &#x27;[&#123;\\&quot;url\\&quot;:\\&quot;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrUWAYdkyAAANt9KDpWU669.jpg\\&quot;,\\&quot;uid\\&quot;:1561719575032,\\&quot;status\\&quot;:\\&quot;success\\&quot;&#125;,&#123;\\&quot;url\\&quot;:\\&quot;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrUWAZ0UTAAAZLmoT79w845.jpg\\&quot;,\\&quot;uid\\&quot;:1561719575039,\\&quot;status\\&quot;:\\&quot;success\\&quot;&#125;,&#123;\\&quot;url\\&quot;:\\&quot;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrUWAXnipAAAQ9pXk-oA727.jpg\\&quot;,\\&quot;uid\\&quot;:1561719575042,\\&quot;status\\&quot;:\\&quot;success\\&quot;&#125;,&#123;\\&quot;url\\&quot;:\\&quot;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrUaAT4sFAAAZIgrXilA369.jpg\\&quot;,\\&quot;uid\\&quot;:1561719575046,\\&quot;status\\&quot;:\\&quot;success\\&quot;&#125;,&#123;\\&quot;url\\&quot;:\\&quot;http://192.168.200.128:8080/group1/M00/00/00/wKjIgF0IrUaADogZAAATxAf-zBo522.jpg\\&quot;,\\&quot;uid\\&quot;:1561719575049,\\&quot;status\\&quot;:\\&quot;success\\&quot;&#125;]&#x27;);-- ------------------------------ Table structure for tb_brand-- ----------------------------DROP TABLE IF EXISTS `tb_brand`;CREATE TABLE `tb_brand` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;品牌id&#x27;, `name` varchar(100) NOT NULL COMMENT &#x27;品牌名称&#x27;, `image` varchar(1000) DEFAULT &#x27;&#x27; COMMENT &#x27;品牌图片地址&#x27;, `letter` char(1) DEFAULT &#x27;&#x27; COMMENT &#x27;品牌的首字母&#x27;, `seq` int(11) DEFAULT NULL COMMENT &#x27;排序&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=325414 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC COMMENT=&#x27;品牌表&#x27;;-- ------------------------------ Records of tb_brand-- ----------------------------INSERT INTO `tb_brand` VALUES (&#x27;1115&#x27;, &#x27;HTC&#x27;, &#x27;&#x27;, &#x27;H&#x27;, &#x27;1&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;1528&#x27;, &#x27;LG&#x27;, &#x27;&#x27;, &#x27;L&#x27;, &#x27;1&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;1912&#x27;, &#x27;NEC&#x27;, &#x27;&#x27;, &#x27;N&#x27;, &#x27;2&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;2032&#x27;, &#x27;OPPO&#x27;, &#x27;http://img10.360buyimg.com/popshop/jfs/t2119/133/2264148064/4303/b8ab3755/56b2f385N8e4eb051.jpg&#x27;, &#x27;O&#x27;, &#x27;3&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;2505&#x27;, &#x27;TCL&#x27;, &#x27;&#x27;, &#x27;T&#x27;, &#x27;2&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;3177&#x27;, &#x27;爱贝多&#x27;, &#x27;&#x27;, &#x27;A&#x27;, &#x27;3&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;3539&#x27;, &#x27;安桥&#x27;, &#x27;&#x27;, &#x27;A&#x27;, &#x27;1&#x27;);INSERT INTO `tb_brand` VALUES (&#x27;3941&#x27;, &#x27;白金&#x27;, &#x27;&#x27;, &#x27;B&#x27;, null);DROP TABLE IF EXISTS `tb_category`;CREATE TABLE `tb_category` ( `id` int(20) NOT NULL AUTO_INCREMENT COMMENT &#x27;分类ID&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;分类名称&#x27;, `goods_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;商品数量&#x27;, `is_show` char(1) DEFAULT NULL COMMENT &#x27;是否显示&#x27;, `is_menu` char(1) DEFAULT NULL COMMENT &#x27;是否导航&#x27;, `seq` int(11) DEFAULT NULL COMMENT &#x27;排序&#x27;, `parent_id` int(20) DEFAULT NULL COMMENT &#x27;上级ID&#x27;, `template_id` int(11) DEFAULT NULL COMMENT &#x27;模板ID&#x27;, PRIMARY KEY (`id`) USING BTREE, KEY `parent_id` (`parent_id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=1217 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC COMMENT=&#x27;商品类目&#x27;;-- ------------------------------ Records of tb_category-- ----------------------------INSERT INTO `tb_category` VALUES (&#x27;1&#x27;, &#x27;图书、音像、电子书刊&#x27;, &#x27;100&#x27;, &#x27;1&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;0&#x27;, &#x27;42&#x27;);INSERT INTO `tb_category` VALUES (&#x27;2&#x27;, &#x27;电子书刊&#x27;, &#x27;100&#x27;, &#x27;1&#x27;, &#x27;0&#x27;, &#x27;1&#x27;, &#x27;1&#x27;, &#x27;42&#x27;);DROP TABLE IF EXISTS `tb_category_brand`;CREATE TABLE `tb_category_brand` ( `category_id` int(11) NOT NULL COMMENT &#x27;分类ID&#x27;, `brand_id` int(11) NOT NULL COMMENT &#x27;品牌ID&#x27;, PRIMARY KEY (`category_id`,`brand_id`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_category_brand-- ----------------------------INSERT INTO `tb_category_brand` VALUES (&#x27;14&#x27;, &#x27;2505&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;42&#x27;, &#x27;12&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;111&#x27;, &#x27;1528&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;189&#x27;, &#x27;1528&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;205&#x27;, &#x27;8557&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;558&#x27;, &#x27;1115&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;558&#x27;, &#x27;2032&#x27;);INSERT INTO `tb_category_brand` VALUES (&#x27;560&#x27;, &#x27;8557&#x27;);-- ------------------------------ Table structure for tb_para-- ----------------------------DROP TABLE IF EXISTS `tb_para`;CREATE TABLE `tb_para` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;名称&#x27;, `options` varchar(2000) DEFAULT NULL COMMENT &#x27;选项&#x27;, `seq` int(11) DEFAULT NULL COMMENT &#x27;排序&#x27;, `template_id` int(11) DEFAULT NULL COMMENT &#x27;模板ID&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_para-- ----------------------------INSERT INTO `tb_para` VALUES (&#x27;1&#x27;, &#x27;出厂年份&#x27;, &#x27;2001,2002,2004,2005&#x27;, &#x27;1&#x27;, &#x27;42&#x27;);INSERT INTO `tb_para` VALUES (&#x27;2&#x27;, &#x27;版本&#x27;, &#x27;10,20,30&#x27;, &#x27;11&#x27;, &#x27;42&#x27;);-- ------------------------------ Table structure for tb_pref-- ----------------------------DROP TABLE IF EXISTS `tb_pref`;CREATE TABLE `tb_pref` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `cate_id` int(11) DEFAULT NULL COMMENT &#x27;分类ID&#x27;, `buy_money` int(11) DEFAULT NULL COMMENT &#x27;消费金额&#x27;, `pre_money` int(11) DEFAULT NULL COMMENT &#x27;优惠金额&#x27;, `start_time` date DEFAULT NULL COMMENT &#x27;活动开始日期&#x27;, `end_time` date DEFAULT NULL COMMENT &#x27;活动截至日期&#x27;, `type` char(1) DEFAULT NULL COMMENT &#x27;类型&#x27;, `state` char(1) DEFAULT NULL COMMENT &#x27;状态&#x27;, PRIMARY KEY (`id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 ROW_FORMAT=DYNAMIC;-- ------------------------------ Records of tb_pref-- ----------------------------INSERT INTO `tb_pref` VALUES (&#x27;1&#x27;, null, &#x27;100&#x27;, &#x27;30&#x27;, null, null, null, null);INSERT INTO `tb_pref` VALUES (&#x27;2&#x27;, null, &#x27;300&#x27;, &#x27;100&#x27;, null, null, null, null);INSERT INTO `tb_pref` VALUES (&#x27;3&#x27;, &#x27;1&#x27;, &#x27;50&#x27;, &#x27;10&#x27;, null, null, null, null);INSERT INTO `tb_pref` VALUES (&#x27;4&#x27;, &#x27;2&#x27;, &#x27;100&#x27;, &#x27;40&#x27;, null, null, null, null); 2 相册模块2 相册管理（实战）目标 需求分析(知道写哪些东西),表结构分析,模块划分, 引入的依赖 2.1 需求分析相册是用于存储图片的管理单元，我们通常会将商品的图片先上传到相册中，在添加商品时可以直接在相册中选择，获取相册中的图片地址，保存到商品表中。 前端交互方式见管理后台的静态原型 2.2 表结构分析tb_album 表（相册表） 字段名称 字段含义 字段类型 备注 id 编号 BIGINT(20) 主键 title 相册名称 VARCHAR(100) image 相册封面 VARCHAR(100) image_items 图片列表 TEXT 表中image_items数据如下示例： 123456789101112[ &#123; &quot;url&quot;: &quot;http://localhost:9101/img/1.jpg&quot;, &quot;uid&quot;: 1548143143154, &quot;status&quot;: &quot;success&quot; &#125;, &#123; &quot;url&quot;: &quot;http://localhost:9101/img/7.jpg&quot;, &quot;uid&quot;: 1548143143155, &quot;status&quot;: &quot;success&quot; &#125;] 3 规格参数模板3.1 需求分析规格参数模板是用于管理规格参数的单元。规格是例如颜色、手机运行内存等信息，参数是例如系统：安卓（Android）后置摄像头像素：2000万及以上 热点：快速充电等信息 。 前端交互方式见管理后台的静态原型 3.2 表结构分析规格参数模板相关的表有3个 tb_template 表（模板表） 字段名称 字段含义 字段类型 字段长度 备注 id ID INT name 模板名称 VARCHAR spec_num 规格数量 INT para_num 参数数量 INT tb_spec 表（ 规格表） 字段名称 字段含义 字段类型 字段长度 备注 id ID INT name 名称 VARCHAR options 规格选项 VARCHAR seq 排序 INT template_id 模板ID INT tb_para 表（参数表） 字段名称 字段含义 字段类型 字段长度 备注 id id INT name 名称 VARCHAR options 选项 VARCHAR seq 排序 INT template_id 模板ID INT 模板与规格是一对多关系 ，模板与参数是一对多关系 4 商品分类4.1 需求分析商品分类一共分三级管理，主要作用是在网站首页中显示商品导航，以及在管理后台管理商品时使用。 前端交互方式见管理后台的静态原型 4.2 表结构分析tb_category 表 （商品分类） 阿里规约中 将is_xxx 设为tinyint 字段名称 字段含义 字段类型 字段长度 备注 id 分类ID INT name 分类名称 VARCHAR goods_num 商品数量 INT is_show 是否显示 CHAR 0 不显示 1显示 is_menu 是否导航 CHAR 0 不时导航 1 为导航 seq 排序 INT parent_id 上级ID INT template_id 模板ID INT 商品分类与模板是多对一关系 5 新增与修改商品5.1 需求分析实现商品的新增与修改功能。 (1)第1个步骤，先选择添加的商品所属分类 这块在第2天的代码中已经有一个根据父节点ID查询分类信息的方法，参考第2天的4.3.4的findByPrantId方法，首先查询顶级分类，也就是pid&#x3D;0，然后根据用户选择的分类，将选择的分类作为pid查询子分类。 （2)第2个步骤，填写SPU的信息 (3)第3个步骤，填写SKU信息 先进入选择商品分类 再填写商品的信息 填写商品的属性添加商品。 5.2 实现思路前端传递给后端的数据格式 是一个spu对象和sku列表组成的对象,如下图: 上图JSON数据如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&#123; &quot;spu&quot;: &#123; &quot;name&quot;: &quot;这个是商品名称&quot;, &quot;caption&quot;: &quot;这个是副标题&quot;, &quot;brandId&quot;: 12, &quot;category1Id&quot;: 558, &quot;category2Id&quot;: 559, &quot;category3Id&quot;: 560, &quot;freightId&quot;: 10, &quot;image&quot;: &quot;http://www.qingcheng.com/image/1.jpg&quot;, &quot;images&quot;: &quot;http://www.qingcheng.com/image/1.jpg,http://www.qingcheng.com/image/2.jpg&quot;, &quot;introduction&quot;: &quot;这个是商品详情，html代码&quot;, &quot;paraItems&quot;: &#123; &quot;出厂年份&quot;: &quot;2019&quot;, &quot;赠品&quot;: &quot;充电器&quot; &#125;, &quot;saleService&quot;: &quot;七天包退,闪电退货&quot;, &quot;sn&quot;: &quot;020102331&quot;, &quot;specItems&quot;: &#123; &quot;颜色&quot;: [ &quot;红&quot;, &quot;绿&quot; ], &quot;机身内存&quot;: [ &quot;64G&quot;, &quot;8G&quot; ] &#125;, &quot;templateId&quot;: 42 &#125;, &quot;skuList&quot;: [ &#123; &quot;sn&quot;: &quot;10192010292&quot;, &quot;num&quot;: 100, &quot;alertNum&quot;: 20, &quot;price&quot;: 900000, &quot;spec&quot;: &#123; &quot;颜色&quot;: &quot;红&quot;, &quot;机身内存&quot;: &quot;64G&quot; &#125;, &quot;image&quot;: &quot;http://www.qingcheng.com/image/1.jpg&quot;, &quot;images&quot;: &quot;http://www.qingcheng.com/image/1.jpg,http://www.qingcheng.com/image/2.jpg&quot;, &quot;status&quot;: &quot;1&quot;, &quot;weight&quot;: 130 &#125;, &#123; &quot;sn&quot;: &quot;10192010293&quot;, &quot;num&quot;: 100, &quot;alertNum&quot;: 20, &quot;price&quot;: 600000, &quot;spec&quot;: &#123; &quot;颜色&quot;: &quot;绿&quot;, &quot;机身内存&quot;: &quot;8G&quot; &#125;, &quot;image&quot;: &quot;http://www.qingcheng.com/image/1.jpg&quot;, &quot;images&quot;: &quot;http://www.qingcheng.com/image/1.jpg,http://www.qingcheng.com/image/2.jpg&quot;, &quot;status&quot;: &quot;1&quot;, &quot;weight&quot;: 130 &#125; ]&#125; 5.3 查询分类 写死了的分类 在实现商品增加之前，需要先选择对应的分类，选择分类的时候，首选选择一级分类，然后根据选中的分类，将选中的分类作为查询的父ID，再查询对应的子分类集合，因此我们可以在后台编写一个方法，根据父类ID查询对应的分类集合即可。 5.4 模板查询(规格参数组)同学作业 如上图，当用户选中了分类后，需要根据分类的ID查询出对应的模板数据，并将模板的名字显示在这里，模板表结构如下： 1234567CREATE TABLE `tb_template` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;模板名称&#x27;, `spec_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;规格数量&#x27;, `para_num` int(11) DEFAULT &#x27;0&#x27; COMMENT &#x27;参数数量&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=44 DEFAULT CHARSET=utf8; 5.5 查询分类品牌数据用户每次选择了分类之后，可以根据用户选择的分类到tb_category_brand表中查询指定的品牌集合ID,然后根据品牌集合ID查询对应的品牌集合数据，再将品牌集合数据拿到这里来展示即可实现上述功能。 5.6 规格查询 用户选择分类后，需要根据所选分类对应的模板ID查询对应的规格，规格表结构如下： 12345678CREATE TABLE `tb_spec` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;名称&#x27;, `options` varchar(2000) DEFAULT NULL COMMENT &#x27;规格选项&#x27;, `seq` int(11) DEFAULT NULL COMMENT &#x27;排序&#x27;, `template_id` int(11) DEFAULT NULL COMMENT &#x27;模板ID&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=40 DEFAULT CHARSET=utf8; 5.7 参数列表查询2.4.5.1 分析 当用户选中分类后，需要根据分类的模板ID查询对应的参数列表，参数表结构如下： 12345678CREATE TABLE `tb_para` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;id&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;名称&#x27;, `options` varchar(2000) DEFAULT NULL COMMENT &#x27;选项&#x27;, `seq` int(11) DEFAULT NULL COMMENT &#x27;排序&#x27;, `template_id` int(11) DEFAULT NULL COMMENT &#x27;模板ID&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8; 5.8 SPU+SKU保存2.4.6.1 分析保存商品数据的时候，需要保存Spu和Sku，一个Spu对应多个Sku，我们可以先构建一个Goods对象，将Spu和List&lt;Sku&gt;组合到一起,前端将2者数据提交过来，再实现添加操作。 5.9 根据ID查询商品2.4.7.1 需求分析需求：根据id 查询SPU和SKU列表 ，显示效果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#123; &quot;spu&quot;: &#123; &quot;brandId&quot;: 0, &quot;caption&quot;: &quot;111&quot;, &quot;category1Id&quot;: 558, &quot;category2Id&quot;: 559, &quot;category3Id&quot;: 560, &quot;commentNum&quot;: null, &quot;freightId&quot;: null, &quot;id&quot;: 149187842867993, &quot;image&quot;: null, &quot;images&quot;: null, &quot;introduction&quot;: null, &quot;isDelete&quot;: null, &quot;isEnableSpec&quot;: &quot;0&quot;, &quot;isMarketable&quot;: &quot;1&quot;, &quot;name&quot;: &quot;黑马智能手机&quot;, &quot;paraItems&quot;: null, &quot;saleNum&quot;: null, &quot;saleService&quot;: null, &quot;sn&quot;: null, &quot;specItems&quot;: null, &quot;status&quot;: null, &quot;templateId&quot;: 42 &#125;, &quot;skuList&quot;: [&#123; &quot;alertNum&quot;: null, &quot;brandName&quot;: &quot;金立（Gionee）&quot;, &quot;categoryId&quot;: 560, &quot;categoryName&quot;: &quot;手机&quot;, &quot;commentNum&quot;: null, &quot;createTime&quot;: &quot;2018-11-06 10:17:08&quot;, &quot;id&quot;: 1369324, &quot;image&quot;: null, &quot;images&quot;: &quot;blob:http://localhost:8080/ec04d1a5-d865-4e7f-a313-2e9a76cfb3f8&quot;, &quot;name&quot;: &quot;黑马智能手机&quot;, &quot;num&quot;: 100, &quot;price&quot;: 900000, &quot;saleNum&quot;: null, &quot;sn&quot;: &quot;&quot;, &quot;spec&quot;: null, &quot;spuId&quot;: 149187842867993, &quot;status&quot;: &quot;1&quot;, &quot;updateTime&quot;: &quot;2018-11-06 10:17:08&quot;, &quot;weight&quot;: null &#125;,&#123; &quot;alertNum&quot;: null, &quot;brandName&quot;: &quot;金立（Gionee）&quot;, &quot;categoryId&quot;: 560, &quot;categoryName&quot;: &quot;手机&quot;, &quot;commentNum&quot;: null, &quot;createTime&quot;: &quot;2018-11-06 10:17:08&quot;, &quot;id&quot;: 1369325, &quot;image&quot;: null, &quot;images&quot;: &quot;blob:http://localhost:8080/ec04d1a5-d865-4e7f-a313-2e9a76cfb3f8&quot;, &quot;name&quot;: &quot;黑马智能手机&quot;, &quot;num&quot;: 100, &quot;price&quot;: 900000, &quot;saleNum&quot;: null, &quot;sn&quot;: &quot;&quot;, &quot;spec&quot;: null, &quot;spuId&quot;: 149187842867993, &quot;status&quot;: &quot;1&quot;, &quot;updateTime&quot;: &quot;2018-11-06 10:17:08&quot;, &quot;weight&quot;: null &#125; ]&#125; 5.10 保存修改5.11 修改SKU库存6 商品审核与上下架6.1 需求分析商品新增后，审核状态为0（未审核），默认为下架状态。 审核商品，需要校验是否是被删除的商品，如果未删除则修改审核状态为1，并自动上架 下架商品，需要校验是否是被删除的商品，如果未删除则修改上架状态为0 上架商品，需要审核通过的商品 6.2 实现思路（1）按照ID查询SPU信息 （2）判断修改审核、上架和下架状态 （3）保存SPU 6.3 代码实现3.3.1 商品审核实现审核通过，自动上架。 3.3.4 批量上架3.3.5 批量下架7 删除与还原商品7.1 需求分析请看管理后台的静态原型 商品列表中的删除商品功能，并非真正的删除，而是将删除标记的字段设置为1， 在回收站中有恢复商品的功能，将删除标记的字段设置为0 在回收站中有删除商品的功能，是真正的物理删除。 7.2 实现思路逻辑删除商品，修改spu表is_delete字段为1 商品回收站显示spu表is_delete字段为1的记录 回收商品，修改spu表is_delete字段为0 还原被删除的商品物理删除商品8 商品列表8.1 需求分析如图所示 展示商品的列表。并实现分页。 思路： 12根据查询的条件 分页查询 并返回分页结果即可。分页查询 采用 pagehelper ，条件查询 通过map进行封装传递给后台即可。 Mysql知识点数据库优化数据库结构优化一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 注意： 冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 阿里Mysql规约is_xxx 使用unsinged tinyint 而不是bit(1) bit实际上是一个二进制字符串 在为1的时候与 unsigned一样为 8bit 但是在应用间的通用性更好,JDBC会将tinyint 转为bit 需要设置 tiny转bit 为false","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"seata分布式事务模块","slug":"畅购商城/seata","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:04:57.899Z","comments":true,"path":"2022/09/01/畅购商城/seata/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/seata/","excerpt":"","text":"第15章 分布式事务学习目标 理解什么是事务 理解什么是分布式事务 &#x3D;&#x3D;理解CAP定理&#x3D;&#x3D; 1CAP不能3者同时成立 &#x3D;&#x3D;能说出相关的分布式事务解决方案&#x3D;&#x3D; 12341.2PC-JTA分布式事务2.本地消息-业务库中添加对应的消息表和业务耦合实现3.MQ事务消息-RocketMQ4.Seata &#x3D;&#x3D;理解Seata工作流程&#x3D;&#x3D; 12AT模式-表TCC模式-代码补偿机制 能实现Seata案例 1Seata使用案例 作业：实现项目中分布式事务控制-下单-&gt;用户微服务（增加积分）-&gt;Goods微服务(库存递减) 1 分布式事务介绍1.1 什么是事务数据库事务(简称：事务，Transaction)是指数据库执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成[由当前业务逻辑多个不同操作构成]。 事务拥有以下四个特性，习惯上被称为ACID特性： **原子性(Atomicity)**：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。 **一致性(Consistency)**：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态是指数据库中的数据应满足完整性约束。除此之外，一致性还有另外一层语义，就是事务的中间状态不能被观察到(这层语义也有说应该属于原子性)。 **隔离性(Isolation)**：多个事务并发执行时，一个事务的执行不应影响其他事务的执行，如同只有这一个操作在被数据库所执行一样。 **持久性(Durability)**：已被提交的事务对数据库的修改应该永久保存在数据库中。在事务结束时，此操作将不可逆转。 1.2 本地事务起初，事务仅限于对单一数据库资源的访问控制,架构服务化以后，事务的概念延伸到了服务中。倘若将一个单一的服务操作作为一个事务，那么整个服务操作只能涉及一个单一的数据库资源,这类基于单个服务单一数据库资源访问的事务，被称为本地事务(Local Transaction)。 1.3 什么是分布式事务分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上,且属于不同的应用，分布式事务需要保证这些操作要么全部成功，要么全部失败。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。 1.4 分布式事务应用架构本地事务主要限制在单个会话内，不涉及多个数据库资源。但是在基于SOA(Service-Oriented Architecture，面向服务架构)的分布式应用环境下，越来越多的应用要求对多个数据库资源，多个服务的访问都能纳入到同一个事务当中，分布式事务应运而生。 1.4.1 单一服务分布式事务最早的分布式事务应用架构很简单，不涉及服务间的访问调用，仅仅是服务内操作涉及到对多个数据库资源的访问。 1.4.2 多服务分布式事务当一个服务操作访问不同的数据库资源，又希望对它们的访问具有事务特性时，就需要采用分布式事务来协调所有的事务参与者。 对于上面介绍的分布式事务应用架构，尽管一个服务操作会访问多个数据库资源，但是毕竟整个事务还是控制在单一服务的内部。如果一个服务操作需要调用另外一个服务，这时的事务就需要跨越多个服务了。在这种情况下，起始于某个服务的事务在调用另外一个服务的时候，需要以某种机制流转到另外一个服务，从而使被调用的服务访问的资源也自动加入到该事务当中来。下图反映了这样一个跨越多个服务的分布式事务： 1.4.3 多服务多数据源分布式事务如果将上面这两种场景(一个服务可以调用多个数据库资源，也可以调用其他服务)结合在一起，对此进行延伸，整个分布式事务的参与者将会组成如下图所示的树形拓扑结构。在一个跨服务的分布式事务中，事务的发起者和提交均系同一个，它可以是整个调用的客户端，也可以是客户端最先调用的那个服务。 较之基于单一数据库资源访问的本地事务，分布式事务的应用架构更为复杂。在不同的分布式应用架构下，实现一个分布式事务要考虑的问题并不完全一样，比如对多资源的协调、事务的跨服务传播等，实现机制也是复杂多变。 事务的作用： 1保证每个事务的数据一致性。 1.5 CAP定理CAP 定理，又被叫作布鲁尔定理。对于设计分布式系统(不仅仅是分布式事务)的架构师来说，CAP 就是你的入门理论。 C (一致性)：对某个指定的客户端来说，读操作能返回最新的写操作。 对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A (可用性)：非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。 合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的，这里的正确指的是比如应该返回 50，而不是返回 40。 P (分区容错性)：当出现网络分区后，系统能够继续工作。打个比方，这里集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以正常工作。 熟悉 CAP 的人都知道，三者不能共有，如果感兴趣可以搜索 CAP 的证明，在分布式系统中，网络无法 100% 可靠，分区其实是一个必然现象。 如果我们选择了 CA 而放弃了 P，那么当发生分区现象时，为了保证一致性，这个时候必须拒绝请求，但是 A 又不允许，所以分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。 对于 CP 来说，放弃可用性，追求一致性和分区容错性，我们的 ZooKeeper 其实就是追求的强一致。 对于 AP 来说，放弃一致性(这里说的一致性是强一致性)，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的 BASE 也是根据 AP 来扩展。 顺便一提，CAP 理论中是忽略网络延迟，也就是当事务提交时，从节点 A 复制到节点 B 没有延迟，但是在现实中这个是明显不可能的，所以总会有一定的时间是不一致。 同时 CAP 中选择两个，比如你选择了 CP，并不是叫你放弃 A。因为 P 出现的概率实在是太小了，大部分的时间你仍然需要保证 CA。 就算分区出现了你也要为后来的 A 做准备，比如通过一些日志的手段，是其他机器回复至可用。 &#x3D;&#x3D;2 分布式事务解决方案&#x3D;&#x3D;1.XA两段提交(低效率)-21 XA JTA分布式事务解决方案 2.TCC三段提交(2段,高效率[不推荐(补偿代码)]) 3.本地消息(MQ+Table) 4.事务消息(RocketMQ[alibaba]) 5.Seata(alibaba) 2.1 基于XA协议的两阶段提交(2PC)两阶段提交协议(Two Phase Commitment Protocol)中，涉及到两种角色 &#x3D;&#x3D;一个事务协调者&#x3D;&#x3D;（coordinator）：负责协调多个参与者进行事务投票及提交(回滚)多个&#x3D;&#x3D;事务参与者&#x3D;&#x3D;（participants）：即本地事务执行者 总共处理步骤有两个（1）投票阶段（voting phase）：协调者将通知事务参与者准备提交或取消事务，然后进入表决过程。参与者将告知协调者自己的决策：同意（事务参与者本地事务执行成功，但未提交）或取消（本地事务执行故障）；（2）提交阶段（commit phase）：收到参与者的通知后，协调者再向参与者发出通知，根据反馈情况决定各参与者是否要提交还是回滚； 如果所示 1-2为第一阶段，2-3为第二阶段 如果任一资源管理器在第一阶段返回准备失败，那么事务管理器会要求所有资源管理器在第二阶段执行回滚操作。通过事务管理器的两阶段协调，最终所有资源管理器要么全部提交，要么全部回滚，最终状态都是一致的 官方解决方案图例如下: 优点： 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。 缺点： 牺牲了可用性，对性能影响较大，不适合高并发高性能场景，如果分布式系统跨接口调用，目前 .NET 界还没有实现方案。 2.2 补偿事务（TCC）TCC 将事务提交分为 Try(method1) - Confirm(method2) - Cancel(method3) 3个操作。其和两阶段提交有点类似，Try为第一阶段，Confirm - Cancel为第二阶段，是一种应用层面侵入业务的两阶段提交。 操作方法 含义 Try 预留业务资源&#x2F;数据效验 Confirm 确认执行业务操作，实际提交数据，不做任何业务检查，try成功，confirm必定成功，需保证幂等 Cancel 取消执行业务操作，实际回滚数据，需保证幂等 其核心在于将业务分为两个操作步骤完成。不依赖 RM 对分布式事务的支持，而是通过对业务逻辑的分解来实现分布式事务。 例如： A要向 B 转账，思路大概是： 1234我们有一个本地方法，里面依次调用 1、首先在 Try 阶段，要先调用远程接口把 B和 A的钱给冻结起来。 2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 12345假设用户user表中有两个字段：可用余额(available_money)、冻结余额(frozen_money)A扣钱对应服务A(ServiceA)B加钱对应服务B(ServiceB)转账订单服务(OrderService)业务转账方法服务(BusinessService) ServiceA，ServiceB，OrderService都需分别实现try()，confirm()，cancle()方法，方法对应业务逻辑如下 操作方法 ServiceA ServiceB OrderService try() 校验余额(并发控制)冻结余额+1000余额-1000 冻结余额+1000 创建转账订单，状态待转账 confirm() 冻结余额-1000 状态变为转账成功 cancle() 冻结余额-1000余额+1000 状态变为转账失败 其中业务调用方BusinessService中就需要调用ServiceA.try()ServiceB.try()OrderService.try() 1、当所有try()方法均执行成功时，对全局事物进行提交，即由事物管理器调用每个微服务的confirm()方法 2、 当任意一个方法try()失败(预留资源不足，抑或网络异常，代码异常等任何异常)，由事物管理器调用每个微服务的cancle()方法对全局事务进行回滚 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。 2.3 本地消息表（异步确保） 本地消息表这种实现方式应该是业界使用最多的，其核心思想是将分布式事务拆分成本地事务进行处理，这种思路是来源于ebay。我们可以从下面的流程图中看出其中的一些细节： 基本思路就是： 消息生产方，需要额外建一个消息表，并记录消息发送状态。消息表和业务数据要在一个事务里提交，也就是说他们要在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送。 消息消费方，需要处理这个消息，并完成自己的业务逻辑。此时如果本地事务处理成功，表明已经处理成功了，如果处理失败，那么就会重试执行。如果是业务上面的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。 生产方和消费方定时扫描本地消息表，把还没处理完成的消息或者失败的消息再发送一遍。如果有靠谱的自动对账补账逻辑，这种方案还是非常实用的。 这种方案遵循BASE理论，采用的是最终一致性，笔者认为是这几种方案里面比较适合实际业务场景的，即不会出现像2PC那样复杂的实现(当调用链很长的时候，2PC的可用性是非常低的)，也不会像TCC那样可能出现确认或者回滚不了的情况。 优点： 一种非常经典的实现，避免了分布式事务，实现了最终一致性。在 .NET中 有现成的解决方案。 缺点： 消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。 2.4 MQ 事务消息有一些第三方的MQ是支持事务消息的，比如RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交，但是市面上一些主流的MQ都是不支持事务消息的，比如 RabbitMQ 和 Kafka 都不支持。 以阿里的 RocketMQ 中间件为例，其思路大致为： 第一阶段Prepared消息，会拿到消息的地址。第二阶段执行本地事务，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。 也就是说在业务方法内要想消息队列提交两次请求，一次发送消息和一次确认消息。如果确认消息发送失败了RocketMQ会定期扫描消息集群中的事务消息，这时候发现了Prepared消息，它会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 优点： 实现了最终一致性，不需要依赖本地数据库事务。 缺点： 目前主流MQ中只有RocketMQ支持事务消息。","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"面试问题","slug":"畅购商城/总结","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:02:59.249Z","comments":true,"path":"2022/09/01/畅购商城/总结/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E6%80%BB%E7%BB%93/","excerpt":"","text":"高访问量的解决方案核心表的设计?哪些时候使用到了数据库,与前端的交互,有进行过压测吗?单点登陆的实现?谈谈对Oauth2.0的理解秒杀模块的实现?具体的问题解决?分布式事务的使用?原理了解吗?购物车与订单模块的实现?遇到的问题?对常见问题的解决有没有遇到什么问题?","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"商品详情页模块","slug":"畅购商城/商品详情页","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:02:00.854Z","comments":true,"path":"2022/09/01/畅购商城/商品详情页/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E5%95%86%E5%93%81%E8%AF%A6%E6%83%85%E9%A1%B5/","excerpt":"","text":"5.畅购商品详情页5.1 需求分析当系统审核完成商品，需要将商品详情页进行展示，那么采用静态页面生成的方式生成，并部署到高性能的web服务器中进行访问是比较合适的。所以，开发流程如下图所示： 执行步骤解释： 系统管理员（商家运维人员）修改或者审核商品的时候，会触发canal监控数据 canal微服务获取修改数据后，调用静态页微服务的方法进行生成静态页 静态页微服务只负责使用thymeleaf的模板技术生成静态页 5.2 商品静态化微服务创建5.2.1 需求分析该微服务只用于生成商品静态页，不做其他事情。 5.2.2 搭建项目（1）在changgou-web下创建一个名称为changgou-web-item的模块,如图： （2）changgou-web-item中添加起步依赖，如下 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;changgou-web&lt;/artifactId&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;changgou-web-item&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!--api 模块--&gt; &lt;dependency&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;artifactId&gt;changgou-service-goods-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; （3）修改application.yml的配置 1234567891011121314151617181920212223server: port: 18085eureka: client: service-url: defaultZone: http://127.0.0.1:7001/eureka instance: prefer-ip-address: truefeign: hystrix: enabled: truespring: thymeleaf: cache: false application: name: item main: allow-bean-definition-overriding: true #rabbitmq: # host: 192.168.25.138# 生成静态页的位置pagepath: D:/project/workspace_changgou/changgou/changgou-parent/changgou-web/changgou-web-item/src/main/resources/templates/items （4）创建系统启动类 12345678@SpringBootApplication@EnableEurekaClient@EnableFeignClients(basePackages = &quot;com.changgou.goods.feign&quot;)public class ItemApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ItemApplication.class,args); &#125;&#125; 5.3 生成静态页5.3.1 需求分析页面发送请求，传递要生成的静态页的的商品的SpuID.后台controller 接收请求，调用thyemleaf的原生API生成商品静态页。 上图是要生成的商品详情页，从图片上可以看出需要查询SPU的3个分类作为面包屑显示，同时还需要查询SKU和SPU信息。 5.3.2 Feign创建一会儿需要查询SPU和SKU以及Category，所以我们需要先创建Feign，修改changgou-service-goods-api,添加CategoryFeign，并在CategoryFeign中添加根据ID查询分类数据，代码如下： 1234567/** * 获取分类的对象信息 * @param id * @return */@GetMapping(&quot;/&#123;id&#125;&quot;)public Result&lt;Category&gt; findById(@PathVariable(name = &quot;id&quot;) Integer id); 在changgou-service-goods-api,添加SkuFeign,并添加根据SpuID查询Sku集合，代码如下： 1234567/** * 根据条件搜索 * @param sku * @return */@PostMapping(value = &quot;/search&quot; )public Result&lt;List&lt;Sku&gt;&gt; findList(@RequestBody(required = false) Sku sku); 在changgou-service-goods-api,添加SpuFeign,并添加根据SpuID查询Spu信息，代码如下： 1234567/*** * 根据SpuID查询Spu信息 * @param id * @return */@GetMapping(&quot;/&#123;id&#125;&quot;)public Result&lt;Spu&gt; findById(@PathVariable(name = &quot;id&quot;) Long id); 5.3.3 静态页生成代码(1)创建Controller 在changgou-web-item中创建com.changgou.item.controller.PageController用于接收请求，测试生成静态页 123456789101112131415161718@RestController@RequestMapping(&quot;/page&quot;)public class PageController &#123; @Autowired private PageService pageService; /** * 生成静态页面 * @param id * @return */ @RequestMapping(&quot;/createHtml/&#123;id&#125;&quot;) public Result createHtml(@PathVariable(name=&quot;id&quot;) Long id)&#123; pageService.createPageHtml(id); return new Result(true, StatusCode.OK,&quot;ok&quot;); &#125;&#125; (2)创建service 接口： 1234567public interface PageService &#123; /** * 根据商品的ID 生成静态页 * @param spuId */ public void createPageHtml(Long spuId) ;&#125; 实现类： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Servicepublic class PageServiceImpl implements PageService &#123; @Autowired private SpuFeign spuFeign; @Autowired private CategoryFeign categoryFeign; @Autowired private SkuFeign skuFeign; @Autowired private TemplateEngine templateEngine; //生成静态文件路径 @Value(&quot;$&#123;pagepath&#125;&quot;) private String pagepath; /** * 构建数据模型 * @param spuId * @return */ private Map&lt;String,Object&gt; buildDataModel(Long spuId)&#123; //构建数据模型 Map&lt;String,Object&gt; dataMap = new HashMap&lt;&gt;(); //获取spu 和SKU列表 Result&lt;Spu&gt; result = spuFeign.findById(spuId); Spu spu = result.getData(); //获取分类信息 dataMap.put(&quot;category1&quot;,categoryFeign.findById(spu.getCategory1Id()).getData()); dataMap.put(&quot;category2&quot;,categoryFeign.findById(spu.getCategory2Id()).getData()); dataMap.put(&quot;category3&quot;,categoryFeign.findById(spu.getCategory3Id()).getData()); if(spu.getImages()!=null) &#123; dataMap.put(&quot;imageList&quot;, spu.getImages().split(&quot;,&quot;)); &#125; dataMap.put(&quot;specificationList&quot;,JSON.parseObject(spu.getSpecItems(),Map.class)); dataMap.put(&quot;spu&quot;,spu); //根据spuId查询Sku集合 Sku skuCondition = new Sku(); skuCondition.setSpuId(spu.getId()); Result&lt;List&lt;Sku&gt;&gt; resultSku = skuFeign.findList(skuCondition); dataMap.put(&quot;skuList&quot;,resultSku.getData()); return dataMap; &#125; /*** * 生成静态页 * @param spuId */ @Override public void createPageHtml(Long spuId) &#123; // 1.上下文 Context context = new Context(); Map&lt;String, Object&gt; dataModel = buildDataModel(spuId); context.setVariables(dataModel); // 2.准备文件 File dir = new File(pagepath); if (!dir.exists()) &#123; dir.mkdirs(); &#125; File dest = new File(dir, spuId + &quot;.html&quot;); // 3.生成页面 try (PrintWriter writer = new PrintWriter(dest, &quot;UTF-8&quot;)) &#123; templateEngine.process(&quot;item&quot;, context, writer); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 5.2.4 模板填充(1)面包屑数据 修改item.html，填充三个分类数据作为面包屑，代码如下： (2)商品图片 修改item.html，将商品图片信息输出，在真实工作中需要做空判断，代码如下： (3)规格输出 (4)默认SKU显示 静态页生成后，需要显示默认的Sku，我们这里默认显示第1个Sku即可，这里可以结合着Vue一起实现。可以先定义一个集合，再定义一个spec和sku，用来存储当前选中的Sku信息和Sku的规格，代码如下： 页面显示默认的Sku信息 (5)记录选中的Sku 在当前Spu的所有Sku中spec值是唯一的，我们可以根据spec来判断用户选中的是哪个Sku，我们可以在Vue中添加代码来实现，代码如下： 添加规格点击事件 (6)样式切换 点击不同规格后，实现样式选中，我们可以根据每个规格判断该规格是否在当前选中的Sku规格中，如果在，则返回true添加selected样式，否则返回false不添加selected样式。 Vue添加代码： 页面添加样式绑定，代码如下： 5.2.5 静态资源过滤生成的静态页我们可以先放到changgou-web-item工程中，后面项目实战的时候可以挪出来放到Nginx指定发布目录。一会儿我们将生成的静态页放到resources&#x2F;templates&#x2F;items目录下,所以请求该目录下的静态页需要直接到该目录查找即可。 我们创建一个EnableMvcConfig类，开启静态资源过滤，代码如下： 12345678910111213@ControllerAdvice@Configurationpublic class EnableMvcConfig implements WebMvcConfigurer&#123; /*** * 静态资源放行 * @param registry */ @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;/items/**&quot;).addResourceLocations(&quot;classpath:/templates/items/&quot;); &#125;&#125; 5.4.6 启动测试启动eurekea服务端 启动商品微服务 启动静态化微服务 changgou-web-item 将静态资源导入到changgou-web-item中，如下图： 生成静态页地址 http://localhost:18085/page/createHtml/1087918019151269888 静态页生成后访问地址 http://localhost:18085/items/1087918019151269888.html 6 canal监听生成静态页 监听到数据的变化,直接调用feign 生成静态页即可. 6.1 需求分析当商品微服务审核商品之后，应当发送消息，这里采用了Canal监控数据变化，数据变化后，调用feign实现生成静态页 6.2 Feign创建在changgou-service-api中创建changgou-web-item-api，该工程中主要创建changgou-web-item的对外依赖抽取信息。 (1)Feign创建 在changgou-web-item-api中创建com.changgou.item.feign.PageFeign,代码如下： 123456789101112@FeignClient(name=&quot;item&quot;)@RequestMapping(&quot;/page&quot;)public interface PageFeign &#123; /*** * 根据SpuID生成静态页 * @param id * @return */ @RequestMapping(&quot;/createHtml/&#123;id&#125;&quot;) Result createHtml(@PathVariable(name=&quot;id&quot;) Long id);&#125; (2)pom.xml依赖 修改changgou-service-canal工程的pom.xml，引入如下依赖： 123456&lt;!--静态页API 服务--&gt;&lt;dependency&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;artifactId&gt;changgou-web-item-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/dependency&gt; (3)修改changgou-service-canal工程中的启动类 12345678910@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)@EnableEurekaClient@EnableCanalClient // 启用canal@EnableFeignClients(basePackages = &#123;&quot;com.changgou.content.feign&quot;,&quot;com.changgou.item.feign&quot;&#125;)public class CanalApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(CanalApplication.class, args); &#125;&#125; 6.3 canal监听数据变化监听类中,监听商品数据库的tb_spu的数据变化,当数据变化的时候生成静态页或者删除静态页 在原来的监听类中添加如下代码即可, 1234567891011121314151617181920212223242526272829303132333435@Autowiredprivate PageFeign pageFeign;@ListenPoint(destination = &quot;example&quot;, schema = &quot;changgou_goods&quot;, table = &#123;&quot;tb_spu&quot;&#125;, eventType = &#123;CanalEntry.EventType.UPDATE, CanalEntry.EventType.INSERT, CanalEntry.EventType.DELETE&#125;)public void onEventCustomSpu(CanalEntry.EventType eventType, CanalEntry.RowData rowData) &#123; //判断操作类型 if (eventType == CanalEntry.EventType.DELETE) &#123; String spuId = &quot;&quot;; List&lt;CanalEntry.Column&gt; beforeColumnsList = rowData.getBeforeColumnsList(); for (CanalEntry.Column column : beforeColumnsList) &#123; if (column.getName().equals(&quot;id&quot;)) &#123; spuId = column.getValue();//spuid break; &#125; &#125; //todo 删除静态页 &#125;else&#123; //新增 或者 更新 List&lt;CanalEntry.Column&gt; afterColumnsList = rowData.getAfterColumnsList(); String spuId = &quot;&quot;; for (CanalEntry.Column column : afterColumnsList) &#123; if (column.getName().equals(&quot;id&quot;)) &#123; spuId = column.getValue(); break; &#125; &#125; //更新 生成静态页 pageFeign.createHtml(Long.valueOf(spuId)); &#125;&#125;","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"用户登陆 & JWT & Oauth2.0","slug":"畅购商城/用户登陆 & JWT & Oauth2.0","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:02:39.179Z","comments":true,"path":"2022/09/01/畅购商城/用户登陆 & JWT & Oauth2.0/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E7%94%A8%E6%88%B7%E7%99%BB%E9%99%86%20&%20JWT%20&%20Oauth2.0/","excerpt":"","text":"3 用户登录项目中有2个重要角色，分别为管理员和用户，下面几章我们将实现购物下单和支付，用户如果没登录是没法下单和支付的，所以我们这里需要实现一个登录功能。 3.1 表结构介绍changgou_user表如下： 用户信息表tb_user 123456789101112131415161718192021222324CREATE TABLE `tb_user` ( `username` varchar(50) NOT NULL COMMENT &#x27;用户名&#x27;, `password` varchar(100) NOT NULL COMMENT &#x27;密码，加密存储&#x27;, `phone` varchar(20) DEFAULT NULL COMMENT &#x27;注册手机号&#x27;, `email` varchar(50) DEFAULT NULL COMMENT &#x27;注册邮箱&#x27;, `created` datetime NOT NULL COMMENT &#x27;创建时间&#x27;, `updated` datetime NOT NULL COMMENT &#x27;修改时间&#x27;, `source_type` varchar(1) DEFAULT NULL COMMENT &#x27;会员来源：1:PC，2：H5，3：Android，4：IOS&#x27;, `nick_name` varchar(50) DEFAULT NULL COMMENT &#x27;昵称&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;真实姓名&#x27;, `status` varchar(1) DEFAULT NULL COMMENT &#x27;使用状态（1正常 0非正常）&#x27;, `head_pic` varchar(150) DEFAULT NULL COMMENT &#x27;头像地址&#x27;, `qq` varchar(20) DEFAULT NULL COMMENT &#x27;QQ号码&#x27;, `is_mobile_check` varchar(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;手机是否验证 （0否 1是）&#x27;, `is_email_check` varchar(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;邮箱是否检测（0否 1是）&#x27;, `sex` varchar(1) DEFAULT &#x27;1&#x27; COMMENT &#x27;性别，1男，0女&#x27;, `user_level` int(11) DEFAULT NULL COMMENT &#x27;会员等级&#x27;, `points` int(11) DEFAULT NULL COMMENT &#x27;积分&#x27;, `experience_value` int(11) DEFAULT NULL COMMENT &#x27;经验值&#x27;, `birthday` datetime DEFAULT NULL COMMENT &#x27;出生年月日&#x27;, `last_login_time` datetime DEFAULT NULL COMMENT &#x27;最后登录时间&#x27;, PRIMARY KEY (`username`), UNIQUE KEY `username` (`username`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;用户表&#x27;; 3.2 用户微服务创建创建工程之前，先使用代码生成器生成对应的业务代码。 (1)公共API创建 在changgou-service-api中创建changgou-service-user-api，并将pojo拷贝到工程中，如下图： 在changgou-service中创建changgou-service-user微服务,并引入生成的业务逻辑代码，如下图： (2)依赖 在changgou-service-user的pom.xml引入如下依赖： 123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;changgou-service&lt;/artifactId&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;changgou-service-user&lt;/artifactId&gt; &lt;!--依赖--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;artifactId&gt;changgou-service-user-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; (3)启动类创建 在changgou-service-user微服务中创建启动类com.changgou.UserApplication，代码如下： 123456789@SpringBootApplication@EnableEurekaClient@MapperScan(&quot;com.changgou.user.dao&quot;)public class UserApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(UserApplication.class,args); &#125;&#125; (4)application.yml配置 在changgou-service-user的resources中创建application.yml配置，代码如下： 12345678910111213141516171819server: port: 18089spring: application: name: user datasource: driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://192.168.211.132:3306/changgou_user?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC username: root password: 123456eureka: client: service-url: defaultZone: http://127.0.0.1:7001/eureka instance: prefer-ip-address: truefeign: hystrix: enabled: true 3.3 登录登录的时候，需要进行密码校验，这里采用了BCryptPasswordEncoder进行加密，需要将资料中的BCrypt导入到common工程中，其中BCrypt.checkpw(“明文”,”密文”)用于对比密码是否一致。 修改changgou-service-user的com.changgou.user.controller.UserController添加登录方法，代码如下： 12345678910111213/*** * 用户登录 */@RequestMapping(value = &quot;/login&quot;)public Result login(String username,String password)&#123; //查询用户信息 User user = userService.findById(username); if(user!=null &amp;&amp; BCrypt.checkpw(password,user.getPassword()))&#123; return new Result(true,StatusCode.OK,&quot;登录成功！&quot;,user); &#125; return new Result(false,StatusCode.LOGINERROR,&quot;账号或者密码错误！&quot;);&#125; 注意：这里密码进行了加密。 使用Postman测试如下： 3.4 网关关联 在我们平时工作中，并不会直接将微服务暴露出去，一般都会使用网关对接，实现对微服务的一个保护作用，如上图，当用户访问/api/user/的时候我们再根据用户请求调用用户微服务的指定方法。当然，除了/api/user/还有/api/address/、/api/areas/、/api/cities/、/api/provinces/都需要由user微服务处理，修改网关工程changgou-gateway-web的application.yml配置文件，如下代码： 上图代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354spring: cloud: gateway: globalcors: corsConfigurations: &#x27;[/**]&#x27;: # 匹配所有请求 allowedOrigins: &quot;*&quot; #跨域处理 允许所有的域 allowedMethods: # 支持的方法 - GET - POST - PUT - DELETE routes: - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/goods/** filters: - StripPrefix=1 - name: RequestRateLimiter #请求数限流 名字不能随便写 ，使用默认的facatory args: key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 #用户微服务 - id: changgou_user_route uri: lb://user predicates: - Path=/api/user/**,/api/address/**,/api/areas/**,/api/cities/**,/api/provinces/** filters: - StripPrefix=1 application: name: gateway-web #Redis配置 redis: host: 192.168.211.132 port: 6379server: port: 8001eureka: client: service-url: defaultZone: http://127.0.0.1:7001/eureka instance: prefer-ip-address: truemanagement: endpoint: gateway: enabled: true web: exposure: include: true 使用Postman访问http://localhost:8001/api/user/login?username=changgou&amp;password=changgou，效果如下： 4 JWT讲解4.1 需求分析我们之前已经搭建过了网关，使用网关在网关系统中比较适合进行权限校验。 那么我们可以采用JWT的方式来实现鉴权校验。 4.2 什么是JWTJSON Web Token（JWT）是一个非常轻巧的规范。这个规范允许我们使用JWT在用户和服务器之间传递安全可靠的信息。 4.3 JWT的构成头部描述信息,+载荷生成base64相当于明文,在加盐签名,盗了怎么办,服务端签发了JWT后用户就可 JWT本身包含认证信息，因此一旦信息泄露，任何人都可以获得令牌的所有权限。为了减少盗用，JWT的有效期不宜设置太长。对于某些重要操作，用户在使用时应该每次都进行进行身份验证。 为了减少盗用和窃取，JWT不建议使用HTTP协议来传输代码，而是使用加密的HTTPS协议进行传输。 一个JWT实际上就是一个字符串，它由三部分组成，头部、载荷与签名。\\ 头部（Header） 头部用于描述关于该JWT的最基本的信息，例如其类型以及签名所用的算法等。这也可以被表示成一个JSON对象。 1&#123;&quot;typ&quot;:&quot;JWT&quot;,&quot;alg&quot;:&quot;HS256&quot;&#125; 在头部指明了签名算法是HS256算法。 我们进行BASE64编码http://base64.xpcha.com/，编码后的字符串如下： 1eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 小知识：Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于2的6次方等于64，所以每6个比特为一个单元，对应某个可打印字符。三个字节有24个比特，对应于4个Base64单元，即3个字节需要用4个可打印字符来表示。JDK 中提供了非常方便的 BASE64Encoder 和 BASE64Decoder，用它们可以非常方便的完成基于 BASE64 的编码和解码 载荷（playload） 载荷就是存放有效信息的地方。这个名字像是特指飞机上承载的货品，这些有效信息包含三个部分 （1）标准中注册的声明（建议但不强制使用） 1234567iss: jwt签发者sub: jwt所面向的用户aud: 接收jwt的一方exp: jwt的过期时间，这个过期时间必须要大于签发时间nbf: 定义在什么时间之前，该jwt都是不可用的.iat: jwt的签发时间jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 （2）公共的声明 公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密. （3）私有的声明 私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 这个指的就是自定义的claim。比如下面面结构举例中的admin和name都属于自定的claim。这些claim跟JWT标准规定的claim区别在于：JWT规定的claim，JWT的接收方在拿到JWT之后，都知道怎么对这些标准的claim进行验证(还不知道是否能够验证)；而private claims不会验证，除非明确告诉接收方要对这些claim进行验证以及规则才行。 定义一个payload: 1&#123;&quot;sub&quot;:&quot;1234567890&quot;,&quot;name&quot;:&quot;John Doe&quot;,&quot;admin&quot;:true&#125; 然后将其进行base64加密，得到Jwt的第二部分。 1eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9 签证（signature） jwt的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的) payload (base64后的) secret 这个部分需要base64加密后的header和base64加密后的payload使用.连接组成的字符串，然后通过header中声明的加密方式进行加盐secret组合加密，然后就构成了jwt的第三部分。 1TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 将这三部分用.连接成一个完整的字符串,构成了最终的jwt: 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥，在任何场景都不应该流露出去。一旦客户端得知这个secret, 那就意味着客户端是可以自我签发jwt了。 4.4 JJWT的介绍和使用JJWT是一个提供端到端的JWT创建和验证的Java库。永远免费和开源(Apache License，版本2.0)，JJWT很容易使用和理解。它被设计成一个以建筑为中心的流畅界面，隐藏了它的大部分复杂性。 官方文档： https://github.com/jwtk/jjwt 4.4.1 创建TOKEN(1)依赖引入 在changgou-parent项目中的pom.xml中添加依赖： 123456&lt;!--鉴权--&gt;&lt;dependency&gt; &lt;groupId&gt;io.jsonwebtoken&lt;/groupId&gt; &lt;artifactId&gt;jjwt&lt;/artifactId&gt; &lt;version&gt;0.9.0&lt;/version&gt;&lt;/dependency&gt; (2)创建测试 在changgou-common的&#x2F;test&#x2F;java下创建测试类，并设置测试方法 12345678910111213141516public class JwtTest &#123; /**** * 创建Jwt令牌 */ @Test public void testCreateJwt()&#123; JwtBuilder builder= Jwts.builder() .setId(&quot;888&quot;) //设置唯一编号 .setSubject(&quot;小白&quot;) //设置主题 可以是JSON数据 .setIssuedAt(new Date()) //设置签发日期 .signWith(SignatureAlgorithm.HS256,&quot;itcast&quot;);//设置签名 使用HS256算法，并设置SecretKey(字符串) //构建 并返回一个字符串 System.out.println( builder.compact() ); &#125;&#125; 运行打印结果： 1eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjIyODd9.RBLpZ79USMplQyfJCZFD2muHV_KLks7M1ZsjTu6Aez4 再次运行，会发现每次运行的结果是不一样的，因为我们的载荷中包含了时间。 4.4.2 TOKEN解析我们刚才已经创建了token ，在web应用中这个操作是由服务端进行然后发给客户端，客户端在下次向服务端发送请求时需要携带这个token（这就好像是拿着一张门票一样），那服务端接到这个token 应该解析出token中的信息（例如用户id）,根据这些信息查询数据库返回相应的结果。 123456789101112/*** * 解析Jwt令牌数据 */@Testpublic void testParseJwt()&#123; String compactJwt=&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjIyODd9.RBLpZ79USMplQyfJCZFD2muHV_KLks7M1ZsjTu6Aez4&quot;; Claims claims = Jwts.parser(). setSigningKey(&quot;itcast&quot;). parseClaimsJws(compactJwt). getBody(); System.out.println(claims);&#125; 运行打印效果： 1&#123;jti=888, sub=小白, iat=1562062287&#125; 试着将token或签名秘钥篡改一下，会发现运行时就会报错，所以解析token也就是验证token. 4.4.3 设置过期时间有很多时候，我们并不希望签发的token是永久生效的，所以我们可以为token添加一个过期时间。 4.4.3.1 token过期设置 解释： 1.setExpiration(date)//用于设置过期时间 ，参数为Date类型数据 运行，打印效果如下： 1eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjI5MjUsImV4cCI6MTU2MjA2MjkyNX0._vs4METaPkCza52LuN0-2NGGWIIO7v51xt40DHY1U1Q 4.4.3.2 解析TOKEN123456789101112/*** * 解析Jwt令牌数据 */@Testpublic void testParseJwt()&#123; String compactJwt=&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjI5MjUsImV4cCI6MTU2MjA2MjkyNX0._vs4METaPkCza52LuN0-2NGGWIIO7v51xt40DHY1U1Q&quot;; Claims claims = Jwts.parser(). setSigningKey(&quot;itcast&quot;). parseClaimsJws(compactJwt). getBody(); System.out.println(claims);&#125; 打印效果： 当前时间超过过期时间，则会报错。 4.4.4 自定义claims我们刚才的例子只是存储了id和subject两个信息，如果你想存储更多的信息（例如角色）可以定义自定义claims。 创建测试类，并设置测试方法： 创建token: 运行打印效果： 1eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjMyOTIsImFkZHJlc3MiOiLmt7HlnLPpu5Hpqazorq3nu4PokKXnqIvluo_lkZjkuK3lv4MiLCJuYW1lIjoi546L5LqUIiwiYWdlIjoyN30.ZSbHt5qrxz0F1Ma9rVHHAIy4jMCBGIHoNaaPQXxV_dk 解析TOKEN: 123456789101112/*** * 解析Jwt令牌数据 */@Testpublic void testParseJwt()&#123; String compactJwt=&quot;eyJhbGciOiJIUzI1NiJ9.eyJqdGkiOiI4ODgiLCJzdWIiOiLlsI_nmb0iLCJpYXQiOjE1NjIwNjMyOTIsImFkZHJlc3MiOiLmt7HlnLPpu5Hpqazorq3nu4PokKXnqIvluo_lkZjkuK3lv4MiLCJuYW1lIjoi546L5LqUIiwiYWdlIjoyN30.ZSbHt5qrxz0F1Ma9rVHHAIy4jMCBGIHoNaaPQXxV_dk&quot;; Claims claims = Jwts.parser(). setSigningKey(&quot;itcast&quot;). parseClaimsJws(compactJwt). getBody(); System.out.println(claims);&#125; 运行效果： 4.5 鉴权处理4.5.1 思路分析 123456781.用户通过访问微服务网关调用微服务，同时携带头文件信息2.在微服务网关这里进行拦截，拦截后获取用户要访问的路径3.识别用户访问的路径是否需要登录，如果需要，识别用户的身份是否能访问该路径[这里可以基于数据库设计一套权限]4.如果需要权限访问，用户已经登录，则放行5.如果需要权限访问，且用户未登录，则提示用户需要登录6.用户通过网关访问用户微服务，进行登录验证7.验证通过后，用户微服务会颁发一个令牌给网关，网关会将用户信息封装到头文件中，并响应用户8.用户下次访问，携带头文件中的令牌信息即可识别是否登录 4.5.2用户登录签发TOKEN(1)生成令牌工具类 在changgou-common中创建类entity.JwtUtil，主要辅助生成Jwt令牌信息，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class JwtUtil &#123; //有效期为 public static final Long JWT_TTL = 3600000L;// 60 * 60 *1000 一个小时 //Jwt令牌信息 public static final String JWT_KEY = &quot;itcast&quot;; public static String createJWT(String id, String subject, Long ttlMillis) &#123; //指定算法 SignatureAlgorithm signatureAlgorithm = SignatureAlgorithm.HS256; //当前系统时间 long nowMillis = System.currentTimeMillis(); //令牌签发时间 Date now = new Date(nowMillis); //如果令牌有效期为null，则默认设置有效期1小时 if(ttlMillis==null)&#123; ttlMillis=JwtUtil.JWT_TTL; &#125; //令牌过期时间设置 long expMillis = nowMillis + ttlMillis; Date expDate = new Date(expMillis); //生成秘钥 SecretKey secretKey = generalKey(); //封装Jwt令牌信息 JwtBuilder builder = Jwts.builder() .setId(id) //唯一的ID .setSubject(subject) // 主题 可以是JSON数据 .setIssuer(&quot;admin&quot;) // 签发者 .setIssuedAt(now) // 签发时间 .signWith(signatureAlgorithm, secretKey) // 签名算法以及密匙 .setExpiration(expDate); // 设置过期时间 return builder.compact(); &#125; /** * 生成加密 secretKey * @return */ public static SecretKey generalKey() &#123; byte[] encodedKey = Base64.getEncoder().encode(JwtUtil.JWT_KEY.getBytes()); SecretKey key = new SecretKeySpec(encodedKey, 0, encodedKey.length, &quot;AES&quot;); return key; &#125; /** * 解析令牌数据 * @param jwt * @return * @throws Exception */ public static Claims parseJWT(String jwt) throws Exception &#123; SecretKey secretKey = generalKey(); return Jwts.parser() .setSigningKey(secretKey) .parseClaimsJws(jwt) .getBody(); &#125;&#125; (2) 用户登录成功 则 签发TOKEN，修改登录的方法： 代码如下： 1234567891011121314151617181920/*** * 用户登录 */@RequestMapping(value = &quot;/login&quot;)public Result login(String username,String password)&#123; //查询用户信息 User user = userService.findById(username); if(user!=null &amp;&amp; BCrypt.checkpw(password,user.getPassword()))&#123; //设置令牌信息 Map&lt;String,Object&gt; info = new HashMap&lt;String,Object&gt;(); info.put(&quot;role&quot;,&quot;USER&quot;); info.put(&quot;success&quot;,&quot;SUCCESS&quot;); info.put(&quot;username&quot;,username); //生成令牌 String jwt = JwtUtil.createJWT(UUID.randomUUID().toString(), JSON.toJSONString(info),null); return new Result(true,StatusCode.OK,&quot;登录成功！&quot;,jwt); &#125; return new Result(false,StatusCode.LOGINERROR,&quot;账号或者密码错误！&quot;);&#125; 4.5.3 网关过滤器拦截请求处理拷贝JwtUtil到changgou-gateway-web中 4.5.4 自定义全局过滤器创建 过滤器类，如图所示： AuthorizeFilter代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Componentpublic class AuthorizeFilter implements GlobalFilter, Ordered &#123; //令牌头名字 private static final String AUTHORIZE_TOKEN = &quot;Authorization&quot;; /*** * 全局过滤器 * @param exchange * @param chain * @return */ @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; //获取Request、Response对象 ServerHttpRequest request = exchange.getRequest(); ServerHttpResponse response = exchange.getResponse(); //获取请求的URI String path = request.getURI().getPath(); //如果是登录、goods等开放的微服务[这里的goods部分开放],则直接放行,这里不做完整演示，完整演示需要设计一套权限系统 if (path.startsWith(&quot;/api/user/login&quot;) || path.startsWith(&quot;/api/brand/search/&quot;)) &#123; //放行 Mono&lt;Void&gt; filter = chain.filter(exchange); return filter; &#125; //获取头文件中的令牌信息 String tokent = request.getHeaders().getFirst(AUTHORIZE_TOKEN); //如果头文件中没有，则从请求参数中获取 if (StringUtils.isEmpty(tokent)) &#123; tokent = request.getQueryParams().getFirst(AUTHORIZE_TOKEN); &#125; //如果为空，则输出错误代码 if (StringUtils.isEmpty(tokent)) &#123; //设置方法不允许被访问，405错误代码 response.setStatusCode(HttpStatus.METHOD_NOT_ALLOWED); return response.setComplete(); &#125; //解析令牌数据 try &#123; Claims claims = JwtUtil.parseJWT(tokent); &#125; catch (Exception e) &#123; e.printStackTrace(); //解析失败，响应401错误 response.setStatusCode(HttpStatus.UNAUTHORIZED); return response.setComplete(); &#125; //放行 return chain.filter(exchange); &#125; /*** * 过滤器执行顺序 * @return */ @Override public int getOrder() &#123; return 0; &#125;&#125; 4.5.5 配置过滤规则修改网关系统的yml文件： 上述代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354spring: cloud: gateway: globalcors: corsConfigurations: &#x27;[/**]&#x27;: # 匹配所有请求 allowedOrigins: &quot;*&quot; #跨域处理 允许所有的域 allowedMethods: # 支持的方法 - GET - POST - PUT - DELETE routes: - id: changgou_goods_route uri: lb://goods predicates: - Path=/api/album/**,/api/brand/**,/api/cache/**,/api/categoryBrand/**,/api/category/**,/api/para/**,/api/pref/**,/api/sku/**,/api/spec/**,/api/spu/**,/api/stockBack/**,/api/template/** filters: - StripPrefix=1 - name: RequestRateLimiter #请求数限流 名字不能随便写 ，使用默认的facatory args: key-resolver: &quot;#&#123;@ipKeyResolver&#125;&quot; redis-rate-limiter.replenishRate: 1 redis-rate-limiter.burstCapacity: 1 #用户微服务 - id: changgou_user_route uri: lb://user predicates: - Path=/api/user/**,/api/address/**,/api/areas/**,/api/cities/**,/api/provinces/** filters: - StripPrefix=1 application: name: gateway-web #Redis配置 redis: host: 192.168.211.132 port: 6379server: port: 8001eureka: client: service-url: defaultZone: http://127.0.0.1:7001/eureka instance: prefer-ip-address: truemanagement: endpoint: gateway: enabled: true web: exposure: include: true 测试访问http://localhost:8001/api/user/login?username=changgou&amp;password=changgou，效果如下： 测试访问http://localhost:8001/api/user，效果如下： 参考官方手册： https://cloud.spring.io/spring-cloud-gateway/spring-cloud-gateway.html#_stripprefix_gatewayfilter_factory 4.6 会话保持用户每次请求的时候，我们都需要获取令牌数据，方法有多重，可以在每次提交的时候，将数据提交到头文件中，也可以将数据存储到Cookie中，每次从Cookie中校验数据，还可以每次将令牌数据以参数的方式提交到网关，这里面采用Cookie的方式比较容易实现。 4.6.1 登录封装Cookie修改user微服务，每次登录的时候，添加令牌信息到Cookie中，修改changgou-service-user的com.changgou.user.controller.UserController的login方法，代码如下： 4.6.2 过滤器获取令牌数据每次在网关中通过过滤器获取Cookie中的令牌，然后对令牌数据进行解析，修改微服务网关changgou-gateway-web中的AuthorizeFilter，代码如下： 登录后测试，可以识别用户身份，不登录无法识别。如下访问http://localhost:8001/api/user会携带令牌数据： 4.6.3 添加Header信息我们还可以在Gateway的全局过滤器中添加请求头信息，例如可以讲令牌信息添加到请求头中，在微服务中获取头信息，如下代码： 修改微服务网关中的AuthorizeFilter过滤器，在令牌信息校验那块将令牌加入到请求头中，如下代码： 在changgou-service-user微服务的UserController的findAll方法中获取请求头测试，代码如下： 后台输出令牌数据如下： 不存储密码明文而是密文 2 认证技术方案2.1 单点登录技术方案分布式系统要实现单点登录，通常将认证系统独立抽取出来，并且将用户身份信息存储在单独的存储介质，比如： MySQL、Redis，考虑性能要求，通常存储在Redis中，如下图： 单点登录的特点是： 1231、认证系统为独立的系统。 2、各子系统通过Http或其它协议与认证系统通信，完成用户认证。 3、用户身份信息存储在Redis集群。 Java中有很多用户认证的框架都可以实现单点登录： 1231、Apache Shiro. 2、CAS 3、Spring security CAS Oauth2.0 2.3 Spring security Oauth2认证解决方案本项目采用 Spring security + Oauth2完成用户认证及用户授权，Spring security 是一个强大的和高度可定制的身份验证和访问控制框架，Spring security 框架集成了Oauth2协议，下图是项目认证架构图： 1、用户请求认证服务完成认证。 2、认证服务下发用户身份令牌，拥有身份令牌表示身份合法。 3、用户携带令牌请求资源服务，请求资源服务必先经过网关。 4、网关校验用户身份令牌的合法，不合法表示用户没有登录，如果合法则放行继续访问。 5、资源服务获取令牌，根据令牌完成授权。 6、资源服务完成授权则响应资源信息。 4 资源服务授权 具体实操了解 4.1 资源服务授权流程(1)传统授权流程 资源服务器授权流程如上图，客户端先去授权服务器申请令牌，申请令牌后，携带令牌访问资源服务器，资源服务器访问授权服务校验令牌的合法性，授权服务会返回校验结果，如果校验成功会返回用户信息给资源服务器，资源服务器如果接收到的校验结果通过了，则返回资源给客户端。 传统授权方法的问题是用户每次请求资源服务，资源服务都需要携带令牌访问认证服务去校验令牌的合法性，并根 据令牌获取用户的相关信息，性能低下。 (2)公钥私钥授权流程 传统的授权模式性能低下，每次都需要请求授权服务校验令牌合法性，我们可以利用公钥私钥完成对令牌的加密，如果加密解密成功，则表示令牌合法，如果加密解密失败，则表示令牌无效不合法，合法则允许访问资源服务器的资源，解密失败，则不允许访问资源服务器资源。 上图的业务流程如下: 123451、客户端请求认证服务申请令牌2、认证服务生成令牌认证服务采用非对称加密算法，使用私钥生成令牌。3、客户端携带令牌访问资源服务客户端在Http header 中添加： Authorization：Bearer 令牌。4、资源服务请求认证服务校验令牌的有效性资源服务接收到令牌，使用公钥校验令牌的合法性。5、令牌有效，资源服务向客户端响应资源信息 5.2 认证服务5.2.1 认证需求分析认证服务需要实现的功能如下： 1、登录接口 前端post提交账号、密码等，用户身份校验通过，生成令牌，并将令牌写入cookie。 2、退出接口 校验当前用户的身份为合法并且为已登录状态。 将令牌从cookie中删除。","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"文档","slug":"畅购商城/文档","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:02:12.831Z","comments":true,"path":"2022/09/01/畅购商城/文档/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E6%96%87%E6%A1%A3/","excerpt":"","text":"","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"秒杀模块","slug":"畅购商城/秒杀模块","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T13:00:38.683Z","comments":true,"path":"2022/09/01/畅购商城/秒杀模块/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E7%A7%92%E6%9D%80%E6%A8%A1%E5%9D%97/","excerpt":"","text":"1 秒杀业务分析1.1 需求分析所谓“秒杀”，就是网络卖家发布一些超低价格的商品，所有买家在同一时间网上抢购的一种销售方式。通俗一点讲就是网络商家为促销等目的组织的网上限时抢购活动。由于商品价格低廉，往往一上架就被抢购一空，有时只用一秒钟。 秒杀商品通常有两种限制：库存限制、时间限制。 需求： 12345（1）录入秒杀商品数据，主要包括：商品标题、原价、秒杀价、商品图片、介绍、秒杀时段等信息（2）秒杀频道首页列出秒杀商品（进行中的）点击秒杀商品图片跳转到秒杀商品详细页。（3）商品详细页显示秒杀商品信息，点击立即抢购实现秒杀下单，下单时扣减库存。当库存为0或不在活动期范围内时无法秒杀。（4）秒杀下单成功，直接跳转到支付页面（微信扫码），支付成功，跳转到成功页，填写收货地址、电话、收件人等信息，完成订单。（5）当用户秒杀下单5分钟内未支付，取消预订单，调用微信支付的关闭订单接口，恢复库存。 1.2 表结构说明秒杀商品信息表 123456789101112131415161718CREATE TABLE `tb_seckill_goods` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `sup_id` bigint(20) DEFAULT NULL COMMENT &#x27;spu ID&#x27;, `sku_id` bigint(20) DEFAULT NULL COMMENT &#x27;sku ID&#x27;, `name` varchar(100) DEFAULT NULL COMMENT &#x27;标题&#x27;, `small_pic` varchar(150) DEFAULT NULL COMMENT &#x27;商品图片&#x27;, `price` decimal(10,2) DEFAULT NULL COMMENT &#x27;原价格&#x27;, `cost_price` decimal(10,2) DEFAULT NULL COMMENT &#x27;秒杀价格&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;添加日期&#x27;, `check_time` datetime DEFAULT NULL COMMENT &#x27;审核日期&#x27;, `status` char(1) DEFAULT NULL COMMENT &#x27;审核状态，0未审核，1审核通过，2审核不通过&#x27;, `start_time` datetime DEFAULT NULL COMMENT &#x27;开始时间&#x27;, `end_time` datetime DEFAULT NULL COMMENT &#x27;结束时间&#x27;, `num` int(11) DEFAULT NULL COMMENT &#x27;秒杀商品数&#x27;, `stock_count` int(11) DEFAULT NULL COMMENT &#x27;剩余库存数&#x27;, `introduction` varchar(2000) DEFAULT NULL COMMENT &#x27;描述&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8; 秒杀订单表 1234567891011121314CREATE TABLE `tb_seckill_order` ( `id` bigint(20) NOT NULL COMMENT &#x27;主键&#x27;, `seckill_id` bigint(20) DEFAULT NULL COMMENT &#x27;秒杀商品ID&#x27;, `money` decimal(10,2) DEFAULT NULL COMMENT &#x27;支付金额&#x27;, `user_id` varchar(50) DEFAULT NULL COMMENT &#x27;用户&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;创建时间&#x27;, `pay_time` datetime DEFAULT NULL COMMENT &#x27;支付时间&#x27;, `status` char(1) DEFAULT NULL COMMENT &#x27;状态，0未支付，1已支付&#x27;, `receiver_address` varchar(200) DEFAULT NULL COMMENT &#x27;收货人地址&#x27;, `receiver_mobile` varchar(20) DEFAULT NULL COMMENT &#x27;收货人电话&#x27;, `receiver` varchar(20) DEFAULT NULL COMMENT &#x27;收货人&#x27;, `transaction_id` varchar(30) DEFAULT NULL COMMENT &#x27;交易流水&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 1.3 秒杀需求分析秒杀技术实现核心思想是运用缓存减少数据库瞬间的访问压力！读取商品详细信息时运用缓存，当用户点击抢购时减少缓存中的库存数量，当库存数为0时或活动期结束时，同步到数据库。 产生的秒杀预订单也不会立刻写到数据库中，而是先写到缓存，当用户付款成功后再写入数据库。 当然，上面实现的思路只是一种最简单的方式，并未考虑其中一些问题，例如并发状况容易产生的问题。我们看看下面这张思路更严谨的图： 2 秒杀商品压入缓存 我们这里秒杀商品列表和秒杀商品详情都是从Redis中取出来的，所以我们首先要将符合参与秒杀的商品定时查询出来，并将数据存入到Redis缓存中。 数据存储类型我们可以选择Hash类型。 秒杀分页列表这里可以通过获取redisTemplate.boundHashOps(key).values()获取结果数据。 秒杀商品详情，可以通过redisTemplate.boundHashOps(key).get(key)获取详情。 2.1 秒杀服务工程我们将商品数据压入到Reids缓存，可以在秒杀工程的服务工程中完成，可以按照如下步骤实现： 12345671.查询活动没结束的所有秒杀商品 1)状态必须为审核通过 status=1 2)商品库存个数&gt;0 3)活动没有结束 endTime&gt;=now() 4)在Redis中没有该商品的缓存 5)执行查询获取对应的结果集2.将活动没有结束的秒杀商品入库 我们首先搭建一个秒杀服务工程，然后按照上面步骤实现。 6 多线程抢单6.1 实现思路分析 在审视秒杀中，操作一般都是比较复杂的，而且并发量特别高，比如，检查当前账号操作是否已经秒杀过该商品，检查该账号是否存在存在刷单行为，记录用户操作日志等。 下订单这里，我们一般采用多线程下单，但多线程中我们又需要保证用户抢单的公平性，也就是先抢先下单。我们可以这样实现，用户进入秒杀抢单，如果用户复合抢单资格，只需要记录用户抢单数据，存入队列，多线程从队列中进行消费即可，存入队列采用左压，多线程下单采用右取的方式。 6.2 异步实现要想使用Spring的异步操作，需要先开启异步操作，用@EnableAsync注解开启，然后在对应的异步方法上添加注解@Async即可。 创建com.changgou.seckill.task.MultiThreadingCreateOrder类，在类中创建一个createOrder方法，并在方法上添加@Async,代码如下： 1234567891011121314151617@Componentpublic class MultiThreadingCreateOrder &#123; /*** * 多线程下单操作 */ @Async public void createOrder()&#123; try &#123; System.out.println(&quot;准备执行....&quot;); Thread.sleep(20000); System.out.println(&quot;开始执行....&quot;); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 上面createOrder方法进行了休眠阻塞操作，我们在下单的方法调用createOrder方法，如果下单的方法没有阻塞，继续执行，说明属于异步操作，如果阻塞了，说明没有执行异步操作。 修改秒杀抢单SeckillOrderServiceImpl代码，注入MultiThreadingCreateOrder,并调用createOrder方法，代码如下： 使用Postman测试如下： http://localhost:18084/seckill/order/add?id=1131814847898587136&amp;time=2019052510 6.3 多线程抢单 用户每次下单的时候，我们都让他们先进行排队，然后采用多线程的方式创建订单，排队我们可以采用Redis的队列实现，多线程下单我们可以采用Spring的异步实现。 6.3.1 多线程下单将之前下单的代码全部挪到多线程的方法中，com.changgou.seckill.service.impl.SeckillOrderServiceImpl类的方法值负责调用即可，代码如下： 多线程下单代码如下图： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Componentpublic class MultiThreadingCreateOrder &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private SeckillGoodsMapper seckillGoodsMapper; @Autowired private IdWorker idWorker; /*** * 多线程下单操作 */ @Async public void createOrder()&#123; try &#123; //时间区间 String time = &quot;2019052510&quot;; //用户登录名 String username=&quot;szitheima&quot;; //用户抢购商品 Long id = 1131814847898587136L; //获取商品数据 SeckillGoods goods = (SeckillGoods) redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).get(id); //如果没有库存，则直接抛出异常 if(goods==null || goods.getStockCount()&lt;=0)&#123; throw new RuntimeException(&quot;已售罄!&quot;); &#125; //如果有库存，则创建秒杀商品订单 SeckillOrder seckillOrder = new SeckillOrder(); seckillOrder.setId(idWorker.nextId()); seckillOrder.setSeckillId(id); seckillOrder.setMoney(goods.getCostPrice()); seckillOrder.setUserId(username); seckillOrder.setCreateTime(new Date()); seckillOrder.setStatus(&quot;0&quot;); //将秒杀订单存入到Redis中 redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).put(username,seckillOrder); //库存减少 goods.setStockCount(goods.getStockCount()-1); //判断当前商品是否还有库存 if(goods.getStockCount()&lt;=0)&#123; //并且将商品数据同步到MySQL中 seckillGoodsMapper.updateByPrimaryKeySelective(goods); //如果没有库存,则清空Redis缓存中该商品 redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).delete(id); &#125;else&#123; //如果有库存，则直数据重置到Reids中 redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).put(id,goods); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 此时测试，是可以正常下单的，但是用户名和订单都写死了，此处需要继续优化。 6.3.2 排队下单6.3.2.1 排队信息封装用户每次下单的时候，我们可以创建一个队列进行排队，然后采用多线程的方式创建订单，排队我们可以采用Redis的队列实现。 排队信息中需要有用户抢单的商品信息，主要包含商品ID，商品抢购时间段，用户登录名。我们可以设计个javabean，如下： 1234567891011121314151617181920212223242526272829303132public class SeckillStatus implements Serializable &#123; //秒杀用户名 private String username; //创建时间 private Date createTime; //秒杀状态 1:排队中，2:秒杀等待支付,3:支付超时，4:秒杀失败,5:支付完成 private Integer status; //秒杀的商品ID private Long goodsId; //应付金额 private Float money; //订单号 private Long orderId; //时间段 private String time; public SeckillStatus() &#123; &#125; public SeckillStatus(String username, Date createTime, Integer status, Long goodsId, String time) &#123; this.username = username; this.createTime = createTime; this.status = status; this.goodsId = goodsId; this.time = time; &#125; //get、set...略&#125; 6.3.2.2 排队实现我们可以将秒杀抢单信息存入到Redis中,这里采用List方式存储,List本身是一个队列，用户点击抢购的时候，就将用户抢购信息存入到Redis中，代码如下： 123456789101112131415161718192021222324252627286@Servicepublic class SeckillOrderServiceImpl implements SeckillOrderService &#123; @Autowired private MultiThreadingCreateOrder multiThreadingCreateOrder; @Autowired private RedisTemplate redisTemplate; /**** * 添加订单 * @param id * @param time * @param username */ @Override public Boolean add(Long id, String time, String username)&#123; //排队信息封装 SeckillStatus seckillStatus = new SeckillStatus(username, new Date(),1, id,time); //将秒杀抢单信息存入到Redis中,这里采用List方式存储,List本身是一个队列 redisTemplate.boundListOps(&quot;SeckillOrderQueue&quot;).leftPush(seckillStatus); //多线程操作 multiThreadingCreateOrder.createOrder(); return true; &#125;&#125; 多线程每次从队列中获取数据，分别获取用户名和订单商品编号以及商品秒杀时间段，进行下单操作，代码如下： 上图代码如下： 12345678910111213141516171819202122/*** * 多线程下单操作 */@Asyncpublic void createOrder()&#123; //从队列中获取排队信息 SeckillStatus seckillStatus = (SeckillStatus) redisTemplate.boundListOps(&quot;SeckillOrderQueue&quot;).rightPop(); try &#123; if(seckillStatus!=null)&#123; //时间区间 String time = seckillStatus.getTime(); //用户登录名 String username=seckillStatus.getUsername(); //用户抢购商品 Long id = seckillStatus.getGoodsId(); //...略 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 6.3.3 下单状态查询按照上面的流程，虽然可以实现用户下单异步操作，但是并不能确定下单是否成功，所以我们需要做一个页面判断，每过1秒钟查询一次下单状态,多线程下单的时候，需要修改抢单状态，支付的时候，清理抢单状态。 6.3.3.1 下单更新抢单状态用户每次点击抢购的时候，如果排队成功，则将用户抢购状态存储到Redis中，多线程抢单的时候，如果抢单成功，则更新抢单状态。 修改SeckillOrderServiceImpl的add方法，记录状态，代码如下： 上图代码如下： 12//将抢单状态存入到Redis中redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).put(username,seckillStatus); 多线程抢单更新状态，修改MultiThreadingCreateOrder的createOrder方法，代码如下： 上图代码如下： 12345//抢单成功，更新抢单状态,排队-&gt;等待支付seckillStatus.setStatus(2);seckillStatus.setOrderId(seckillOrder.getId());seckillStatus.setMoney(seckillOrder.getMoney().floatValue());redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).put(username,seckillStatus); 6.3.3.2 后台查询抢单状态后台提供抢单状态查询方法，修改SeckillOrderService，添加如下查询方法： 12345/*** * 抢单状态查询 * @param username */SeckillStatus queryStatus(String username); 修改SeckillOrderServiceImpl,添加如下实现方法： 123456789/*** * 抢单状态查询 * @param username * @return */@Overridepublic SeckillStatus queryStatus(String username) &#123; return (SeckillStatus) redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).get(username);&#125; 修改SeckillOrderController,添加如下查询方法： 上图代码如下： 123456789101112131415161718/**** * 查询抢购 * @return */@RequestMapping(value = &quot;/query&quot;)public Result queryStatus()&#123; //获取用户名 String username = tokenDcode.getUserInfo().get(&quot;username&quot;); //根据用户名查询用户抢购状态 SeckillStatus seckillStatus = seckillOrderService.queryStatus(username); if(seckillStatus!=null)&#123; return new Result(true,seckillStatus.getStatus(),&quot;抢购状态&quot;); &#125; //NOTFOUNDERROR =20006,没有对应的抢购数据 return new Result(false,StatusCode.NOTFOUNDERROR,&quot;没有抢购信息&quot;);&#125; 6.3.3.3 测试使用Postman测试查询状态 http://localhost:18084/seckill/order/query 第14章 秒杀学习目标 &#x3D;&#x3D;防止秒杀重复排队&#x3D;&#x3D; 1重复排队：一个人抢购商品，如果没有支付，不允许重复排队抢购 &#x3D;&#x3D;并发超卖问题解决&#x3D;&#x3D; 11个商品卖给多个人：1商品多订单 &#x3D;&#x3D;秒杀订单支付&#x3D;&#x3D; 1秒杀支付：支付流程需要调整 &#x3D;&#x3D;超时支付订单库存回滚&#x3D;&#x3D; 121.RabbitMQ延时队列2.利用延时队列实现支付订单的监听，根据订单支付状况进行订单数据库回滚 1 防止秒杀重复排队用户每次抢单的时候，一旦排队，我们设置一个自增值，让该值的初始值为1，每次进入抢单的时候，对它进行递增，如果值&gt;1，则表明已经排队,不允许重复排队,如果重复排队，则对外抛出异常，并抛出异常信息100表示已经正在排队。 1.1 后台排队记录修改SeckillOrderServiceImpl的add方法，新增递增值判断是否排队中，代码如下： 上图代码如下： 123456//递增，判断是否排队Long userQueueCount = redisTemplate.boundHashOps(&quot;UserQueueCount&quot;).increment(username, 1);if(userQueueCount&gt;1)&#123; //100：表示有重复抢单 throw new RuntimeException(String.valueOf(StatusCode.REPERROR));&#125; 2 并发超卖问题解决超卖问题，这里是指多人抢购同一商品的时候，多人同时判断是否有库存，如果只剩一个，则都会判断有库存，此时会导致超卖现象产生，也就是一个商品下了多个订单的现象。 2.1 思路分析 解决超卖问题，可以利用Redis队列实现，给每件商品创建一个独立的商品个数队列，例如：A商品有2个，A商品的ID为1001，则可以创建一个队列,key&#x3D;SeckillGoodsCountList_1001,往该队列中塞2次该商品ID。 每次给用户下单的时候，先从队列中取数据，如果能取到数据，则表明有库存，如果取不到，则表明没有库存，这样就可以防止超卖问题产生了。 在我们队Redis进行操作的时候，很多时候，都是先将数据查询出来，在内存中修改，然后存入到Redis，在并发场景，会出现数据错乱问题，为了控制数量准确，我们单独将商品数量整一个自增键，自增键是线程安全的，所以不担心并发场景的问题。 2.2 代码实现每次将商品压入Redis缓存的时候，另外多创建一个商品的队列。 修改SeckillGoodsPushTask,添加一个pushIds方法，用于将指定商品ID放入到指定的数字中，代码如下： 12345678910111213/*** * 将商品ID存入到数组中 * @param len:长度 * @param id :值 * @return */public Long[] pushIds(int len,Long id)&#123; Long[] ids = new Long[len]; for (int i = 0; i &lt;ids.length ; i++) &#123; ids[i]=id; &#125; return ids;&#125; 修改SeckillGoodsPushTask的loadGoodsPushRedis方法，添加队列操作，代码如下： 上图代码如下： 12345//商品数据队列存储,防止高并发超卖Long[] ids = pushIds(seckillGood.getStockCount(), seckillGood.getId());redisTemplate.boundListOps(&quot;SeckillGoodsCountList_&quot;+seckillGood.getId()).leftPushAll(ids);//自增计数器redisTemplate.boundHashOps(&quot;SeckillGoodsCount&quot;).increment(seckillGood.getId(),seckillGood.getStockCount()); 2.3 超卖控制修改多线程下单方法，分别修改数量控制，以及售罄后用户抢单排队信息的清理，修改代码如下图： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/*** * 多线程下单操作 */@Asyncpublic void createOrder()&#123; //从队列中获取排队信息 SeckillStatus seckillStatus = (SeckillStatus) redisTemplate.boundListOps(&quot;SeckillOrderQueue&quot;).rightPop(); try &#123; //从队列中获取一个商品 Object sgood = redisTemplate.boundListOps(&quot;SeckillGoodsCountList_&quot; + seckillStatus.getGoodsId()).rightPop(); if(sgood==null)&#123; //清理当前用户的排队信息 clearQueue(seckillStatus); return; &#125; //时间区间 String time = seckillStatus.getTime(); //用户登录名 String username=seckillStatus.getUsername(); //用户抢购商品 Long id = seckillStatus.getGoodsId(); //获取商品数据 SeckillGoods goods = (SeckillGoods) redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).get(id); //如果有库存，则创建秒杀商品订单 SeckillOrder seckillOrder = new SeckillOrder(); seckillOrder.setId(idWorker.nextId()); seckillOrder.setSeckillId(id); seckillOrder.setMoney(goods.getCostPrice()); seckillOrder.setUserId(username); seckillOrder.setCreateTime(new Date()); seckillOrder.setStatus(&quot;0&quot;); //将秒杀订单存入到Redis中 redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).put(username,seckillOrder); //商品库存-1 Long surplusCount = redisTemplate.boundHashOps(&quot;SeckillGoodsCount&quot;).increment(id, -1);//商品数量递减 goods.setStockCount(surplusCount.intValue()); //根据计数器统计 //判断当前商品是否还有库存 if(surplusCount&lt;=0)&#123; //并且将商品数据同步到MySQL中 seckillGoodsMapper.updateByPrimaryKeySelective(goods); //如果没有库存,则清空Redis缓存中该商品 redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).delete(id); &#125;else&#123; //如果有库存，则直数据重置到Reids中 redisTemplate.boundHashOps(&quot;SeckillGoods_&quot; + time).put(id,goods); &#125; //抢单成功，更新抢单状态,排队-&gt;等待支付 seckillStatus.setStatus(2); seckillStatus.setOrderId(seckillOrder.getId()); seckillStatus.setMoney(seckillOrder.getMoney().floatValue()); redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).put(username,seckillStatus); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125;/*** * 清理用户排队信息 * @param seckillStatus */public void clearQueue(SeckillStatus seckillStatus)&#123; //清理排队标示 redisTemplate.boundHashOps(&quot;UserQueueCount&quot;).delete(seckillStatus.getUsername()); //清理抢单标示 redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).delete(seckillStatus.getUsername());&#125; 3 订单支付 完成秒杀下订单后，进入支付页面，此时前端会每3秒中向后台发送一次请求用于判断当前用户订单是否完成支付，如果完成了支付，则需要清理掉排队信息，并且需要修改订单状态信息。 3.2 创建支付二维码下单成功后，会跳转到支付选择页面，在支付选择页面要显示订单编号和订单金额，所以我们需要在下单的时候，将订单金额以及订单编号信息存储到用户查询对象中。 选择微信支付后，会跳转到微信支付页面，微信支付页面会根据用户名查看用户秒杀订单，并根据用户秒杀订单的ID创建预支付信息并获取二维码信息，展示给用户看,此时页面每3秒查询一次支付状态，如果支付成功，需要修改订单状态信息。 3.2.1 回显订单号、金额下单后，进入支付选择页面，需要显示订单号和订单金额，所以需要在用户下单后将该数据传入到pay.html页面，所以查询订单状态的时候，需要将订单号和金额封装到查询的信息中，修改查询订单装的方法加入他们即可。 修改SeckillOrderController的queryStatus方法，代码如下： 上图代码如下： 1return new Result(true,seckillStatus.getStatus(),&quot;抢购状态&quot;,seckillStatus); 使用Postman测试，效果如下： http://localhost:18084/seckill/order/query 3.2.2 创建二维码用户创建二维码，可以先查询用户的秒杀订单抢单信息，然后再发送请求到支付微服务中创建二维码，将订单编号以及订单对应的金额传递到支付微服务:/weixin/pay/create/native。 使用Postman测试效果如下： http://localhost:9022/weixin/pay/create/native?outtradeno=1132510782836314112&amp;money=1 3.3 支付流程分析 如上图，步骤分析如下： 12341.用户抢单，经过秒杀系统实现抢单，下单后会将向MQ发送一个延时队列消息，包含抢单信息，延时半小时后才能监听到2.秒杀系统同时启用延时消息监听，一旦监听到订单抢单信息，判断Redis缓存中是否存在订单信息，如果存在，则回滚3.秒杀系统还启动支付回调信息监听，如果支付完成，则将订单吃句话到MySQL，如果没完成，清理排队信息回滚库存4.每次秒杀下单后调用支付系统，创建二维码，如果用户支付成功了，微信系统会将支付信息发送给支付系统指定的回调地址，支付系统收到信息后，将信息发送给MQ，第3个步骤就可以监听到消息了。 3.4 支付回调更新支付回调这一块代码已经实现了，但之前实现的是订单信息的回调数据发送给MQ，指定了对应的队列，不过现在需要实现的是秒杀信息发送给指定队列，所以之前的代码那块需要动态指定队列。 3.4.1 支付回调队列指定关于指定队列如下： 121.创建支付二维码需要指定队列2.回调地址回调的时候，获取支付二维码指定的队列，将支付信息发送到指定队列中 在微信支付统一下单API中，有一个附加参数,如下： 1attach:附加数据,String(127)，在查询API和支付通知中原样返回，可作为自定义参数使用。 我们可以在创建二维码的时候，指定该参数，该参数用于指定回调支付信息的对应队列，每次回调的时候，会获取该参数，然后将回调信息发送到该参数对应的队列去。 3.4.1.1 改造支付方法修改支付微服务的WeixinPayController的createNative方法，代码如下： 修改支付微服务的WeixinPayService的createNative方法，代码如下： 修改支付微服务的WeixinPayServiceImpl的createNative方法，代码如下： 我们创建二维码的时候，需要将下面几个参数传递过去 1234username:用户名,可以根据用户名查询用户排队信息outtradeno：商户订单号，下单必须money：支付金额，支付必须queue：队列名字，回调的时候，可以知道将支付信息发送到哪个队列 修改WeixinPayApplication，添加对应队列以及对应交换机绑定，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@SpringBootApplication@EnableEurekaClient@EnableFeignClientspublic class WeixinPayApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(WeixinPayApplication.class,args); &#125; @Autowired private Environment env; /*** * 创建DirectExchange交换机 * @return */ @Bean public DirectExchange basicExchange()&#123; return new DirectExchange(env.getProperty(&quot;mq.pay.exchange.order&quot;), true,false); &#125; /*** * 创建队列 * @return */ @Bean(name = &quot;queueOrder&quot;) public Queue queueOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.order&quot;), true); &#125; /*** * 创建秒杀队列 * @return */ @Bean(name = &quot;queueSeckillOrder&quot;) public Queue queueSeckillOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.seckillorder&quot;), true); &#125; /**** * 队列绑定到交换机上 * @return */ @Bean public Binding basicBindingOrder()&#123; return BindingBuilder .bind(queueOrder()) .to(basicExchange()) .with(env.getProperty(&quot;mq.pay.routing.orderkey&quot;)); &#125; /**** * 队列绑定到交换机上 * @return */ @Bean public Binding basicBindingSeckillOrder()&#123; return BindingBuilder .bind(queueSeckillOrder()) .to(basicExchange()) .with(env.getProperty(&quot;mq.pay.routing.seckillorderkey&quot;)); &#125;&#125; 修改application.yml，添加如下配置 123456789101112#位置支付交换机和队列mq: pay: exchange: order: exchange.order seckillorder: exchange.seckillorder queue: order: queue.order seckillorder: queue.seckillorder routing: key: queue.order seckillkey: queue.seckillorder 3.4.1.2 测试使用Postman创建二维码测试 http://localhost:18092/weixin/pay/create/native?username=szitheima&amp;out_trade_no=1132510782836314121&amp;total_fee=1&amp;queue=queue.seckillorder&amp;routingkey=queue.seckillorder&amp;exchange=exchange.seckillorder 以后每次支付，都需要带上对应的参数，包括前面的订单支付。 3.4.1.3 改造支付回调方法修改com.changgou.pay.controller.WeixinPayController的notifyUrl方法，获取自定义参数，并转成Map，获取queue地址，并将支付信息发送到绑定的queue中，代码如下： 3.4.2 支付状态监听支付状态通过回调地址发送给MQ之后，我们需要在秒杀系统中监听支付信息，如果用户已支付，则修改用户订单状态，如果支付失败，则直接删除订单，回滚库存。 在秒杀工程中创建com.changgou.seckill.consumer.SeckillOrderPayMessageListener,实现监听消息，代码如下: 1234567891011121314151617@Component@RabbitListener(queues = &quot;$&#123;mq.pay.queue.seckillorder&#125;&quot;)public class SeckillOrderPayMessageListener &#123; /** * 监听消费消息 * @param message */ @RabbitHandler public void consumeMessage(@Payload String message)&#123; System.out.println(message); //将消息转换成Map对象 Map&lt;String,String&gt; resultMap = JSON.parseObject(message,Map.class); System.out.println(&quot;监听到的消息:&quot;+resultMap); &#125;&#125; 修改SeckillApplication创建对应的队列以及绑定对应交换机。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@SpringBootApplication@EnableEurekaClient@EnableFeignClients@MapperScan(basePackages = &#123;&quot;com.changgou.seckill.dao&quot;&#125;)@EnableScheduling@EnableAsyncpublic class SeckillApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SeckillApplication.class,args); &#125; @Bean public IdWorker idWorker()&#123; return new IdWorker(1,1); &#125; @Autowired private Environment env; /*** * 创建DirectExchange交换机 * @return */ @Bean public DirectExchange basicExchange()&#123; return new DirectExchange(env.getProperty(&quot;mq.pay.exchange.order&quot;), true,false); &#125; /*** * 创建队列 * @return */ @Bean(name = &quot;queueOrder&quot;) public Queue queueOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.order&quot;), true); &#125; /*** * 创建秒杀队列 * @return */ @Bean(name = &quot;queueSeckillOrder&quot;) public Queue queueSeckillOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.seckillorder&quot;), true); &#125; /**** * 队列绑定到交换机上 * @return */ @Bean public Binding basicBindingOrder()&#123; return BindingBuilder .bind(queueOrder()) .to(basicExchange()) .with(env.getProperty(&quot;mq.pay.routing.orderkey&quot;)); &#125; /**** * 队列绑定到交换机上 * @return */ @Bean public Binding basicBindingSeckillOrder()&#123; return BindingBuilder .bind(queueSeckillOrder()) .to(basicExchange()) .with(env.getProperty(&quot;mq.pay.routing.seckillorderkey&quot;)); &#125;&#125; 修改application.yml文件，添加如下配置： 123456789101112#位置支付交换机和队列mq: pay: exchange: order: exchange.order seckillorder: exchange.seckillorder queue: order: queue.order seckillorder: queue.seckillorder routing: key: queue.order seckillkey: queue.seckillorder 3.4.3 修改订单状态监听到支付信息后，根据支付信息判断，如果用户支付成功，则修改订单信息，并将订单入库，删除用户排队信息，如果用户支付失败，则删除订单信息，回滚库存，删除用户排队信息。 3.4.3.1 业务层修改SeckillOrderService，添加修改订单方法，代码如下 1234567/*** * 更新订单状态 * @param out_trade_no * @param transaction_id * @param username */void updatePayStatus(String out_trade_no, String transaction_id,String username); 修改SeckillOrderServiceImpl，添加修改订单方法实现，代码如下： 123456789101112131415161718192021222324252627/*** * 更新订单状态 * @param out_trade_no * @param transaction_id * @param username */@Overridepublic void updatePayStatus(String out_trade_no, String transaction_id,String username) &#123; //订单数据从Redis数据库查询出来 SeckillOrder seckillOrder = (SeckillOrder) redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).get(username); //修改状态 seckillOrder.setStatus(&quot;1&quot;); //支付时间 seckillOrder.setPayTime(new Date()); //同步到MySQL中 seckillOrderMapper.insertSelective(seckillOrder); //清空Redis缓存 redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).delete(username); //清空用户排队数据 redisTemplate.boundHashOps(&quot;UserQueueCount&quot;).delete(username); //删除抢购状态信息 redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).delete(username);&#125; 3.4.3.2 修改订单对接修改微信支付状态监听的代码，当用户支付成功后，修改订单状态，也就是调用上面的方法，代码如下： 3.4.4 删除订单回滚库存如果用户支付失败，我们需要删除用户订单数据，并回滚库存。 3.4.4.1 业务层实现修改SeckillOrderService，创建一个关闭订单方法，代码如下： 1234/*** * 关闭订单，回滚库存 */void closeOrder(String username); 修改SeckillOrderServiceImpl，创建一个关闭订单实现方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839/*** * 关闭订单，回滚库存 * @param username */@Overridepublic void closeOrder(String username) &#123; //将消息转换成SeckillStatus SeckillStatus seckillStatus = (SeckillStatus) redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).get(username); //获取Redis中订单信息 SeckillOrder seckillOrder = (SeckillOrder) redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).get(username); //如果Redis中有订单信息，说明用户未支付 if(seckillStatus!=null &amp;&amp; seckillOrder!=null)&#123; //删除订单 redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).delete(username); //回滚库存 //1)从Redis中获取该商品 SeckillGoods seckillGoods = (SeckillGoods) redisTemplate.boundHashOps(&quot;SeckillGoods_&quot;+seckillStatus.getTime()).get(seckillStatus.getGoodsId()); //2)如果Redis中没有，则从数据库中加载 if(seckillGoods==null)&#123; seckillGoods = seckillGoodsMapper.selectByPrimaryKey(seckillStatus.getGoodsId()); &#125; //3)数量+1 (递增数量+1，队列数量+1) Long surplusCount = redisTemplate.boundHashOps(&quot;SeckillGoodsCount&quot;).increment(seckillStatus.getGoodsId(), 1); seckillGoods.setStockCount(surplusCount.intValue()); redisTemplate.boundListOps(&quot;SeckillGoodsCountList_&quot; + seckillStatus.getGoodsId()).leftPush(seckillStatus.getGoodsId()); //4)数据同步到Redis中 redisTemplate.boundHashOps(&quot;SeckillGoods_&quot;+seckillStatus.getTime()).put(seckillStatus.getGoodsId(),seckillGoods); //清理排队标示 redisTemplate.boundHashOps(&quot;UserQueueCount&quot;).delete(seckillStatus.getUsername()); //清理抢单标示 redisTemplate.boundHashOps(&quot;UserQueueStatus&quot;).delete(seckillStatus.getUsername()); &#125;&#125; 3.4.4.2 调用删除订单修改SeckillOrderPayMessageListener，在用户支付失败后调用关闭订单方法，代码如下： 12//支付失败,删除订单seckillOrderService.closeOrder(attachMap.get(&quot;username&quot;)); 3.4.4.3 测试使用Postman完整请求创建二维码下单测试一次。 商品ID：1131814854034853888 数量：49 下单： http://localhost:18084/seckill/order/add?id=1131814854034853888&amp;time=2019052614 下单后，Redis数据 下单查询： http://localhost:18084/seckill/order/query 创建二维码： http://localhost:9022/weixin/pay/create/native?username=szitheima&amp;outtradeno=1132530879663575040&amp;money=1&amp;queue=queue.seckillorder 秒杀抢单后，商品数量变化： 支付微服务回调方法控制台： 12345678910111213&#123; nonce_str=Mnv06RIaIwxzg3bA, code_url=weixin://wxpay/bizpayurl?pr=iTidd5h, appid=wx8397f8696b538317, sign=1436E43FBA8A171D79A9B78B61F0A7AB, trade_type=NATIVE, return_msg=OK, result_code=SUCCESS, mch_id=1473426802, return_code=SUCCESS, prepay_id=wx2614182102123859e3869a853739004200&#125;&#123;money=1, queue=queue.seckillorder, username=szitheima, outtradeno=1132530879663575040&#125; 订单微服务控制台输出 123456789101112131415161718192021&#123; transaction_id=4200000289201905268232990890, nonce_str=a1aefe00a9bc4e8bb66a892dba38eb42, bank_type=CMB_CREDIT, openid=oNpSGwUp-194-Svy3JnVlAxtdLkc, sign=56679BC02CC82204635434817C1FCA46, fee_type=CNY, mch_id=1473426802, cash_fee=1, out_trade_no=1132530879663575040, appid=wx8397f8696b538317, total_fee=1, trade_type=NATIVE, result_code=SUCCESS, attach=&#123; &quot;username&quot;: &quot;szitheima&quot;, &quot;outtradeno&quot;: &quot;1132530879663575040&quot;, &quot;money&quot;: &quot;1&quot;, &quot;queue&quot;: &quot;queue.seckillorder&quot; &#125;, time_end=20190526141849, is_subscribe=N, return_code=SUCCESS&#125; 附录： 支付微服务application.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849server: port: 9022spring: application: name: pay main: allow-bean-definition-overriding: true rabbitmq: host: 127.0.0.1 #mq的服务器地址 username: guest #账号 password: guest #密码eureka: client: service-url: defaultZone: http://127.0.0.1:6868/eureka instance: prefer-ip-address: truefeign: hystrix: enabled: true#hystrix 配置hystrix: command: default: execution: timeout: #如果enabled设置为false，则请求超时交给ribbon控制 enabled: true isolation: strategy: SEMAPHORE#微信支付信息配置weixin: appid: wx8397f8696b538317 partner: 1473426802 partnerkey: T6m9iK73b0kn9g5v426MKfHQH7X8rKwb notifyurl: http://2cw4969042.wicp.vip:36446/weixin/pay/notify/url#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order seckillorder: queue.seckillorder routing: orderkey: queue.order seckillorderkey: queue.seckillorder 秒杀微服务application.yml配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354server: port: 18084spring: application: name: seckill datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://127.0.0.1:3306/changgou_seckill?useUnicode=true&amp;characterEncoding=UTF-8&amp;serverTimezone=UTC username: root password: itcast rabbitmq: host: 127.0.0.1 #mq的服务器地址 username: guest #账号 password: guest #密码 main: allow-bean-definition-overriding: trueeureka: client: service-url: defaultZone: http://127.0.0.1:6868/eureka instance: prefer-ip-address: truefeign: hystrix: enabled: truemybatis: configuration: map-underscore-to-camel-case: true mapper-locations: classpath:mapper/*Mapper.xml type-aliases-package: com.changgou.seckill.pojo#hystrix 配置hystrix: command: default: execution: timeout: #如果enabled设置为false，则请求超时交给ribbon控制 enabled: true isolation: thread: timeoutInMilliseconds: 10000 strategy: SEMAPHORE#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order seckillorder: queue.seckillorder routing: orderkey: queue.order seckillorderkey: queue.seckillorder 4 RabbitMQ延时消息队列4.1 延时队列介绍延时队列即放置在该队列里面的消息是不需要立即消费的，而是等待一段时间之后取出消费。那么，为什么需要延迟消费呢？我们来看以下的场景 网上商城下订单后30分钟后没有完成支付，取消订单(如：淘宝、去哪儿网)系统创建了预约之后，需要在预约时间到达前一小时提醒被预约的双方参会系统中的业务失败之后，需要重试这些场景都非常常见，我们可以思考，比如第二个需求，系统创建了预约之后，需要在预约时间到达前一小时提醒被预约的双方参会。那么一天之中肯定是会有很多个预约的，时间也是不一定的，假设现在有1点 2点 3点 三个预约，如何让系统知道在当前时间等于0点 1点 2点给用户发送信息呢，是不是需要一个轮询，一直去查看所有的预约，比对当前的系统时间和预约提前一小时的时间是否相等呢？这样做非常浪费资源而且轮询的时间间隔不好控制。如果我们使用延时消息队列呢，我们在创建时把需要通知的预约放入消息中间件中，并且设置该消息的过期时间，等过期时间到达时再取出消费即可。 Rabbitmq实现延时队列一般而言有两种形式：第一种方式：利用两个特性： Time To Live(TTL)、Dead Letter Exchanges（DLX）[A队列过期-&gt;转发给B队列] 第二种方式：利用rabbitmq中的插件x-delay-message 4.2 TTL DLX实现延时队列4.2.1 TTL DLX介绍TTLRabbitMQ可以针对队列设置x-expires(则队列中所有的消息都有相同的过期时间)或者针对Message设置x-message-ttl(对消息进行单独设置，每条消息TTL可以不同)，来控制消息的生存时间，如果超时(两者同时设置以最先到期的时间为准)，则消息变为dead letter(死信) Dead Letter Exchanges（DLX）RabbitMQ的Queue可以配置x-dead-letter-exchange和x-dead-letter-routing-key（可选）两个参数，如果队列内出现了dead letter，则按照这两个参数重新路由转发到指定的队列。x-dead-letter-exchange：出现dead letter之后将dead letter重新发送到指定exchange x-dead-letter-routing-key：出现dead letter之后将dead letter重新按照指定的routing-key发送 4.2.3 DLX延时队列实现4.2.3.1 创建工程创建springboot_rabbitmq_delay工程，并引入相关依赖 1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;changgou_parent&lt;/artifactId&gt; &lt;groupId&gt;com.changgou&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;springboot_rabbitmq_delay&lt;/artifactId&gt; &lt;dependencies&gt; &lt;!--starter-web--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--加入ampq--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--测试--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; application.yml配置 12345678spring: application: name: springboot-demo rabbitmq: host: 127.0.0.1 port: 5672 password: guest username: guest 4.2.3.2 队列创建创建2个队列，用于接收消息的叫延时队列queue.message.delay，用于转发消息的队列叫queue.message，同时创建一个交换机，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Configurationpublic class QueueConfig &#123; /** 短信发送队列 */ public static final String QUEUE_MESSAGE = &quot;queue.message&quot;; /** 交换机 */ public static final String DLX_EXCHANGE = &quot;dlx.exchange&quot;; /** 短信发送队列 延迟缓冲（按消息） */ public static final String QUEUE_MESSAGE_DELAY = &quot;queue.message.delay&quot;; /** * 短信发送队列 * @return */ @Bean public Queue messageQueue() &#123; return new Queue(QUEUE_MESSAGE, true); &#125; /** * 短信发送队列 * @return */ @Bean public Queue delayMessageQueue() &#123; return QueueBuilder.durable(QUEUE_MESSAGE_DELAY) .withArgument(&quot;x-dead-letter-exchange&quot;, DLX_EXCHANGE) // 消息超时进入死信队列，绑定死信队列交换机 .withArgument(&quot;x-dead-letter-routing-key&quot;, QUEUE_MESSAGE) // 绑定指定的routing-key .build(); &#125; /*** * 创建交换机 * @return */ @Bean public DirectExchange directExchange()&#123; return new DirectExchange(DLX_EXCHANGE); &#125; /*** * 交换机与队列绑定 * @param messageQueue * @param directExchange * @return */ @Bean public Binding basicBinding(Queue messageQueue, DirectExchange directExchange) &#123; return BindingBuilder.bind(messageQueue) .to(directExchange) .with(QUEUE_MESSAGE); &#125;&#125; 4.2.3.3 消息监听创建MessageListener用于监听消息，代码如下： 1234567891011121314151617@Component@RabbitListener(queues = QueueConfig.QUEUE_MESSAGE)public class MessageListener &#123; /*** * 监听消息 * @param msg */ @RabbitHandler public void msg(@Payload Object msg)&#123; SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(&quot;当前时间:&quot;+dateFormat.format(new Date())); System.out.println(&quot;收到信息:&quot;+msg); &#125;&#125; 4.2.3.4 创建启动类12345678@SpringBootApplication@EnableRabbitpublic class SpringRabbitMQApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringRabbitMQApplication.class,args); &#125;&#125; 4.2.3.5 测试123456789101112131415161718192021222324252627@SpringBootTest@RunWith(SpringRunner.class)public class RabbitMQTest &#123; @Autowired private RabbitTemplate rabbitTemplate; /*** * 发送消息 */ @Test public void sendMessage() throws InterruptedException, IOException &#123; SimpleDateFormat dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); System.out.println(&quot;发送当前时间:&quot;+dateFormat.format(new Date())); Map&lt;String,String&gt; message = new HashMap&lt;&gt;(); message.put(&quot;name&quot;,&quot;szitheima&quot;); rabbitTemplate.convertAndSend(QueueConfig.QUEUE_MESSAGE_DELAY, message, new MessagePostProcessor() &#123; @Override public Message postProcessMessage(Message message) throws AmqpException &#123; message.getMessageProperties().setExpiration(&quot;10000&quot;); return message; &#125; &#125;); System.in.read(); &#125;&#125; 其中message.getMessageProperties().setExpiration(“10000”);设置消息超时时间,超时后，会将消息转入到另外一个队列。 测试效果如下： 5 库存回滚(作业)5.1 秒杀流程回顾 如上图，步骤分析如下： 12341.用户抢单，经过秒杀系统实现抢单，下单后会将向MQ发送一个延时队列消息，包含抢单信息，延时半小时后才能监听到2.秒杀系统同时启用延时消息监听，一旦监听到订单抢单信息，判断Redis缓存中是否存在订单信息，如果存在，则回滚3.秒杀系统还启动支付回调信息监听，如果支付完成，则将订单吃句话到MySQL，如果没完成，清理排队信息回滚库存4.每次秒杀下单后调用支付系统，创建二维码，如果用户支付成功了，微信系统会将支付信息发送给支付系统指定的回调地址，支付系统收到信息后，将信息发送给MQ，第3个步骤就可以监听到消息了。 延时队列实现订单关闭回滚库存： 123456781.创建一个过期队列 Queue12.接收消息的队列 Queue23.中转交换机4.监听Queue2 1)SeckillStatus-&gt;检查Redis中是否有订单信息 2)如果有订单信息，调用删除订单回滚库存-&gt;[需要先关闭微信支付] 3)如果关闭订单时，用于已支付，修改订单状态即可 4)如果关闭订单时，发生了别的错误，记录日志，人工处理 5.2 关闭支付用户如果半个小时没有支付，我们会关闭支付订单，但在关闭之前，需要先关闭微信支付，防止中途用户支付。 修改支付微服务的WeixinPayService，添加关闭支付方法，代码如下： 123456/*** * 关闭支付 * @param orderId * @return */Map&lt;String,String&gt; closePay(Long orderId) throws Exception; 修改WeixinPayServiceImpl，实现关闭微信支付方法，代码如下： 12345678910111213141516171819202122232425262728293031323334353637/*** * 关闭微信支付 * @param orderId * @return * @throws Exception */@Overridepublic Map&lt;String, String&gt; closePay(Long orderId) throws Exception &#123; //参数设置 Map&lt;String,String&gt; paramMap = new HashMap&lt;String,String&gt;(); paramMap.put(&quot;appid&quot;,appid); //应用ID paramMap.put(&quot;mch_id&quot;,partner); //商户编号 paramMap.put(&quot;nonce_str&quot;,WXPayUtil.generateNonceStr());//随机字符 paramMap.put(&quot;out_trade_no&quot;,String.valueOf(orderId)); //商家的唯一编号 //将Map数据转成XML字符 String xmlParam = WXPayUtil.generateSignedXml(paramMap,partnerkey); //确定url String url = &quot;https://api.mch.weixin.qq.com/pay/closeorder&quot;; //发送请求 HttpClient httpClient = new HttpClient(url); //https httpClient.setHttps(true); //提交参数 httpClient.setXmlParam(xmlParam); //提交 httpClient.post(); //获取返回数据 String content = httpClient.getContent(); //将返回数据解析成Map return WXPayUtil.xmlToMap(content);&#125; 5.3 关闭订单回滚库存5.3.1 配置延时队列在application.yml文件中引入队列信息配置，如下： 12345678910111213#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order seckillorder: queue.seckillorder seckillordertimer: queue.seckillordertimer seckillordertimerdelay: queue.seckillordertimerdelay routing: orderkey: queue.order seckillorderkey: queue.seckillorder 配置队列与交换机,在SeckillApplication中添加如下方法 12345678910111213141516171819202122232425262728293031/** * 到期数据队列 * @return */@Beanpublic Queue seckillOrderTimerQueue() &#123; return new Queue(env.getProperty(&quot;mq.pay.queue.seckillordertimer&quot;), true);&#125;/** * 超时数据队列 * @return */@Beanpublic Queue delaySeckillOrderTimerQueue() &#123; return QueueBuilder.durable(env.getProperty(&quot;mq.pay.queue.seckillordertimerdelay&quot;)) .withArgument(&quot;x-dead-letter-exchange&quot;, env.getProperty(&quot;mq.pay.exchange.order&quot;)) // 消息超时进入死信队列，绑定死信队列交换机 .withArgument(&quot;x-dead-letter-routing-key&quot;, env.getProperty(&quot;mq.pay.queue.seckillordertimer&quot;)) // 绑定指定的routing-key .build();&#125;/*** * 交换机与队列绑定 * @return */@Beanpublic Binding basicBinding() &#123; return BindingBuilder.bind(seckillOrderTimerQueue()) .to(basicExchange()) .with(env.getProperty(&quot;mq.pay.queue.seckillordertimer&quot;));&#125; 5.3.2 发送延时消息修改MultiThreadingCreateOrder，添加如下方法： 12345678910111213/*** * 发送延时消息到RabbitMQ中 * @param seckillStatus */public void sendTimerMessage(SeckillStatus seckillStatus)&#123; rabbitTemplate.convertAndSend(env.getProperty(&quot;mq.pay.queue.seckillordertimerdelay&quot;), (Object) JSON.toJSONString(seckillStatus), new MessagePostProcessor() &#123; @Override public Message postProcessMessage(Message message) throws AmqpException &#123; message.getMessageProperties().setExpiration(&quot;10000&quot;); return message; &#125; &#125;);&#125; 在createOrder方法中调用上面方法，如下代码： 12//发送延时消息到MQ中sendTimerMessage(seckillStatus); 5.3.3 库存回滚创建SeckillOrderDelayMessageListener实现监听消息，并回滚库存，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Component@RabbitListener(queues = &quot;$&#123;mq.pay.queue.seckillordertimer&#125;&quot;)public class SeckillOrderDelayMessageListener &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private SeckillOrderService seckillOrderService; @Autowired private WeixinPayFeign weixinPayFeign; /*** * 读取消息 * 判断Redis中是否存在对应的订单 * 如果存在，则关闭支付，再关闭订单 * @param message */ @RabbitHandler public void consumeMessage(@Payload String message)&#123; //读取消息 SeckillStatus seckillStatus = JSON.parseObject(message,SeckillStatus.class); //获取Redis中订单信息 String username = seckillStatus.getUsername(); SeckillOrder seckillOrder = (SeckillOrder) redisTemplate.boundHashOps(&quot;SeckillOrder&quot;).get(username); //如果Redis中有订单信息，说明用户未支付 if(seckillOrder!=null)&#123; System.out.println(&quot;准备回滚---&quot;+seckillStatus); //关闭支付 Result closeResult = weixinPayFeign.closePay(seckillStatus.getOrderId()); Map&lt;String,String&gt; closeMap = (Map&lt;String, String&gt;) closeResult.getData(); if(closeMap!=null &amp;&amp; closeMap.get(&quot;return_code&quot;).equalsIgnoreCase(&quot;success&quot;) &amp;&amp; closeMap.get(&quot;result_code&quot;).equalsIgnoreCase(&quot;success&quot;) )&#123; //关闭订单 seckillOrderService.closeOrder(username); &#125; &#125; &#125;&#125; 总结需求分析 + 设计 + 技术选型 + 引入demo并测试 + 编写业务逻辑 +","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"购物车模块","slug":"畅购商城/购物车模块","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:59:59.271Z","comments":true,"path":"2022/09/01/畅购商城/购物车模块/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E8%B4%AD%E7%89%A9%E8%BD%A6%E6%A8%A1%E5%9D%97/","excerpt":"","text":"(3)表结构分析 用户登录后将商品加入购物车，需要存储商品详情以及购买数量，购物车详情表如下： changgou_order数据中tb_order_item表： 123456789101112131415161718192021CREATE TABLE `tb_order_item` ( `id` varchar(20) COLLATE utf8_bin NOT NULL COMMENT &#x27;ID&#x27;, `category_id1` int(11) DEFAULT NULL COMMENT &#x27;1级分类&#x27;, `category_id2` int(11) DEFAULT NULL COMMENT &#x27;2级分类&#x27;, `category_id3` int(11) DEFAULT NULL COMMENT &#x27;3级分类&#x27;, `spu_id` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;SPU_ID&#x27;, `sku_id` bigint(20) NOT NULL COMMENT &#x27;SKU_ID&#x27;, `order_id` bigint(20) NOT NULL COMMENT &#x27;订单ID&#x27;, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;商品名称&#x27;, `price` int(20) DEFAULT NULL COMMENT &#x27;单价&#x27;, `num` int(10) DEFAULT NULL COMMENT &#x27;数量&#x27;, `money` int(20) DEFAULT NULL COMMENT &#x27;总金额&#x27;, `pay_money` int(11) DEFAULT NULL COMMENT &#x27;实付金额&#x27;, `image` varchar(200) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;图片地址&#x27;, `weight` int(11) DEFAULT NULL COMMENT &#x27;重量&#x27;, `post_fee` int(11) DEFAULT NULL COMMENT &#x27;运费&#x27;, `is_return` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;是否退货&#x27;, PRIMARY KEY (`id`), KEY `item_id` (`sku_id`), KEY `order_id` (`order_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 购物车详情表其实就是订单详情表结构，只是目前临时存储数据到Redis，等用户下单后才将数据从Redis取出存入到MySQL中。 在商城中一般都会有不同分类商品做折扣活动，下面有一张表记录了对应分类商品的折扣活动，每类商品只允许参与一次折扣活动。 changgou_goods数据中tb_pref表： 1234567891011CREATE TABLE `tb_pref` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &#x27;ID&#x27;, `cate_id` int(11) DEFAULT NULL COMMENT &#x27;分类ID&#x27;, `buy_money` int(11) DEFAULT NULL COMMENT &#x27;消费金额&#x27;, `pre_money` int(11) DEFAULT NULL COMMENT &#x27;优惠金额&#x27;, `start_time` date DEFAULT NULL COMMENT &#x27;活动开始日期&#x27;, `end_time` date DEFAULT NULL COMMENT &#x27;活动截至日期&#x27;, `type` char(1) DEFAULT NULL COMMENT &#x27;类型,1:普通订单，2：限时活动&#x27;, `state` char(1) DEFAULT NULL COMMENT &#x27;状态,1:有效，0：无效&#x27;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8; 1234567891011121314151617Sku sku = resultSku.getData(); //获取SPU Result&lt;Spu&gt; resultSpu = spuFeign.findById(sku.getSpuId()); //将SKU转换成OrderItem OrderItem orderItem = sku2OrderItem(sku,resultSpu.getData(), num); /****** * 购物车数据存入到Redis * namespace = Cart_[username] * key=id(sku) * value=OrderItem */ redisTemplate.boundHashOps(&quot;Cart_&quot;+username).put(id,orderItem);","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]},{"title":"订单模块","slug":"畅购商城/订单模块","date":"2022-09-01T03:51:56.000Z","updated":"2022-10-17T12:58:08.528Z","comments":true,"path":"2022/09/01/畅购商城/订单模块/","link":"","permalink":"https://gouguoqiang.github.io/2022/09/01/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/%E8%AE%A2%E5%8D%95%E6%A8%A1%E5%9D%97/","excerpt":"","text":"3 下单3.1 业务分析点击结算页的时候，会立即创建订单数据，创建订单数据会将数据存入到2张表中，分别是订单表和订单明细表，此处还需要修改商品对应的库存数量。 订单表结构如下： 123456789101112131415161718192021222324252627282930313233CREATE TABLE `tb_order` ( `id` varchar(50) COLLATE utf8_bin NOT NULL COMMENT &#x27;订单id&#x27;, `total_num` int(11) DEFAULT NULL COMMENT &#x27;数量合计&#x27;, `total_money` int(11) DEFAULT NULL COMMENT &#x27;金额合计&#x27;, `pre_money` int(11) DEFAULT NULL COMMENT &#x27;优惠金额&#x27;, `post_fee` int(11) DEFAULT NULL COMMENT &#x27;邮费&#x27;, `pay_money` int(11) DEFAULT NULL COMMENT &#x27;实付金额&#x27;, `pay_type` varchar(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;支付类型，1、在线支付、0 货到付款&#x27;, `create_time` datetime DEFAULT NULL COMMENT &#x27;订单创建时间&#x27;, `update_time` datetime DEFAULT NULL COMMENT &#x27;订单更新时间&#x27;, `pay_time` datetime DEFAULT NULL COMMENT &#x27;付款时间&#x27;, `consign_time` datetime DEFAULT NULL COMMENT &#x27;发货时间&#x27;, `end_time` datetime DEFAULT NULL COMMENT &#x27;交易完成时间&#x27;, `close_time` datetime DEFAULT NULL COMMENT &#x27;交易关闭时间&#x27;, `shipping_name` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;物流名称&#x27;, `shipping_code` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;物流单号&#x27;, `username` varchar(50) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;用户名称&#x27;, `buyer_message` varchar(1000) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;买家留言&#x27;, `buyer_rate` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;是否评价&#x27;, `receiver_contact` varchar(50) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;收货人&#x27;, `receiver_mobile` varchar(12) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;收货人手机&#x27;, `receiver_address` varchar(200) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;收货人地址&#x27;, `source_type` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;订单来源：1:web，2：app，3：微信公众号，4：微信小程序 5 H5手机页面&#x27;, `transaction_id` varchar(30) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;交易流水号&#x27;, `order_status` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;订单状态,0:未完成,1:已完成，2：已退货&#x27;, `pay_status` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;支付状态,0:未支付，1：已支付，2：支付失败&#x27;, `consign_status` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;发货状态,0:未发货，1：已发货，2：已收货&#x27;, `is_delete` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;是否删除&#x27;, PRIMARY KEY (`id`), KEY `create_time` (`create_time`), KEY `status` (`order_status`), KEY `payment_type` (`pay_type`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 订单明细表结构如下： 123456789101112131415161718192021CREATE TABLE `tb_order_item` ( `id` varchar(50) COLLATE utf8_bin NOT NULL COMMENT &#x27;ID&#x27;, `category_id1` int(11) DEFAULT NULL COMMENT &#x27;1级分类&#x27;, `category_id2` int(11) DEFAULT NULL COMMENT &#x27;2级分类&#x27;, `category_id3` int(11) DEFAULT NULL COMMENT &#x27;3级分类&#x27;, `spu_id` varchar(20) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;SPU_ID&#x27;, `sku_id` bigint(20) NOT NULL COMMENT &#x27;SKU_ID&#x27;, `order_id` bigint(20) NOT NULL COMMENT &#x27;订单ID&#x27;, `name` varchar(200) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;商品名称&#x27;, `price` int(20) DEFAULT NULL COMMENT &#x27;单价&#x27;, `num` int(10) DEFAULT NULL COMMENT &#x27;数量&#x27;, `money` int(20) DEFAULT NULL COMMENT &#x27;总金额&#x27;, `pay_money` int(11) DEFAULT NULL COMMENT &#x27;实付金额&#x27;, `image` varchar(200) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;图片地址&#x27;, `weight` int(11) DEFAULT NULL COMMENT &#x27;重量&#x27;, `post_fee` int(11) DEFAULT NULL COMMENT &#x27;运费&#x27;, `is_return` char(1) COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;是否退货,0:未退货，1：已退货&#x27;, PRIMARY KEY (`id`), KEY `item_id` (`sku_id`), KEY `order_id` (`order_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 3.2 下单实现下单的时候，先添加订单往tb_order表中增加数据，再添加订单明细，往tb_order_item表中增加数据. 3.3 库存变更3.3.1 业务分析上面操作只实现了下单操作，但对应的库存还没跟着一起减少，我们在下单之后，应该调用商品微服务，将下单的商品库存减少，销量增加。每次订单微服务只需要将用户名传到商品微服务，商品微服务通过用户名到Redis中查询对应的购物车数据，然后执行库存减少，库存减少需要控制当前商品库存&gt;&#x3D;销售数量。 如何控制库存数量&gt;&#x3D;销售数量呢？其实可以通过SQL语句实现，每次减少数量的时候，加个条件判断。 where num&gt;=#&#123;num&#125;即可。 3.3.3 调用库存递减修改changgou-service-order微服务的com.changgou.order.service.impl.OrderServiceImpl类的add方法，增加库存递减的调用。 先注入SkuFeign 12@Autowiredprivate SkuFeign skuFeign; 再调用库存递减方法 12//库存减库存skuFeign.decrCount(order.getUsername()); 完整代码如下： 3.4 增加积分比如每次下单完成之后，给用户增加10个积分，支付完成后赠送优惠券，优惠券可用于支付时再次抵扣。我们先完成增加积分功能。如下表：points表示用户积分 123456789101112131415161718192021222324CREATE TABLE `tb_user` ( `username` varchar(50) NOT NULL COMMENT &#x27;用户名&#x27;, `password` varchar(100) NOT NULL COMMENT &#x27;密码，加密存储&#x27;, `phone` varchar(20) DEFAULT NULL COMMENT &#x27;注册手机号&#x27;, `email` varchar(50) DEFAULT NULL COMMENT &#x27;注册邮箱&#x27;, `created` datetime NOT NULL COMMENT &#x27;创建时间&#x27;, `updated` datetime NOT NULL COMMENT &#x27;修改时间&#x27;, `source_type` varchar(1) DEFAULT NULL COMMENT &#x27;会员来源：1:PC，2：H5，3：Android，4：IOS&#x27;, `nick_name` varchar(50) DEFAULT NULL COMMENT &#x27;昵称&#x27;, `name` varchar(50) DEFAULT NULL COMMENT &#x27;真实姓名&#x27;, `status` varchar(1) DEFAULT NULL COMMENT &#x27;使用状态（1正常 0非正常）&#x27;, `head_pic` varchar(150) DEFAULT NULL COMMENT &#x27;头像地址&#x27;, `qq` varchar(20) DEFAULT NULL COMMENT &#x27;QQ号码&#x27;, `is_mobile_check` varchar(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;手机是否验证 （0否 1是）&#x27;, `is_email_check` varchar(1) DEFAULT &#x27;0&#x27; COMMENT &#x27;邮箱是否检测（0否 1是）&#x27;, `sex` varchar(1) DEFAULT &#x27;1&#x27; COMMENT &#x27;性别，1男，0女&#x27;, `user_level` int(11) DEFAULT NULL COMMENT &#x27;会员等级&#x27;, `points` int(11) DEFAULT NULL COMMENT &#x27;积分&#x27;, `experience_value` int(11) DEFAULT NULL COMMENT &#x27;经验值&#x27;, `birthday` datetime DEFAULT NULL COMMENT &#x27;出生年月日&#x27;, `last_login_time` datetime DEFAULT NULL COMMENT &#x27;最后登录时间&#x27;, PRIMARY KEY (`username`), UNIQUE KEY `username` (`username`) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&#x27;用户表&#x27;; 4 支付流程分析4.1 订单支付分析 如上图，步骤分析如下： 1234561.用户下单之后，订单数据会存入到MySQL中，同时会将订单对应的支付日志存入到Redis，以队列的方式存储。2.用户下单后，进入支付页面，支付页面调用支付系统，从微信支付获取二维码数据，并在页面生成支付二维码。3.用户扫码支付后，微信支付服务器会通调用前预留的回调地址，并携带支付状态信息。4.支付系统接到支付状态信息后，将支付状态信息发送给RabbitMQ5.订单系统监听RabbitMQ中的消息获取支付状态，并根据支付状态修改订单状态6.为了防止网络问题导致notifyurl没有接到对应数据，定时任务定时获取Redis中队列数据去微信支付接口查询状态，并定时更新对应状态。 4.2 二维码创建(了解)今天主要讲微信支付，后面为了看到效果，我们简单说下利用qrious制作二维码插件。 qrious是一款基于HTML5 Canvas的纯JS二维码生成插件。通过qrious.js可以快速生成各种二维码，你可以控制二维码的尺寸颜色，还可以将生成的二维码进行Base64编码。 qrious.js二维码插件的可用配置参数如下： 参数 类型 默认值 描述 background String “white” 二维码的背景颜色。 foreground String “black” 二维码的前景颜色。 level String “L” 二维码的误差校正级别(L, M, Q, H)。 mime String “image&#x2F;png” 二维码输出为图片时的MIME类型。 size Number 100 二维码的尺寸，单位像素。 value String “” 需要编码为二维码的值 下面的代码即可生成一张二维码 1234567891011121314151617&lt;html&gt;&lt;head&gt;&lt;title&gt;二维码入门小demo&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;img id=&quot;qrious&quot;&gt;&lt;script src=&quot;qrious.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var qr = new QRious(&#123; element:document.getElementById(&#x27;qrious&#x27;), size:250, level:&#x27;H&#x27;, value:&#x27;http://www.itheima.com&#x27; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 运行效果： 大家掏出手机，扫一下看看是否会看到黑马的官网呢？ 5 微信扫码支付简介5.1微信扫码支付申请微信扫码支付是商户系统按微信支付协议生成支付二维码，用户再用微信“扫一扫”完成支付的模式。该模式适用于PC网站支付、实体店单品或订单支付、媒体广告支付等场景。 申请步骤：（了解） 第一步：注册公众号（类型须为：服务号） 请根据营业执照类型选择以下主体注册：个体工商户| 企业&#x2F;公司| 政府| 媒体| 其他类型。 第二步：认证公众号 公众号认证后才可申请微信支付，认证费：300元&#x2F;次。 第三步：提交资料申请微信支付 登录公众平台，点击左侧菜单【微信支付】，开始填写资料等待审核，审核时间为1-5个工作日内。 第四步：开户成功，登录商户平台进行验证 资料审核通过后，请登录联系人邮箱查收商户号和密码，并登录商户平台填写财付通备付金打的小额资金数额，完成账户验证。 第五步：在线签署协议 本协议为线上电子协议，签署后方可进行交易及资金结算，签署完立即生效。 本课程已经提供好“传智播客”的微信支付账号，学员无需申请。 5.2 开发文档微信支付接口调用的整体思路： 按API要求组装参数，以XML方式发送（POST）给微信支付接口（URL）,微信支付接口也是以XML方式给予响应。程序根据返回的结果（其中包括支付URL）生成二维码或判断订单状态。 在线微信支付开发文档： https://pay.weixin.qq.com/wiki/doc/api/index.html 如果你不能联网，请查阅讲义配套资源 （资源\\配套软件\\微信扫码支付\\开发文档） 我们在本章课程中会用到”统一下单”和”查询订单”两组API 12341. appid：微信公众账号或开放平台APP的唯一标识2. mch_id：商户号 (配置文件中的partner)3. partnerkey：商户密钥4. sign:数字签名, 根据微信官方提供的密钥和一套算法生成的一个加密信息, 就是为了保证交易的安全性 5.3 微信支付模式介绍5.3.1 模式一 业务流程说明： 1234567891011121314151.商户后台系统根据微信支付规定格式生成二维码（规则见下文），展示给用户扫码。2.用户打开微信“扫一扫”扫描二维码，微信客户端将扫码内容发送到微信支付系统。3.微信支付系统收到客户端请求，发起对商户后台系统支付回调URL的调用。调用请求将带productid和用户的openid等参数，并要求商户系统返回交数据包,详细请见&quot;本节3.1回调数据输入参数&quot;4.商户后台系统收到微信支付系统的回调请求，根据productid生成商户系统的订单。5.商户系统调用微信支付【统一下单API】请求下单，获取交易会话标识（prepay_id）6.微信支付系统根据商户系统的请求生成预支付交易，并返回交易会话标识（prepay_id）。7.商户后台系统得到交易会话标识prepay_id（2小时内有效）。8.商户后台系统将prepay_id返回给微信支付系统。返回数据见&quot;本节3.2回调数据输出参数&quot;9.微信支付系统根据交易会话标识，发起用户端授权支付流程。10.用户在微信客户端输入密码，确认支付后，微信客户端提交支付授权。11.微信支付系统验证后扣款，完成支付交易。12.微信支付系统完成支付交易后给微信客户端返回交易结果，并将交易结果通过短信、微信消息提示用户。微信客户端展示支付交易结果页面。13.微信支付系统通过发送异步消息通知商户后台系统支付结果。商户后台系统需回复接收情况，通知微信后台系统不再发送该单的支付通知。14.未收到支付通知的情况，商户后台系统调用【查询订单API】。15.商户确认订单已支付后给用户发货。 5.3.2 模式二 业务流程说明： 1234567891011121.商户后台系统根据用户选购的商品生成订单。2.用户确认支付后调用微信支付【统一下单API】生成预支付交易；3.微信支付系统收到请求后生成预支付交易单，并返回交易会话的二维码链接code_url。4.商户后台系统根据返回的code_url生成二维码。5.用户打开微信“扫一扫”扫描二维码，微信客户端将扫码内容发送到微信支付系统。6.微信支付系统收到客户端请求，验证链接有效性后发起用户支付，要求用户授权。7.用户在微信客户端输入密码，确认支付后，微信客户端提交授权。8.微信支付系统根据用户授权完成支付交易。9.微信支付系统完成支付交易后给微信客户端返回交易结果，并将交易结果通过短信、微信消息提示用户。微信客户端展示支付交易结果页面。10.微信支付系统通过发送异步消息通知商户后台系统支付结果。商户后台系统需回复接收情况，通知微信后台系统不再发送该单的支付通知。11.未收到支付通知的情况，商户后台系统调用【查询订单API】。12.商户确认订单已支付后给用户发货。 3 检测支付状态3.1 需求分析当用户支付成功后跳转到成功页面 当返回异常时跳转到错误页面 3.2 实现思路我们通过HttpClient工具类实现对远程支付接口的调用。 接口链接：https://api.mch.weixin.qq.com/pay/orderquery 具体参数参见“查询订单”API, 我们在controller方法中轮询调用查询订单（间隔3秒），当返回状态为success时，我们会在controller方法返回结果。前端代码收到结果后跳转到成功页面。 4 订单状态操作准备工作4.1 需求分析 我们现在系统还有个问题需要解决：支付后订单状态没有改变 流程回顾： 1234561.用户下单之后，订单数据会存入到MySQL中，同时会将订单对应的支付日志存入到Redis，以List+Hash的方式存储。2.用户下单后，进入支付页面，支付页面调用支付系统，从微信支付获取二维码数据，并在页面生成支付二维码。3.用户扫码支付后，微信支付服务器会通调用前预留的回调地址，并携带支付状态信息。4.支付系统接到支付状态信息后，将支付状态信息发送给RabbitMQ5.订单系统监听RabbitMQ中的消息获取支付状态，并根据支付状态修改订单状态6.为了防止网络问题导致notifyurl没有接到对应数据，定时任务定时获取Redis中队列数据去微信支付接口查询状态，并定时更新对应状态。 需要做的工作： 123451.创建订单时，同时将订单信息放到Redis中，以List和Hash各存一份2.实现回调地址接收支付状态信息3.将订单支付状态信息发送给RabbitMQ4.订单系统中监听支付状态信息，如果是支付成功，修改订单状态，如果是支付失败，删除订单(或者改成支付失败)5.防止网络异常无法接收到回调地址的支付信息，定时任务从Redis List中读取数据判断是否支付，如果支付了，修改订单状态，如果未支付，将支付信息放入队列，下次再检测，如果支付失败删除订单(或者改成支付失败)。 4.2 Redis存储订单信息每次添加订单后，会根据订单检查用户是否是否支付成功，我们不建议每次都操作数据库，每次操作数据库会增加数据库的负载，我们可以选择将用户的订单信息存入一份到Redis中，提升读取速度。 修改changgou-service-order微服务的com.changgou.order.service.impl.OrderServiceImpl类中的add方法，如果是线上支付，将用户订单数据存入到Redis中,由于每次创建二维码，需要用到订单编号 ，所以也需要将添加的订单信息返回。 上图代码如下： 1234567891011121314151617181920212223242526/** * 增加Order * 金额校验:后台校验 * @param order */@Overridepublic Order add(Order order)&#123; //...略 //修改库存 skuFeign.decrCount(order.getUsername()); //添加用户积分 userFeign.addPoints(2); //线上支付，记录订单 if(order.getPayType().equalsIgnoreCase(&quot;1&quot;))&#123; //将支付记录存入到Reids namespace key value redisTemplate.boundHashOps(&quot;Order&quot;).put(order.getId(),order); &#125; //删除购物车信息 //redisTemplate.delete(&quot;Cart_&quot; + order.getUsername()); return order;&#125; 修改com.changgou.order.controller.OrderController的add方法，将订单对象返回，因为页面需要获取订单的金额和订单号用于创建二维码，代码如下： 4.3 修改订单状态订单支付成功后，需要修改订单状态并持久化到数据库，修改订单的同时，需要将Redis中的订单删除，所以修改订单状态需要将订单日志也传过来，实现代码如下： 修改com.changgou.order.service.OrderService，添加修改订单状态方法，代码如下： 123456/*** * 根据订单ID修改订单状态 * @param transactionid 交易流水号 * @param orderId */void updateStatus(String orderId,String transactionid); 修改com.changgou.order.service.impl.OrderServiceImpl，添加修改订单状态实现方法，代码如下： 123456789101112131415161718/*** * 订单修改 * @param orderId * @param transactionid 微信支付的交易流水号 */@Overridepublic void updateStatus(String orderId,String transactionid) &#123; //1.修改订单 Order order = orderMapper.selectByPrimaryKey(orderId); order.setUpdateTime(new Date()); //时间也可以从微信接口返回过来，这里为了方便，我们就直接使用当前时间了 order.setPayTime(order.getUpdateTime()); //不允许这么写 order.setTransactionId(transactionid); //交易流水号 order.setPayStatus(&quot;1&quot;); //已支付 orderMapper.updateByPrimaryKeySelective(order); //2.删除Redis中的订单记录 redisTemplate.boundHashOps(&quot;Order&quot;).delete(orderId);&#125; 4.4 删除订单如果用户订单支付失败了，或者支付超时了，我们需要删除用户订单，删除订单的同时需要回滚库存，这里回滚库存我们就不实现了，作为同学们的作业。实现如下： 修改changgou-service-order的com.changgou.order.service.OrderService，添加删除订单方法，我们只需要将订单id传入进来即可实现，代码如下： 12345/*** * 删除订单操作 * @param id */void deleteOrder(String id); 修改changgou-service-order的com.changgou.order.service.impl.OrderServiceImpl，添加删除订单实现方法，代码如下： 1234567891011121314/*** * 订单的删除操作 */@Overridepublic void deleteOrder(String id) &#123; //改状态 Order order = (Order) redisTemplate.boundHashOps(&quot;Order&quot;).get(id); order.setUpdateTime(new Date()); order.setPayStatus(&quot;2&quot;); //支付失败 orderMapper.updateByPrimaryKeySelective(order); //删除缓存 redisTemplate.boundHashOps(&quot;Order&quot;).delete(id);&#125; 5 支付信息回调5.1 接口分析每次实现支付之后，微信支付都会将用户支付结果返回到指定路径，而指定路径是指创建二维码的时候填写的notifyurl参数,响应的数据以及相关文档参考一下地址：https://pay.weixin.qq.com/wiki/doc/api/native.php?chapter=9_7&amp;index=8 5.1.1 返回参数分析通知参数如下： 字段名 变量名 必填 类型 示例值 描述 返回状态码 return_code 是 String(16) SUCCESS SUCCESS 返回信息 return_msg 是 String(128) OK OK 以下字段在return_code为SUCCESS的时候有返回 字段名 变量名 必填 类型 示例值 描述 公众账号ID appid 是 String(32) wx8888888888888888 微信分配的公众账号ID（企业号corpid即为此appId） 业务结果 result_code 是 String(16) SUCCESS SUCCESS&#x2F;FAIL 商户订单号 out_trade_no 是 String(32) 1212321211201407033568112322 商户系统内部订单号 微信支付订单号 transaction_id 是 String(32) 1217752501201407033233368018 微信支付订单号 5.1.2 响应分析回调地址接收到数据后，需要响应信息给微信服务器，告知已经收到数据，不然微信服务器会再次发送4次请求推送支付信息。 字段名 变量名 必填 类型 示例值 描述 返回状态码 return_code 是 String(16) SUCCESS 请按示例值填写 返回信息 return_msg 是 String(128) OK 请按示例值填写 举例如下： 1234&lt;xml&gt; &lt;return_code&gt;&lt;![CDATA[SUCCESS]]&gt;&lt;/return_code&gt; &lt;return_msg&gt;&lt;![CDATA[OK]]&gt;&lt;/return_msg&gt;&lt;/xml&gt; 5.2 回调接收数据实现修改changgou-service-pay微服务的com.changgou.pay.controller.WeixinPayController,添加回调方法，代码如下： 1234567891011121314151617181920212223242526272829303132333435/*** * 支付回调 * @param request * @return */@RequestMapping(value = &quot;/notify/url&quot;)public String notifyUrl(HttpServletRequest request)&#123; InputStream inStream; try &#123; //读取支付回调数据 inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); // 将支付回调数据转换成xml字符串 String result = new String(outSteam.toByteArray(), &quot;utf-8&quot;); //将xml字符串转换成Map结构 Map&lt;String, String&gt; map = WXPayUtil.xmlToMap(result); //响应数据设置 Map respMap = new HashMap(); respMap.put(&quot;return_code&quot;,&quot;SUCCESS&quot;); respMap.put(&quot;return_msg&quot;,&quot;OK&quot;); return WXPayUtil.mapToXml(respMap); &#125; catch (Exception e) &#123; e.printStackTrace(); //记录错误日志 &#125; return null;&#125; 6 MQ处理支付回调状态6.1 业务分析 支付系统是独立于其他系统的服务，不做相关业务逻辑操作，只做支付处理，所以回调地址接收微信服务返回的支付状态后，立即将消息发送给RabbitMQ，订单系统再监听支付状态数据，根据状态数据做出修改订单状态或者删除订单操作。 6.2 发送支付状态(1)集成RabbitMQ 修改支付微服务，集成RabbitMQ，添加如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 这里我们建议在后台手动创建队列，并绑定队列。如果使用程序创建队列，可以按照如下方式实现。 修改application.yml，配置支付队列和交换机信息，代码如下： 123456789#位置支付交换机和队列mq: pay: exchange: order: exchange.order queue: order: queue.order routing: key: queue.order 创建队列以及交换机并让队列和交换机绑定，修改com.changgou.WeixinPayApplication,添加如下代码： 1234567891011121314151617181920212223242526/*** * 创建DirectExchange交换机 * @return */@Beanpublic DirectExchange basicExchange()&#123; return new DirectExchange(env.getProperty(&quot;mq.pay.exchange.order&quot;), true,false);&#125;/*** * 创建队列 * @return */@Bean(name = &quot;queueOrder&quot;)public Queue queueOrder()&#123; return new Queue(env.getProperty(&quot;mq.pay.queue.order&quot;), true);&#125;/**** * 队列绑定到交换机上 * @return */@Beanpublic Binding basicBinding()&#123; return BindingBuilder.bind(queueOrder()).to(basicExchange()).with(env.getProperty(&quot;mq.pay.routing.key&quot;));&#125; 6.2.2 发送MQ消息修改回调方法，在接到支付信息后，立即将支付信息发送给RabbitMQ，代码如下： 上图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Value(&quot;$&#123;mq.pay.exchange.order&#125;&quot;)private String exchange;@Value(&quot;$&#123;mq.pay.queue.order&#125;&quot;)private String queue;@Value(&quot;$&#123;mq.pay.routing.key&#125;&quot;)private String routing;@Autowiredprivate WeixinPayService weixinPayService;@Autowiredprivate RabbitTemplate rabbitTemplate;/*** * 支付回调 * @param request * @return */@RequestMapping(value = &quot;/notify/url&quot;)public String notifyUrl(HttpServletRequest request)&#123; InputStream inStream; try &#123; //读取支付回调数据 inStream = request.getInputStream(); ByteArrayOutputStream outSteam = new ByteArrayOutputStream(); byte[] buffer = new byte[1024]; int len = 0; while ((len = inStream.read(buffer)) != -1) &#123; outSteam.write(buffer, 0, len); &#125; outSteam.close(); inStream.close(); // 将支付回调数据转换成xml字符串 String result = new String(outSteam.toByteArray(), &quot;utf-8&quot;); //将xml字符串转换成Map结构 Map&lt;String, String&gt; map = WXPayUtil.xmlToMap(result); //将消息发送给RabbitMQ rabbitTemplate.convertAndSend(exchange,routing, JSON.toJSONString(map)); //响应数据设置 Map respMap = new HashMap(); respMap.put(&quot;return_code&quot;,&quot;SUCCESS&quot;); respMap.put(&quot;return_msg&quot;,&quot;OK&quot;); return WXPayUtil.mapToXml(respMap); &#125; catch (Exception e) &#123; e.printStackTrace(); //记录错误日志 &#125; return null;&#125; 6.3 监听MQ消息处理订单在订单微服务中，我们需要监听MQ支付状态消息，并实现订单数据操作。 6.3.1 集成RabbitMQ在订单微服务中，先集成RabbitMQ，再监听队列消息。 在pom.xml中引入如下依赖： 12345&lt;!--加入ampq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 在application.yml中配置rabbitmq配置，代码如下： 在application.yml中配置队列名字，代码如下： 12345#位置支付交换机和队列mq: pay: queue: order: queue.order 6.3.2 监听消息修改订单在订单微服务于中创建com.changgou.order.consumer.OrderPayMessageListener，并在该类中consumeMessage方法，用于监听消息，并根据支付状态处理订单，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041@Component@RabbitListener(queues = &#123;&quot;$&#123;mq.pay.queue.order&#125;&quot;&#125;)public class OrderPayMessageListener &#123; @Autowired private RedisTemplate redisTemplate; @Autowired private OrderService orderService; /*** * 接收消息 */ @RabbitHandler public void consumeMessage(String msg)&#123; //将数据转成Map Map&lt;String,String&gt; result = JSON.parseObject(msg,Map.class); //return_code=SUCCESS String return_code = result.get(&quot;return_code&quot;); //业务结果 String result_code = result.get(&quot;result_code&quot;); //业务结果 result_code=SUCCESS/FAIL，修改订单状态 if(return_code.equalsIgnoreCase(&quot;success&quot;) )&#123; //获取订单号 String outtradeno = result.get(&quot;out_trade_no&quot;); //业务结果 if(result_code.equalsIgnoreCase(&quot;success&quot;))&#123; if(outtradeno!=null)&#123; //修改订单状态 out_trade_no orderService.updateStatus(outtradeno,result.get(&quot;transaction_id&quot;)); &#125; &#125;else&#123; //订单删除 orderService.deleteOrder(outtradeno); &#125; &#125; &#125;&#125; 7 定时处理订单状态(学员完成)7.1 业务分析在现实场景中，可能会出现这么种情况，就是用户支付后，有可能畅购服务网络不通或者服务器挂了，此时会导致回调地址无法接收到用户支付状态，这时候我们需要取微信服务器查询。所以我们之前订单信息的ID存入到了Redis队列，主要用于解决这种网络不可达造成支付状态无法回调获取的问题。 实现思路如下： 123451.每次下单，都将订单存入到Reids List队列中2.定时每5秒检查一次Redis 队列中是否有数据，如果有，则再去查询微信服务器支付状态3.如果已支付，则修改订单状态4.如果没有支付，是等待支付，则再将订单存入到Redis队列中，等会再次检查5.如果是支付失败，直接删除订单信息并修改订单状态 总结流程回顾： 1234561.用户下单之后，订单数据会存入到MySQL中，同时会将订单对应的支付日志存入到Redis，以List+Hash的方式存储。2.用户下单后，进入支付页面，支付页面调用支付系统，从微信支付获取二维码数据，并在页面生成支付二维码。3.用户扫码支付后，微信支付服务器会通调用前预留的回调地址，并携带支付状态信息。4.支付系统接到支付状态信息后，将支付状态信息发送给RabbitMQ5.订单系统监听RabbitMQ中的消息获取支付状态，并根据支付状态修改订单状态6.为了防止网络问题导致notifyurl没有接到对应数据，定时任务定时获取Redis中队列数据去微信支付接口查询状态，并定时更新对应状态。 需要做的工作： 123451.创建订单时，同时将订单信息放到Redis中，以List和Hash各存一份2.实现回调地址接收支付状态信息3.将订单支付状态信息发送给RabbitMQ4.订单系统中监听支付状态信息，如果是支付成功，修改订单状态，如果是支付失败，删除订单(或者改成支付失败)5.防止网络异常无法接收到回调地址的支付信息，定时任务从Redis List中读取数据判断是否支付，如果支付了，修改订单状态，如果未支付，将支付信息放入队列，下次再检测，如果支付失败删除订单(或者改成支付失败)。 订单表结构下单流程及库存变更,增加积分功能支付流程微信扫码支付支付信息回调支付系统是独立于其他系统的服务，不做相关业务逻辑操作，只做支付处理，所以回调地址接收微信服务返回的支付状态后，立即将消息发送给RabbitMQ，订单系统再监听支付状态数据，根据状态数据做出修改订单状态或者删除订单操作。 MQ处理定时处理","categories":[{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"}]}],"categories":[{"name":"算法","slug":"算法","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"1基础知识","slug":"算法/1基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"2搜索","slug":"算法/2搜索","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/2%E6%90%9C%E7%B4%A2/"},{"name":"4图论","slug":"算法/4图论","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/4%E5%9B%BE%E8%AE%BA/"},{"name":"5dp","slug":"算法/5dp","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/5dp/"},{"name":"6基础知识","slug":"算法/6基础知识","permalink":"https://gouguoqiang.github.io/categories/%E7%AE%97%E6%B3%95/6%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/categories/suibi/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"ES","slug":"ES","permalink":"https://gouguoqiang.github.io/categories/ES/"},{"name":"OS","slug":"OS","permalink":"https://gouguoqiang.github.io/categories/OS/"},{"name":"Net","slug":"Net","permalink":"https://gouguoqiang.github.io/categories/Net/"},{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/categories/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"项目","slug":"项目","permalink":"https://gouguoqiang.github.io/categories/%E9%A1%B9%E7%9B%AE/"},{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/categories/JVM/"},{"name":"畅购商城项目","slug":"畅购商城项目","permalink":"https://gouguoqiang.github.io/categories/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE/"}],"tags":[{"name":"算法基础","slug":"算法基础","permalink":"https://gouguoqiang.github.io/tags/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/"},{"name":"二分","slug":"二分","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86/"},{"name":"排序","slug":"排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8E%92%E5%BA%8F/"},{"name":"搜索","slug":"搜索","permalink":"https://gouguoqiang.github.io/tags/%E6%90%9C%E7%B4%A2/"},{"name":"图论","slug":"图论","permalink":"https://gouguoqiang.github.io/tags/%E5%9B%BE%E8%AE%BA/"},{"name":"Floyd","slug":"Floyd","permalink":"https://gouguoqiang.github.io/tags/Floyd/"},{"name":"二分图","slug":"二分图","permalink":"https://gouguoqiang.github.io/tags/%E4%BA%8C%E5%88%86%E5%9B%BE/"},{"name":"拓扑排序","slug":"拓扑排序","permalink":"https://gouguoqiang.github.io/tags/%E6%8B%93%E6%89%91%E6%8E%92%E5%BA%8F/"},{"name":"最小生成树","slug":"最小生成树","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"},{"name":"最近公共祖先","slug":"最近公共祖先","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E8%BF%91%E5%85%AC%E5%85%B1%E7%A5%96%E5%85%88/"},{"name":"单源最短路","slug":"单源最短路","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%95%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF/"},{"name":"负环","slug":"负环","permalink":"https://gouguoqiang.github.io/tags/%E8%B4%9F%E7%8E%AF/"},{"name":"dp","slug":"dp","permalink":"https://gouguoqiang.github.io/tags/dp/"},{"name":"区间dp","slug":"区间dp","permalink":"https://gouguoqiang.github.io/tags/%E5%8C%BA%E9%97%B4dp/"},{"name":"数字三角形模型","slug":"数字三角形模型","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%97%E4%B8%89%E8%A7%92%E5%BD%A2%E6%A8%A1%E5%9E%8B/"},{"name":"最长上升子序列模型","slug":"最长上升子序列模型","permalink":"https://gouguoqiang.github.io/tags/%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/"},{"name":"背包","slug":"背包","permalink":"https://gouguoqiang.github.io/tags/%E8%83%8C%E5%8C%85/"},{"name":"状态机","slug":"状态机","permalink":"https://gouguoqiang.github.io/tags/%E7%8A%B6%E6%80%81%E6%9C%BA/"},{"name":"数学知识","slug":"数学知识","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E5%AD%A6%E7%9F%A5%E8%AF%86/"},{"name":"容斥原理","slug":"容斥原理","permalink":"https://gouguoqiang.github.io/tags/%E5%AE%B9%E6%96%A5%E5%8E%9F%E7%90%86/"},{"name":"博弈论","slug":"博弈论","permalink":"https://gouguoqiang.github.io/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/"},{"name":"suibi","slug":"suibi","permalink":"https://gouguoqiang.github.io/tags/suibi/"},{"name":"缓存","slug":"缓存","permalink":"https://gouguoqiang.github.io/tags/%E7%BC%93%E5%AD%98/"},{"name":"中间件","slug":"中间件","permalink":"https://gouguoqiang.github.io/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"数据库","slug":"数据库","permalink":"https://gouguoqiang.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"多线程","slug":"多线程","permalink":"https://gouguoqiang.github.io/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"Spring","slug":"Spring","permalink":"https://gouguoqiang.github.io/tags/Spring/"},{"name":"Mybatis","slug":"Mybatis","permalink":"https://gouguoqiang.github.io/tags/Mybatis/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://gouguoqiang.github.io/tags/SpringBoot/"},{"name":"商城","slug":"商城","permalink":"https://gouguoqiang.github.io/tags/%E5%95%86%E5%9F%8E/"},{"name":"JVM","slug":"JVM","permalink":"https://gouguoqiang.github.io/tags/JVM/"},{"name":"虚拟机","slug":"虚拟机","permalink":"https://gouguoqiang.github.io/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"Nginx","slug":"Nginx","permalink":"https://gouguoqiang.github.io/tags/Nginx/"}]}