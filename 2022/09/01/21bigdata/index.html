<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>大数据 | ggq</title><meta name="keywords" content="大数据"><meta name="author" content="ggq"><meta name="copyright" content="ggq"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大数据笔记。">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据">
<meta property="og:url" content="https://gouguoqiang.github.io/2022/09/01/21bigdata/index.html">
<meta property="og:site_name" content="ggq">
<meta property="og:description" content="大数据笔记。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-09-01T03:51:56.000Z">
<meta property="article:modified_time" content="2023-02-19T04:43:14.804Z">
<meta property="article:author" content="ggq">
<meta property="article:tag" content="大数据">
<meta name="twitter:card" content="summary"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://gouguoqiang.github.io/2022/09/01/21bigdata/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大数据',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-19 12:43:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">51</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ggq</a></span><div id="menus"><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">大数据</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-09-01T03:51:56.000Z" title="发表于 2022-09-01 11:51:56">2022-09-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-19T04:43:14.804Z" title="更新于 2023-02-19 12:43:14">2023-02-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="大数据"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="大数据组件笔记"><a href="#大数据组件笔记" class="headerlink" title="大数据组件笔记"></a>大数据组件笔记</h1><h2 id="一、-Hadoop"><a href="#一、-Hadoop" class="headerlink" title="一、 Hadoop"></a>一、 Hadoop</h2><p>HDFS：分布式文件存储系统，大数据环境的基石<br>MapReduce(MR)：基于磁盘计算，主要用于大量数据的批处理计算<br>Spark(RDD)：基于内存计算<br>SparkSQL：一般情况都是基于离线数据处理<br>Spark Streaming：一般情况是基于微批(实时)处理</p>
<p>Flink 流式计算引擎<br>Flink SQL：类似SparkSQL，可以写SQL，更快的使用批处理操作<br>Flink Streaming：流式数据，(开发思路)生产库产生数据一部分发送至kafka、一部分落库，后续Filnk对接kafka中的Topic ，实时对kafka中数据进行去重、清洗、汇总、计算，维度可以存放至redis中。。。</p>
<h2 id="二、-消息队列"><a href="#二、-消息队列" class="headerlink" title="二、 消息队列"></a>二、 消息队列</h2><p>Kafka：可理解生产者和消费者之间的数据传递，大数据量发送传递及消费，主要功能点是削峰填谷<br>MQ：主要运用于事务性消息队列，但是对于数据量来看，量级很小</p>
<h2 id="三、-数据库-关系型、非关系型"><a href="#三、-数据库-关系型、非关系型" class="headerlink" title="三、 数据库(关系型、非关系型)"></a>三、 数据库(关系型、非关系型)</h2><p>Hive：hive只是有个管理工具，不存储任何数据，将hdfs中的文件映射成表的形式进行数据处理，主要面向于(离线)数据仓库使用，内部执行引擎(MapReduce&#x2F;Spark&#x2F;Tez)，也可以开发一下自定义的UTF函数。<br>Impala：号称是当前大数据领域最快的查询sql工具，占用内存比较多，我们在工作中都是使用Hive+Impala做离线数仓。(即席查询)<br>Presto：一个分布式SQL查询引擎，整体性能还是可以的。(即席查询)<br>HBases： 典型的NoSQL、分布式存储的数据库，速度够快。<br>Kudu：在更新更及时的基础上实现更快的数据分析，个人感觉是在大量数据中做到更快的查询速度。<br>Kylin：分布式分析引擎，我们主要用于做OLAP多维数据立方体数据，就类似与Cognos中的动态CUBE。<br>ClickHouse：(不基于Hadoop集群，可独立安装)列式数据库，主要用于实时数据仓库，这个也是基于内存的，特点就是快。单表查询块，多表关联是弊端。(即席查询)<br>Doris：一个基于 MPP 架构的高性能、实时的分析型数据库，尽量使用星型模型，单表不要超过100列，(即席查询)</p>
<h2 id="四、-ETL工具"><a href="#四、-ETL工具" class="headerlink" title="四、 ETL工具"></a>四、 ETL工具</h2><p>Sqoop：主要是用于关系型数据与分布式数据库的数据抽取任务，类似MYSQL数据抽取至HDFS&#x2F;Hive。sqoop底层运用的计算引擎也是MR，只不顾没有用到Reduce而已<br>SeaTunnel（WaterDrop）：大数据集群数据同步工具，主要引擎有spark和flink，Kakfa-&gt;PostgreSQL、ClinkHouse-&gt;PostgreSQL、MongoDB-&gt;PostgreSQL，PostgreSQL-&gt;HDFS等等。我就使用了这么多，其他的功能还需小伙伴继续钻研<br>DataX：阿里开发，可多线程抽取数据，但是会有一些数据丢失问题，可能是生产库数据有脏数据问题，有待考证。<br>FlinkX： 袋鼠云开源，有待考证。</p>
<h2 id="五、-数据可视化"><a href="#五、-数据可视化" class="headerlink" title="五、 数据可视化"></a>五、 数据可视化</h2><p>DataV：阿里开发<br>Datart(Davinci)：宜信开发，可支持中国式复杂报表开发，也支持图表等二三十种图表样式支持，可自定义开发图表组件<br>Superset：百度开源，现已贡献给Apache 开源基金会，图表绚丽</p>
<h2 id="六、-任务调度工具"><a href="#六、-任务调度工具" class="headerlink" title="六、 任务调度工具"></a>六、 任务调度工具</h2><p>Azkaban：一个脚本任务调度工具，一般用于ETL脚本执行调度，需要单独配置调度文件。在文件中需要配置依赖脚本等信息，全程在WEB端开发，在查看调度的时候有点费眼睛，DAG图不可以缩得太小或太大<br>DolphinScheduler：俗称小海豚，国人开发的脚本调度工具，但是每次每一个脚本都得需要上传到服务器上，如果该工具能够支持git自动同步脚本文件的话，将是一个很好的脚本工具，开发及迁移都是很好使用的<br>Kettle：传统的ETL工具+调度工具，有两大特性：job和转换，win和linux都可以兼容，很好使的一个ETL处理工具<br>————————————————<br>版权声明：本文为CSDN博主「W-DW」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36712507/article/details/106933379">https://blog.csdn.net/qq_36712507/article/details/106933379</a></p>
<h1 id="Windows-环境搭建"><a href="#Windows-环境搭建" class="headerlink" title="Windows 环境搭建"></a>Windows 环境搭建</h1><p>看过的博客:</p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1818595">https://cloud.tencent.com/developer/article/1818595</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44186838/article/details/119965991">https://blog.csdn.net/qq_44186838/article/details/119965991</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/508841769">https://zhuanlan.zhihu.com/p/508841769</a></p>
<h2 id="hive"><a href="#hive" class="headerlink" title="hive"></a>hive</h2><p>在Hive中 如果本地数据库是8.0 驱动jar包换成8.的版本 hive –service metastore 也有可能报错 如果报错了 把hive库删了重新手动初始化下</p>
<p>报错信息：</p>
<p>FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:An exception was thrown while adding&#x2F;validating class(es) : You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘[CHARACTER SET charset_name] [COLLATE collation_name] NULL,<br>    <code>VIEW_ORIGINAL_T&#39; at line 14 java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &#39;[CHARACTER SET charset_name] [COLLATE collation_name] NULL,     </code>VIEW_ORIGINAL_T’ at line 14</p>
<p>hive –service metastore SQL语法报错解决</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">手动初始化</span><br><span class="line">hive --service schematool -dbType mysql -initSchema</span><br><span class="line"></span><br><span class="line">直接启动hive</span><br><span class="line">hive Starting Hive Metastore Server</span><br></pre></td></tr></table></figure>



<p><img src="/../images/image-20221127220533291.png" alt="image-20221127220533291"></p>
<h2 id="安装scala"><a href="#安装scala" class="headerlink" title="安装scala"></a>安装scala</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_46864949/article/details/123203397">https://blog.csdn.net/qq_46864949/article/details/123203397</a></p>
<p>不知道在安装啥的时候把scala安装好了 怕影响其他大数据组件,目录有空格将环境变量去掉了</p>
<p><img src="/../images/image-20221127230718424.png" alt="image-20221127230718424"></p>
<p>需要在IDEA 中安装Scala插件</p>
<p><img src="/../images/image-20221207161010782.png" alt="image-20221207161010782"></p>
<p><img src="/../images/image-20221207161020008.png" alt="image-20221207161020008"></p>
<p>ok后就可正常开发scala了</p>
<h2 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/moshowgame/article/details/80379402">https://blog.csdn.net/moshowgame/article/details/80379402</a></p>
<p>Hive与Hbase区别</p>
<p>一、Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据(十亿、百亿)的随机实时查询，如日志明细、交易清单、轨迹行为等。</p>
<p>二、Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。</p>
<p>\1. Hive用于批处理，而HBase用于事务处理。</p>
<p>\2. Hive是查询引擎,而Hbase是 非结构化数据的数据存储。</p>
<p>\3. Hive是运行MapReduce作业的类似SQL的引擎，而HBase 是Hadoop上的NoSQL键&#x2F;值数据库</p>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p>启动spark : spark-shell.cmd</p>
<h2 id="sparkSQL集成-hive-hbase-mysql"><a href="#sparkSQL集成-hive-hbase-mysql" class="headerlink" title="sparkSQL集成 hive hbase mysql"></a>sparkSQL集成 hive hbase mysql</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011254180/article/details/79395227">https://blog.csdn.net/u011254180/article/details/79395227</a></p>
<p><img src="/../images/image-20221204211016774.png" alt="image-20221204211016774"></p>
<h1 id="spark1-6-gt-2-4升级问题"><a href="#spark1-6-gt-2-4升级问题" class="headerlink" title="spark1.6 -&gt;2.4升级问题"></a>spark1.6 -&gt;2.4升级问题</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/aoyugang0603/article/details/102442007">https://blog.csdn.net/aoyugang0603/article/details/102442007</a></p>
<h1 id="项目搭建"><a href="#项目搭建" class="headerlink" title="项目搭建"></a>项目搭建</h1><h2 id="JavaWeb"><a href="#JavaWeb" class="headerlink" title="JavaWeb"></a>JavaWeb</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u014636209/article/details/104261350/">https://blog.csdn.net/u014636209/article/details/104261350/</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43977226/article/details/118614918">https://blog.csdn.net/weixin_43977226/article/details/118614918</a></p>
<p>根据代码 修改组件的端口号设置</p>
<p>hadoop</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">2）修改配置文件</span><br><span class="line"></span><br><span class="line">            A）vi /home/hadoop-2.6.4/etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line">                   修改为export JAVA_HOME=/usr/java/jdk1.7.0_79（jdk地址）</span><br><span class="line"></span><br><span class="line">            B）vi /home/hadoop-2.6.4/etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    //配置namenode主机和端口</span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://spark1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    //配置hadoop的临时文件目录以及fsimage文件目录，不能放在temp下</span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop-2.6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            C）vi /home/hadoop-2.6.4/etc/hadoop/hdfs-site.xml，配置secondary namenode主机名和端口</span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">             <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2）修改配置文件</span><br><span class="line"></span><br><span class="line">            A）vi /home/hadoop-2.6.4/etc/hadoop/hadoop-env.sh</span><br><span class="line"></span><br><span class="line">                   修改为export JAVA_HOME=/usr/java/jdk1.7.0_79（jdk地址）</span><br><span class="line"></span><br><span class="line">            B）vi /home/hadoop-2.6.4/etc/hadoop/core-site.xml</span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    //配置namenode主机和端口</span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://spark1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span>  </span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    //配置hadoop的临时文件目录以及fsimage文件目录，不能放在temp下</span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop-2.6<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            C）vi /home/hadoop-2.6.4/etc/hadoop/hdfs-site.xml，配置secondary namenode主机名和端口</span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                 <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.https-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                     <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark1:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">                  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">             <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            D）vi /home/hadoop-2.6.4/etc/hadoop/slaves，配置datanode的主机名</span><br><span class="line">                spark1</span><br><span class="line">                spark2</span><br><span class="line">                spark3</span><br><span class="line">            E）vi /home/hadoop-2.6.4/etc/hadoop/masters（这个文件原本是没有的）</span><br><span class="line">                spark1（只需输入这个，为namenode主机）</span><br><span class="line">            F）配置hadoop的环境变量</span><br><span class="line">                vi ~/.bash_profile</span><br><span class="line">                export HADOOP_HOME=/home/hadoop-2.6.4</span><br><span class="line">                export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"></span><br><span class="line">    3）格式化hdfs，只能在namenode主机（spark1）上执行</span><br><span class="line">        hdfs namenode -format</span><br><span class="line">    4）启动hdfs（在namenode上启动，已经配置免密码登录了，具体配置见免密码登录）</span><br><span class="line">        start-dfs.sh   （stop-dfs.sh为关闭）</span><br><span class="line">    5）在浏览器登录查看</span><br><span class="line">            spark1:50070       //50070为namenode的http监控的端口，spark1已经在hosts文件中与ip相映射</span><br><span class="line"></span><br><span class="line">            spark1:50090       //secondary namenode的监控端口</span><br><span class="line"></span><br></pre></td></tr></table></figure>







<p>代码位置</p>
<p>D:\IDEA\projects\Movie_Recommend-master\Spark_Movie\src\main\scala\com\zxl&gt;</p>
<p>数据清洗</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">**数据的清洗：** （上传数据至hdfs中，[hdfs操作](http://blog.csdn.net/u011254180/article/details/<span class="number">79399422</span>)）</span><br><span class="line"></span><br><span class="line">到自己配置的响应的文件目录</span><br><span class="line"></span><br><span class="line"><span class="number">1</span>）启动 hdfs：  [root@spark1 ~]<span class="comment"># start-dfs.sh</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span>）启动 yarn：  [root@spark1 ~]<span class="comment"># start-yarn.sh</span></span><br><span class="line"></span><br><span class="line"><span class="number">3</span>）启动 mysql： [root@spark2 ~]<span class="comment"># service mysqld start</span></span><br><span class="line"></span><br><span class="line">**这一步配置好 hive后就不用了** <span class="number">4</span>）启动 hive：  [root@spark1 ~]<span class="comment"># hive --service metastore</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5）启动 spark集群： [root@spark1 spark-1.6.1]# ./sbin/start-all.sh </span></span><br><span class="line">启动spark ..\spark-<span class="number">2.4</span><span class="number">.2</span>-<span class="built_in">bin</span>-hadoop2<span class="number">.7</span>\<span class="built_in">bin</span>\spark-shell.cmd</span><br><span class="line"></span><br><span class="line"><span class="number">6</span>）代码(com.zxl.datacleaner.ETL)打包上传（[spark-sql与hive集成](http://blog.csdn.net/u011254180/article/details/<span class="number">79395227</span>)）</span><br><span class="line"></span><br><span class="line">- 代码位于 package com.zxl.datacleaner.ETL，打包为 ETL.jar</span><br><span class="line"><span class="comment"># 打包操作 todo</span></span><br><span class="line">D:\IDEA\projects\Movie_Recommend-master\Spark_Movie\src\main\scala\com\zxl&gt;</span><br><span class="line">- 运行代码 spark-submit --<span class="keyword">class</span> <span class="title class_">com</span>.zxl.datacleaner.ETL --total-executor-cores <span class="number">2</span> --executor-memory 2g lib/ETL.jar</span><br><span class="line">- 成功于hive中建表</span><br></pre></td></tr></table></figure>



<p><strong>5.数据的加工，</strong> 根据ALS算法对数据建立模型(<a target="_blank" rel="noopener" href="https://github.com/ZzXxL1994/Machine-Learning-Papers/tree/master/ALS">ALS论文</a>)</p>
<p>1）启动 hdfs：  [root@spark1 ~]# start-dfs.sh</p>
<p>2）启动 yarn：  [root@spark1 ~]# start-yarn.sh</p>
<p>3）启动 mysql： [root@spark2 ~]# service mysqld start</p>
<p>4）启动 hive：  [root@spark1 ~]# hive –service metastore</p>
<p>5）启动 spark集群： [root@spark1 spark-1.6.1]# .&#x2F;sbin&#x2F;start-all.sh</p>
<p>6）代码(com.zxl.datacleaner.RatingData)打包上传，测试建立模型</p>
<p><strong>6.建立模型，</strong> 根据RMSE(均方根误差)选取较好的模型</p>
<p>1）启动上述的服务</p>
<p>2）代码(com.zxl.ml.ModelTraining)打包上传，建立模型</p>
<p>注：com.zxl.ml.ModelTraining2中代码训练单个模型，其中参数 rank&#x3D;50, iteration &#x3D; 10, lambda &#x3D; 0.01</p>
<ul>
<li>代码位于 package com.zxl.ml.ModelTraining，打包为 Spark_Movie.jar</li>
<li>运行代码 spark-submit –class com.zxl.ml.ModelTraining lib&#x2F;Spark_Movie.jar</li>
</ul>
<p><strong>7.产生推荐结果</strong></p>
<p>1）启动上述的服务</p>
<p>2）代码(com.zxl.ml.Recommender)打包上传，产生推荐结果</p>
<p><strong>8.数据入库，</strong> 存储为所有用户推荐的电影结果，mysql中存入的格式为(userid, movieid,rating)</p>
<p>1）启动上述的服务</p>
<p>2）代码(com.zxl.ml.RecommendForAllUsers)打包上传，数据入库</p>
<ul>
<li>运行代码 spark-submit –class com.zxl.ml.RecommendForAllUsers –jars lib&#x2F;mysql-connector-java-5.1.35-bin.jar lib&#x2F;Spark_Movie.jar</li>
</ul>
<p><strong>9.实时数据的发送</strong></p>
<p>1）<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011254180/article/details/77897663">安装nginx</a>，用来接收电影网站上用户的点击信息，写入本地文件</p>
<p>2）<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011254180/article/details/80000763">安装flume</a>，实时监控本地文件，将数据发送至kafka消息队列中</p>
<p><strong>10.实时数据的接收处理</strong> ，如果打包到服务器运行错误，也可在本地IDEA上运行</p>
<p>1）<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011254180/article/details/79480234">安装zookeeper</a></p>
<p>2）<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011254180/article/details/79481088">安装kafka</a>，用来接收发送数据</p>
<p>3）启动上述的服务</p>
<p>4）启动zookeeper：  [root@spark1 soft]# zkServer.sh start</p>
<p>4）启动flume：[root@spark1 flume]# bin&#x2F;flume-ng agent -c .&#x2F;conf&#x2F; -f conf&#x2F;flume-conf.properties -Dflume.root.logger&#x3D;DEBUG,console -n a1</p>
<p>5）启动kafka：  [root@spark1 kafka_2.11-0.10.1.0]# bin&#x2F;kafka-server-start.sh config&#x2F;server.properties</p>
<p>6）代码(com.zxl.datacleaner.PopularMovies2)运行，用于为没有登录或新用户推荐，默认推荐观看最多的5部电影</p>
<p>7）代码运行(需指定jar包 kafka-clients-0.10.1.0.jar)</p>
<ul>
<li>spark-submit –class com.zxl.streaming.SparkDrStreamALS –total-executor-cores 2 –executor-memory 1g –jars lib&#x2F;kafka-clients-0.10.1.0.jar lib&#x2F;Spark_Movie.jar</li>
</ul>
<h1 id="项目学习"><a href="#项目学习" class="headerlink" title="项目学习"></a>项目学习</h1><p>下载数据集</p>
<p>spark操作将数据集上传到hdfs     “hdfs:&#x2F;&#x2F;spark1:9000&#x2F;movie&#x2F;data&#x2F;links.txt”</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ratings = sc.textFile(<span class="string">&quot;hdfs://spark1:9000/movie/data/ratings.txt&quot;</span>, minPartitions).filter &#123; !_.endsWith(<span class="string">&quot;,&quot;</span>) &#125;</span><br><span class="line">     .map(_.split(<span class="string">&quot;,&quot;</span>)).map(x =&gt; <span class="type">Ratings</span>(x(<span class="number">0</span>).trim().toInt, x(<span class="number">1</span>).trim().toInt, x(<span class="number">2</span>).trim().toDouble, x(<span class="number">3</span>).trim().toInt)).toDF()</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里面spark的作用</p>
<p>Spark(RDD)：基于内存计算<br>SparkSQL：一般情况都是基于离线数据处理</p>
</blockquote>
<p>ETL:  将表存到hive, 数据写入hdfs</p>
<blockquote>
<p>hive 与  hdfs:</p>
<p>7）Hbase：HBase 是一个分布式的、面向列的开源数据库。HBase 不同于一般的关系数据库， 它是一个适合于非结构化数据存储的数据库。 </p>
<p>8）Hive：Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张 数据库表，并提供简单的 SQL 查询功能，可以将 SQL 语句转换为 MapReduce 任务进行运 行。其优点是学习成本低，可以通过类 SQL 语句快速实现简单的 MapReduce 统计，不必开 发专门的 MapReduce 应用，十分适合数据仓库的统计分析。</p>
<p>hdfs的分布式: 是因为一个系统存储不下所有数据</p>
</blockquote>
<p>Hive：hive只是有个管理工具，不存储任何数据，将hdfs中的文件映射成表的形式进行数据处理，主要面向于(离线)数据仓库使用，内部执行引擎</p>
<h1 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h1><h1 id="Spark-1"><a href="#Spark-1" class="headerlink" title="Spark"></a>Spark</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p><img src="/../images/image-20221127235013033.png" alt="image-20221127235013033"></p>
<p><img src="/../images/image-20221127235111321.png" alt="image-20221127235111321"></p>
<img src="../images/image-20221127235323671.png" alt="image-20221127235323671">



<p><img src="/../images/image-20221127235409719.png" alt="image-20221127235409719"></p>
<p><img src="/../images/image-20221127235528675.png" alt="image-20221127235528675"></p>
<p><img src="/../images/image-20221127235656222.png" alt="image-20221127235656222"></p>
<p><img src="/../images/image-20221207110637786.png" alt="image-20221207110637786"></p>
<p><img src="/../images/image-20221207110717135.png" alt="image-20221207110717135"></p>
<p><img src="/../images/image-20221127235842327.png" alt="image-20221127235842327"></p>
<p><img src="/../images/image-20221127235930039.png" alt="image-20221127235930039"></p>
<p><img src="/../images/image-20221128000229791.png" alt="image-20221128000229791"></p>
<p><img src="/../images/image-20221128000351592.png" alt="image-20221128000351592"></p>
<p><img src="/../images/image-20221128000817044.png" alt="image-20221128000817044"></p>
<p><img src="/../images/image-20221128000838330.png" alt="image-20221128000838330"></p>
<p><img src="/../images/image-20221128000913365.png" alt="image-20221128000913365"></p>
<p><img src="/../images/image-20221128001031532.png" alt="image-20221128001031532"></p>
<p><img src="/../images/image-20221128001301311.png" alt="image-20221128001301311"></p>
<p><img src="/../images/image-20221128001320708.png" alt="image-20221128001320708"></p>
<p><img src="/../images/image-20221128102043021.png" alt="image-20221128102043021"></p>
<p><img src="/../images/image-20221128102022432.png" alt="image-20221128102022432"></p>
<p><img src="/../images/image-20221128102209922.png" alt="image-20221128102209922"></p>
<p>DataSet 是将DataFrame转换为对象的结果</p>
<p><img src="/../images/image-20221128102430686.png" alt="image-20221128102430686"></p>
<p><img src="/../images/image-20221128102501297.png" alt="image-20221128102501297"></p>
<h2 id="操作外置Hive"><a href="#操作外置Hive" class="headerlink" title="操作外置Hive"></a>操作外置Hive</h2><p><img src="/../images/image-20221128103258992.png" alt="image-20221128103258992"></p>
<p><img src="/../images/image-20221128103409094.png" alt="image-20221128103409094"></p>
<h2 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h2><p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能 够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里 的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p>
<h3 id="RDD算子"><a href="#RDD算子" class="headerlink" title="RDD算子"></a><strong>RDD算子</strong></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//map</span></span><br><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2: <span class="type">RDD</span>[<span class="type">String</span>] = dataRDD.map(</span><br><span class="line"> num =&gt; &#123;</span><br><span class="line"> <span class="string">&quot;&quot;</span> + num</span><br><span class="line"> &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">//mapPartitions </span></span><br><span class="line"><span class="comment">//将处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处</span></span><br><span class="line"><span class="comment">//理，哪怕是过滤数据。</span></span><br><span class="line"><span class="comment">//Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处</span></span><br><span class="line"><span class="comment">//理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能</span></span><br><span class="line"><span class="comment">//不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作。</span></span><br><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.mapPartitions(</span><br><span class="line"> datas =&gt; &#123;</span><br><span class="line"> datas.filter(_==<span class="number">2</span>)</span><br><span class="line"> &#125;</span><br><span class="line">)</span><br><span class="line"><span class="comment">//flatmap 扁平映射</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line"> <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.flatMap(</span><br><span class="line"> list =&gt; list</span><br><span class="line">)</span><br><span class="line"><span class="comment">//groupby</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.groupBy(</span><br><span class="line"> _%<span class="number">2</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// filter 符合规则的数据保留，不符合规则的数据丢弃</span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line"> <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.filter(_%<span class="number">2</span> == <span class="number">0</span>)</span><br><span class="line"><span class="comment">//distinct </span></span><br><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line"> <span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span></span><br><span class="line">),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.distinct()</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD.distinct(<span class="number">2</span>)</span><br><span class="line"><span class="comment">//collect</span></span><br><span class="line"><span class="comment">//在驱动程序中，以数组 Array 的形式返回数据集的所有元素</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 收集数据到 Driver</span></span><br><span class="line">rdd.collect().foreach(println)</span><br><span class="line"><span class="comment">//take</span></span><br><span class="line">返回一个由 <span class="type">RDD</span> 的前 n 个元素组成的数组</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 返回 RDD 中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> takeResult: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.take(<span class="number">2</span>)</span><br><span class="line">println(takeResult.mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 返回 RDD 中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">Array</span>[<span class="type">Int</span>] = rdd.takeOrdered(<span class="number">2</span>)</span><br><span class="line"><span class="comment">// 将数据保存到不同格式的文件中</span></span><br><span class="line"><span class="comment">// 保存成 Text 文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"><span class="comment">// 序列化成对象保存到文件</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"><span class="comment">// 保存成 Sequencefile 文件</span></span><br><span class="line">rdd.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br><span class="line"><span class="comment">//foreach</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 收集后打印</span></span><br><span class="line">rdd.map(num=&gt;num).collect().foreach(println)</span><br><span class="line">println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line"><span class="comment">// 分布式打印</span></span><br><span class="line">rdd.foreach(println)</span><br></pre></td></tr></table></figure>



<h3 id="RDD文件读取与保存"><a href="#RDD文件读取与保存" class="headerlink" title="RDD文件读取与保存"></a>RDD文件读取与保存</h3><p><img src="/../images/image-20221207110253173.png" alt="image-20221207110253173"></p>
<h3 id="创建df"><a href="#创建df" class="headerlink" title="创建df"></a>创建df</h3><p>1从spark数据源进行创建</p>
<p><img src="/../images/image-20221207110939536.png" alt="image-20221207110939536"></p>
<p>2RDD转换</p>
<p><img src="/../images/image-20221207111139164.png" alt="image-20221207111139164"></p>
<p><img src="/../images/image-20221207111152745.png" alt="image-20221207111152745"></p>
<p><img src="/../images/image-20221207111210474.png" alt="image-20221207111210474"></p>
<p><img src="/../images/image-20221207111256961.png" alt="image-20221207111256961"> </p>
<p><img src="/../images/image-20221207111324429.png" alt="image-20221207111324429"></p>
<p><img src="/../images/image-20221207111337061.png" alt="image-20221207111337061"></p>
<p><img src="/../images/image-20221207111427301.png" alt="image-20221207111427301"></p>
<p><img src="/../images/image-20221207111439824.png" alt="image-20221207111439824"></p>
<p><img src="/../images/image-20221207111458188.png" alt="image-20221207111458188"></p>
<p>3HIVE Table 进行查询返回</p>
<h2 id="打包代码"><a href="#打包代码" class="headerlink" title="打包代码"></a>打包代码</h2><p><img src="/../images/image-20221128104011441.png"></p>
<p><img src="/../images/image-20221128104026046.png" alt="image-20221128104026046"></p>
<p><img src="/../images/image-20221128104255372.png" alt="image-20221128104255372"></p>
<p>build之后打成jar包</p>
<p>运行jar包:</p>
<p>启动spark : 了解下master  worker</p>
<p><img src="/../images/image-20221128104624470.png" alt="image-20221128104624470"></p>
<p><img src="/../images/image-20221128104514434.png" alt="image-20221128104514434"></p>
<h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><p>Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式 的操作系统平台，而 MapReduce 等运算程序则相当于运行于操作系统之上的应用程序。</p>
<p>YARN 主要由 ResourceManager、NodeManager、ApplicationMaster 和 Container 等组件 构成。</p>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p><img src="/../images/image-20221119205150711.png" alt="image-20221119205150711"></p>
<p><img src="/../images/image-20221119210611866.png" alt="image-20221119210611866"></p>
<p><img src="/../images/image-20221204192014734.png" alt="image-20221204192014734"></p>
<p><img src="/../images/image-20221204192806088.png" alt="image-20221204192806088"></p>
<h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">【scala】下划线用法总结</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 用于变量的初始化</span><br><span class="line">在<span class="type">Scala</span>中，变量在声明时必须显示指定，可以使用下划线对变量进行初始化。而且该语法只适用于成员变量，不适用于局部变量。例：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">对于<span class="type">Int</span>来说，它是<span class="number">0</span>。</span><br><span class="line">对于<span class="type">Double</span>来说，它是<span class="number">0.0</span></span><br><span class="line">对于引用类型，它是<span class="literal">null</span></span><br><span class="line"></span><br><span class="line"><span class="number">2.</span> 用于导包引入</span><br><span class="line">导包引入时使用_导入该包下所有内容，类比<span class="type">Java</span>中的*。例如：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">3.</span> 用于将方法转变为函数</span><br><span class="line">在<span class="type">Scala</span>中方法不是值，而函数是。所以一个方法不能赋值给一个<span class="keyword">val</span>变量，而函数可以。方法可以转换为函数赋值给变量，例：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">4.</span> 用于模式匹配</span><br><span class="line">模式匹配中可以用下划线来作为<span class="type">Java</span>中<span class="keyword">default</span>的类比使用，也可以在匹配集合类型时，用于代表集合中元素，例：</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">5.</span> 用户访问tuple元素</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">6.</span> 下划线与星号</span><br><span class="line">向函数或方法传入可变参数时不能直接传入<span class="type">Range</span>或集合或数组对象，需要使用:_*转换才可传入</span><br><span class="line"></span><br><span class="line"><span class="number">6.1</span> 变长参数</span><br><span class="line">例如定义一个变长参数的方法sum，然后计算<span class="number">1</span><span class="number">-5</span>的和，可以写为</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="function"><span class="keyword">def</span> <span class="title">sum</span></span>(args: <span class="type">Int</span>*) = &#123;</span><br><span class="line">     | <span class="keyword">var</span> result = <span class="number">0</span></span><br><span class="line">     | <span class="keyword">for</span> (arg &lt;- args) result += arg</span><br><span class="line">     | result</span><br><span class="line">     | &#125;</span><br><span class="line">sum: (args: <span class="type">Int</span>*)<span class="type">Int</span></span><br><span class="line">​</span><br><span class="line">scala&gt; <span class="keyword">val</span> s = sum(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">s: <span class="type">Int</span> = <span class="number">15</span></span><br><span class="line"></span><br><span class="line">但是如果写成这种方式就会报错</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> s = sum(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">&lt;console&gt;:<span class="number">12</span>: error: <span class="class"><span class="keyword">type</span> <span class="title">mismatch</span></span>;</span><br><span class="line"> found : scala.collection.immutable.<span class="type">Range</span>.<span class="type">Inclusive</span></span><br><span class="line"> required: <span class="type">Int</span></span><br><span class="line">       <span class="keyword">val</span> s = sum(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">                     ^</span><br><span class="line"></span><br><span class="line">这种情况必须在后面写上: _*将<span class="number">1</span> to <span class="number">5</span>转化为参数序列</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> s = sum(<span class="number">1</span> to <span class="number">5</span>: _*)</span><br><span class="line">s: <span class="type">Int</span> = <span class="number">15</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">6.2</span> 变量声明中的模式</span><br><span class="line">下面代码分别将arr中的第一个和第二个值赋给first和second</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">​</span><br><span class="line">scala&gt; <span class="keyword">val</span> <span class="type">Array</span>(first, second, _*) = arr</span><br><span class="line">first: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">second: <span class="type">Int</span> = <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）说明</span><br><span class="line">（<span class="number">1</span>）过滤</span><br><span class="line">遍历一个集合并从中获取满足指定条件的元素组成一个新的集合</span><br><span class="line">（<span class="number">2</span>）转化/映射（map）</span><br><span class="line"></span><br><span class="line"> 将集合中的每一个元素映射到某一个函数</span><br><span class="line">（<span class="number">3</span>）扁平化</span><br><span class="line">（<span class="number">4</span>）扁平化+映射 注：flatMap 相当于先进行 map 操作，在进行 flatten 操作</span><br><span class="line">集合中的每个元素的子元素映射到某个函数并返回新集合</span><br><span class="line">（<span class="number">5</span>）分组(group)</span><br><span class="line">按照指定的规则对集合的元素进行分组</span><br><span class="line">（<span class="number">6</span>）简化（归约）</span><br><span class="line"> （<span class="number">7</span>）折叠</span><br><span class="line"><span class="number">2</span>）实操</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestList</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br><span class="line"> <span class="keyword">val</span> nestedList: <span class="type">List</span>[<span class="type">List</span>[<span class="type">Int</span>]] = <span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">List</span>(<span class="number">4</span>,</span><br><span class="line"><span class="number">5</span>, <span class="number">6</span>), <span class="type">List</span>(<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>))</span><br><span class="line"> <span class="keyword">val</span> wordList: <span class="type">List</span>[<span class="type">String</span>] = <span class="type">List</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello</span></span><br><span class="line"><span class="string">atguigu&quot;</span>, <span class="string">&quot;hello scala&quot;</span>)</span><br><span class="line"> <span class="comment">//（1）过滤</span></span><br><span class="line"> println(list.filter(x =&gt; x % <span class="number">2</span> == <span class="number">0</span>))</span><br><span class="line"> <span class="comment">//（2）转化/映射</span></span><br><span class="line"> println(list.map(x =&gt; x + <span class="number">1</span>))</span><br><span class="line"> <span class="comment">//（3）扁平化</span></span><br><span class="line"> println(nestedList.flatten)</span><br><span class="line"> <span class="comment">//（4）扁平化+映射 注：flatMap 相当于先进行 map 操作，在进行 flatten</span></span><br><span class="line">操作</span><br><span class="line"> println(wordList.flatMap(x =&gt; x.split(<span class="string">&quot; &quot;</span>)))</span><br><span class="line"> <span class="comment">//（5）分组</span></span><br><span class="line"> println(list.groupBy(x =&gt; x % <span class="number">2</span>))</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">3</span>）<span class="type">Reduce</span> 方法</span><br><span class="line"></span><br><span class="line"><span class="type">Reduce</span> 简化（归约） ：通过指定的逻辑将集合中的数据进行聚合，从而减少数据，最</span><br><span class="line">终获取结果。</span><br><span class="line">案例实操</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestReduce</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"> <span class="comment">// 将数据两两结合，实现运算规则</span></span><br><span class="line"> <span class="keyword">val</span> i: <span class="type">Int</span> = list.reduce( (x,y) =&gt; x-y )</span><br><span class="line"> println(<span class="string">&quot;i = &quot;</span> + i)</span><br><span class="line"> <span class="comment">// 从源码的角度，reduce 底层调用的其实就是 reduceLeft</span></span><br><span class="line"> <span class="comment">//val i1 = list.reduceLeft((x,y) =&gt; x-y)</span></span><br><span class="line"> <span class="comment">// ((4-3)-2-1) = -2</span></span><br><span class="line"> <span class="keyword">val</span> i2 = list.reduceRight((x,y) =&gt; x-y)</span><br><span class="line"> println(i2)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="number">4</span>）<span class="type">Fold</span> 方法</span><br><span class="line"><span class="type">Fold</span> 折叠：化简的一种特殊情况。</span><br><span class="line">（<span class="number">1</span>）案例实操：fold 基本使用</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFold</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"> <span class="comment">// fold 方法使用了函数柯里化，存在两个参数列表</span></span><br><span class="line"> <span class="comment">// 第一个参数列表为 ： 零值（初始值）</span></span><br><span class="line"> <span class="comment">// 第二个参数列表为： 简化规则</span></span><br><span class="line"> <span class="comment">// fold 底层其实为 foldLeft</span></span><br><span class="line"> <span class="keyword">val</span> i = list.foldLeft(<span class="number">1</span>)((x,y)=&gt;x-y)</span><br><span class="line"> <span class="keyword">val</span> i1 = list.foldRight(<span class="number">10</span>)((x,y)=&gt;x-y)</span><br><span class="line"> println(i)</span><br><span class="line"> println(i1)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestFold</span> </span>&#123;</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="comment">// 两个 Map 的数据合并</span></span><br><span class="line"> <span class="keyword">val</span> map1 = mutable.<span class="type">Map</span>(<span class="string">&quot;a&quot;</span>-&gt;<span class="number">1</span>, <span class="string">&quot;b&quot;</span>-&gt;<span class="number">2</span>, <span class="string">&quot;c&quot;</span>-&gt;<span class="number">3</span>)</span><br><span class="line"> <span class="keyword">val</span> map2 = mutable.<span class="type">Map</span>(<span class="string">&quot;a&quot;</span>-&gt;<span class="number">4</span>, <span class="string">&quot;b&quot;</span>-&gt;<span class="number">5</span>, <span class="string">&quot;d&quot;</span>-&gt;<span class="number">6</span>)</span><br><span class="line"> <span class="keyword">val</span> map3: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = map2.foldLeft(map1)</span><br><span class="line">&#123;</span><br><span class="line"> (map, kv) =&gt; &#123;</span><br><span class="line"> <span class="keyword">val</span> k = kv._1</span><br><span class="line"> <span class="keyword">val</span> v = kv._2</span><br><span class="line"> map(k) = map.getOrElse(k, <span class="number">0</span>) + v</span><br><span class="line"> map</span><br><span class="line"> &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> println(map3)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>







<p>case class，它其实就是一个普通的class。但是它又和普通的class略有区别，如下：<br>　　<strong>1、初始化的时候可以不用new，当然你也可以加上，普通类一定需要加new；</strong></p>
<p>object</p>
<ol>
<li>伴生对象中的内容,都可以通过类名访问,来模拟java中的静态语法</li>
<li>伴生对象的语法规则：使用object声明[加上就一定能够使用类名来访问]</li>
</ol>
<p><img src="/../images/image-20221205222940475.png" alt="image-20221205222940475"></p>
<h1 id="Flume-数据采集"><a href="#Flume-数据采集" class="headerlink" title="Flume 数据采集"></a>Flume 数据采集</h1><p><img src="/../images/image-20221129173804948.png" alt="image-20221129173804948"></p>
<h1 id="机器学习和推荐系统"><a href="#机器学习和推荐系统" class="headerlink" title="机器学习和推荐系统"></a>机器学习和推荐系统</h1><h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><p>矩阵乘法: 行定列移</p>
<p>矩阵转置: 行列交换</p>
<p>倒数: 沿x正方向斜率 <img src="/../images/image-20221201111103240.png" alt="image-20221201111103240"></p>
<p><img src="/../images/image-20221201111447576.png" alt="image-20221201111447576"></p>
<p><img src="/../images/image-20221201111918703.png" alt="image-20221201111918703"></p>
<p>变化大的地方 好求最大值最小值</p>
<p><img src="/../images/image-20221201112233468.png" alt="image-20221201112233468"></p>
<p><img src="/../images/image-20221201112435690.png" alt="image-20221201112435690"></p>
<p><img src="/../images/image-20221201112617731.png" alt="image-20221201112617731"></p>
<p><img src="/../images/image-20221201112737880.png" alt="image-20221201112737880"></p>
<p><img src="/../images/image-20221201113054920.png" alt="image-20221201113054920"></p>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p><img src="/../images/image-20221201113348268.png" alt="image-20221201113348268"></p>
<p><img src="/../images/image-20221201113433293.png" alt="image-20221201113433293"></p>
<h2 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h2><p><img src="/../images/image-20221201114122575.png" alt="image-20221201114122575"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">基于内容的推荐算法</span><br><span class="line">Content-based Recommendations (CB)根据推荐物品或内容的元数据,发</span><br><span class="line">现物品的相关性，再基于用户过去的喜好记录，为用户推荐相似的物品。</span><br><span class="line">通过抽取物品内在或者外在的特征值，实现相似度计算。</span><br><span class="line">-比如一个电影，有导演、演员、用户标签UGC、用户评论、时长、风格等等，都可以算是特</span><br><span class="line">征。</span><br><span class="line">， 将用户(user) 个人信息的特征(基于喜好记录或是预设兴趣标签)，和物</span><br><span class="line">品(item)的特征相匹配，就能得到用户对物品感兴趣的程度</span><br><span class="line">-在一些电影、音乐、图书的社交网站有很成功的应用，有些网站还请专业的人员对物品进行基</span><br><span class="line">因编码/打标签(PGC)</span><br></pre></td></tr></table></figure>



<h3 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h3><p><img src="/../images/image-20221201114416962.png" alt="image-20221201114416962"></p>
<h3 id="基于内容"><a href="#基于内容" class="headerlink" title="基于内容"></a>基于内容</h3><p><img src="/../images/image-20221201114526350.png" alt="image-20221201114526350"></p>
<p><img src="/../images/image-20221201114621689.png" alt="image-20221201114621689"></p>
<h3 id="基于UGC"><a href="#基于UGC" class="headerlink" title="基于UGC"></a>基于UGC</h3><p><img src="/../images/image-20221201114701678.png" alt="image-20221201114701678"></p>
<h3 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h3><p><img src="/../images/image-20221201115023091.png" alt="image-20221201115023091"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">基于内容的推荐算法</span><br><span class="line">Content-based Recommendations (CB)根据推荐物品或内容的元数据,发</span><br><span class="line">现物品的相关性，再基于用户过去的喜好记录，为用户推荐相似的物品。</span><br><span class="line">通过抽取物品内在或者外在的特征值，实现相似度计算。</span><br><span class="line">-比如一个电影，有导演、演员、用户标签UGC、用户评论、时长、风格等等，都可以算是特</span><br><span class="line">征。</span><br><span class="line">， 将用户(user) 个人信息的特征(基于喜好记录或是预设兴趣标签)，和物</span><br><span class="line">品(item)的特征相匹配，就能得到用户对物品感兴趣的程度</span><br><span class="line">-在一些电影、音乐、图书的社交网站有很成功的应用，有些网站还请专业的人员对物品进行基</span><br><span class="line">因编码/打标签(PGC)</span><br></pre></td></tr></table></figure>



<h3 id="基于近邻"><a href="#基于近邻" class="headerlink" title="基于近邻"></a>基于近邻</h3><p><img src="/../images/image-20221201115113934.png" alt="image-20221201115113934"></p>
<p><img src="/../images/image-20221201115444619.png" alt="image-20221201115444619"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">LFM降维方法一矩阵因子分解</span><br><span class="line">●假设用户物品评分矩阵为R, 现在有m个用户，n个物品</span><br><span class="line">●我们想要发现k 个隐类,我们的任务就是找到两个矩阵P和Q,使这两个矩阵的乘积</span><br><span class="line">近似等于R,即将用户物品评分矩阵R分解成为两个低维矩阵相乘:</span><br></pre></td></tr></table></figure>



<p><img src="/../images/image-20221201115625706.png" alt="image-20221201115625706"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">矩阵因子分解</span><br><span class="line">如果得到的预测评分矩阵R与原评分矩阵R在已知评分位置上的值都近似，那么我们认</span><br><span class="line">为它们在预测位置上的值也是近似的</span><br></pre></td></tr></table></figure>



<p><img src="/../images/image-20221201115732569.png" alt="image-20221201115732569"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">LFM的进一步理解</span><br><span class="line">●我们可以认为，用户之所以给电影打出这样的分数,是有内在原因的，我们可以挖掘</span><br><span class="line">出影响用户打分的隐藏因素，进而根据未评分电影与这些隐藏因素的关联度,决定此</span><br><span class="line">未评分电影的预测评分</span><br><span class="line">● 应该有一些隐藏的因素，影响用户的打分，比如电影:演员、题材、年代..甚至不- -定</span><br><span class="line">是人直接可以理解的隐藏因子</span><br><span class="line">●找到隐藏因子，可以对user和item 进行关联(找到是由于什么使得user 喜欢/不喜</span><br><span class="line">欢此item,什么会决定user喜欢/不喜欢此item)，就可以推测用户是否会喜欢某一部未看过的电影</span><br></pre></td></tr></table></figure>



<p><img src="/../images/image-20221201115752421.png" alt="image-20221201115752421"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">●对于用户看过的电影， 会有相应的打分，但一个用户不可能看过所有电影,对于用户没</span><br><span class="line">有看过的电影是没有评分的，因此用户评分矩阵大部分项都是空的，是一个稀疏矩阵</span><br><span class="line">●如果我们能够根据用户给已有电影的打分推测出用户会给没有看过的电影的打分,那么</span><br><span class="line">就可以根据预测结果给用户推荐他可能打高分的电影</span><br></pre></td></tr></table></figure>



<p>预测的误差 就是损失函数</p>
<p><img src="/../images/image-20221201120045970.png" alt="image-20221201120045970"></p>
<p><img src="/../images/image-20221201120220086.png" alt="image-20221201120220086"></p>
<p><img src="/../images/image-20221201120254285.png" alt="image-20221201120254285"></p>
<p><img src="/../images/image-20221201120311936.png" alt="image-20221201120311936"></p>
<p><img src="/../images/image-20221201120327299.png" alt="image-20221201120327299"></p>
<p><img src="/../images/image-20221201120352298.png" alt="image-20221201120352298"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// rank：对应的是隐因子的个数，这个值设置越高越准，但是也会产生更多的计算量。一般将这个值设置为10-200</span><br><span class="line">//      隐因子：如一部电影，决定它评分的有导演、主演、特效和剧本4个隐因子</span><br><span class="line">// iterations：对应迭代次数，一般设置个10就够了；</span><br><span class="line">// lambda：该参数控制正则化过程，其值越高，正则化程度就越深。一般设置为0.01</span><br><span class="line">// val model = ALS.train(ratingRDD, 1, 10, 0.01)</span><br></pre></td></tr></table></figure>

<h2 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h2><p><img src="/../images/image-20221201120600112.png" alt="image-20221201120600112"></p>
<h3 id="离线"><a href="#离线" class="headerlink" title="离线"></a>离线</h3><p><img src="/../images/image-20221201121248189.png" alt="image-20221201121248189"></p>
<p><img src="/../images/image-20221201121300678.png" alt="image-20221201121300678"></p>
<p><img src="/../images/image-20221201121506567.png" alt="image-20221201121506567"></p>
<p><img src="/../images/image-20221205145015085.png" alt="image-20221205145015085"></p>
<p>矩阵存成表的问题</p>
<p><img src="/../images/image-20221206203732771.png" alt="image-20221206203732771"></p>
<p><img src="/../images/image-20221205145252314.png" alt="image-20221205145252314"></p>
<h3 id="实时"><a href="#实时" class="headerlink" title="实时"></a>实时</h3><p><img src="/../images/image-20221206204709786.png" alt="image-20221206204709786"></p>
<p><img src="/../images/image-20221206204821959.png" alt="image-20221206204821959"></p>
<p><img src="/../images/image-20221206205228736.png" alt="image-20221206205228736"></p>
<p><img src="/../images/image-20221206205305475.png" alt="image-20221206205305475"></p>
<h1 id="Spark实操"><a href="#Spark实操" class="headerlink" title="Spark实操"></a>Spark实操</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>spark 处理数据:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h2 id><a href="#" class="headerlink" title></a></h2><h2 id="项目-1"><a href="#项目-1" class="headerlink" title="项目"></a>项目</h2><p><img src="/../images/image-20221205141953259.png" alt="image-20221205141953259"></p>
<p>方法里面定义隐式参数, 后面调用的时候省去传参, 就会在当前 全局作用域里找到同类型的隐式变量传入</p>
<p> jblas java中跟线性代数相关的库</p>
<p>模型训练 ALS参数</p>
<p><img src="/../images/image-20221205151402080.png" alt="image-20221205151402080"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://gouguoqiang.github.io">ggq</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://gouguoqiang.github.io/2022/09/01/21bigdata/">https://gouguoqiang.github.io/2022/09/01/21bigdata/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://gouguoqiang.github.io" target="_blank">ggq</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></div><div class="post_share"><div class="social-share" data-image="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/09/01/20Django/"><img class="prev-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Django</div></div></a></div><div class="next-post pull-right"><a href="/2022/09/01/22CloudNative/"><img class="next-cover" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">云原生</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ggq</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">74</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">51</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">23</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">大数据组件笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81-Hadoop"><span class="toc-number">1.1.</span> <span class="toc-text">一、 Hadoop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><span class="toc-number">1.2.</span> <span class="toc-text">二、 消息队列</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81-%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%85%B3%E7%B3%BB%E5%9E%8B%E3%80%81%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">三、 数据库(关系型、非关系型)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81-ETL%E5%B7%A5%E5%85%B7"><span class="toc-number">1.4.</span> <span class="toc-text">四、 ETL工具</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81-%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">五、 数据可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%B7%A5%E5%85%B7"><span class="toc-number">1.6.</span> <span class="toc-text">六、 任务调度工具</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Windows-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">2.</span> <span class="toc-text">Windows 环境搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#hive"><span class="toc-number">2.1.</span> <span class="toc-text">hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85scala"><span class="toc-number">2.2.</span> <span class="toc-text">安装scala</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Flume"><span class="toc-number">2.3.</span> <span class="toc-text">Flume</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark"><span class="toc-number">2.4.</span> <span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sparkSQL%E9%9B%86%E6%88%90-hive-hbase-mysql"><span class="toc-number">2.5.</span> <span class="toc-text">sparkSQL集成 hive hbase mysql</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark1-6-gt-2-4%E5%8D%87%E7%BA%A7%E9%97%AE%E9%A2%98"><span class="toc-number">3.</span> <span class="toc-text">spark1.6 -&gt;2.4升级问题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E6%90%AD%E5%BB%BA"><span class="toc-number">4.</span> <span class="toc-text">项目搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#JavaWeb"><span class="toc-number">4.1.</span> <span class="toc-text">JavaWeb</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.</span> <span class="toc-text">项目学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%88%AC%E8%99%AB"><span class="toc-number">6.</span> <span class="toc-text">爬虫</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-1"><span class="toc-number">7.</span> <span class="toc-text">Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">7.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%A4%96%E7%BD%AEHive"><span class="toc-number">7.2.</span> <span class="toc-text">操作外置Hive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%93%8D"><span class="toc-number">7.3.</span> <span class="toc-text">实操</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E7%AE%97%E5%AD%90"><span class="toc-number">7.3.1.</span> <span class="toc-text">RDD算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="toc-number">7.3.2.</span> <span class="toc-text">RDD文件读取与保存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAdf"><span class="toc-number">7.3.3.</span> <span class="toc-text">创建df</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%93%E5%8C%85%E4%BB%A3%E7%A0%81"><span class="toc-number">7.4.</span> <span class="toc-text">打包代码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadoop"><span class="toc-number">8.</span> <span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Yarn"><span class="toc-number">8.1.</span> <span class="toc-text">Yarn</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS"><span class="toc-number">8.2.</span> <span class="toc-text">HDFS</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Scala"><span class="toc-number">9.</span> <span class="toc-text">Scala</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Flume-%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86"><span class="toc-number">10.</span> <span class="toc-text">Flume 数据采集</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F"><span class="toc-number">11.</span> <span class="toc-text">机器学习和推荐系统</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="toc-number">11.1.</span> <span class="toc-text">数学基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.2.</span> <span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95"><span class="toc-number">11.3.</span> <span class="toc-text">推荐算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="toc-number">11.3.1.</span> <span class="toc-text">相似度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9"><span class="toc-number">11.3.2.</span> <span class="toc-text">基于内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8EUGC"><span class="toc-number">11.3.3.</span> <span class="toc-text">基于UGC</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-number">11.3.4.</span> <span class="toc-text">协同过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E8%BF%91%E9%82%BB"><span class="toc-number">11.3.5.</span> <span class="toc-text">基于近邻</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE"><span class="toc-number">11.4.</span> <span class="toc-text">项目</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF"><span class="toc-number">11.4.1.</span> <span class="toc-text">离线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6"><span class="toc-number">11.4.2.</span> <span class="toc-text">实时</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark%E5%AE%9E%E6%93%8D"><span class="toc-number">12.</span> <span class="toc-text">Spark实操</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">12.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">12.2.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE-1"><span class="toc-number">12.3.</span> <span class="toc-text">项目</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/0%E7%AE%80%E4%BB%8B/" title="无题"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/12/12/%E7%95%85%E8%B4%AD%E5%95%86%E5%9F%8E/0%E7%AE%80%E4%BB%8B/" title="无题">无题</a><time datetime="2022-12-12T15:06:58.895Z" title="发表于 2022-12-12 23:06:58">2022-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/09/7JVM/" title="无题"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2022/10/09/7JVM/" title="无题">无题</a><time datetime="2022-10-09T14:12:36.051Z" title="发表于 2022-10-09 22:12:36">2022-10-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/5%E4%BD%8D%E8%BF%90%E7%AE%97/" title="基础知识-位运算"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="基础知识-位运算"/></a><div class="content"><a class="title" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/5%E4%BD%8D%E8%BF%90%E7%AE%97/" title="基础知识-位运算">基础知识-位运算</a><time datetime="2022-09-02T03:51:56.000Z" title="发表于 2022-09-02 11:51:56">2022-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/6java%E6%A8%A1%E6%9D%BF/" title="一. 基础知识Java模板"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一. 基础知识Java模板"/></a><div class="content"><a class="title" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/6java%E6%A8%A1%E6%9D%BF/" title="一. 基础知识Java模板">一. 基础知识Java模板</a><time datetime="2022-09-02T03:51:56.000Z" title="发表于 2022-09-02 11:51:56">2022-09-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/AcWing/" title="一. 基础知识AcWing题目"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="一. 基础知识AcWing题目"/></a><div class="content"><a class="title" href="/2022/09/02/%E7%AE%97%E6%B3%95/1%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/AcWing/" title="一. 基础知识AcWing题目">一. 基础知识AcWing题目</a><time datetime="2022-09-02T03:51:56.000Z" title="发表于 2022-09-02 11:51:56">2022-09-02</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By ggq</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>